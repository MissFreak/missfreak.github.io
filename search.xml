<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>神经网络基本概念汇总</title>
    <url>/DL/</url>
    <content><![CDATA[<h1 id="前馈神经网络">前馈神经网络</h1>
<figure>
<img src="https://github.com/martisak/dotnets/raw/master/test.png" alt="简单网" /><figcaption aria-hidden="true">简单网</figcaption>
</figure>
<span id="more"></span>
<h1 id="encoder">Encoder</h1>
<figure>
<img src="https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png" alt="Multi-head attention mechanism: “queries”, “keys”, and “values,” over and over again – Data Science Blog" /><figcaption aria-hidden="true">Multi-head attention mechanism: “queries”, “keys”, and “values,” over and over again – Data Science Blog</figcaption>
</figure>
<h1 id="seq2seq-模型通常具有编码器-解码器架构">seq2seq 模型通常具有编码器-解码器架构</h1>
<h1 id="对齐分数矩阵">对齐分数矩阵</h1>
<p>The matrix of alignment scores is a nice byproduct to explicitly show the correlation between source and target words.对齐分数矩阵是一个很好的副产品，可以明确显示源词和目标词之间的相关性。</p>
<figure>
<img src="https://lilianweng.github.io/lil-log/assets/images/bahdanau-fig3.png" alt="alignment matrix" /><figcaption aria-hidden="true">alignment matrix</figcaption>
</figure>
<h1 id="相关性">相关性</h1>
<figure>
<img src="https://lilianweng.github.io/lil-log/assets/images/cheng2016-fig1.png" alt="注意内" /><figcaption aria-hidden="true">注意内</figcaption>
</figure>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>神经网络</category>
      </categories>
  </entry>
  <entry>
    <title>Bahdanau Attention机制初探</title>
    <url>/NMT/</url>
    <content><![CDATA[<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.0473">Bahdanau Attention</a> 是一种非常经典的注意力机制，本文将从原论文“神经机器翻译-通过共同学习来调整和翻译”（<a href="https://arxiv.org/pdf/1409.0473.pdf">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>）来探秘背后的数学原理。</p>
<p>传统的编码器-解码器架构的缺点在于，随着输入句子长度的增加，会忘记前面的信息。本方法的区别在于不是将源句的所有信息压缩为单个固定长度的向量，而是一连串的向量，并在解码时自适应地选择这些向量的一个子集。</p>
<h1 id="rnn编码器-解码器">RNN编码器-解码器</h1>
<p>在编码器-解码器框架中，编码器将输入的句子，一连串的向量<span class="math inline">\(\mathbf{x}=\left(x_{1}, \cdots, x_{T_{x}}\right)\)</span>读成向量<span class="math inline">\(c\)</span>：</p>
<p><span class="math inline">\(h_{t}=f\left(x_{t}, h_{t-1}\right)\)</span></p>
<p><span class="math inline">\(c=q\left(\left\{h_{1}, \cdots, h_{T_{x}}\right\}\right)\)</span></p>
<p>其中<span class="math inline">\(h_{t} \in \mathbb{R}^{n}\)</span>是时间t的隐藏状态，<span class="math inline">\(c\)</span>是由隐藏状态的序列生成的向量。<span class="math inline">\(f\)</span>和<span class="math inline">\(q\)</span>是一些非线性函数。例如，Sutskever等人（2014）使用LSTM作为<span class="math inline">\(f\)</span>和<span class="math inline">\(q\left(\left\{h_{1}, \cdots, h_{T}\right\}\right)=h_{T}\)</span>。</p>
<p><img src="https://i.loli.net/2021/09/16/TVBwpkZL2urPbmQ.png" width="300"/></p>
<p>（图片来自论文：learning phrase representations using rnn encoder–decoder for statistical machine translation）</p>
<p>解码器通常被训练成在给定的上下文向量<span class="math inline">\(c\)</span>和所有先前预测的词<span class="math inline">\(\left\{y_{1}, \cdots, y_{t^{\prime}-1}\right\}\)</span>的情况下预测下一个词<span class="math inline">\(y_t\)</span>。换句话说，解码器通过将联合概率分解为有序条件，对译文<span class="math inline">\(\mathbf{y}\)</span>定义了一个概率：</p>
<p><span class="math inline">\(p(\mathbf{y})=\prod_{t=1}^{T} p\left(y_{t} \mid\left\{y_{1}, \cdots, y_{t-1}\right\}, c\right)\)</span></p>
<p>其中<span class="math inline">\(\mathbf{y}=\left(y_{1}, \cdots, y_{T_{y}}\right)\)</span>，在RNN中，每个条件概率被建模为</p>
<p><span class="math inline">\(p\left(y_{t} \mid\left\{y_{1}, \cdots, y_{t-1}\right\}, c\right)=g\left(y_{t-1}, s_{t}, c\right)\)</span></p>
<p>其中<span class="math inline">\(g\)</span>是一个非线性的、可能是多层次的输出<span class="math inline">\(y_t\)</span>的概率的函数，<span class="math inline">\(s_t\)</span>是RNN的隐藏状态。应该指出的是，可以使用其他架构，如RNN和去卷积神经网络的混合体（Kalchbrenner和Blunsom，2013）。</p>
<h1 id="attention机制">Attention机制</h1>
<h2 id="encoder双向rnn标注序列">Encoder：双向RNN标注序列</h2>
<p>BiRNN由前向和后向RNN组成，前向RNN按顺序读取输入序列并计算出前向隐藏状态的序列<span class="math inline">\(\left(\vec{h}_{1}, \cdots, \vec{h}_{T_{x}}\right)\)</span>​，后向RNN以相反的顺序读取序列，从而得到一个后向隐藏状态序列<span class="math inline">\(\left(\overleftarrow{h}_{1}, \cdots, \overleftarrow{h}_{T_{x}}\right)\)</span></p>
<p>我们通过concatenate前向和后向隐藏状态<span class="math inline">\(h_{j}=\left[\vec{h}_{j}^{\top} ; \overleftarrow{h}_{j}^{\top}\right]^{\top}\)</span>得到每个单词<span class="math inline">\(x_i\)</span>的标注。每个标注<span class="math inline">\(h_i\)</span>都包含整个输入序列的信息，主要集中在输入序列的第<span class="math inline">\(i\)</span>个词周围的部分。</p>
<h2 id="decoder">Decoder</h2>
<p>在新的模型架构中，我们定义上述的条件概率为：</p>
<p><span class="math inline">\(p\left(y_{i} \mid y_{1}, \ldots, y_{i-1}, \mathbf{x}\right)=g\left(y_{i-1}, s_{i}, c_{i}\right)\)</span></p>
<p>其中<span class="math inline">\(s_i\)</span>是时间<span class="math inline">\(i\)</span>的RNN隐藏状态，计算公式为：</p>
<p><span class="math inline">\(s_{i}=f\left(s_{i-1}, y_{i-1}, c_{i}\right)\)</span></p>
<p>这里的概率是以每个目标词<span class="math inline">\(y_i\)</span>的不同语境向量<span class="math inline">\(c_i\)</span>为条件。语境向量<span class="math inline">\(c_i\)</span>取决于编码器输出的标注序列<span class="math inline">\(\left(h_{1}, \cdots, h_{T_{x}}\right)\)</span>。</p>
<p><img src="https://i.loli.net/2021/09/16/MFufLp3qjTJGlHx.png" width="300"/></p>
<p>语境向量<span class="math inline">\(c_i\)</span>被计算为这些标注<span class="math inline">\(h_i\)</span>的加权和：</p>
<p><span class="math inline">\(c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j}\)</span></p>
<p>每个标注<span class="math inline">\(h_j\)</span>的权重<span class="math inline">\(\alpha\)</span>的计算方法是：</p>
<p><span class="math inline">\(\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T_{x}} \exp \left(e_{i k}\right)}\)</span></p>
<p>其中<span class="math inline">\(e_{i j}=a\left(s_{i-1}, h_{j}\right)\)</span>是一个对齐模型，对位置<span class="math inline">\(j\)</span>附近的输入和位置<span class="math inline">\(i\)</span>的输出的匹配程度进行评分。这个分数是基于RNN的隐藏状态<span class="math inline">\(s_{i-1}\)</span>（就在发射出<span class="math inline">\(y_i\)</span>之前，公式（4））和输入句子的第<span class="math inline">\(j\)</span>个标注<span class="math inline">\(h_j\)</span>。 我们将对齐模型<span class="math inline">\(a\)</span>参数化为一个前馈神经网络，与拟议系统的所有其他组件共同训练。请注意，与传统的机器翻译不同，对齐方式不被认为是一个潜在的变量。相反，对齐模型直接计算软对齐，这允许成本函数的梯度被反向传播。这个梯度可以用来训练对齐模型以及整个翻译模型的联合。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语言模型</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>目前共有108篇文章</title>
    <url>/all-posts/</url>
    <content><![CDATA[<center>
『 好奇心造就诗人与科学家。』<br>Knowledge of languages is the doorway to wisdom. – Roger Bacon<br><br>
<details class="category_1">
<summary>
代码
</summary>
<details open>
<summary>
Git
</summary>
<br> <a href="git">Git和Github常用操作大全</a><br>
</details>
<details open>
<summary>
pandas
</summary>
<br> <a href="mind-graph">我的知识库</a><br> <a href="pandas">Pandas的一些常用操作</a><br>
</details>
<details open>
<summary>
文本处理
</summary>
<br> <a href="json">用python处理json数据</a><br> <a href="pdf-explained">PDF相关知识大全</a><br> <a href="pdf">PDF文件处理大全</a><br> <a href="txt">纯文本文件的处理</a><br>
</details>
<details open>
<summary>
爬虫
</summary>
<br> <a href="scraping">Python爬虫代码-全</a><br>
</details>
<details open>
<summary>
编辑器
</summary>
<br> <a href="atom">从小白到起飞，一站解决Atom编辑器各种骚操作</a><br> <a href="programming-methodology">编程方法论</a><br>
</details>
</details>
<details class="category_1">
<summary>
数学
</summary>
<details open>
<summary>
微积分
</summary>
<br> <a href="calculus-I-0">第0章：为什么要学习微积分？</a><br>
</details>
<details open>
<summary>
方法
</summary>
<br> <a href="math-0">数学中的英语名词汇</a><br> <a href="math-1">如何学习数学</a><br>
</details>
</details>
<details class="category_1">
<summary>
未分类
</summary>
<br><a href="all-posts">目前共有88篇文章</a><br> <a href="all_papers">NLP领域经典论文</a><br> <a href="bert-keyword">使用 BERT 提取关键字</a><br> <a href="bert">BERT和SentenceBERT</a><br> <a href="bootstrap-2">Bootstrapping 2.0</a><br> <a href="bootstrap">Bootstrapping 1.0</a><br> <a href="cognitive-linguistics">认知语言学大纲</a><br> <a href="constituency">提取句子中的每个子树</a><br> <a href="content">我的电脑文件目录管理</a><br> <a href="excel">python批量处理Excel表格</a><br> <a href="hello-world">Hello World</a><br> <a href="hexo-next">Hexo博客Next主题搭建全过程</a><br> <a href="information-theory-1">信息论知识与资源汇总</a><br> <a href="KG-1">知识图谱与知识表示</a><br> <a href="knowledge-graph-1">knowledge-graph-1</a><br> <a href="knowledge-graph-2">用Python搭建学术知识图谱</a><br> <a href="middle-test">中期考核的复习计划</a><br> <a href="paper-3">用分布式语义bootstrap关系</a><br> <a href="prime-citation">priming</a><br> <a href="priming-content">syntactic priming-2</a><br> <a href="priming-pre">syntactic priming</a><br> <a href="pyperclip">利用python监听剪贴板内容并修改-工具开发</a><br> <a href="resources">免费好用网站软件资源集锦：从学习、办公到娱乐</a><br> <a href="semeval">SemEval 2017任务10：ScienceIE--从科学出版物中提取关键词和关系</a><br> <a href="tensorflow-0">tensorflow基础知识</a><br> <a href="tensorflow-1">使用 BERT 对文本进行分类</a><br> <a href="web-dev-all">网站开发需要的工具：前端和后端</a><br> <a href="wiki-api">好用的API集锦</a><br>
</details>
</details>
<details class="category_1">
<summary>
网站开发
</summary>
<details open>
<summary>
django
</summary>
<br> <a href="django-1">Django网站开发全过程实录-1</a><br>
</details>
<details open>
<summary>
flask
</summary>
<br> <a href="flask-2">一文读懂Flask Web开发实战！</a><br> <a href="flask-api-1">基础：用flask搭建RESTful API</a><br> <a href="flask-api-2">部署Flask开发的API到Heroku</a><br> <a href="flask-api-3">如何在curl和python中使用API</a><br> <a href="flask-web">一个人开发信息检索与抽取网站的全过程</a><br> <a href="flask">flask干货总结</a><br>
</details>
</details>
<details class="category_1">
<summary>
自然语言处理
</summary>
<details open>
<summary>
依存分析
</summary>
<br> <a href="dependency-parsing-1">《自然语言处理综论》第14章-依存分析（上）</a><br> <a href="dependency-parsing-2">《自然语言处理综论》第14章-依存分析（中）</a><br> <a href="dependency-parsing">英文依存句法分析</a><br> <a href="pos-tagging">英文词性标记（POS Tagging）</a><br>
</details>
<details open>
<summary>
信息抽取
</summary>
<br> <a href="information-retrieval-1">《自然语言处理综论》第17章-信息抽取（上）</a><br> <a href="information-retrieval-2">《自然语言处理综论》第17章-信息抽取（中）</a><br> <a href="information-retrieval-3">《自然语言处理综论》第17章-信息抽取（下）</a><br> <a href="information-retrieval-4">信息抽取</a><br> <a href="information-retrieval">信息抽取技术综述</a><br> <a href="named-entity-recognition">英文文献的命名实体识别（上）</a><br> <a href="paper-0">SemEval 2017任务10：ScienceIE--从科学出版物中提取关键词和关系</a><br> <a href="paper-1">数值开放信息抽取的自助法</a><br> <a href="paper-2">具有分布式语义的关系提取器的半监督性引导 </a><br> <a href="paper-4">数值开放信息抽取的自助法</a><br> <a href="paper-5">用于从文本中半监督的网络论点语料库的清洗 </a><br> <a href="phd">数值开放信息抽取的自助法</a><br>
</details>
<details open>
<summary>
句子切分
</summary>
<br> <a href="sentence-segmentation">英文句子切分的要点和工具</a><br>
</details>
<details open>
<summary>
神经网络
</summary>
<br> <a href="DL">神经网络基本概念汇总</a><br>
</details>
<details open>
<summary>
表示学习
</summary>
<br> <a href="sentence-embedding">句子嵌入技术笔记</a><br>
</details>
<details open>
<summary>
词向量
</summary>
<br> <a href="word_embedding">词向量空间可视化</a><br>
</details>
<details open>
<summary>
词向量
</summary>
<br> <a href="word_embeddings2">认知语言学论文代码</a><br>
</details>
<details open>
<summary>
语料库
</summary>
<br> <a href="corpus">语料库资源大全</a><br>
</details>
<details open>
<summary>
语言模型
</summary>
<br> <a href="transformer">Transformer详解</a><br>
</details>
<details open>
<summary>
预处理
</summary>
<br> <a href="encoding">文本编码格式大全</a><br> <a href="spacy-1">spaCy超强指南之文本预处理和语言特征表</a><br>
</details>
</details>
<details class="category_1">
<summary>
计算机
</summary>
<details open>
<summary>
正则表达式
</summary>
<br> <a href="regex-1">正则表达式进阶</a><br>
</details>
<details open>
<summary>
算法
</summary>
<br> <a href="finite-state-machines">有限状态机</a><br>
</details>
<details open>
<summary>
课程
</summary>
<br> <a href="courses">计算机公开课目录</a><br>
</details>
</details>
<details class="category_1">
<summary>
语言学
</summary>
<details open>
<summary>
二语习得
</summary>
<br> <a href="sla-3">二语习得课本</a><br> <a href="sla-4">二语习得最终</a><br> <a href="sla">二语习得</a><br>
</details>
<details open>
<summary>
句法
</summary>
<br> <a href="syntax-final">句法学最终</a><br> <a href="syntax-main">句法学主要内容</a><br> <a href="syntax-terms">句法学框架及术语大全</a><br> <a href="syntax-trees">句法树大全</a><br> <a href="syntax">句法学超人大总结</a><br>
</details>
<details open>
<summary>
心理语言学
</summary>
<br> <a href="psycho-linguistics-content">心理语言学：资源和知识整理</a><br> <a href="psycho-linguistics">心理语言学：资源和知识整理</a><br>
</details>
<details open>
<summary>
认知语言学
</summary>
<br> <a href="concepts">Reclaiming Concepts，Eleanor Rosch</a><br>
</details>
<details open>
<summary>
语义
</summary>
<br> <a href="semantics-concepts">语义学框架</a><br> <a href="semantics-final">语义学最终</a><br> <a href="semantics-terms">语义学术语大全</a><br> <a href="semantics">语义学术语</a><br>
</details>
<details open>
<summary>
语言测试
</summary>
<br> <a href="language-assessment">语言测试Syllabus</a><br> <a href="language-assessment2">语言测试glossary</a><br> <a href="language-testing">机器学习驱动的语言评估</a><br>
</details>
</details>
<details class="category_1">
<summary>
项目
</summary>
<details open>
<summary>
信息抽取
</summary>
<br> <a href="attention">Attention模型中注意力的多尺度视觉化 </a><br> <a href="dataset">10000篇计算语言学文献语料库搭建</a><br> <a href="ir-project-0">信息抽取-Pilot Study</a><br> <a href="ir-project-1">爬虫构建语料库的方法</a><br> <a href="ir-project-2">根据POS生成候选词</a><br> <a href="ir-project">信息抽取的毕业论文准备</a><br> <a href="noun-chunks">名词短语抽取方法汇总和比较</a><br> <a href="paper-6">一袋什么？用于文本分析的简单名词短语抽取</a><br> <a href="papers">文献整理</a><br> <a href="pattern_match">用模式匹配的方法抽取关键词</a><br> <a href="pattern_mining">用bootstrapping迭代的方式发掘新的模式</a><br> <a href="project">项目最终使用代码汇总与实时更新</a><br> <a href="stopwords">计算stopwords</a><br> <a href="visualization">计算语言学文献语料库的可视化</a><br>
</details>
</details>
<br><br>关注公众号：鸽婆打字机！风里雨里，鸽鸽陪你~
</center>
<p><img align="center" width="120" height="120" alt="微信公众号：鸽婆打字机" src="https://i.loli.net/2021/03/04/dXVUZiRfW2o7wCy.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>NLP领域经典论文</title>
    <url>/all_papers/</url>
    <content><![CDATA[<p><strong>NLP paper</strong></p>
<p><strong>1、baseline paper（文本分类）</strong></p>
<ol type="1">
<li>ICLR2013，Word2Vec 词向量扛鼎之作</li>
</ol>
<ul>
<li><ol type="1">
<li>Efficient Estimation of Word Representation in Vector Space</li>
</ol></li>
</ul>
<p>https://arxiv.org/pdf/1301.3781v3.pdf</p>
<ol type="1">
<li>EMNLP2014，Glove：最出名的词向量训练方法之一</li>
</ol>
<ul>
<li><ol type="1">
<li>GloVe: Global Vectors for Word Representation</li>
</ol></li>
</ul>
<p>http://emnlp2014.org/papers/pdf/EMNLP2014162.pdf</p>
<ol type="1">
<li>EMNLP2015，Char Embedding 第一篇介绍字符嵌入的论文</li>
</ol>
<ul>
<li><ol type="1">
<li>Compositional character models for open vocabulary word representation</li>
</ol></li>
</ul>
<p>https://arxiv.org/pdf/1508.02096.pdf</p>
<ol type="1">
<li>EMNLP2014，TextCNN 第一篇CNN用于文本分类的文章</li>
</ol>
<ul>
<li><ol type="1">
<li>Convolutional Neural Network for Sentence Classification</li>
</ol></li>
</ul>
<p>https://arxiv.org/pdf/1408.5882.pdf</p>
<ol type="1">
<li>NIPS2015，CharTextCNN 第一篇字符级别文本分类模型</li>
</ol>
<ul>
<li><ol type="1">
<li>Character-level Convolutional Networks for Text Classification</li>
</ol></li>
</ul>
<p>https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf</p>
<ol type="1">
<li>EACL2017，FastText 细粒度的文本分类</li>
</ol>
<ul>
<li><ol type="1">
<li>Bag of Tricks for Efficient Text Classification</li>
</ol></li>
</ul>
<p>https://arxiv.org/pdf/1607.01759v2.pdf</p>
<ol type="1">
<li>NLPS 2014，Deep_NMT 使用LSTM解决机器翻译问题</li>
</ol>
<ul>
<li><ol type="1">
<li>Sequence to Sequence Learning with Neural Networks</li>
</ol></li>
</ul>
<p>https://arxiv.org/pdf/1409.3215.pdf</p>
<ol type="1">
<li>ICLR2015，Bahdanau_NMT 第一篇介绍attention的论文</li>
</ol>
<ul>
<li><ol type="1">
<li>Neural Machine Translation by Jointly Learning to Align and Translate</li>
</ol></li>
</ul>
<ol type="1">
<li>NAACL2016，Han_Attention attention用于文本分类</li>
</ol>
<ul>
<li><ol type="1">
<li>Hierarchical Attention Networks for Document</li>
</ol></li>
</ul>
<ol type="1">
<li>Coling2018，SGM 第一篇使用序列生成做多标签文本分类</li>
</ol>
<ul>
<li><ol type="1">
<li>SGM: Sequence Generation Model for Multi-label Classification</li>
</ol></li>
</ul>
<p><strong>2、细分领域</strong></p>
<p><strong>① 信息抽取</strong></p>
<p><strong>命名实体识别</strong></p>
<ol type="1">
<li>Bidirectional LSTM-CRF Models for Sequence Tagging</li>
</ol>
<p>​ 深度学习应用NER的经典模型</p>
<ol type="1">
<li>Chinese NER Using Lattice LSTM</li>
</ol>
<p>​ 融合字词向量的中文NER</p>
<ol type="1">
<li>CNN-Based Chinese NER with Lexicon Rethinking</li>
</ol>
<p>​ Rethinking机制的CNN网络解决中文NER问题</p>
<ol type="1">
<li>A Lexicon-Based Graph Neural Network for Chinese NER</li>
</ol>
<p>​ 图神经网络解决中文NER任务</p>
<ol type="1">
<li>TENER- Adapting Transformer Encoder for Named Entity Recognition</li>
</ol>
<p>​ 改进Transformer应用在NER任务</p>
<ol type="1">
<li>2020ACL，Simplify the Usage of Lexicon in Chinese NER</li>
</ol>
<p>​ 自适应Embedding融合词典信息解决中文NER任务</p>
<p><strong>关系抽取</strong></p>
<ol type="1">
<li>Relation Classification via Convolutional Deep Neural Network(2014,cnn)</li>
<li>CNN识别改进 CRCNN+PCNN一起(2015ACL,rank loss)</li>
<li>Attention-based bidirectional long short-term memory networks for relation classification(2016ACL,lstm+att)</li>
<li>joint extractions of entities and relations based on a novel tagging scheme(标注转换,将分类任务转换为序列任务ACL2017)</li>
<li>A Novel Cascade Binary Tagging Framework for Relational Triple Extraction.(2020ACL实体+关系识别统一训练,半指针半标注解决标签重合问题)</li>
</ol>
<p><strong>② 预训练模型</strong></p>
<ol type="1">
<li>2017 NIPS，transformer：预训练模型的基石</li>
</ol>
<ul>
<li><ol type="1">
<li>Attention is all your need</li>
</ol></li>
</ul>
<ol type="1">
<li>2017 ICLR，transformer-xl:文本生成任务经典模型</li>
</ol>
<ul>
<li><ol type="1">
<li>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</li>
</ol></li>
</ul>
<ol type="1">
<li>2018 NAACL，elmo：经典动态词向量，预训练模型三巨头之一</li>
</ol>
<ul>
<li><ol type="1">
<li>Deep contextualized word representations------&gt;elmo</li>
</ol></li>
</ul>
<ol type="1">
<li>2018 gpt：文本生成任务上的巨人</li>
</ol>
<ul>
<li><ol type="1">
<li>Improving Language Understanding by Generative Pre-Training-----&gt;gpt</li>
</ol></li>
</ul>
<ol type="1">
<li>2018 ACL，bert:预训练模型最耀眼的那颗星，基于Transformer 的双向深度语言模型</li>
</ol>
<ul>
<li><ol type="1">
<li>Pre-training of Deep Bidirectional Transformers for Language Understanding------&gt;bert</li>
</ol></li>
</ul>
<ol type="1">
<li>2018，ACL，ulmfit：少量样本训练的预训练模型</li>
</ol>
<ul>
<li><ol type="1">
<li>Universal Language Model Fine-tuning for Text Classification-------&gt;ulmfit</li>
</ol></li>
</ul>
<ol type="1">
<li>2018 ICLR，albert：轻量级bert的代表之作</li>
</ol>
<ul>
<li><ol type="1">
<li>ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS----&gt;albert</li>
</ol></li>
</ul>
<ol type="1">
<li>2019 PMLR，mass：包含gpt和bert的预训练模型</li>
</ol>
<ul>
<li><ol type="1">
<li>MASS: Masked Sequence to Sequence Pre-training for Language Generation-------------&gt;mass</li>
</ol></li>
</ul>
<ol type="1">
<li>2019 NeurIPS，xlnet:自回归预训练模型代表之作</li>
</ol>
<ul>
<li><ol type="1">
<li>XLNet: Generalized Autoregressive Pretraining for Language Understanding--------&gt;xlnet</li>
</ol></li>
</ul>
<ol type="1">
<li>2020 ICLR，electra:轻量级新生代预训练模型</li>
</ol>
<ul>
<li><ol type="1">
<li>ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS---&gt;electra</li>
</ol></li>
</ul>
<p><strong>③ 图神经网络</strong></p>
<ol type="1">
<li>KDD 2016，Node2vec 经典必读第一篇，平衡同质性和结构性</li>
</ol>
<ul>
<li><ol type="1">
<li>《node2vec: Scalable Feature Learning for Networks》</li>
</ol></li>
</ul>
<ol type="1">
<li>WWW2015，LINE 1阶+2阶相似度</li>
</ol>
<ul>
<li><ol type="1">
<li>《Line: Large-scale information network embedding》</li>
</ol></li>
</ul>
<ol type="1">
<li>KDD 2016，SDNE 多层自编码器</li>
</ol>
<ul>
<li><ol type="1">
<li>《Structural deep network embedding》</li>
</ol></li>
</ul>
<ol type="1">
<li>KDD 2017，metapath2vec 异构图网络</li>
</ol>
<ul>
<li><ol type="1">
<li>《metapath2vec: Scalable representation learning for heterogeneous networks》</li>
</ol></li>
</ul>
<ol type="1">
<li>NIPS 2013，TransE 知识图谱奠基</li>
</ol>
<ul>
<li><ol type="1">
<li>《Translating Embeddings for Modeling Multi-relational Data》</li>
</ol></li>
</ul>
<ol type="1">
<li>ICLR 2018，GAT attention机制</li>
</ol>
<ul>
<li><ol type="1">
<li>《Graph Attention Network》</li>
</ol></li>
</ul>
<ol type="1">
<li>NIPS 2017，GraphSAGE 归纳式学习框架</li>
</ol>
<ul>
<li><ol type="1">
<li>《Inductive Representation Learning on Large Graphs 》</li>
</ol></li>
</ul>
<ol type="1">
<li>ICLR 2017，GCN 图神经开山之作</li>
</ol>
<ul>
<li><ol type="1">
<li>《SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS》</li>
</ol></li>
</ul>
<ol type="1">
<li>ICLR 2016，GGNN 门控图神经网络</li>
</ol>
<ul>
<li><ol type="1">
<li>《Gated Graph Sequence Neural Networks》</li>
</ol></li>
</ul>
<ol type="1">
<li>ICML 2017，MPNN 空域卷积消息传递框架</li>
</ol>
<ul>
<li><ol type="1">
<li>《Neural Message Passing for Quantum Chemistry》</li>
</ol></li>
</ul>
<p><strong>④ 文本匹配</strong></p>
<ol type="1">
<li>DSSM：第一篇深度学习领域文本匹配文章</li>
<li>SiamseNet：利用孪生网络计算文本相似度</li>
<li>Compare-Aggreaget：多角度提取文本特征，利用CNN进行特征融合</li>
<li>ESIM：最流行、经典的文本匹配模型，优秀的baseline</li>
<li>BiMPM：多通道、多角度匹配，充分挖掘文本特征</li>
<li>RE2, acl 2019，基于更丰富特征对齐结构的简单高效文本匹配</li>
</ol>
<p>《imple and Effective Text Matching with Richer Alignment Features》</p>
<p>https://www.aclweb.org/anthology/P19-1465.pdf</p>
<ol type="1">
<li>ICLR 2020，BERT/Poly-encoders, 基于BERT的高效文本匹配模型</li>
</ol>
<p>《Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring》</p>
<p>https://arxiv.org/pdf/1905.01969v2.pdf</p>
<ol type="1">
<li>AAAI 2016，MatchPyramid，用图像识别的思路进行文本匹配</li>
</ol>
<p>《Text Matching as Image Recognition》</p>
<p>https://arxiv.org/pdf/1602.06359.pdf</p>
<ol type="1">
<li>MGCN,acl 2019，基于图形分解和卷积匹配的长文档匹配</li>
</ol>
<p>《Matching Article Pairs with Graphical Decomposition and Convolutions》</p>
<p>https://arxiv.org/pdf/1802.07459.pdf</p>
<ol type="1">
<li>SemBERT,AAAI 2020，使用BERT融合上下文语义信息实现文本匹配的模型</li>
</ol>
<p>《Semantics-aware BERT for Language Understanding》</p>
<p>https://arxiv.org/pdf/1909.02209.pdf</p>
<p><strong>⑤ 机器翻译</strong></p>
<ol type="1">
<li>EMNLP 2015， <strong>Luong NMT，</strong>经典的使用attention做神经机器翻译文章。</li>
</ol>
<ul>
<li><ol type="1">
<li><a href="https://arxiv.org/pdf/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a>.</li>
</ol></li>
</ul>
<ol type="1">
<li>ACL 2016， <strong>Coverage NMT</strong>，解决过度翻译和漏翻译问题。</li>
</ol>
<ul>
<li><ol type="1">
<li>Modeling Coverage for Neural Machine Translation 在nmt中使用coverage</li>
</ol></li>
</ul>
<ol type="1">
<li>EMNLP 2017，<strong>Google NMT，</strong> 神经机器翻译中的参数设置</li>
</ol>
<ul>
<li><ol type="1">
<li>Massive Exploration of Neural Machine Translation Architectures</li>
</ol></li>
</ul>
<ol type="1">
<li>ACL 2016，<strong>BPE NMT</strong>， Bpe子词的方法做机器翻译</li>
</ol>
<ul>
<li><ol type="1">
<li><a href="https://arxiv.org/pdf/1508.07909.pdf">Neural Machine Translation of Rare Words with Subword Units</a></li>
</ol></li>
</ul>
<ol type="1">
<li>EMNLP 2017，<strong>Recon MNT</strong>，一种新奇的加上一个auto-encoder解决机器翻译中的漏翻问题的论文</li>
</ol>
<ul>
<li><ol type="1">
<li>Neural Machine Translation with Reconstruction</li>
</ol></li>
</ul>
<ol type="1">
<li>EMNLP 2018，<strong>Back-Translation</strong>，机器翻译数据增强方法</li>
</ol>
<ul>
<li><ol type="1">
<li>Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation.</li>
</ol></li>
</ul>
<p><strong>⑥ 情感分析</strong></p>
<p>\1. 1 TextRNN,Recurrent Neural Network for Text Classification-使用循环神经网络进行句子的情感分类.2014——为主</p>
<p>1.2. FastText，Bag of Tricks for Efficient Text Classification-基于FastText的快速情感分类方法。2016</p>
<p>1.3. TextCNN,Convolutional Neural Networks for Sentence Classification-使用卷积神经网络进行句子的情感分类.2014</p>
<p>\2. TreeLSTM,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks-基于树结构的长短期记忆网络进行的语义表示上的改进.2015</p>
<p>3.1. TD-LSTM,Effective LSTMs for Target-Dependent Sentiment Classification-基于LSTM进行目标依赖的情感分类。2016——为主5</p>
<p>3.2. AT-LSTM,Attention-based lstm for aspect-level sentiment classification.-基于Attention的LSTM模型用于aspect-level的情感分类。2016</p>
<p>4.1. MemNet,Aspect Level Sentiment Classification with Deep Memory Network.-深度记忆网络在用于aspect-level情感分类.2016——为主</p>
<p>4.2. IAN,Interactive Attention Networks for Aspect-Level Sentiment Classification.-交互式注意力网络用于aspect-level情感分类.2017</p>
<p>5.1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.基于Bert模型进行情感分类。2018——为主</p>
<p>5.2. ERNIE 2.0: A Continual Pre-training Framework for Language Understanding.最大语义评测SemEval 2020上，ERNIE摘得5项世界冠军。2019</p>
<p><strong>⑦ 阅读理解</strong></p>
<p>\1. Teaching Machines to Read and Comprehend（<strong>完形填空</strong>）</p>
<p>论文以及机构：2015 nips Google DeepMind</p>
<p>文章提供一个比较大的数据集，同时首次给了三个 baseline 神经网络模型。从此基于神经网络的MRC任务开始迅速发展。</p>
<p>\2. Bi-Directional Attention Flow for Machine Comprehension（<strong>抽取式问答</strong>）</p>
<p>论文以及机构：2017 ICLR Allen实验室和University of Washington</p>
<p>首次提出了双向的attention流应用在MRC任务，在SQuAD数据上取得sota的结果，后面的许多论文都是在此基础上进行的。</p>
<p>\3. Get To The Point: Summarization with Pointer-Generator Networks（<strong>答案生成</strong>）</p>
<p>论文机构：斯坦福和谷歌</p>
<p>从文本摘要中借鉴过来的，主要解决的是生成任务中OOV等问题，在生成式阅读理解任务里可以通过引入PG-Net来达到更好的效果。</p>
<p>\4. Improving the Robustness of Question Answering Systems to Question Paraphrasing（考察模型的抗干扰能力）</p>
<p>论文和机构：2019ACL新加坡国立大学</p>
<p>研究发现很多模型在问题被攻击之后（即替换问题中的某些词语造成干扰），效果会大大降低。针对这类问题，主要从数据增强以及对抗训练的角度出发，提升模型抗干扰的能力。</p>
<p>5.XLNet: Generalized Autoregressive Pretraining for Language Understanding</p>
<p>论文和机构：2020 Google 卡内基梅隆大学</p>
<p>结合了自回归和自编码与训练语言模型的优势，提出双流自注意力机制，本次主要聚焦在中文的XLNet，也会对原文进行简单的核心介绍</p>
]]></content>
  </entry>
  <entry>
    <title>从小白到起飞，一站解决Atom编辑器各种骚操作</title>
    <url>/atom/</url>
    <content><![CDATA[<blockquote>
<p>一站到底解决IDE搭建问题！从基础设置、插件安装、snippets填充、到github版本控制、代码调试……本文将不断更新以求完善！</p>
</blockquote>
<h2 id="安装文本编辑器atom">安装文本编辑器Atom</h2>
<p>入坑Sublime、VSCode无数次后，还是回头选择了Atom IDE，因为它颜值惊艳、操作便捷、界面简单，除了许多编辑器都有的<strong>代码折叠</strong>和<strong>自动补全</strong>，还自带原生Markdown支持！！！这漂亮的实时预览和代码高亮真让人春心荡漾！并且，插件非常丰富！ <span id="more"></span> 于是，果断选择Atom作为本博主的<strong>创作神器</strong>，为Python开发之旅保驾护航！</p>
<p>官网一键安装：<a href="https://atom.io/">AtomSetup-x64</a></p>
<p>确认操作系统无误，点击download，打开下载好的AtomSetup-x64.exe，极速体验！</p>
<figure>
<img src="https://pic2.zhimg.com/v2-e2e305deea69303a19d71da452138945_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>缺点是启动速度不如sublime</em></p>
<h2 id="安装插件失足卡顿的惨痛经历">安装插件：失足卡顿的惨痛经历</h2>
<p>此时你一定急不可耐地冲向Install a Package，风风火火地下载了一堆插件：minimap用来预览全貌，atom-beautify用来格式化（想到令人头痛的html），file-icons小图标好可爱呀，material主题貌似很热门吼，markdown-preview-enhanced吊打我的Typora呢（Typora是我常用的md编辑器），script可以运行代码嗷，autocomplete-python自动补全呢，python-autopep8调格式也不错哟，linter-flake8检查语法错误呢，Hydrogen简直是jupyter的孪生姐妹……你在界面乐此不疲地倒腾……</p>
<p>A few hours later...</p>
<p>突然，你意识到此时的atom一片混乱卡顿缓慢，再也不是当初清纯活泼的模样！你蹙起眉，两行清泪润湿了乌黑的下眼眶！</p>
<p>一怒之下，你卸载了atom，并剿杀了一切软件残留：<a href="https://cn.compbs.com/how-uninstall-atom-windows">如何彻底删除atom</a>，<a href="https://www.coder.work/article/552966">如何更彻底地删除</a>！</p>
<h2 id="正确的打开方式是什么">正确的打开方式是什么？</h2>
<p>正文从这里开始！</p>
<p>那么，配置环境和安装插件的正确方式是什么呢？</p>
<ol type="1">
<li>打开Editor Settings，<strong>勾选Scroll Past End和Show Indent Guide，设置Tab Length为4，Font Size为20</strong>，护眼第一啦！</li>
<li>主题我喜欢atom自带的<strong>One Light，</strong>打开themes-&gt;One Light UI-&gt;Settings-&gt;<strong>Font Size设置为15</strong>，语法主题我选择<strong>Monokai</strong>。还是为了护眼！码字时还可以随时ctrl+和ctrl-调整字体大小。</li>
<li>下载插件：<strong>minimap、file-icons、atom-beautify、markdown-preview-enhanced、python-autopep8、markdown-writer</strong>。大道至简，这是我目前选择的基础组件，可能以后会更新！进入python-autopep8-&gt;Settings，勾选Format On Save。</li>
</ol>
<figure>
<img src="https://pic3.zhimg.com/v2-62a1e257adc2ca9978dee539798d3342_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>atom-material-syntax-dark语法主题也好看，只是Python语法高亮略丑，就不放了</em></p>
<p>个人认为，<strong>编辑器只需提供高效、舒适的代码体验即可</strong>，各种花里胡哨的功能比较鸡肋，反而会加重的负担。</p>
<p><a href="https://atom.io/themes/list?direction=desc&amp;sort=downloads">atom热门主题排行榜</a>：material、monokai、seti。</p>
<p><a href="https://atom.io/packages/list?direction=desc&amp;sort=stars">atom热门插件排行榜</a>：minimap、file-icons、atom-beautify、linter、script。</p>
<p><strong>下面重点来了，下载插件的方式！！！</strong></p>
<p>打开cmd，执行以下命令（apm是Atom Package Manager的缩写）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install autopep8</span><br><span class="line">apm install monokai, minimap, file-icons, atom-beautify, markdown-preview-enhanced, python-autopep8</span><br></pre></td></tr></table></figure>
<p>如果一起下载速度太慢，也可以apm install <package_name>分开下载。</p>
<p>晃悠了一杯茶的时间，已经全部done啦！Voila！</p>
<p>重启Atom，Ctrl+Shift+P打开Settings-&gt;Packages，是不是整齐陈列着我们要的插件呢？（不知为什么ctrl+逗号不能打开settings）</p>
<p>码上行动叭！！！</p>
<figure>
<img src="https://pic3.zhimg.com/v2-873f85f52366e6ab44b3eda548930102_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>这是最终的windows界面</em></p>
<p>另外，有童鞋选择github上面的源码git clone，然后cd进文件夹、npm install来安装，也不错呢~</p>
<p>对了，卸载插件的话，apm uninstall <package_name> 就好啦！</p>
<p>----------2021-03-04更新---------</p>
<h2 id="代码填充功能snippets">代码填充功能Snippets</h2>
<p>当我们需要重复使用一套模板时，不如试试<a href="https://www.jianshu.com/p/2ee34d8da142">Atom自带的Snippets代码块功能</a>，这种快捷填充，省去了重复码字的时间。打开终端输入下面指令，用atom打开snippets.cson：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">atom C:\Users\用户名\.atom\snippets.cson</span><br></pre></td></tr></table></figure>
<p>可以看到目前空空如也，在这个文件里面打“snip”，然后敲下tab键，会跳出来用于创建snippets的snippets（像套娃一样耶）。换上你想存储的代码块吧，示例如下：</p>
<figure>
<img src="https://pic1.zhimg.com/v2-d5d6ec618868382045aa1044eae6e988_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以后每次想插入代码块，直接输我们预设的“暗号”，然后按tab键即可，迅如闪电呀！</p>
<p><strong>注意</strong>：snippets只在相应语言中有效！并且，除了常见的Python、Java、JS等，<strong>其他有些语言是不支持snippets功能的</strong>，此时有两种方法：</p>
<ol type="1">
<li>通过apm install language-语言，安装相应语言的支持包；</li>
<li>将.source.语言 改成* 。</li>
</ol>
<p>我比较建议第二种方法。</p>
<p>另外，mardown中输入table、img、L等按下tab键，可以快捷插入表格、图片、链接等。</p>
<figure>
<img src="https://pic2.zhimg.com/v2-4d43fef791abc2195246b9621b37f4b9_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>成功啦，美滋滋~</p>
<h2 id="代码调试">代码调试</h2>
<p>暂时用不到，先占个坑，下次完善！</p>
<h2 id="使用github远程版本控制">使用Github远程版本控制</h2>
<p>又解锁atom连接github和git啦！赶紧更新日志！</p>
<p>话说，atom本来就是github开发的编辑器好嘛！我们来测试下好用吗！</p>
<p>Github注册无须多言吧，我创建了一个新的repository，命名blog，用来云端存储我的写作素材与稿件。这时我们看到页面提供了一系列指令：</p>
<figure>
<img src="https://pic1.zhimg.com/v2-6daf82cf178c578fd4cf4c19da68cb88_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>这里其实是blog仓库，但我为了演示又重新建了名为test的仓库</em></p>
<p>我们只需要打开windows系统的git bash，一句一句输入。我输入的是以下指令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd blog</span><br><span class="line">echo &quot;#blog&quot; &gt;&gt; README.md</span><br><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;first commit&quot;</span><br><span class="line">git branch -M main</span><br><span class="line">git remote add origin https:&#x2F;&#x2F;github.com&#x2F;MissFreak&#x2F;blog.git</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>
<p>成功后，在Atom打开blog这个文件夹（也就是你想要进行版本控制的项目），我们看到右侧显示github登录界面，提示我们打开github.atom.io，复制GitHub token到Atom的登录表单。Success！</p>
<figure>
<img src="https://pic1.zhimg.com/v2-6e1aa37602728c2eb6bdc8701b013270_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>接下来就随心所欲地增改文件，然后stage all-&gt;commit to main-&gt;pull 吧！</p>
<p>终于不用在终端敲指令，也可以和Github Desktop说拜拜啦！</p>
<figure>
<img src="https://pic4.zhimg.com/v2-713ea328565e6babf7e9559c20bc2e8b_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>参考资料：</strong><a href="https://flight-manual.atom.io/using-atom/sections/github-package/">Atom Documentation-GitHub package</a></p>
<h2 id="你可能不知道的快捷键">你可能不知道的快捷键</h2>
<table>
<thead>
<tr class="header">
<th>功能</th>
<th>快捷键</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>在所有项目文件中查找字符串</td>
<td>Ctrl + Shift + f</td>
</tr>
<tr class="even">
<td>打开导航栏的File、Edit、View等</td>
<td>Alt+导航栏首字母</td>
</tr>
<tr class="odd">
<td>向上/下移动该行</td>
<td>Ctrl + up/down</td>
</tr>
</tbody>
</table>
<p><a href="https://yanyinhong.github.io/2017/07/23/Atom-keyboard-shortcuts/">Windows环境下的Atom快捷键</a></p>
<p><a href="https://www.itread01.com/content/1549978931.html">mac下Atom编辑器快捷键大全</a></p>
<figure>
<img src="https://pic4.zhimg.com/v2-e0ed2bebdccf67e44a7c2bb61ff7b26b_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>在项目所有文件中搜索“正则”</em></p>
<p>----------2021-03-04更新---------</p>
<h2 id="html神器">HTML神器</h2>
<p>又屁颠屁颠跑来更新啦，虽然只有我一个人自娱自乐。</p>
<p>Python开发网站离不开的插件，亲测好用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apm install atom-html-preview, pigments, highlight-selected, autoclose-html-plus, color-picker, color-tabs</span><br></pre></td></tr></table></figure>
<p>效果如下：</p>
<figure>
<img src="https://pic3.zhimg.com/v2-692783c713218dc64a68c8faca88c096_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="markdown神器">Markdown神器</h2>
<p>还有，大家赶紧把这款神器mardown-preview-enhanced用起来吧，功能齐全到不可想象！<strong>可以折叠一级、二级、三级标题，专注于当前标题下的内容！（但是我刚用了貌似会卡）</strong></p>
<p>可以运行代码，并渲染运行结果：</p>
<figure>
<img src="https://pic3.zhimg.com/v2-12f773fb3a40b67afa0a432aa3dbc9b2_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>各种流程图、结构图：</p>
<figure>
<img src="https://pic2.zhimg.com/v2-9c4fd8ff9abd02d7cbb3f6566bf0549d_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>甚至制作幻灯片：</p>
<figure>
<img src="https://pic4.zhimg.com/v2-cdcf68324e69b094ea314250405b9e9b_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>还有太多亮瞎眼的操作不一一列举，详情请戳：https://shd101wyy.github.io/markdown-preview-enhanced/#/zh-cn/</p>
<p>VScode也可以用~真的很想把sublime扔进垃圾桶了！虽然sublime打开速度确实快！</p>
<h2 id="terminal-神器">Terminal 神器</h2>
<p>我又发现了一款终端程序包，terminal-plus！可以更改主题、查看终端当前运行的命令进程！用颜色标记状态图标和排序！从文本编辑器插入并运行文本！还有太多功能大家自己查阅！再次挖到宝贝啦！</p>
<p><a href="https://github.com/jeremyramin/terminal-plus">jeremyramin/terminal-plus</a></p>
<figure>
<img src="https://pic1.zhimg.com/v2-a579e162c502f89da0d9036ac21abfb8_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>总结：我目前安装的插件和主题如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── atom-beautify@0.33.4</span><br><span class="line">├── atom-html-preview@0.2.6</span><br><span class="line">├── autoclose-html-plus@0.27.2</span><br><span class="line">├── color-picker@2.3.0</span><br><span class="line">├── color-tabs@0.1.8</span><br><span class="line">├── file-icons@2.1.46</span><br><span class="line">├── highlight-selected@0.17.0</span><br><span class="line">├── markdown-preview-enhanced@0.18.8</span><br><span class="line">├── markdown-writer@2.11.11</span><br><span class="line">├── minimap@4.39.9</span><br><span class="line">├── monokai@0.27.0</span><br><span class="line">├── pigments@0.40.6</span><br><span class="line">├── python-autopep8@0.1.3</span><br><span class="line">└── python-tools@0.6.9</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>atom</tag>
        <tag>editor</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention模型中注意力的多尺度视觉化</title>
    <url>/attention/</url>
    <content><![CDATA[<p>Transformer是一个序列模型，它放弃了传统的递归架构，而采用了完全基于注意力的方法。除了提高性能外，使用注意力的一个好处是，它还可以通过显示模型如何为不同的输入元素分配权重来帮助解释一个模型。然而，Transformer模型中的多层、多头注意力机制可能难以解读。为了使该模型更容易理解，我们介绍了一个开源工具，该工具在多个尺度上将注意力可视化，每个尺度都为注意力机制提供一个独特的视角。我们在BERT和OpenAI GPT-2上演示了该工具，并提出了三个用例：检测模型的偏差，定位相关的注意力头，以及将神经元与模型行为联系起来。</p>
<span id="more"></span>
<h1 id="引言">1 引言</h1>
<p>2018年，BERT（Bidirectional Encoder Representations from Transformers）语言表示模型在从情感分析到问题回答的NLP任务中取得了最先进的性能（Devlin等人，2018）。最近，OpenAI GPT-2（Generative Pretrained Transformer-2）模型在几个语言建模基准上的表现超过了其他模型的零点设置（Radford等人，2019）。</p>
<p>BERT和GPT-2的基础是Transformer模型，它使用了一种完全基于注意力的方法，与基于递归架构的传统序列模型形成对比（Vaswani等人，2017）。使用注意力的一个好处是，它可以通过显示模型如何为不同的输入元素分配权重来帮助解释一个模型（Bahdanau等人，2015；Belinkov和Glass，2019），尽管它在解释单个预测方面的价值可能有限（Jain和Wallace，2019）。已经开发了各种工具来可视化NLP模型中的注意力，从注意力矩阵热图（Bahdanau等人，2015年；Rush等人，2015年；Rocktaschel等人 ¨ ，2016年）到二方图表示（Liu等人，2018年；Lee等人，2017年；Strobelt等人，2018年）。</p>
<p>在Transformer中实现注意力可视化的一个挑战是，它使用了一个多层、多头的注意力机制，为每一层和每一个头产生不同的注意力模式。例如，BERT-Large有24层和16个头，对每个输入产生24×16=384个独特的注意结构。Jones（2017年）专门为多头注意力设计了一个可视化工具，通过叠加它们的注意力模式，将一层中多个头的注意力可视化（Vaswani等人，2017，2018）。</p>
<p>在本文中，我们扩展了Jones（2017）的工作，在Transformer中实现了多尺度的注意力可视化。我们引入了一个高层次的模型视图，在一个界面中可视化所有的层和注意力头，以及一个低层次的神经元视图，显示单个神经元如何互动以产生注意力。我们还将该工具从原来的编码器-解码器的实现调整为仅有解码器的GPT-2模型和仅有编码器的BERT模型。</p>
<p>https://arxiv.org/pdf/1608.05604.pdf</p>
<p>https://arxiv.org/pdf/1906.05714.pdf</p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>BERT和SentenceBERT</title>
    <url>/bert/</url>
    <content><![CDATA[<h1 id="bert模型输入">BERT模型输入</h1>
<p>BERT与Transformer 的编码方式一样。将固定长度的字符串作为输入，数据由下而上传递计算，每一层都用到了self-attention，并通过前馈神经网络传递其结果，将其交给下一个编码器。</p>
<h1 id="bert与transformer-不同之处">BERT与Transformer 不同之处</h1>
<h3 id="模型输出">模型输出</h3>
<p>每个位置返回的输出都是一个隐藏层大小的向量（基本版本BERT为768）。以文本分类为例，我们重点关注第一个位置上的输出（第一个位置是分类标识[CLS]） 。</p>
<p>该向量现在可以用作分类器的输入，在论文中指出使用单层神经网络作为分类器就可以取得很好的效果。</p>
<figure>
<img src="https://img-blog.csdnimg.cn/2018120509180072.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjY0ODQ1,size_16,color_FFFFFF,t_70" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h1 id="参考">参考</h1>
<h2 id="经典讲解">经典讲解</h2>
<p>https://jalammar.github.io/illustrated-transformer/</p>
<p>https://zhuanlan.zhihu.com/p/47063917</p>
<p>https://zhuanlan.zhihu.com/p/47282410</p>
<h2 id="视频">视频</h2>
<p>唐宇迪：基本就是按照上面的文献来。</p>
<h2 id="其他">其他</h2>
<p>https://zhuanlan.zhihu.com/p/113133510</p>
<p>https://arxiv.org/pdf/1908.10084.pdf</p>
<p>https://www.gushiciku.cn/pl/pWzZ</p>
<p>https://zhuanlan.zhihu.com/p/49271699</p>
<p>https://zhuanlan.zhihu.com/p/51337063</p>
]]></content>
  </entry>
  <entry>
    <title>使用 BERT 提取关键字</title>
    <url>/bert-keyword/</url>
    <content><![CDATA[<p>使用<a href="https://github.com/aneesha/RAKE">Rake</a>和<a href="https://github.com/LIAAD/yake">YAKE</a>等方法<a href="https://github.com/LIAAD/yake">！</a>我们已经有易于使用的包，可用于提取关键字和关键短语。然而，这些模型通常基于文本的统计属性而不是语义相似性。</p>
<p>在谈到<strong>BERT</strong>。BERT 是一种双向转换器模型，它允许我们将短语和文档转换为能够捕捉其含义的向量。</p>
<blockquote>
<p>如果我们使用 BERT 而不是统计模型会怎样？</p>
</blockquote>
<p>尽管有许多使用 BERT 嵌入的优秀论文和解决方案（例如<a href="https://github.com/pranav-ust/BERT-keyphrase-extraction">1</a>、<a href="https://github.com/ibatra/BERT-Keyword-Extractor">2</a>、<a href="https://www.preprints.org/manuscript/201908.0073/download/final_file">3</a>、 ），但我找不到简单且易于使用的基于 BERT 的解决方案。相反，我决定创建<a href="https://github.com/MaartenGr/KeyBERT/">KeyBERT，这</a>是一种利用 BERT 嵌入的最小且易于使用的关键字提取技术。</p>
<p>现在，本文的主要主题将不是<a href="https://github.com/MaartenGr/KeyBERT">KeyBERT</a>的使用，而是关于如何使用 BERT 创建自己的<strong>关键字提取模型</strong>的<strong>教程</strong>。</p>
<h1 id="数据">1. 数据</h1>
<p>在本教程中，我们将使用有关<strong>监督机器学习</strong>的文档：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">doc = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">         Supervised learning is the machine learning task of </span></span><br><span class="line"><span class="string">         learning a function that maps an input to an output based </span></span><br><span class="line"><span class="string">         on example input-output pairs.[1] It infers a function </span></span><br><span class="line"><span class="string">         from labeled training data consisting of a set of </span></span><br><span class="line"><span class="string">         training examples.[2] In supervised learning, each </span></span><br><span class="line"><span class="string">         example is a pair consisting of an input object </span></span><br><span class="line"><span class="string">         (typically a vector) and a desired output value (also </span></span><br><span class="line"><span class="string">         called the supervisory signal). A supervised learning </span></span><br><span class="line"><span class="string">         algorithm analyzes the training data and produces an </span></span><br><span class="line"><span class="string">         inferred function, which can be used for mapping new </span></span><br><span class="line"><span class="string">         examples. An optimal scenario will allow for the algorithm </span></span><br><span class="line"><span class="string">         to correctly determine the class labels for unseen </span></span><br><span class="line"><span class="string">         instances. This requires the learning algorithm to  </span></span><br><span class="line"><span class="string">         generalize from the training data to unseen situations </span></span><br><span class="line"><span class="string">         in a &#x27;reasonable&#x27; way (see inductive bias).</span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>我相信，使用有关读者非常了解的主题的文档可以帮助您了解生成的关键短语是否具有质量。</p>
<h1 id="候选关键词关键词">2. 候选关键词/关键词</h1>
<p>我们首先从文档中创建候选关键字或关键短语的列表。尽管许多人关注名词短语，但我们将通过使用 Scikit-Learns 来保持简单<code>CountVectorizer</code>。这允许我们指定关键字的长度并将它们变成关键短语。这也是一种快速去除停用词的好方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">n_gram_range = (<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">stop_words = <span class="string">&quot;english&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Extract candidate words/phrases</span></span><br><span class="line">count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])</span><br><span class="line">candidates = count.get_feature_names()</span><br></pre></td></tr></table></figure>
<p>我们可以使用<code>n_gram_range</code>来改变结果候选的大小。例如，如果我们将其设置为，<code>(3, 3)</code>那么结果候选将是包含<strong>3 个关键字的</strong>短语。</p>
<p>然后，该变量<code>candidates</code>只是一个字符串列表，其中包含我们的候选关键字/关键短语。</p>
<p><strong>注意</strong>：您可以玩弄<code>n_gram_range</code>以创建不同长度的关键短语。然后，您可能不想删除 stop_words，因为它们可以将更长的关键词组合在一起。</p>
<h1 id="嵌入">3. 嵌入</h1>
<p>接下来，我们将文档以及候选关键字/关键短语都转换为数字数据。我们为此目的使用<strong>BERT</strong>，因为它在相似性和释义任务中都显示出很好的结果。</p>
<p>生成 BERT 嵌入的方法有很多，例如<a href="https://github.com/flairNLP/">Flair</a>、<a href="https://github.com/huggingface/transformers">Hugginface Transformers</a>，现在甚至<a href="https://nightly.spacy.io/">还有</a>3.0 版本的<a href="https://nightly.spacy.io/">spaCy</a>！但是，我更喜欢使用该<code>sentence-transformers</code>包，因为它允许我快速创建高质量的嵌入，这些嵌入非常适用于句子级和文档级嵌入。</p>
<p>我们使用<code>pip install sentence-transformers</code>. 如果您在安装此软件包时遇到问题，那么先安装<a href="https://pytorch.org/get-started/locally/">Pytorch</a>可能会有所帮助。</p>
<p>现在，我们将运行以下代码将我们的文档和候选者转换为向量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line"></span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line">doc_embedding = model.encode([doc])</span><br><span class="line">candidate_embeddings = model.encode(candidates)</span><br></pre></td></tr></table></figure>
<p>我们是<strong>Distilbert，</strong>因为它在相似性任务中表现出出色的性能，这就是我们使用关键字/关键短语提取的目标！</p>
<p>由于转换器模型有令牌限制，因此在输入大型文档时可能会遇到一些错误。在这种情况下，您可以考虑将文档拆分为多个段落，并对结果向量进行平均池化（取平均值）。</p>
<p><strong>注意</strong>：有许多<a href="https://www.sbert.net/docs/pretrained_models.html">预训练的基于 BERT 的模型</a>可用于关键字提取。但是，我建议您使用<code>distilbert — base-nli-stsb-mean-tokens</code> 或 ，<code>xlm-r-distilroberta-base-paraphase-v1</code> 因为它们分别在<strong>语义相似性</strong>和<strong>释义识别</strong>方面表现出色。</p>
<h1 id="余弦相似度">4.余弦相似度</h1>
<p>在最后一步，我们要找到与文档最相似的候选者。我们假设与文档最相似的候选者是代表文档的良好关键字/关键短语。</p>
<p>为了计算候选和文档之间的相似度，我们将使用向量之间的<strong>余弦相似度</strong>，因为它在高维上表现得非常好：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line">top_n = <span class="number">5</span></span><br><span class="line">distances = cosine_similarity(doc_embedding, candidate_embeddings)</span><br><span class="line">keywords = [candidates[index] <span class="keyword">for</span> index <span class="keyword">in</span> distances.argsort()[<span class="number">0</span>][-top_n:]]</span><br></pre></td></tr></table></figure>
<p>还有……就是这样！我们将与输入文档最相似的前 5 个候选作为结果关键字。</p>
<p>结果看起来很棒！这些术语看起来确实像是在描述有关监督机器学习的文档。</p>
<p>现在，让我们看看会发生什么，如果我们改变<code>n_gram_range</code>到<code>(3,3)</code>：</p>
<p><img src="https://miro.medium.com/max/1050/1*5DwToI85spdjK4dbvpmehQ.png" /></p>
<p>似乎我们现在得到了<strong>关键短语</strong>而不是<strong>关键字</strong>！这些关键词本身似乎很好地代表了文档。然而，我并不高兴所有的关键短语都如此相似。</p>
<p>为了解决这个问题，让我们来看看我们结果的<strong>多样化</strong>。</p>
<h1 id="多元化">5. 多元化</h1>
<p>返回类似结果是有原因的……它们最能代表文档！如果我们要使关键字/关键短语多样化，那么它们就不太可能很好地代表文档作为一个<strong>集合。</strong></p>
<p>因此，我们结果的<strong>多样化</strong>需要在关键字/关键短语的准确性和它们之间的多样性之间取得微妙的平衡。</p>
<p>我们将使用两种算法来使结果多样化：</p>
<ul>
<li>最大和相似度</li>
<li>最大边际相关性</li>
</ul>
<h2 id="最大和相似度">最大和相似度</h2>
<p>数据对之间的最大总和距离被定义为它们之间的距离被最大化的数据对。在我们的例子中，我们希望最大化候选与文档的相似度，同时最小化候选之间的相似度。</p>
<p>为此，我们选择前 20 个关键字/关键短语，然后从这 20 个中选择彼此最不相似的 5 个：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_sum_sim</span>(<span class="params">doc_embedding, word_embeddings, words, top_n, nr_candidates</span>):</span></span><br><span class="line">    <span class="comment"># Calculate distances and extract keywords</span></span><br><span class="line">    distances = cosine_similarity(doc_embedding, candidate_embeddings)</span><br><span class="line">    distances_candidates = cosine_similarity(candidate_embeddings, </span><br><span class="line">                                            candidate_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get top_n words as candidates based on cosine similarity</span></span><br><span class="line">    words_idx = <span class="built_in">list</span>(distances.argsort()[<span class="number">0</span>][-nr_candidates:])</span><br><span class="line">    words_vals = [candidates[index] <span class="keyword">for</span> index <span class="keyword">in</span> words_idx]</span><br><span class="line">    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the combination of words that are the least similar to each other</span></span><br><span class="line">    min_sim = np.inf</span><br><span class="line">    candidate = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> combination <span class="keyword">in</span> itertools.combinations(<span class="built_in">range</span>(<span class="built_in">len</span>(words_idx)), top_n):</span><br><span class="line">        sim = <span class="built_in">sum</span>([distances_candidates[i][j] <span class="keyword">for</span> i <span class="keyword">in</span> combination <span class="keyword">for</span> j <span class="keyword">in</span> combination <span class="keyword">if</span> i != j])</span><br><span class="line">        <span class="keyword">if</span> sim &lt; min_sim:</span><br><span class="line">            candidate = combination</span><br><span class="line">            min_sim = sim</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [words_vals[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> candidate]</span><br></pre></td></tr></table></figure>
<p>如果我们设置一个<strong>低</strong><code>nr_candidates</code>，那么我们的结果似乎与我们原来的余弦相似度方法非常相似：</p>
<p><img src="https://miro.medium.com/max/1050/1*fG-QkmlyF3FGW6AgnzinIw.png" /></p>
<p>但是，相对<strong>较高的值</strong><code>nr_candidates</code>会产生更多样化的关键词：</p>
<p><img src="https://miro.medium.com/max/1050/1*_uMu3fdZifQk2JMxPpTKHA.png" /></p>
<p>如前所述，您必须牢记准确性和多样性之间的权衡。如果您增加<code>nr_candidates</code>，则很有可能您会获得非常多样化的关键字，但这并不是文档的很好表示。</p>
<p>我建议您<code>nr_candidates</code>在文档中保留少于 20% 的唯一单词总数。</p>
<h2 id="最大边际相关性">最大边际相关性</h2>
<p>使我们的结果多样化的最后一种方法是<strong>最大边际相关性</strong>(MMR)。MMR 尝试在文本摘要任务中最小化冗余并最大化结果的多样性。幸运的是，名为<a href="https://arxiv.org/pdf/1801.04470.pdf">EmbedRank</a>的关键字提取算法实现了一个 MMR 版本，允许我们使用它来使我们的关键字/关键短语多样化。</p>
<p>我们首先选择与文档最相似的关键字/关键短语。然后，我们迭代地选择与文档相似但与已选择的关键字/关键短语不相似的新候选词：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmr</span>(<span class="params">doc_embedding, word_embeddings, words, top_n, diversity</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Extract similarity within words, and between words and the document</span></span><br><span class="line">    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)</span><br><span class="line">    word_similarity = cosine_similarity(word_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize candidates and already choose best keyword/keyphras</span></span><br><span class="line">    keywords_idx = [np.argmax(word_doc_similarity)]</span><br><span class="line">    candidates_idx = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(words)) <span class="keyword">if</span> i != keywords_idx[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(top_n - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Extract similarities within candidates and</span></span><br><span class="line">        <span class="comment"># between candidates and selected keywords/phrases</span></span><br><span class="line">        candidate_similarities = word_doc_similarity[candidates_idx, :]</span><br><span class="line">        target_similarities = np.<span class="built_in">max</span>(word_similarity[candidates_idx][:, keywords_idx], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate MMR</span></span><br><span class="line">        mmr = (<span class="number">1</span>-diversity) * candidate_similarities - diversity * target_similarities.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        mmr_idx = candidates_idx[np.argmax(mmr)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update keywords &amp; candidates</span></span><br><span class="line">        keywords_idx.append(mmr_idx)</span><br><span class="line">        candidates_idx.remove(mmr_idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [words[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> keywords_idx]</span><br></pre></td></tr></table></figure>
<p>如果我们设置一个相对<strong>较低的diversity</strong>，那么我们的结果似乎与我们原来的余弦相似度方法非常相似：</p>
<p><img src="https://miro.medium.com/max/1050/1*fZb31IpYV3UAc7Jhkbv7xw.png" /></p>
<p>但是，相对<strong>较高的多样性</strong>得分会产生非常多样化的关键短语：</p>
<p><img src="https://miro.medium.com/max/1050/1*70FekE90MTceTBr4tIGyYQ.png" /></p>
<h1 id="参考">参考</h1>
<p><a href="https://github.com/MaartenGr/KeyBERT">KeyBERT</a>, <a href="https://pythonawesome.com/a-toolkit-for-multilingual-keyword-extraction-based-on-googles-bert/">BERT关键词提取工具包</a>， <a href="https://jaketae.github.io/study/keyword-extraction/">使用 BERT 提取关键字</a>，<strong><a href="https://github.com/ibatra/BERT-Keyword-Extractor">BERT-关键字提取器</a></strong></p>
<p>https://arxiv.org/pdf/1910.05786.pdf</p>
<p>http://jaitc.ki-it.or.kr/xml/24833/24833.pdf</p>
]]></content>
  </entry>
  <entry>
    <title>Bootstrapping 2.0</title>
    <url>/bootstrap-2/</url>
    <content><![CDATA[<h2 id="代码-v2">代码-V2</h2>
<p><span class="math display">\[ \begin{aligned} \operatorname{Sim}\left(S_{n}, S_{j}\right)=&amp; \alpha \cdot \cos \left(Head_{i}, Head_{j}\right) +\beta \cdot \cos \left(HeadDep_{i}, HeadDep_{j}\right) +\gamma \cdot \cos \left(HeadHead_{i}, HeadHead_{j}\right) \end{aligned} \]</span> (1)</p>
<p>怎么判断一个pattern是否成立？</p>
<ul>
<li>看产生种子的情况！（这个种子就从现存术语库中取）</li>
</ul>
<p>怎么判断一个实例是否可以加入种子集？</p>
<p>这个实例要满足一些条件：比如长度、不存在stopwords、大小写、pos、一些特定的词汇（名词是model等）、出现词汇的等。</p>
<p>开始先用大写抽取出一些种子</p>
<p>Termhood</p>
<p>怎么判断是term还是不是？</p>
]]></content>
  </entry>
  <entry>
    <title>Bootstrapping 1.0</title>
    <url>/bootstrap/</url>
    <content><![CDATA[<h2 id="代码-v1.1">代码-V1.1</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;../result/candidates.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处仅仅是方法的种子</span></span><br><span class="line">seed_list = [<span class="string">&quot;multi-head self-attention&quot;</span>, <span class="string">&quot;gated recurrent units&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 如果种子出现在文章中，就把种子所在的pattern提取出来</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_pattern</span>(<span class="params">cell</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tokenized_candidates = re.split(<span class="string">r&quot;[^0-9a-z]&quot;</span>, <span class="built_in">str</span>(cell.lower()))</span><br><span class="line">        <span class="keyword">for</span> seed <span class="keyword">in</span> seed_list:</span><br><span class="line">            same = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> re.split(<span class="string">r&quot;[^0-9a-z]&quot;</span>, seed.lower()):</span><br><span class="line">                <span class="keyword">if</span> token <span class="keyword">in</span> tokenized_candidates:</span><br><span class="line">                    same += <span class="number">1</span></span><br><span class="line">            different = <span class="built_in">len</span>(tokenized_candidates) - same</span><br><span class="line">            <span class="keyword">if</span> same &gt; <span class="number">2</span> <span class="keyword">and</span> different &lt; <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">pattern_df = df[df[<span class="string">&quot;candidates&quot;</span>].<span class="built_in">map</span>(find_pattern).values]</span><br><span class="line">print(pattern_df)</span><br><span class="line">pattern_df.to_csv(<span class="string">&quot;../result/patterns.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_seed</span>(<span class="params">row</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    pattern_df：抽取的patterns.csv</span></span><br><span class="line"><span class="string">    pattern_row: 抽取出来的pattern中的一行，代表一个pattern</span></span><br><span class="line"><span class="string">    row: 原来的candidates的一行</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># print(row.loc[[&quot;head_dep&quot;]])</span></span><br><span class="line">    <span class="keyword">for</span> _, pattern_row <span class="keyword">in</span> pattern_df.iterrows():</span><br><span class="line">        <span class="comment"># if pattern_row[&quot;dep&quot;] == &quot;pobj&quot;:</span></span><br><span class="line">        <span class="comment">#     print(pattern_row[&quot;dep&quot;])</span></span><br><span class="line">        <span class="keyword">if</span> row[<span class="string">&quot;head_dep&quot;</span>] == pattern_row[<span class="string">&quot;head_dep&quot;</span>] <span class="keyword">and</span> row[<span class="string">&quot;head&quot;</span>] == pattern_row[<span class="string">&quot;head&quot;</span>] <span class="keyword">and</span> row[<span class="string">&quot;head_head_lemma&quot;</span>] == pattern_row[<span class="string">&quot;head_head_lemma&quot;</span>]:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">seed_df = df[df.apply(find_seed,axis=<span class="number">1</span>).values]</span><br><span class="line">print(seed_df)</span><br><span class="line">seed_df.to_csv(<span class="string">&quot;../result/seeds.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>第0章：为什么要学习微积分？</title>
    <url>/calculus-I-0/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>本章的目的是诱使你学习一些微积分。</p>
<p><strong>主题</strong></p>
<p>0.1 你应该知道什么</p>
<p>0.2 什么是微积分，我们为什么要学习它？</p>
<span id="more"></span>
<h1 id="你应该知道什么">0.1 你应该知道什么</h1>
<p>要学习微积分，你必须要有呼吸的能力。没有这种能力，你很快就会死亡，无法继续下去。 除此之外，你还需要熟悉两个概念：一个是数的概念，一个是函数的概念。</p>
<p><strong>假设我已经忘记了我所知道的关于数字和函数的一切？</strong> 不要担心。我们将回顾它们的属性。</p>
<p><strong>如果我知道所有关于数和函数的知识呢？</strong> 那么你已经了解了微积分，不需要再继续了。</p>
<p>在提醒自己关于数和函数的知识之前，你可以问以下问题。</p>
<p><strong>什么是微积分？</strong> <strong>我为什么要学习它？</strong></p>
<h1 id="什么是微积分为什么我们要研究它">0.2 什么是微积分，为什么我们要研究它？</h1>
<p>微积分是关于事物如何变化的研究。它提供了一个对存在变化的系统进行建模的框架，以及一种推导这种模型的预测的方法。</p>
<p><strong>我已经待了一段时间，或多或少知道事物是如何变化的。微积分能给我带来什么？</strong></p>
<p>我确信你对事物的变化有很多了解。而且你对微积分有一个定性的概念。例如，运动速度的概念是一个直接来自微积分的概念，尽管它肯定在微积分之前就已经存在了，而且你对它了解很多。</p>
<p><strong>那么，微积分为我增加了什么？</strong></p>
<p>它为我们提供了一种方法来构建相对简单的变化的定量模型，并推断其结果。</p>
<p><strong>达到什么目的呢？</strong></p>
<p>有了它，你就有能力找到变化的条件对正在研究的系统的影响。通过研究这些，你可以学习如何控制系统，使其做你想做的事情。微积分使工程师和你有能力对系统进行建模和控制，从而使他们（以及可能是你）对物质世界拥有非凡的力量。</p>
<p>微积分的发展及其在物理学和工程学中的应用，可能是现代科学发展的最重要因素，超过了阿基米德时代的水平。而这也是工业革命以及随之而来的一切，包括过去几个世纪几乎所有的重大进步的原因。</p>
<p><strong>你是否试图声称我将知道足够的微积分来建立系统模型，并推导出足够的控制它们的方法？</strong></p>
<p>如果你在1990年问我这个问题，我会说没有。现在，对于一些非微不足道的系统，在你使用你的笔记本电脑或台式电脑的情况下，它是在可能的范围内。</p>
<p><strong>好吧，但是微积分模型是如何变化的？微积分是什么样子的？</strong></p>
<p>微积分的基本思想是通过研究“瞬时”变化来研究变化，我们指的是微小时间间隔内的变化。</p>
<p><strong>这有什么好处呢？</strong></p>
<p>事实证明，这种变化往往比有限时间间隔的变化要简单得多。这意味着它们更容易建模。事实上，微积分是由牛顿发明的，他发现加速度，也就是物体的速度变化，可以通过他相对简单的运动定律来建模。</p>
<p><strong>那又如何？</strong></p>
<p>这就给我们留下了一个问题，即从物体的速度或加速度的信息中推导出关于物体运动的信息。而微积分的细节涉及速度和加速度所代表的概念与位置所代表的概念之间的相互关系。</p>
<p><strong>那么，在学习微积分的过程中，我们要学习什么呢？</strong></p>
<p>首先，你必须有一个框架来描述诸如位置、速度和加速度等概念。</p>
<p>单变量微积分，也就是我们开始学习的，可以处理一个物体沿固定路径的运动。更普遍的问题是，当运动可以发生在表面或空间时，可以用多变量微积分处理。我们研究这后一个主题时，要找到使用一维思想和方法来处理更普遍问题的巧妙技巧。所以单变量微积分也是一般问题的关键。</p>
<p>当我们处理一个沿着路径运动的物体时，它的位置随时间变化，我们可以用一个单一的数字来描述它在任何时候的位置，这个数字可以是距离该路径上的某个固定点（称为我们坐标系的原点）的一些单位。我们在这个距离上加一个符号，如果物体在原点后面，这个符号将是负的）。</p>
<p>然后，物体的运动由其在相关时间点的数字位置集来描述。</p>
<p>我们用来描述运动的位置和时间集合就是我们所说的函数。在所有应用微积分的系统中，类似的函数被用来描述所关注的量。</p>
<p>这里的课程从回顾数字和函数及其属性开始。毫无疑问，你对其中的大部分内容都很熟悉，所以我们试图增加一些不熟悉的材料，让你在看的时候保持注意力。</p>
<p><strong>如果我读到这样的东西，我就会陷入困境。我必须这样做吗？</strong></p>
<p>我很想让你看看，因为这是我写的，但如果你不愿意，你无疑可以跳过它，在需要时再参考它。但是你会错过新的信息，这样做可能会使你永远受到困扰。（虽然我对此表示怀疑）。</p>
<p><strong>那么在数字和函数之后是什么呢</strong>？</p>
<p>一个典型的微积分课程包括以下主题。</p>
<ol type="1">
<li>如何找到各种函数的瞬时变化（称为 "导数"）。这样做的过程被称为 "微分"）。</li>
<li>如何使用导数来解决各种问题。</li>
<li>如何从一个函数的导数回到函数本身。这个过程被称为 "积分"）。</li>
<li>研究某些种类的函数积分的详细方法。</li>
<li>如何使用积分来解决各种几何问题，如某些区域的面积和体积的计算。</li>
</ol>
<p>在这样的课程中还有一些其他的标准课题。其中包括用幂级数来描述函数，以及研究无限级数何时 "收敛 "到一个数字。</p>
<p><strong>那么，这让我有什么权力去做什么呢？</strong></p>
<p>它并没有真正这样做。问题是，这些课程最初是在几个世纪前设计的，它们的目的不是增强能力（在当时是完全不可能的），而是让听众熟悉一些思想、概念和符号，以便理解更高级的工作。数学家、科学家和工程师在各种背景下使用微积分的概念，并使用行话和符号，如果你不学习微积分，你将完全无法理解。学习微积分的目的通常是让你具备 "数学素养"，以便与这些更高级的工作联系起来。</p>
<p><strong>那么，为什么要对赋权一统胡说八道呢？</strong></p>
<p>本课程将努力做到与众不同，在实现其他通常目标的同时，也致力于增强能力。它可能不会成功，但至少会尝试。</p>
<p><strong>那么，它将如何尝试演绎这一奇迹呢？</strong></p>
<p>传统的微积分课程强调用代数方法进行微分和积分。我们将描述这些方法，但也将展示如何在计算机电子表格上以可接受的工作量进行微分和积分（以及普通微分方程的解法）。我们还将提供一些小程序，这些小程序可以自动进行同样的操作，而且工作量更小。通过这些小程序或电子表格，你可以比以前更容易和更灵活地应用微积分的工具。</p>
<p>还有一些更高级的程序，如MAPLE和Mathematica，它们可以让你以类似的方式做更多的事情。有了它们，你可以在各种各样的背景下推导出各种模型的结果。一旦你理解了微积分，它们可以使微积分的使用变得更加容易，但它们提供的答案是给定的输入，这并不提供对它们如何做的理解)。</p>
<p>另外，我们将更多地强调建模系统。有了关于建模的想法和解决它们导致的微分方程的方法，你可以实现我们所声称的授权。</p>
<p><strong>我将能够利用这一点达到一些有价值的目的？</strong></p>
<p>好吧，可能不会。但你可能会。还有，你可能会被激起去学习更多关于你想研究的系统或数学的知识，以提高你这样做的机会。另外，你可能会比现在更好地理解模型的可能后果。另外你可能会爱上微积分的概念和思想。</p>
<p><strong>那么，关于数字的介绍性章节有哪些内容呢？</strong></p>
<p>我们从自然数<span class="math inline">\((1,2,3,...)\)</span>开始，并注意到减法、除法和取平方根的运算是如何引导我们将数系统扩展到包括负数、分数（称为有理数）和复数。我们还描述了十进制扩展（描述 "实数"），并研究了可数性的概念。我们也嘀咕了复数。</p>
<p><strong>在关于函数的章节里呢？</strong></p>
<p>我们从函数的抽象定义开始（作为一组参数-值对），然后描述标准函数。这些是通过从同一函数（值=参数）和指数函数开始，并对它们进行各种运算得到的。</p>
<p><strong>运算，什么运算？</strong></p>
<p>这些是加法、减法、乘法、除法、替换和反转。</p>
<p><strong>但什么是指数函数，什么是置换和反转？</strong></p>
<p>这里有一句话的答案：如果你想知道更多的内容，请阅读本章!</p>
<p>指数函数是用微积分神秘地定义的：它是自己的导数的函数，定义为在参数0处的值为1，然而，它原来是你以前见过的东西。而且，它与三角学的正弦函数有着密切的关系。</p>
<p>将一个函数<span class="math inline">\(f\)</span>替换成另一个函数<span class="math inline">\(g\)</span>，会产生一个新的函数，这个函数被定义为在参数<span class="math inline">\(x\)</span>处有<span class="math inline">\(f\)</span>的值，而<span class="math inline">\(f\)</span>在参数<span class="math inline">\(x\)</span>处的值就是<span class="math inline">\(g\)</span>的值，这比听起来要简单。例如，假设<span class="math inline">\(y=x^2\)</span>，而<span class="math inline">\(x=2z\)</span>，那么<span class="math inline">\(y(x(z))\)</span>就是<span class="math inline">\((2z)^2\)</span>。</p>
<p>一个函数的逆函数是通过将其值与参数交换而得到的函数。例如平方函数，通常写成<span class="math inline">\(x^2\)</span>有一个平方根函数作为其逆函数。</p>
<p>用路易斯-卡罗尔（Lewis Carroll）写给他侄子的威廉神父的不朽名言来说，他是一位数学家：</p>
<blockquote>
<p>我已经回答了三个问题，这就够了。</p>
<p>圣人说，不要给自己找借口。</p>
<p>你认为我可以整天听这些东西吗？</p>
<p>快走吧，否则我就把你踢下楼去！</p>
</blockquote>
<h1 id="原文链接">原文链接</h1>
<p>http://www-math.mit.edu/~djk/calculus_beginners/index.html</p>
]]></content>
      <categories>
        <category>数学</category>
        <category>微积分</category>
      </categories>
      <tags>
        <tag>calculus</tag>
      </tags>
  </entry>
  <entry>
    <title>认知语言学大纲</title>
    <url>/cognitive-linguistics/</url>
    <content><![CDATA[<p><strong><em>*Syllabus (Spring*</em></strong> <strong><em>*S*</em><em>*emester, 2021)*</em></strong></p>
<p><strong><em>*COURSE TITLE:*</em></strong> <strong><em>*Introduction to C*</em><em>*ognitive*</em></strong> <strong><em>*Linguistics*</em></strong> <strong><em>*认知语言学*</em></strong></p>
<figure>
<img src="file:///C:\Users\13607\AppData\Local\Temp\ksohtml15088\wps1.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><em>*INSTRUCTOR: LAN CHUN EMAIL:*</em></strong> beiwailanchun@126.com</p>
<p><strong><em>*STUDENTS: SEIS postgraduate*</em></strong> grade 2018</p>
<p><strong><em>*TIME: 10:10-12:00,Thursday*</em></strong> <strong><em>*PLACE*</em><em>*：S*</em><em>*EIS Building 417*</em></strong></p>
<p><strong><em>*OFFICE:*</em></strong> Room 202, SEIS Building <strong><em>*OFFICE HOURS:*</em></strong> 10:10-12.00, Friday</p>
<figure>
<img src="file:///C:\Users\13607\AppData\Local\Temp\ksohtml15088\wps2.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><em>*I. OBJECTIVES*</em></strong></p>
<p>This course aims at introducing to students the basic concepts, theories, approaches, research methods and hot issues of cognitive linguistics. It is expected that at the end of the course, students can have a general idea of the intricate relationships between language and cognition and of the various cognitive perspectives of doing linguistic research. Students are also expected to be able to apply the basic theoretical frameworks and methods of cognitive linguistics to the analysis of authentic data from different languages and cultures.</p>
<p><strong><em>*II. CLASS SCHEDULE*</em></strong></p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th><strong><em>*Week*</em></strong></th>
<th><strong><em>*Date*</em></strong></th>
<th><strong><em>*Content*</em></strong></th>
<th><strong><em>*Assignment*</em></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Mar. 4</td>
<td>An overview of cognitive science</td>
<td>Assigned reading 1: <strong>Metaphors We Live By</strong></td>
</tr>
<tr class="even">
<td>2</td>
<td>Mar. 11</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Mar. 18</td>
<td>The theory of prototype and basic-level categories</td>
<td>Assigned reading 2:<strong>Women, Fire and Dangerous Things: What Categories Reveal about the Mind</strong></td>
</tr>
<tr class="even">
<td>4</td>
<td>Mar. 25</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>April 1</td>
<td>Objectivism, subjectivism and embodied philosophy</td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>April 8</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>April 15</td>
<td>The prominence view and the attentional view of cognitive linguistics</td>
<td>Assigned reading 3:<strong>Cognitive Poetics: An Introduction</strong></td>
</tr>
<tr class="even">
<td>8</td>
<td>April 22</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>9</td>
<td>April 29</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>10</td>
<td>May 6</td>
<td>Conceptual metaphor and conceptual metonymy</td>
<td></td>
</tr>
<tr class="odd">
<td>11</td>
<td>May 13</td>
<td>Mini-research projects on assigned topics</td>
<td></td>
</tr>
<tr class="even">
<td>12</td>
<td>May 20</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>13</td>
<td>May 27</td>
<td>Workshop</td>
<td></td>
</tr>
<tr class="even">
<td>14</td>
<td>June 3</td>
<td>Cognitive grammar and cognitive semantics</td>
<td>Preparing for final exam/term paper</td>
</tr>
<tr class="odd">
<td>15</td>
<td>June 10</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>16</td>
<td>June 17</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>17</td>
<td>Exam Week</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>18</td>
<td>Exam Week</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong><em>*III. COURSE REQUIREMENTS*</em></strong></p>
<p>Students are expected to attend all the lectures, finish all the assigned reading and written work properly, contribute actively to both in-class and outside-class group discussions and team work, and take the final exam / submit a term paper at the end of the course.</p>
<p><strong><em>*IV. TEACHING APPROACH*</em></strong></p>
<p>This is a lecture-based course. The instructor will prepare detailed handouts and will use PPT to assist her teaching. Group discussions will be organized in class. Students are also expected to carry out mini research projects in small groups on assigned topics.</p>
<p><strong><em>*V. COURSE MATERIALS*</em></strong></p>
<p><strong><em>*1. Required Materials (or Readings)*</em></strong></p>
<p>蓝纯 2005 《认知语言学与隐喻研究》 北京：外语教学与研究出版社。</p>
<p>Evans, V. &amp; M. Green. 2006. <strong>Cognitive Linguistics: An Introduction</strong>. Berlin: Lawrence Erlbaum Associates Publishers.</p>
<p>Ungerer, F., &amp; Schmid, H.-J. 2006. <strong>An Introduction to Cognitive Linguistics</strong>. Beijing: FLTRP.</p>
<p><strong><em>*2. Recommended Materials (or Readings)*</em></strong></p>
<p>Croft, William and Alan Cruse. 2004. <strong>Cognitive Linguistics</strong>. Cambridge: Cambridge University Press.</p>
<p>Evans, Vyvyan, Benjamin K. Bergen and Jörg Zinken (eds.). 2007. <strong>The Cognitive Linguistic Reader</strong>. London: Equinox Publishing Company.</p>
<p>Lakoff, George, &amp; Johnson, Mark, 1980. <strong>Metaphors We Live By</strong>. Chicago: University of Chicago Press.</p>
<p>Johnson, Mark. 1987. <strong>The Body in the Mind</strong>. Chicago: University of Chicago Press.</p>
<p>Lakoff, George. 1987. <strong>Women, Fire, and Dangerous Things</strong>. Chicago: University of Chicago Press.</p>
<p>Langacker, Ronald W. 1987. <strong>Foundations of Cognitive Grammar: Theoretical Prerequisites</strong>. Stanford: Stanford University Press.</p>
<p>Langacker, Ronald W. 1991. <strong>Foundations of Cognitive Grammar: Descriptive Application</strong>. Stanford: Stanford University Press.</p>
<p>Lefrancois, G. R. 2004. <strong>Theories of Human Learning</strong>. Beijing: FLTRP.</p>
<p>Stockwell, Peter. 2002. <strong>Cognitive Poetics: An Introduction</strong>. London/New York: Routledge.</p>
<p>Talmy, Leonard. 2000. <strong>Toward a Cognitive Semantics</strong>. New York: MIT Press.</p>
<p><strong><em>*VI. ASSESSMENT*</em></strong></p>
<p>Students’ final score will consist of the following parts:</p>
<p><strong>1.</strong> <strong><em>*Post-class Work/Quiz/*</em></strong> <strong><em>*Assignments/*</em><em>*…*</em><em>*: 30%*</em></strong></p>
<p><strong>2.</strong> <strong><em>*C*</em><em>*lass*</em></strong> <strong><em>*Participation*</em></strong> <strong><em>*and Attendance: 20%*</em></strong></p>
<p>(<strong><em>*Absence of one third of class time*</em></strong>, excused or unexcused, will disqualify you from earning <strong><em>*credits for the course*</em></strong>. Showing up late for class or leaving before class is dismissed will also be penalized. <strong><em>*Two such records*</em></strong> will take one point off from your final marks.)</p>
<p><strong>3.</strong> <strong><em>*Final Exam and/or Term Paper: 50%*</em></strong></p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>认知语言学</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
      </tags>
  </entry>
  <entry>
    <title>语料库资源大全</title>
    <url>/corpus/</url>
    <content><![CDATA[<h1 id="现存语料库">现存语料库</h1>
<h2 id="词语">词语</h2>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">条目</th>
<th style="text-align: left;">长度</th>
<th style="text-align: left;">数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">汉字</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">16142</td>
</tr>
<tr class="even">
<td style="text-align: left;">词语</td>
<td style="text-align: left;">2~3</td>
<td style="text-align: left;">264434</td>
</tr>
<tr class="odd">
<td style="text-align: left;">成语</td>
<td style="text-align: left;">4~5</td>
<td style="text-align: left;">31648</td>
</tr>
<tr class="even">
<td style="text-align: left;">歇后语</td>
<td style="text-align: left;">6+</td>
<td style="text-align: left;">14032</td>
</tr>
</tbody>
</table>
<p>新华网成语、歇后语和词语：https://github.com/pwxcoo/chinese-xinhua</p>
<p>查询接口：https://github.com/netnr/zidian</p>
<p>近反义词：https://github.com/guotong1988/chinese_dictionary</p>
<span id="more"></span>
<h2 id="句子">句子</h2>
<p>微博短句、句子迷</p>
<h3 id="idea">idea</h3>
<p>除了欣赏，也可以收集美句美文，并表达不同心情和志趣。</p>
<h2 id="文章">文章</h2>
<p>新闻：https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset</p>
<p>微信公众号语料库：https://github.com/nonamestreet/weixin_public_corpus</p>
<p>每行文章，是JSON格式，名称是微信公众号名字，帐户是微信公众号ID，标题是译文，内容是正文。</p>
<h3 id="idea-1">idea</h3>
<p>文本分类！主题建模！可以按照风格分类！</p>
<h2 id="名字">名字</h2>
<p>豆瓣影视和书籍的名字</p>
<p>微信、知乎文章标题</p>
<p>书名号</p>
<h3 id="idea-2">idea</h3>
<p>抓取所有名字，动态展现，且按照不同风格分类！就叫标题网！</p>
<p>功能：帮助大家取名、让标题更吸引人、更有文采！并研究怎样的文章更吸引读者。</p>
<h2 id="诗词">诗词</h2>
<p>中华古诗词数据库：</p>
<p>https://github.com/chinese-poetry/chinese-poetry</p>
<h2 id="维基百科">维基百科</h2>
<p>信息检索？</p>
<h2 id="学术语料">学术语料</h2>
<p>信息检索？</p>
<h1 id="抓取原则">抓取原则</h1>
<h2 id="频率tf-idf">频率tf-idf</h2>
<h2 id="打分">打分</h2>
<ul>
<li>通过文章和书籍的阅读量和点赞</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语料库</category>
      </categories>
      <tags>
        <tag>corpus</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机公开课目录</title>
    <url>/courses/</url>
    <content><![CDATA[<p>转：这是一份公开课的目录，这里的视频大多来自 YouTube 等国内无法访问的网站，为了方便国内的朋友观看，我将这些视频搬运到了 <a href="https://space.bilibili.com/12721139">Bilibili</a>。</p>
<span id="more"></span>
<h2 id="其他目录说明">其他目录说明</h2>
<ol type="1">
<li><p><a href="https://github.com/wenhan-wu/OpenCourseCatalog/blob/master/台湾等地区公开课目录.md">台湾等地区公开课目录</a></p>
<p>由于某些你懂的原因，台湾地区公开课不易过审，我将这些课程单独列出。</p></li>
<li><p><a href="https://github.com/wenhan-wu/OpenCourseCatalog/blob/master/edX公开课目录.md">edX公开课目录</a></p>
<p>搬运自 edX</p></li>
</ol>
<h2 id="数学-math">数学 / Math</h2>
<p><a href="https://www.bilibili.com/video/av52983739/">新南威尔士大学 数学史 (UNSW, Math History: A course in the History of Mathematics)</a></p>
<h3 id="数学分析-mathematical-analysis"><a href="https://github.com/wenhan-wu/OpenCourseCatalog/blob/master/台湾等地区公开课目录.md#数学分析--mathematical-analysis">数学分析 / Mathematical Analysis</a></h3>
<h3 id="微积分-calculus">微积分 / Calculus</h3>
<p><a href="https://www.bilibili.com/video/av34481507">麻省理工 单变量微积分 (MIT 18.01, Single Variable Calculus, David Jerison)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av34563978">麻省理工 多变量微积分 (MIT 18.02, Multivariable Calculus, Denis Auroux)【英】</a></p>
<h3 id="分析-analysis">分析 / Analysis</h3>
<p><a href="http://www.bilibili.com/video/av36435282">麻省理工 微分方程 (18.03, Differential Equations, Haynes R. Miller, Sp 2006)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av38525832">国际理论物理中心 偏微分方程 (ICTP MTH-PDE, Partial Differential Equations, 2012-2013)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av38112145">斯坦福 傅立叶变换及其应用 (Stanford EE 261, The Fourier Transforms and its Applications)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av38166490">国际理论物理中心 实分析(2018) (ICTP MTH-RA, Real Analysis)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av38415812">国际理论物理中心 复分析 (ICTP MTH-CA, Complex Analysis, 2013-2014)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av56957518/">ICTP 泛函分析 (ICTP MTH-FA 2018, Functional Analysis)</a></p>
<p><a href="https://www.bilibili.com/video/av39098628/">斯克利普斯学院 泛函分析与调和分析 (Scripps College, Basic Functional and Harmonic Analysis)【无字幕】</a></p>
<h3 id="代数-algebra">代数 / Algebra</h3>
<p><a href="https://www.bilibili.com/video/av34573725">麻省理工 线性代数 (MIT 18.06, Linear Algebra, Gilbert Strang)【中英】</a></p>
<p><a href="https://www.bilibili.com/video/av38339505">国际理论物理中心 抽象代数 (ICTP MTH-AA, Abstract Algebra, 2013-2014)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av53067875/">美国国家数学科学研究所 交换代数入门 (MSRI W-196, Introductory Workshop in Commutative Algebra)</a></p>
<p><a href="https://www.bilibili.com/video/av56611018/">国际理论物理中心 表示论 (ICTP MTH-RT 2018, Representation Theory)</a></p>
<p><a href="https://www.bilibili.com/video/av38913620/">国际理论物理中心 李群与李代数 (ICTP HEP-LL, Lie Groups and Lie Algebras, 2018-2019)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av39162675/">切比雪夫实验室 连通簇上有理点的算术论题(Chebyshev Lab, Arithmetic on rationally connected varieties)【无字幕】</a></p>
<p><a href="https://www.bilibili.com/video/av41015190/">傅立叶研究所 Arakelov几何和丢番图应用 (Géométrie d'Arakelov et applications diophantiennes)【无】</a></p>
<p><a href="https://www.bilibili.com/video/av50893071/">哥伦比亚大学 表示论与数论 (Columbia University, Representation theory and number theory)</a></p>
<p><a href="https://www.bilibili.com/video/av53029148/">印度马德拉斯理工学院 黎曼曲面和代数曲线入门 一维复环面和椭圆曲线 (NPTEL, An Introduction to Riemann Surfaces and Algebraic Curves/ Complex 1-Tori and Elliptic Curves)</a></p>
<p><a href="https://www.bilibili.com/video/av57111511/">西江大学 代数曲面 (Sogang, Lectures on Algebraic Surfaces)</a></p>
<p><a href="https://www.bilibili.com/video/av57696030/">BIRS 代数栈：进度与展望 (BIRS 12w5027, Algebraic Stacks: Progress and Prospects)</a></p>
<p><a href="https://www.bilibili.com/video/av53120240/">美国国家数学科学研究所 代数栈上的相交理论 (MSRI W-138, Intersection Theory on Stacks)</a></p>
<p><a href="https://www.bilibili.com/video/av57864978/">IHES 代数分析 (IHES 1914, Algebraic Analysis in honor of Masaki Kashiwara)</a></p>
<h3 id="拓扑-topology">拓扑 / Topology</h3>
<p><a href="https://www.bilibili.com/video/av51368305/">伊利诺伊大学 同伦论：工具与应用 (Illinois, Homotopy: Theory tools and applications)</a></p>
<p>[<a href="https://www.bilibili.com/video/av82719978/">UChicago] Second Chicago Summer School in Geometry and Topology</a></p>
<p>[<a href="https://www.bilibili.com/video/av86880312/">NDGeoTop] Topics in Differential Topology</a></p>
<h3 id="几何-geometry">几何 / Geometry</h3>
<p><a href="https://www.bilibili.com/video/av56849404/">ICTP 微分几何 (ICTP MTH-DG 2018, Differential Geometry)</a></p>
<p><a href="https://www.bilibili.com/video/av38085326">国际理论物理中心 微分几何 (ICTP MTH-DG, Differential Geometry)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av39041115/">克雷数学研究所 算术几何 (CMI, Arithmetic geometry)【无字幕】</a></p>
<p><a href="https://www.bilibili.com/video/av56182412/">巴黎数学科学基金会 双有理几何 (Sciences Maths Paris, Birational Geometry)</a></p>
<p>[<a href="https://www.bilibili.com/video/av84401726/">IHES] 2015 Summer School on Moduli Problems in Symplectic Geometry</a></p>
<h3 id="数论-number-theory">数论 / Number Theory</h3>
<p><a href="https://www.bilibili.com/video/av51105699/">加利福尼亚大学圣迭戈分校 数论 (UCSD Math 205, Topics in Number Theory)</a></p>
<p><a href="https://www.bilibili.com/video/av39795355/">代数数论 (Algebraic Number Theory)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av47039642">解析数论 (Hindi Urdu MTH435, Analytic Number Theory in Hindi Urdu)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av47425317">法国高等科学研究所 Faltings Heights and L-series (IHES)</a></p>
<p><a href="https://www.bilibili.com/video/av47569089">法国高等科学研究所 Indsheaves, temperate holomorphic functions and irregular RH correspondence (IHES)</a></p>
<p><a href="https://www.bilibili.com/video/av47821521">普林斯顿高等研究院 岩泽理论 (IAS, Iwasawa Theory)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av48796466">霍奇理论 (Лаборатория Зеркальной симметрии, Hodge Theory: Old and New)</a></p>
<p><a href="https://www.bilibili.com/video/av48932077">2013 Sep. 25-27 ANRThéorie计划中期会议 p-adic Hodge Theory and developments</a></p>
<p><a href="https://www.bilibili.com/video/av51899830/">国际理论物理中心 神奇的模形式 (ICTP, The Magic of Modular Forms)</a></p>
<h3 id="概率论-probability-theory">概率论 / Probability Theory</h3>
<p><a href="https://www.bilibili.com/video/av34761771">麻省理工 概率分析及应用 (MIT 6.041,Probability Systems Analysis and Applied Probability)【英】</a></p>
<h3 id="动力系统-dynamical-systems">动力系统 / Dynamical Systems</h3>
<p><a href="https://www.bilibili.com/video/av38365622">国际理论物理中心 动力系统 (ICTP MTH-ODE, Dynamical Systems, 2013-2014)【英】</a></p>
<p><a href="http://www.bilibili.com/video/av38374012">斯坦福 线性动力系统入门 (Stanford EE263, Introduction to Linear Dynamical Systems)【英】</a></p>
<h3 id="代数几何-algebaric-geometry">代数几何 / Algebaric Geometry</h3>
<p><a href="https://www.bilibili.com/video/av56806940/">ICTP 代数几何 (ICTP MTH-AG 2018, Algebraic Geometry)</a></p>
<p><a href="https://www.bilibili.com/video/av38456981">国际理论物理中心 代数几何 (ICTP MTH-AG, Algebraic Geometry, 2013-2014)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av51687046/">哈佛大学数学科学与应用中心 导出代数几何 (Harvard CMSA, Derived Algebraic/Differential Geo)</a></p>
<p><a href="https://www.bilibili.com/video/av58262354/">MSRI 导出代数几何、双有理几何与模空间 (MSRI W-862, Introductory Workshop: Derived Algebraic Geometry and Birational Geometry and Moduli Spaces)</a></p>
<p><a href="https://www.bilibili.com/video/av51788414/">美国国家数学科学研究所 导出代数几何及其应用 (MSRI, Derived algebraic geometry and its applications)</a></p>
<p><a href="https://www.bilibili.com/video/av51914008/">美国国家数学科学研究所 p进数几何 (MSRI, p-adic Geometry)</a></p>
<p><a href="https://www.bilibili.com/video/av39156109/">美国国家数学科学研究所 非交换代数几何 (MSRI 648, Noncommutative Algebraic Geometry)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av57723097/">BIRS 代数几何中的Syzygies和与弦理论的联系 (BIRS 12w5117, Syzygies in Algebraic Geometry)</a></p>
<p><a href="https://www.bilibili.com/video/av57947306/">MSRI 现代模理论 (MSRI W-472, Modern Moduli Theory)</a></p>
<p><a href="https://www.bilibili.com/video/av57944105/">MSRI 代数几何中的形变理论与模空间 (MSRI S-419, Deformation Theory and Moduli in Algebaric Geometry)</a></p>
<h3 id="朗兰兹纲领-langlands-program">朗兰兹纲领 / Langlands Program</h3>
<p><a href="https://www.bilibili.com/video/av55649612/">MSRI 朗兰兹纲领的近期进展 (MSRI W-855, Recent progress in Langlands Program)</a></p>
<p><a href="https://www.bilibili.com/video/av55827530/">班夫国际研究所 稳定迹公式，自守形式和伽罗瓦表示 (BIRS, The stable trace formula, automorphic forms, and Galois representations)</a></p>
<p><a href="https://www.bilibili.com/video/av57651093/">IAS 走近几何朗兰兹纲领猜想的证明 (IAS, Towards the proof of the geometric Langlands)</a></p>
<p>[<a href="https://www.bilibili.com/video/av80863269/">IHES] Mathematical Aspects of Six-Dimensional Quantum Field Theories</a></p>
<h3 id="会议-meeting">会议 / Meeting</h3>
<p><a href="https://www.bilibili.com/video/av78861981/">ICM 2014: Plenary Lectures</a></p>
<h3 id="访谈-interviews">访谈 / Interviews</h3>
<p><a href="https://www.bilibili.com/video/av82225921/">The Abel Prize Interviews</a></p>
<h3 id="其他-other">其他 / Other</h3>
<p><a href="https://www.bilibili.com/video/BV1ct411i7j3/">斯坦福 凸优化 (Stanford EE364, Convex Optimization)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av37749598">麻省理工 工程数学方法 (MIT 18.086 Mathematical Methods for Engineers II)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av34841911">MIT 6.042J&amp;18.062J, Mathematics for Computer Science, Tom Leighton【英】</a></p>
<p><a href="http://www.bilibili.com/video/av36916092">耶鲁 博弈论 (ECON-159, Game Theory)</a></p>
<p>[<a href="https://www.bilibili.com/video/av85437699/">IHES] Summer School 2018: Supersymmetric Localization and Exact Results</a></p>
<h2 id="计算机-computer-science">计算机 / Computer Science</h2>
<h3 id="计算机历史-computer-history">计算机历史 / Computer History</h3>
<p>[<a href="https://www.bilibili.com/video/BV1r5411p7ex/">UWashington] 计算的历史 (CSE P 590A: History of Computing)</a></p>
<h3 id="计算机数学-computer-math">计算机数学 / Computer Math</h3>
<p><a href="https://www.bilibili.com/video/av34841911">MIT 6.042J&amp;18.062J, Mathematics for Computer Science, Tom Leighton</a></p>
<h4 id="范畴论-category-theory">范畴论 / Category Theory</h4>
<p>[<a href="https://www.bilibili.com/video/BV1TD4y1Q7r1">Bartosz Milewski] 面向程序员的范畴论 Part I (Category Theory Part I)</a></p>
<p>[<a href="https://www.bilibili.com/video/BV1EA411i7DT">Bartosz Milewski] 面向程序员的范畴论 Part II (Category Theory Part II)</a></p>
<p>[<a href="https://www.bilibili.com/video/BV19D4y1D7um">Bartosz Milewski] 面向程序员的范畴论 Part III (Category Theory Part III)</a></p>
<h3 id="基础课-introduction">基础课 / Introduction</h3>
<p>[**<a href="https://www.bilibili.com/video/BV1AA411t7Wk/">MIT] 6.001 计算机程序的构造与解释 (Structure and Interpretation, 1986)**</a></p>
<p><a href="https://www.bilibili.com/video/av39221579/">卡内基梅隆大学 计算机系统入门 (CMU 15-213, Introduction to Computer Systems)【英】</a></p>
<p><a href="http://www.bilibili.com/video/av37520581">麻省理工 计算机科学与编程导论 (6.00, Introduction to Computer Science and Programming)</a></p>
<p><a href="https://www.bilibili.com/video/av38057765">麻省理工 计算机科学与编程导论(Python) (6.00SC Introduction to Computer Science &amp; Programming)</a></p>
<p><a href="https://www.bilibili.com/video/av36204486">斯坦福 编程方法论 (CS106A, Programming Methodology, Sahami Mehran)</a></p>
<p><a href="https://www.bilibili.com/video/av36256674">斯坦福 编程方法论 (CS106A, Programming Methodology, Marty Stepp, Sp 2017)</a></p>
<p><a href="https://www.bilibili.com/video/av36310870">斯坦福 抽象编程 (CS106B, Programming Abstractions in C++, Zelenski Julie)</a></p>
<p><a href="https://www.bilibili.com/video/av36351060">斯坦福 抽象编程 (CS106B, Programming Abstractions in C++, Marty Stepp, Win 2018)</a></p>
<p><a href="https://www.bilibili.com/video/av36373995">斯坦福 编程范式 (CS107, Programming Paradigms, Cain Jerry)</a></p>
<h3 id="算法-数据结构-algorithm-data-structure">算法 &amp; 数据结构 / Algorithm &amp; Data Structure</h3>
<p><a href="https://www.bilibili.com/video/av34605246">麻省理工 算法入门 (MIT 6.006, Introduction to Algorithms, Peak Finding)</a></p>
<p><a href="https://www.bilibili.com/video/av34952152"><strong>麻省理工 算法设计与分析 (6.046J&amp;18.410J, Design and Analysis of Algorithms, C Leiserson)</strong></a></p>
<p><a href="https://www.bilibili.com/video/av34892415">麻省理工 算法设计与分析 (6.046J, Design and Analysis of Algorithms, 2015, Erik Demaine)</a></p>
<p><a href="https://www.bilibili.com/video/av35009511">麻省理工 高级数据结构 (6.851, Advanced Data Structures, Erik Demaine)</a></p>
<p><a href="https://www.bilibili.com/video/av35371440">麻省理工 高级算法 (6.854&amp;18.415, Advanced Algorithms, Ankur Moitra)</a></p>
<h3 id="计算机架构-computer-architecture">计算机架构 / Computer Architecture</h3>
<p><a href="https://www.bilibili.com/video/av37375242">麻省理工 计算结构 (MIT 6.004, Computation Structures, Chris Terman)</a></p>
<p><a href="https://www.bilibili.com/video/av47656678">伯克利 计算机结构与工程 (CS 152, Computer Architecture and Engineering)【英】</a></p>
<h3 id="操作系统-operating-system">操作系统 / Operating System</h3>
<p><a href="https://www.bilibili.com/video/av40296618/">伯克利 计算机操作系统 (CS 162, Operating Systems and Systems Programming)【英】</a></p>
<h3 id="编译原理-compiling-principle">编译原理 / Compiling Principle</h3>
<p><a href="https://www.bilibili.com/video/av40376453/">伯克利 编译原理 (CS 164, Programming Languages and Compilers)【无字幕】</a></p>
<h3 id="数据库-database">数据库 / Database</h3>
<p><a href="https://www.bilibili.com/video/av39731185/">卡内基梅隆 数据库导论 (CMU 15-445/645, Intro to Database Systems, Fall 2018)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av39753181/">卡内基梅隆 高级数据库 (CMU 15-721 Advanced Database Systems, Spring 2018)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av50204001/">卡内基梅隆 高级数据库 2019 (CMU 15-721 Advanced Database Systems, Spring 2019)【英】</a></p>
<p><a href="https://www.bilibili.com/video/av51476054/">卡内基梅隆 七周七数据库 (CMU, Seven Databases in Seven Weeks)</a></p>
<p><a href="https://www.bilibili.com/video/av51462138/">卡内基梅隆 数据库学 (CMU, The Databaseology)</a></p>
<p><a href="https://www.bilibili.com/video/av51118320/">卡内基梅隆 时间序列数据库 (CMU, Time Series Database)</a></p>
<p><a href="https://www.bilibili.com/video/av51652730/">卡内基梅隆 硬件加速数据库 (CMU, Hardware Accelerated Database)</a></p>
<h3 id="计算机网络-network">计算机网络 / Network</h3>
<p><a href="https://www.bilibili.com/video/av37715284">麻省理工 数字通信系统 (6.02, Digital Communication Systems)</a></p>
<p><a href="https://www.bilibili.com/video/av38761670/">麻省理工 数字通信原理 I (MIT 6.450, Principles of Digital Communications I)</a></p>
<p><a href="https://www.bilibili.com/video/av38670528">麻省理工 数字通信原理 II (MIT 6.451, Principles of Digital Communications II)</a></p>
<h3 id="并行计算-分布式系统-parallel-computing-distributed-system">并行计算 &amp; 分布式系统 / Parallel Computing &amp; Distributed System</h3>
<p><a href="https://www.bilibili.com/video/av37937571">麻省理工 多核编程 (MIT 6.189 Multicore Programming Primer)</a></p>
<p><a href="https://www.bilibili.com/video/av38073607">麻省理工 分布式系统 (MIT 6.824, Distributed Systems)</a></p>
<h3 id="计算机安全-security">计算机安全 / Security</h3>
<p>[<a href="https://www.bilibili.com/video/BV19741117Ng/">Stanford] 网络安全 (CS 253 Web Security)</a></p>
<p><a href="https://www.bilibili.com/video/av58961353/">MIT 密码学 (MIT 6.875, Cryptography Sp 2018)</a></p>
<h3 id="其他-other-1">其他 / Other</h3>
<p><a href="https://www.bilibili.com/video/av36929946">麻省理工 计算机系统工程 (6.033, Computer System Engineering, Sp 2005)</a></p>
<p><a href="https://www.bilibili.com/video/av37833446">麻省理工 软件系统性能工程 (MIT 6.172, Performance Engineering of Software Systems)</a></p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>10000篇计算语言学文献语料库搭建</title>
    <url>/dataset/</url>
    <content><![CDATA[<p>从ACL anthology（bibtex）下载并处理为json文件，使用bibtexparser和并行运算。</p>
<span id="more"></span>
<h1 id="最终版">最终版</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bibtexparser</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time, json</span><br><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocessing</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&#x27;C:\Users\13607\Downloads\anthology+abstracts.bib\anthology+abstracts.bib&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> bibtex_file:</span><br><span class="line">    bibtex = bibtex_file.read()</span><br><span class="line">bibtex = re.sub(<span class="string">r&#x27;@proceedings&#123;[^&#123;&#125;]+&#125;&#x27;</span>, <span class="string">&#x27;&#x27;</span>, bibtex)</span><br><span class="line">bibtex = re.sub(<span class="string">r&#x27;(month|pages|address) = .*\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>, bibtex)</span><br><span class="line">bibtex_list = re.findall(<span class="string">r&#x27;@inproceedings&#123;[^&#123;&#125;]+&#125;&#x27;</span>, bibtex)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">bibtex_str</span>):</span></span><br><span class="line">    bib_database = bibtexparser.loads(bibtex_str)</span><br><span class="line">    <span class="keyword">return</span> bib_database.entries[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    papers_json = []</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> executor.<span class="built_in">map</span>(parse, bibtex_list):</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;abstract&#x27;</span> <span class="keyword">in</span> item:</span><br><span class="line">                papers_json.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;../result/papers.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">        json.dump(papers_json, write_file, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    t2 = time.perf_counter()</span><br><span class="line">    print(<span class="string">f&#x27;Finished in <span class="subst">&#123;t2-t1&#125;</span> seconds&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="历史版本">历史版本</h1>
<p>有关multiprocessing的代码样例，参考youtube和莫凡。</p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bibtexparser</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time, os, json</span><br><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocessing</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&#x27;C:\Users\13607\Downloads\anthology+abstracts.bib\anthology+abstracts.bib&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> bibtex_file:</span><br><span class="line">    bibtex = bibtex_file.read()</span><br><span class="line">bibtex = re.sub(<span class="string">r&#x27;@proceedings&#123;[^&#123;&#125;]+&#125;&#x27;</span>, <span class="string">&#x27;&#x27;</span>, bibtex)</span><br><span class="line">bibtex = re.sub(<span class="string">r&#x27;(month|pages|address) = .*\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>, bibtex)</span><br><span class="line">bibtex_list = re.findall(<span class="string">r&#x27;@inproceedings&#123;[^&#123;&#125;]+&#125;&#x27;</span>, bibtex)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">bibtex_str, i</span>):</span></span><br><span class="line">    fname = <span class="built_in">str</span>(i)+<span class="string">&quot;.json&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(fname):</span><br><span class="line">        bib_database = bibtexparser.loads(bibtex_str)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">            json.dump(bib_database.entries[<span class="number">0</span>], write_file, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">        executor.<span class="built_in">map</span>(parse, bibtex_list[:<span class="number">10000</span>], <span class="built_in">range</span>(<span class="built_in">len</span>(bibtex_list[:<span class="number">10000</span>])))</span><br><span class="line"></span><br><span class="line">    t2 = time.perf_counter()</span><br><span class="line">    print(<span class="string">f&#x27;Finished in <span class="subst">&#123;t2-t1&#125;</span> seconds&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>一万个json文件，Finished in 65.8221881 seconds，速度很快。本来想把所有bibtex_str生成的字典保存在一个list，再dump到json中，但是似乎executor.map()不能改变或存储任何全局变量，只能进行文件操作、打印等等，所以改成了分开保存json。</p>
<p>当我们需要观察语料时，我们就加载前1000篇文章的title和abstract：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">file_list = os.listdir()</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">abstracts = <span class="string">&#x27;&#x27;</span></span><br><span class="line">titles = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> one_file <span class="keyword">in</span> file_list[:<span class="number">1000</span>]:</span><br><span class="line">    <span class="keyword">if</span> one_file[-<span class="number">4</span>:] == <span class="string">&#x27;json&#x27;</span>:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(one_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">            data = json.load(read_file)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                abstracts += data[<span class="string">&#x27;abstract&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">                titles += data[<span class="string">&#x27;title&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(one_file + <span class="string">&#x27;: do not have abstract&#x27;</span>)</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">print(<span class="built_in">str</span>(i) + <span class="string">&#x27; files skipped&#x27;</span>)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;titles.txt&#x27;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(titles)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;abstracts.txt&#x27;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(abstracts)</span><br></pre></td></tr></table></figure>
<p>由于25篇文章没有abstract所以只剩975篇。</p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>《自然语言处理综论》第14章-依存分析（上）</title>
    <url>/dependency-parsing-1/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/14.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<p>前两章的重点是<strong>上下文无关语法</strong>（context-free grammars）<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>，及其用于自动生成基于成分的表征。 我们在这里介绍另一种称为<strong>依存语法</strong>（dependency grammars）的语法形式主义派系，它在当代语音和语言处理系统中极其重要。 在这些形式主义中，短语成分（phrasal constituents）和短语结构规则（phrase-structure rules）并不直接发挥作用。相反，一个句子的句法结构，仅根据句中单词（或词元lemmas）以及单词间存在的一组关联的有向二元语法关系来描述。</p>
下图展示了标准的依存分析风格的句法图示。
<center>
<img src="https://i.loli.net/2021/03/17/3XaE6umlQIgpZrs.png"  alt="" width="700" />
</center>
<p>在句子上方，用从<strong>头部</strong>（heads）到<strong>依存项</strong>（dependents）的有向的标记弧来表示单词之间的关系。我们称之为<strong>类型依存结构</strong>（typed dependency structure），因为标签是从固定的语法关系清单中提取的。它还包括一个根（root）节点，显式地标记句法树的根，即整个结构的中心。</p>
<span id="more"></span>
<p>图14.1显示了与第12章中给出的相应短语结构分析相同的依存分析及树形结构。注意依存分析中没有对应短语成分或词汇类别的节点；<strong>其内部结构仅由句子中词汇项之间的定向关系组成。</strong>这些关系<strong>直接编码重要信息</strong>，这些信息往往隐藏在更复杂的短语结构分析中。例如，动词prefer的<strong>论元</strong>（arguments）在依存结构中直接链接到它，而在短语结构树中它们与主动词的连接不那么紧密。类似地，flight的修饰语morning和Denver在依存结构中直接与之链接。</p>
<center>
<img src="https://i.loli.net/2021/03/17/c1yRPAQCefvESZW.png"  alt="" width="700" />
</center>
<p><strong>依存语法的一个主要优势是能够处理形态丰富、词序相对自由（free word order）的语言。</strong>例如，捷克语的词序可能比英语灵活得多；宾语可能出现在位置状语之前或之后。短语结构语法会需要为解析树中每个可能出现这样一个状语短语的位置单独制定一条规则。基于依存关系的方法只用一种连接类型来表示这种特殊的状语关系。因此，依存分析的方法抽象出了词序信息，只表示解析所需的信息。</p>
<p>使用依存分析的另一个实际性的动机是，<strong>头部-依存（head-dependent）关系</strong>提供了一种近似于谓词及其论元之间的语义关系，这使得它们对指代消歧、自动问答和信息提取之类的许多应用都能产生直接的帮助。基于成分（constituent-based）的语法解析也提供了类似的信息，但通常必须通过诸如第12章讨论的中心语规则等技术从树中提炼出来。</p>
<p>在下面的章节，我们将更详细地讨论依存分析中使用的关系清单，以及这些依存结构的形式基础。然后我们将继续讨论用于自动生成这些结构的主流算法派系。最后，我们将讨论如何评估依存分析器，并指出它们在语言处理中应用的一些方式。</p>
<h1 id="依存关系">14.1 依存关系</h1>
<p>传统语言学的语法关系概念为构成这些依存结构的二元语法关系提供了基础。这些头关系（head relations）的参数由一个<strong>头部</strong>（heads）和一个<strong>依存项</strong>（dependents）组成。在第12章和附录C中，我们已经在成分结构的语境下中讨论过头部的依存项这个概念。在那里，一个成分的头部是一个更大成分的中心组织词（例如名词短语中的关键名词，或动词短语中的动词）。成分中其余的词都是其头部的直接或间接的依存项。在基于依存关系的方法中，通过直接将头部与紧靠头部的词连接起来，绕过成分结构，使头部-依存关系变得明确。</p>
<p>除了指定头部-依存对，依存语法还允许我们根据依存项相对于头部的作用，进一步划分语法关系种类或<strong>语法功能</strong>（grammatical function）。我们熟悉的主语、直接宾语和间接宾语等概念都是可能会想到的关系种类。在英语中，这些概念虽然与一个词在句中的位置和成分类型密切相关，但不起决定性作用，因此与短语结构树中提供的信息重复累赘。然而，在更灵活的语言中，直接编码这些语法关系中的信息是至关重要的，因为基于短语的成分句法提供的帮助很小。</p>
<p>毫不奇怪，语言学家们已经发明了远远超出我们熟悉的主语和宾语概念的关系分类学。虽然不同的理论之间大相径庭，但有足够的共性使其发展出一个在计算上有用的标准。<strong>通用依存关系</strong>（Universal Dependencies）项目（Nivre et al.，2016）提供了一个语言驱动的、利于计算的、跨语言适用的依存关系清单。</p>
<table>
<caption>图14.2 通用依存关系集中的部分依存关系 (de Marneffe et al., 2014)</caption>
<thead>
<tr class="header">
<th>Clausal Argument Relations</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NSUBJ</td>
<td>Nominal subject</td>
</tr>
<tr class="even">
<td>DOBJ</td>
<td>Direct object</td>
</tr>
<tr class="odd">
<td>IOBJ</td>
<td>Indirect object</td>
</tr>
<tr class="even">
<td>CCOMP</td>
<td>Clausal complement</td>
</tr>
<tr class="odd">
<td>XCOMP</td>
<td>Open clausal complement</td>
</tr>
<tr class="even">
<td><strong>Nominal Modifier Relations</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="odd">
<td>NMOD</td>
<td>Nominal modifier</td>
</tr>
<tr class="even">
<td>AMOD</td>
<td>Adjectival modifier</td>
</tr>
<tr class="odd">
<td>NUMMOD</td>
<td>Numeric modifier</td>
</tr>
<tr class="even">
<td>APPOS</td>
<td>Appositional modifier</td>
</tr>
<tr class="odd">
<td>DET</td>
<td>Determiner</td>
</tr>
<tr class="even">
<td>CASE</td>
<td>Prepositions, postpositions and other case markers</td>
</tr>
<tr class="odd">
<td><strong>Other Notable Relations</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td>CONJ</td>
<td>Conjunct</td>
</tr>
<tr class="odd">
<td>CC</td>
<td>Coordinating conjunction</td>
</tr>
</tbody>
</table>
<p>图14.2显示了这项工作中的关系子集。图 14.3 提供了一些例句来说明选定的关系。通用依存方案中所有关系的来由超出了本章的范围，但常用关系的核心集可以分成两组：描述与谓语（通常是动词）有关的句法角色的子句关系（clausal relations），以及对头部修饰词进行分类的修饰关系（modifier relations）。</p>
<table>
<caption>Figure 14.3 Examples of core Universal Dependency relations.</caption>
<thead>
<tr class="header">
<th>Relation</th>
<th>Examples with <em>head</em> and dependent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NSUBJ</td>
<td><strong>United</strong> <em>canceled</em> the flight.</td>
</tr>
<tr class="even">
<td>DOBJ</td>
<td>United <em>diverted</em> the <strong>flight</strong> to Reno.</td>
</tr>
<tr class="odd">
<td></td>
<td>We <em>booked</em> her the first <strong>flight</strong> to Miami.</td>
</tr>
<tr class="even">
<td>IOBJ</td>
<td>We <em>booked</em> <strong>her</strong> the flight to Miami.</td>
</tr>
<tr class="odd">
<td>NMOD</td>
<td>We took the <strong>morning</strong> <em>flight</em>.</td>
</tr>
<tr class="even">
<td>AMOD</td>
<td>Book the <strong>cheapest</strong> <em>flight</em>.</td>
</tr>
<tr class="odd">
<td>NUMMOD</td>
<td>Before the storm JetBlue canceled <strong>1000</strong> <em>flights</em>.</td>
</tr>
<tr class="even">
<td>APPOS</td>
<td><em>United</em>, a <strong>unit</strong> of UAL, matched the fares.</td>
</tr>
<tr class="odd">
<td>DET</td>
<td><strong>The</strong> <em>flight</em> was canceled.</td>
</tr>
<tr class="even">
<td></td>
<td><strong>Which</strong> <em>flight</em> was delayed?</td>
</tr>
<tr class="odd">
<td>CONJ</td>
<td>We <em>flew</em> to Denver and <strong>drove</strong> to Steamboat.</td>
</tr>
<tr class="even">
<td>CC</td>
<td>We flew to Denver <strong>and</strong> <em>drove</em> to Steamboat.</td>
</tr>
<tr class="odd">
<td>CASE</td>
<td>Book the flight <strong>through</strong> <em>Houston</em>.</td>
</tr>
</tbody>
</table>
<p>参考以下例句，子句关系NSUBJ和DOBJ分别表示主语和谓语cancel的直接宾语，而NMOD、DET和CASE关系表示名词flights和Houston的修饰语。</p>
<p><img src="https://i.loli.net/2021/03/17/XgwzFkLVSdaIfDj.png" /></p>
<h1 id="依存形式主义">14.2 依存形式主义</h1>
<p>在最普通的形式中，我们讨论的依存关系结构仅仅是有向图，即由一组顶点<span class="math inline">\(V\)</span>和一组有序的顶点对<span class="math inline">\(A\)</span>组成的结构<span class="math inline">\(G=(V, A)\)</span>，我们称之为弧（arcs）。</p>
<p>大多数情况下，我们假设顶点集<span class="math inline">\(V\)</span>完全对应于给定句子中单词的集合。然而，它们也可能对应于标点符号，或者当处理形态复杂的语言时，顶点集可能由词干和词缀组成。弧线集<span class="math inline">\(A\)</span>捕获了<span class="math inline">\(V\)</span>中元素之间的头部-依存关系和语法功能关系。</p>
<p>对这些依存结构的进一步限制是针对底层语法理论或形式主义的。其中比较常见的限制是，这些结构必须是连接的、有一个指定的根节点，并且是无环或平面的。与本章讨论的解析方法最相关的是对有根树的常见的、以计算为目的的限制。也就是说，<strong>依存树</strong>（dependency tree）是一个满足以下约束的有向图。</p>
<ol type="1">
<li>有一个指定的根结点，它没有传入弧。</li>
<li>除根节点外，每个顶点恰好有一个传出弧。</li>
<li>从根节点到<span class="math inline">\(V\)</span>中的每个顶点有一条唯一的路径。</li>
</ol>
<p>综上所述，这些约束条件保证了每个词都有一个头，依存结构是连接的，并且有唯一的根节点，从这个根节点可以沿着唯一的定向路径到句子中的每个词。</p>
<h2 id="投射性">14.2.1 投射性</h2>
<p><strong>投射性</strong>（projectivity）的概念施加了一个额外的约束条件，这个约束条件来自于输入（input）中词的顺序。如果在句子中<strong>存在一条从头部到位于头部和依存项之间的每个词的路径</strong>，那么就说这条从头部到依存项的弧线具有投射性。如果组成依存树的所有弧线都有投射性，那么就可以说它是投射的。到目前为止，我们看到的所有依存树都是投射的。然而，有许多完全合乎规则的结构会生成非投射树，特别是在词序相对灵活的语言中。</p>
<p>请看下面的例子。</p>
<p><img src="https://i.loli.net/2021/03/17/2Jicfx1AThkBFId.png" /></p>
<p>在这个例子中，从flight到它的修饰词was的弧线是非投射的，因为从flight到中间的单词this和morning没有路径。正如我们从这张图中看到的，投射性（和非投射性）可以通过画树的方式来检测。<strong>如果能画出没有交叉边的依存树，那么它就是投射性的。</strong>在这里，如果不跨越连接morning和它的头部的弧线，就无法将flight和它的依存项was联系起来。</p>
<p>我们对投射性的关注来自于两个相关的问题。首先，最广泛使用的英语依存关系树库是通过使用中心语查找规则从短语结构树库中自动导出的（第12章）。以这种方式生成的树是保证投射性的，因为它们是由上下文无关语法生成的。第二，最广泛使用的一系列解析算法存在计算上的限制。第14.4节中讨论的基于转换的方法只能生成投射树，因此任何具有非投射结构的句子都必然会出错。这个限制是第14.5节中描述的更灵活的基于图的解析方法的动机之一。</p>
<h1 id="依存树库">14.3 依存树库</h1>
<p>与基于成分的方法一样，树库（treebanks）在依存分析器（dependency parsers）的开发和评估中起着至关重要的作用。<strong>依存树库</strong>（dependency treebanks）的创建方法与第12章中讨论的方法类似——让人类标注者直接为给定的语料库生成依存结构，或者使用自动解析器（automatic parsers ）提供初始解析，然后让标注者手动修正这些解析器。我们也可以用一个确定性过程<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>（deterministic process）通过标注中心语规则将现有的基于成分的树库翻译成依存树。</p>
<p>大多形态丰富的语言（如捷克语、印地语和芬兰语）都已经建立了直接标注的依存树库用于依存分析，其中捷克语的Prague依存树库（Bejcek et al., 2013）是最著名的工作。主流英语依存树库主要是从现有资源中提取出来的，比如Penn树库的华尔街日报部分（Marcus等人，1993）。最近的OntoNotes项目（Hovy et al. 2006, Weischedel et al. 2011）扩展了这种方法，超越了传统的新闻文本，涵盖了英语、汉语和阿拉伯语的电话对话、网络日志、usenet新闻组、广播和脱口秀。</p>
<p>从成分结构到依存结构的翻译过程有两个子任务：识别结构中所有的头部-依存关系，以及正确识别这些关系的种类。第一个任务主要依赖于第12章中讨论的中心语规则（head rules）的使用，这些规则最早是为词汇化概率解析器（ lexicalized probabilistic parsers）而开发的(Magerman 1994, Collins 1999, Collins 2003)。下面是Xia和Palmer（2001）提出的一个简单有效的算法。</p>
<ol type="1">
<li>使用适当的中心语规则，标记短语结构中每个节点的头部子节点。</li>
<li>在依存结构中，让每个非头部子节点的头部依存于头部子节点的头部。</li>
</ol>
<p>当一个短语结构解析包含了额外的语法关系和函数标签形式的信息时，如在Penn Treebank的情况下，这些标签可以用来标记生成的树的边。当应用于图14.4中的解析树时，这种算法将产生例14.4中的依存结构。这些提取方法的主要缺点是它们受到原始结构树中存在的信息的限制。其中最重要的问题是未能将形态学信息与短语结构树整合在一起，不能轻易地表示非宾语结构，以及大多数名词短语缺乏内部结构，这反映在大多数树库语法通常所使用的平面规则中。由于这些原因，除了英语之外，大多数依存树库都是直接靠人类标注者开发的。</p>
<hr />
<p><strong>本章剩余内容见：<a href="http://nlpcourse.cn/dependency-parsing-2/">《自然语言处理综论》第14章-依存分析（中）</a></strong></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>也称为短语结构语法 (phrase-structure grammar)<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>译者注：对应<a href="https://baike.baidu.com/item/随机过程">随机过程（<em>Stochastic Process</em>）</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>dependency</tag>
      </tags>
  </entry>
  <entry>
    <title>英文依存句法分析</title>
    <url>/dependency-parsing/</url>
    <content><![CDATA[<blockquote>
<p>依存分析是根据句子中单词之间的依存关系来分析句子语法结构的过程。</p>
</blockquote>
<p>在依存分析中，各种标签代表了一个句子中两两词语之间的关系。例如，在'rainy weather'这个短语中，rainy是修饰名词weather的形容词。weather -&gt; rainy 形成了依存关系，其中weather是head（中心词），而rainy则是dependent（依赖）。该依存关系用amod标签表示，即形容词修饰语。我们可以用依存关系箭头标注语法关系： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     root</span><br><span class="line">      |</span><br><span class="line">      | +-------dobj---------+</span><br><span class="line">      | |                    |</span><br><span class="line">nsubj | |   +------det-----+ | +-----nmod------+</span><br><span class="line">+--+  | |   |              | | |               |</span><br><span class="line">|  |  | |   |      +-nmod-+| | |      +-case-+ |</span><br><span class="line">+  |  + |   +      +      || + |      +      | |</span><br><span class="line">I  prefer  the  morning   flight  through  Denver</span><br></pre></td></tr></table></figure></p>
<p>也可以表示为依存句法树（Dependency Tree Graph）：</p>
<p><img src="https://i.loli.net/2021/03/16/JtlAKsfoXhZwLjP.png" /></p>
<p>图片来源：<a href="https://zhuanlan.zhihu.com/p/66268929">CS224N笔记(五):Dependency Parsing</a></p>
<h1 id="通用依存关系">通用依存关系</h1>
<p>截至目前，UD项目的通用依存关系共有37个，这些关系的完整<a href="https://universaldependencies.org/u/dep/">列表</a>可以在这里查看并深入研究。</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Nominals</strong></th>
<th><strong>Clauses</strong></th>
<th><strong>Modifier words</strong></th>
<th><strong>Function Words</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Core arguments</strong></td>
<td><a href="https://universaldependencies.org/u/dep/nsubj.html">nsubj</a> <a href="https://universaldependencies.org/u/dep/obj.html">obj</a> <a href="https://universaldependencies.org/u/dep/iobj.html">iobj</a></td>
<td><a href="https://universaldependencies.org/u/dep/csubj.html">csubj</a> <a href="https://universaldependencies.org/u/dep/ccomp.html">ccomp</a> <a href="https://universaldependencies.org/u/dep/xcomp.html">xcomp</a></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Non-core dependents</strong></td>
<td><a href="https://universaldependencies.org/u/dep/obl.html">obl</a> <a href="https://universaldependencies.org/u/dep/vocative.html">vocative</a> <a href="https://universaldependencies.org/u/dep/expl.html">expl</a> <a href="https://universaldependencies.org/u/dep/dislocated.html">dislocated</a></td>
<td><a href="https://universaldependencies.org/u/dep/advcl.html">advcl</a></td>
<td><a href="https://universaldependencies.org/u/dep/advmod.html">advmod</a>* <a href="https://universaldependencies.org/u/dep/discourse.html">discourse</a></td>
<td><a href="https://universaldependencies.org/u/dep/aux_.html">aux</a> <a href="https://universaldependencies.org/u/dep/cop.html">cop</a> <a href="https://universaldependencies.org/u/dep/mark.html">mark</a></td>
</tr>
<tr class="odd">
<td><strong>Nominal dependents</strong></td>
<td><a href="https://universaldependencies.org/u/dep/nmod.html">nmod</a> <a href="https://universaldependencies.org/u/dep/appos.html">appos</a> <a href="https://universaldependencies.org/u/dep/nummod.html">nummod</a></td>
<td><a href="https://universaldependencies.org/u/dep/acl.html">acl</a></td>
<td><a href="https://universaldependencies.org/u/dep/amod.html">amod</a></td>
<td><a href="https://universaldependencies.org/u/dep/det.html">det</a> <a href="https://universaldependencies.org/u/dep/clf.html">clf</a> <a href="https://universaldependencies.org/u/dep/case.html">case</a></td>
</tr>
<tr class="even">
<td><strong>Coordination</strong></td>
<td><strong>MWE</strong></td>
<td><strong>Loose</strong></td>
<td><strong>Special</strong></td>
<td><strong>Other</strong></td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/dep/conj.html">conj</a> <a href="https://universaldependencies.org/u/dep/cc.html">cc</a></td>
<td><a href="https://universaldependencies.org/u/dep/fixed.html">fixed</a> <a href="https://universaldependencies.org/u/dep/flat.html">flat</a> <a href="https://universaldependencies.org/u/dep/compound.html">compound</a></td>
<td><a href="https://universaldependencies.org/u/dep/list.html">list</a> <a href="https://universaldependencies.org/u/dep/parataxis.html">parataxis</a></td>
<td><a href="https://universaldependencies.org/u/dep/orphan.html">orphan</a> <a href="https://universaldependencies.org/u/dep/goeswith.html">goeswith</a> <a href="https://universaldependencies.org/u/dep/reparandum.html">reparandum</a></td>
<td><a href="https://universaldependencies.org/u/dep/punct.html">punct</a> <a href="https://universaldependencies.org/u/dep/root.html">root</a> <a href="https://universaldependencies.org/u/dep/dep.html">dep</a></td>
</tr>
</tbody>
</table>
<p>此外还有一些基于特定语言的依存关系。斯坦福依存分析定义了接近50个依存关系，具体的定义可参考：<a href="https://nlp.stanford.edu/software/dependencies_manual.pdf">Stanford typed dependencies manual</a>。</p>
<p>关于依存句法分析，也可以参考Daniel Jurafsky的经典NLP书籍Speech and Language Processing相关<a href="https://web.stanford.edu/~jurafsky/slp3/14.pdf">章节</a>。</p>
<span id="more"></span>
<h1 id="工具推荐">工具推荐</h1>
<h2 id="stanford-parser句法分析">Stanford Parser句法分析</h2>
<p>比较有名的工具是Stanford Parser，我们可以在<a href="http://nlp.stanford.edu:8080/corenlp/">这里</a>在线使用并进行可视化。</p>
<p>或者安装斯坦福的Python NLP软件包<a href="https://stanfordnlp.github.io/stanza/installation_usage.html">Stanza</a>，里面集成了<a href="https://stanfordnlp.github.io/stanza/depparse.html">依存关系分析</a>工具。</p>
<p>如何解决download()下载异常的问题可以参考<a href="https://blog.csdn.net/superyangtze/article/details/105252193">这里</a>。</p>
<p>我们可以进行词性标注：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanza</span><br><span class="line"><span class="comment"># stanza.download(&#x27;en&#x27;)</span></span><br><span class="line">nlp = stanza.Pipeline(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">doc = nlp(<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> doc.sentences:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence.words:</span><br><span class="line">        print(word.text, word.lemma, word.pos)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">It it PRON</span></span><br><span class="line"><span class="string">took take VERB</span></span><br><span class="line"><span class="string">me I PRON</span></span><br><span class="line"><span class="string">more more ADJ</span></span><br><span class="line"><span class="string">than than ADP</span></span><br><span class="line"><span class="string">two two NUM</span></span><br><span class="line"><span class="string">hours hour NOUN</span></span><br><span class="line"><span class="string">to to PART</span></span><br><span class="line"><span class="string">translate translate VERB</span></span><br><span class="line"><span class="string">a a DET</span></span><br><span class="line"><span class="string">few few ADJ</span></span><br><span class="line"><span class="string">pages page NOUN</span></span><br><span class="line"><span class="string">of of ADP</span></span><br><span class="line"><span class="string">English English PROPN</span></span><br><span class="line"><span class="string">. . PUNCT</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>以及依存句法分析： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanza</span><br><span class="line"><span class="comment"># stanza.download(&#x27;en&#x27;)</span></span><br><span class="line">nlp = stanza.Pipeline(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">doc = nlp(<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span>)</span><br><span class="line">print(*[<span class="string">f&#x27;id: <span class="subst">&#123;word.<span class="built_in">id</span>&#125;</span>\tword: <span class="subst">&#123;word.text&#125;</span>\thead id: <span class="subst">&#123;word.head&#125;</span>\thead: <span class="subst">&#123;sent.words[word.head-<span class="number">1</span>].text <span class="keyword">if</span> word.head &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&quot;root&quot;</span>&#125;</span>\tdeprel: <span class="subst">&#123;word.deprel&#125;</span>&#x27;</span> <span class="keyword">for</span> sent <span class="keyword">in</span> doc.sentences <span class="keyword">for</span> word <span class="keyword">in</span> sent.words], sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="built_in">id</span>: <span class="number">1</span>	word: It	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: expl</span><br><span class="line"><span class="built_in">id</span>: <span class="number">2</span>	word: took	head <span class="built_in">id</span>: <span class="number">0</span>	head: root	deprel: root</span><br><span class="line"><span class="built_in">id</span>: <span class="number">3</span>	word: me	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: iobj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">4</span>	word: more	head <span class="built_in">id</span>: <span class="number">6</span>	head: two	deprel: advmod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">5</span>	word: than	head <span class="built_in">id</span>: <span class="number">4</span>	head: more	deprel: fixed</span><br><span class="line"><span class="built_in">id</span>: <span class="number">6</span>	word: two	head <span class="built_in">id</span>: <span class="number">7</span>	head: hours	deprel: nummod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">7</span>	word: hours	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: obj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">8</span>	word: to	head <span class="built_in">id</span>: <span class="number">9</span>	head: translate	deprel: mark</span><br><span class="line"><span class="built_in">id</span>: <span class="number">9</span>	word: translate	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: csubj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">10</span>	word: a	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: det</span><br><span class="line"><span class="built_in">id</span>: <span class="number">11</span>	word: few	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: amod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">12</span>	word: pages	head <span class="built_in">id</span>: <span class="number">9</span>	head: translate	deprel: obj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">13</span>	word: of	head <span class="built_in">id</span>: <span class="number">14</span>	head: English	deprel: case</span><br><span class="line"><span class="built_in">id</span>: <span class="number">14</span>	word: English	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: nmod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">15</span>	word: .	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: punct</span><br></pre></td></tr></table></figure> ## Spacy依存句法分析</p>
<p>我们也可以使用Spacy进行依存句法分析并画出句法树。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $python -m spacy download en_core_web_sm</span></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp=spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line">text=<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(text):</span><br><span class="line">	print(token.text,<span class="string">&#x27;=&gt;&#x27;</span>,token.dep_,<span class="string">&#x27;=&gt;&#x27;</span>,token.head.text)</span><br></pre></td></tr></table></figure>
<p>输出结果如下，对比stanza的结果，还是有明显差异，例如此处it被标记为<code>nsubj</code>，但其实这种情况下it是虚词expletive（我们都学过形式主语），不担任谓词的语义角色，因此应该标记为expl。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">It &#x3D;&gt; nsubj &#x3D;&gt; took</span><br><span class="line">took &#x3D;&gt; ROOT &#x3D;&gt; took</span><br><span class="line">me &#x3D;&gt; dative &#x3D;&gt; took</span><br><span class="line">more &#x3D;&gt; amod &#x3D;&gt; two</span><br><span class="line">than &#x3D;&gt; quantmod &#x3D;&gt; two</span><br><span class="line">two &#x3D;&gt; nummod &#x3D;&gt; hours</span><br><span class="line">hours &#x3D;&gt; dobj &#x3D;&gt; took</span><br><span class="line">to &#x3D;&gt; aux &#x3D;&gt; translate</span><br><span class="line">translate &#x3D;&gt; xcomp &#x3D;&gt; took</span><br><span class="line">a &#x3D;&gt; det &#x3D;&gt; pages</span><br><span class="line">few &#x3D;&gt; amod &#x3D;&gt; pages</span><br><span class="line">pages &#x3D;&gt; dobj &#x3D;&gt; translate</span><br><span class="line">of &#x3D;&gt; prep &#x3D;&gt; pages</span><br><span class="line">English &#x3D;&gt; pobj &#x3D;&gt; of</span><br><span class="line">. &#x3D;&gt; punct &#x3D;&gt; took</span><br></pre></td></tr></table></figure>
<p>画图的方法参考<a href="https://spacy.io/usage/visualizers">spacy可视化工具</a>，如果是在sublime text之类的编辑器内运行代码，我们需要打开浏览器地址http://localhost:5000/查看画图的结果。如果是jupyter notebook，则可以直接显示图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">doc = nlp(<span class="string">&quot;It took me more than two hours to translate a few pages of English.&quot;</span>)</span><br><span class="line">displacy.serve(doc, style=<span class="string">&quot;dep&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2021/03/16/dInSpW2aBw1Dg4u.png" /></p>
<p>spacy中的依存关系有45个，完整的标签如下，具体的定义可以参考：<a href="https://nlp.stanford.edu/software/dependencies_manual.pdf">Stanford typed dependencies manual</a>。</p>
<table>
<thead>
<tr class="header">
<th>标签</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ROOT</code></td>
<td>root</td>
</tr>
<tr class="even">
<td><code>acl</code></td>
<td>clausal modifier of noun (adjectival clause)</td>
</tr>
<tr class="odd">
<td><code>acomp</code></td>
<td>adjectival complement</td>
</tr>
<tr class="even">
<td><code>advcl</code></td>
<td>adverbial clause modifier</td>
</tr>
<tr class="odd">
<td><code>advmod</code></td>
<td>adverbial modifier</td>
</tr>
<tr class="even">
<td><code>agent</code></td>
<td>agent</td>
</tr>
<tr class="odd">
<td><code>amod</code></td>
<td>adjectival modifier</td>
</tr>
<tr class="even">
<td><code>appos</code></td>
<td>appositional modifier</td>
</tr>
<tr class="odd">
<td><code>attr</code></td>
<td>attribute</td>
</tr>
<tr class="even">
<td><code>aux</code></td>
<td>auxiliary</td>
</tr>
<tr class="odd">
<td><code>auxpass</code></td>
<td>auxiliary (passive)</td>
</tr>
<tr class="even">
<td><code>case</code></td>
<td>case marking</td>
</tr>
<tr class="odd">
<td><code>cc</code></td>
<td>coordinating conjunction</td>
</tr>
<tr class="even">
<td><code>ccomp</code></td>
<td>clausal complement</td>
</tr>
<tr class="odd">
<td><code>compound</code></td>
<td>compound</td>
</tr>
<tr class="even">
<td><code>conj</code></td>
<td>conjunct</td>
</tr>
<tr class="odd">
<td><code>csubj</code></td>
<td>clausal subject</td>
</tr>
<tr class="even">
<td><code>csubjpass</code></td>
<td>clausal subject (passive)</td>
</tr>
<tr class="odd">
<td><code>dative</code></td>
<td>dative</td>
</tr>
<tr class="even">
<td><code>dep</code></td>
<td>unclassified dependent</td>
</tr>
<tr class="odd">
<td><code>det</code></td>
<td>determiner</td>
</tr>
<tr class="even">
<td><code>dobj</code></td>
<td>direct object</td>
</tr>
<tr class="odd">
<td><code>expl</code></td>
<td>expletive</td>
</tr>
<tr class="even">
<td><code>intj</code></td>
<td>interjection</td>
</tr>
<tr class="odd">
<td><code>mark</code></td>
<td>marker</td>
</tr>
<tr class="even">
<td><code>meta</code></td>
<td>meta modifier</td>
</tr>
<tr class="odd">
<td><code>neg</code></td>
<td>negation modifier</td>
</tr>
<tr class="even">
<td><code>nmod</code></td>
<td>modifier of nominal</td>
</tr>
<tr class="odd">
<td><code>npadvmod</code></td>
<td>noun phrase as adverbial modifier</td>
</tr>
<tr class="even">
<td><code>nsubj</code></td>
<td>nominal subject</td>
</tr>
<tr class="odd">
<td><code>nsubjpass</code></td>
<td>nominal subject (passive)</td>
</tr>
<tr class="even">
<td><code>nummod</code></td>
<td>numeric modifier</td>
</tr>
<tr class="odd">
<td><code>oprd</code></td>
<td>object predicate</td>
</tr>
<tr class="even">
<td><code>parataxis</code></td>
<td>parataxis</td>
</tr>
<tr class="odd">
<td><code>pcomp</code></td>
<td>complement of preposition</td>
</tr>
<tr class="even">
<td><code>pobj</code></td>
<td>object of preposition</td>
</tr>
<tr class="odd">
<td><code>poss</code></td>
<td>possession modifier</td>
</tr>
<tr class="even">
<td><code>preconj</code></td>
<td>pre-correlative conjunction</td>
</tr>
<tr class="odd">
<td><code>predet</code></td>
<td>predeterminer</td>
</tr>
<tr class="even">
<td><code>prep</code></td>
<td>prepositional modifier</td>
</tr>
<tr class="odd">
<td><code>prt</code></td>
<td>particle</td>
</tr>
<tr class="even">
<td><code>punct</code></td>
<td>punctuation</td>
</tr>
<tr class="odd">
<td><code>quantmod</code></td>
<td>modifier of quantifier</td>
</tr>
<tr class="even">
<td><code>relcl</code></td>
<td>relative clause modifier</td>
</tr>
<tr class="odd">
<td><code>xcomp</code></td>
<td>open clausal complement</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>dependency</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第14章-依存分析（中）</title>
    <url>/dependency-parsing-2/</url>
    <content><![CDATA[<h1 id="基于转换的依存分析">14.4 基于转换的依存分析</h1>
<p>我们的第一个依存分析方法是由一种基于堆栈的方法启发的，这种方法被称为shift-reduce parsing，最初是为分析程序语言而开发的(Aho and Ullman, 1972)。这个经典的方法简单而优雅，采用了一个上下文无关语法、一个堆栈和一个待解析的标记列表。输入的标记被连续地移动到堆栈上，堆栈的前两个元素与语法中的右侧规则进行匹配；当发现匹配时，匹配的元素在堆栈上被匹配的规则左侧的非终端替换（还原）。在将这种方法改编为依存性解析时，我们放弃了对语法的明确使用，并改变了reduce操作，使其不是在解析树上添加一个非终端，而是引入了一个词与其头部之间的依存关系。更具体地说，reduce操作被两种可能的操作所取代：在堆栈顶部的词和它下面的词之间断言一个词头依存关系，或者反之。图14.5说明了这种解析器的基本操作。 配置 在基于过渡的解析中，一个关键的元素是配置的概念，它由一个堆栈、一个词或标记的输入缓冲区和一组代表依存树的关系组成。在这个框架下，解析过程由一个通过可能配置空间的过渡序列组成。这个过程的目标是找到一个最终的配置，在这个配置中，所有的词都已经被计算在内，并且已经合成了一个合适的依存树。 为了实现这样的搜索，我们将定义一组过渡运算符，当它们应用于一个配置时，会产生新的配置。考虑到这个设置，我们可以将解析器的操作看作是在配置空间中搜索从起始状态到目标状态的过渡序列。在这个过程的开始，我们创建一个初始配置，其中堆栈包含ROOT节点，t</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>dependency</tag>
      </tags>
  </entry>
  <entry>
    <title>心怀热爱的人，有梦去追</title>
    <url>/diary-1/</url>
    <content><![CDATA[<blockquote>
<p>上帝啊，请赐给我一个年轻人，他必须有足够的胆识去做别人心目中的傻事。</p>
<p>——罗勃特·路易斯·史蒂文生</p>
</blockquote>
<p>由于学业繁忙、生活也起起落落，鸽鸽许久未曾打理公众号，这个已经蒙尘的小角落。</p>
<p>从2019年毕业以来，很多朋友或许不曾知道，我留在母校继续攻读了英语学院语言学方向的硕士，并自学成为了程序媛。</p>
<p>转眼间，已是研三。读研的时光并不轻松，辗转于学校20门必修与选修课程之余，更多的时间是在自开小灶，夯实编程和数学基础。好在，我从两年前不知Python为何物，到现在神经网络炼丹，也算是小有成就。</p>
<p>但我始终是被边缘化的个体：女生，文科，英语，北外……这些字眼生生把我包裹成了异类，哪一边都不能融入。正是因为北外英语系的牌子足够响亮，才让我的转行更加不被理解。</p>
<p><em>耳边总会有反对的声音：</em></p>
<p><em>辛辛苦苦读了十几年英语，就这么不要了么？</em></p>
<p><em>你读这么多年书是为了什么？</em></p>
<p><em>英语系的也要来计算机专业分一杯羹？</em></p>
<p><em>二十多岁了，还折腾什么，安安稳稳找一份好工作不好吗？</em></p>
<p>尽管我的聪颖和能干被些许理工科和从事计算机的人赏识，尽管我知道自己代码能力并不差，但我始终缺乏自信和勇气去投递实习简历，因为我知道一旦HR看到北京外国语大学英语系的字眼，这封简历必定是石沉大海。市场本已卷到不行，谁有闲情关注你这个自学的文科生呢？碰壁、碰壁、接着碰壁！</p>
<p>其实面对这些阻碍，我并不气馁，因为我知道自己胸中有一股热血、一股汹涌澎湃的激情在燃烧，其实高中数学竞赛的功底还流淌在我的骨子里，当我看到数学公式、跳跃的数字、复杂的图表，我潜藏已久的对数学的热爱就喷发了出来；刷题，解决问题，那种滋味的酸爽；沉浸在代码的世界，自动化、机器运算的魅力让我着迷到无法入睡！</p>
<p>我成为了彻头彻尾的学渣，但我庆幸，当初做了对的选择。也正是因为自己是学渣，我才能如饥似渴地汲取知识、不在乎周围的眼光。我也成为了不修边幅、只穿运动装的攻城狮，最大的爱好就是抱着电脑敲敲敲。</p>
<p>Coursera上课、刷LeetCode、开发网站、写写爬虫和文本处理小工具、参加AI比赛，有些是为了训练能力，但更多是想利用编程这个工具，为个人和社会做些真正有用的事、实现价值，科技的力量真的所向披靡，深深感染和震撼了我，我找到了生命的意义。</p>
<p>由文转理，我重新活了一次，满含热泪、青春洋溢、眼里有光、胸中有希望。我不再是从前那个不知所措、随波逐流的自己，而是笃定前行、每天都有任务、有收获、有目标的逐梦者。</p>
<p>现在的我，不计得失，淬炼出一颗更纯粹、简单的求知之心。</p>
<p>这是最后一个学年，我硕士即将毕业，或许父母会劝我为了生计、找份工作。但是短短两年多的时间不足以让我成为功底扎实的计算机专家，在学习的道路上我还有太多路要走，太多坎坷要经历，我不知道未来会发生什么，但我满怀期待迎接所有的挑战。口号可以喊，但<strong>重在行动</strong>。</p>
<p>从今年9月起，硕士阶段还有整整一年的奋斗时光。我准备好了，你呢？</p>
<p>“其实，成功的真正意义就是找到一份你所热爱的工作并努力去做。”</p>
<p>谨以此文，致敬所有勇敢逐梦的朋友！因为热爱，无惧失败。</p>
<p>另外，抱歉各位关于英语的文章将一律停更，今后会不定期聊技术话题。感谢老朋友们曾经的支持与喜爱，有缘再会！</p>
]]></content>
      <tags>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title>文本编码格式大全</title>
    <url>/encoding/</url>
    <content><![CDATA[<p><strong>字符——（翻译过程）——二进制数字</strong></p>
<p>这个过程实际就是一个字符如何对应一个特定数字的标准，这个标准称之为字符编码。</p>
<h2 id="字符编码发展史与编码类型">字符编码发展史与编码类型</h2>
<h3 id="ascii-码美国标准信息交换码">ASCII 码（美国标准信息交换码）</h3>
<p>ASCII 码一共规定了128个字符的编码，比如空格SPACE是32（二进制00100000），大写的字母A是65（二进制01000001）。这128个符号（包括32个不能打印出来的控制符号），只占用了一个字节的后面7位，最前面的一位统一规定为0。</p>
<span id="more"></span>
<h3 id="非-ascii-编码">非 ASCII 编码</h3>
<p>英语用128个符号编码就够了，但是用来表示其他语言，128个符号是不够的。比如，在法语中，字母上方有注音符号，它就无法用 ASCII 码表示。于是，一些欧洲国家就决定，利用字节中闲置的最高位编入新的符号。比如，法语中的é的编码为130（二进制10000010）。这样一来，这些欧洲国家使用的编码体系，可以表示最多256个符号。</p>
<h3 id="gb2312-码">GB2312 码</h3>
<p>中文字符的每个字节最高位规定为 1，这便是 GB2312 编码。</p>
<h3 id="unicode">Unicode</h3>
<p>可以想象，如果有一种编码，将世界上所有的符号都纳入其中。每一个符号都给予一个独一无二的编码，那么乱码问题就会消失。<u>这就是 Unicode，就像它的名字都表示的，这是一种所有符号的编码。</u></p>
<p>Unicode 当然是一个很大的集合，现在的规模可以容纳100多万个符号。每个符号的编码都不一样，比如，<code>U+0639</code>表示阿拉伯字母<code>Ain</code>，<code>U+0041</code>表示英语的大写字母<code>A</code>，<code>U+4E25</code>表示汉字<code>严</code>。具体的符号对应表，可以查询<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.unicode.org%2F">unicode.org</a>，或者专门的<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.chi2ko.com%2Ftool%2FCJK.htm">汉字对应表</a>。</p>
<h3 id="unicode-的问题">Unicode 的问题</h3>
<p>这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。</p>
<h3 id="utf-8">UTF-8</h3>
<p><strong>重复一遍，这里的关系是，UTF-8 是 Unicode 的实现方式之一。</strong></p>
<p>UTF-8 最大的一个特点，就是它是一种<strong>变长</strong>的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。</p>
<p>UTF-8 的编码规则很简单，只有二条：</p>
<p>1）对于单字节的符号，字节的第一位设为<code>0</code>，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。</p>
<p>2）对于<code>n</code>字节的符号（<code>n &gt; 1</code>），第一个字节的前<code>n</code>位都设为<code>1</code>，第<code>n + 1</code>位设为<code>0</code>，后面字节的前两位一律设为<code>10</code>。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。</p>
<p>参考：</p>
<p><a href="https://www.jianshu.com/p/51b40820848a">Python编码避坑指南——编码基础知识</a></p>
<p><a href="http://www.cnblogs.com/ysocean/p/6850811.html">Java 字符编码与解码</a></p>
<p><a href="https://blog.csdn.net/sinat_36972314/article/details/79745438">Python常见字符编码及其之间的转换</a></p>
<p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896">字符串和编码python</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Django网站开发全过程实录-1</title>
    <url>/django-1/</url>
    <content><![CDATA[<p>网站一般需要实现三种基本功能：<strong>连接数据库、处理用户请求、页面设计的删改</strong>。Django的优势在于将这些功能设计成独立的模块，形成一套web框架。利用Django框架开发网站，能让我们专注于编写应用程序而无需重新造轮子。 <span id="more"></span> Django 采用了 <strong>MVT 的软件设计模式</strong>，即<strong>模型</strong>（Model）、<strong>视图</strong>（View）和<strong>模板</strong>（Template）。这种设计模式的优势在于<strong>各个组件松散结合</strong>，每个APP应用都有明确的目的，并且可独立更改而不影响其它部分。如此，使得页面设计与业务逻辑互不影响。同时，Django是一套出色的<strong>动态内容管理系统</strong>，擅长动态提供数据库驱动的信息。</p>
<p>以下是我使用Django 3.1.7搭建网站过程的实录，力求完整、准确、无误地记录每个操作步骤与细节。</p>
<h2 id="环境搭建与工具安装"><strong>环境搭建与工具安装</strong></h2>
<blockquote>
<p><em>参考：</em><a href="https://stormsha.com/article/2026/"><em>https://stormsha.com/article/2026/</em></a></p>
</blockquote>
<p>我们需要在合适的目录内创建一个<strong>虚拟环境</strong>（用virtualenv, virtualenvwrapper皆可，参考<a href="https://blog.csdn.net/a200822146085/article/details/89048172">virtualenvwrapper的使用</a>），我给它取名为webdev。</p>
<p>在该虚拟环境，安装<strong>django</strong>和<strong>psycopg2</strong>工具包（用于管理PostgreSQL数据库）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install virtualenv, virtualenvwrapper-win</span><br><span class="line">mkvirtualenv webdev</span><br><span class="line">workon webdev</span><br><span class="line">pip install django</span><br><span class="line">pip install psycopg2</span><br><span class="line">pip list</span><br><span class="line">pip freeze</span><br><span class="line"></span><br><span class="line"># 如果需要退出或删除虚拟环境</span><br><span class="line">deactivate</span><br><span class="line">rmvirtualenv webdev</span><br></pre></td></tr></table></figure>
<p><strong>安装数据库：</strong></p>
<p>Django支持四种数据库：PostgreSQL、SQLite 3、MySQL、Oracle。</p>
<p>我选择PostgreSQL，它比MySQL更适合Django，Django的创建者如是说：</p>
<blockquote>
<p>如果您不受任何遗留系统的束缚，并且可以自由选择数据库后端，那么我们建议您使用PostgreSQL，它可以在成本、功能、速度和稳定性之间取得很好的平衡。（《 Django权威指南》第15页）</p>
</blockquote>
<p>PostgreSQL的安装步骤参考：https://www.runoob.com/postgresql/windows-install-postgresql.html</p>
<p>打开后设置语言为中文，然后关闭。</p>
<h2 id="创建项目project"><strong>创建项目（project）</strong></h2>
<p>下面在刚刚的虚拟环境webdev内创建一个项目mysite（你可以选择任意其他名字），项目是我们所建立的网站上所有应用程序的集合，并共用一套数据库配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd webdev</span><br><span class="line">django-admin startproject mysite</span><br></pre></td></tr></table></figure>
<p>我们看到新建了一个文件夹mysite及下面的子文件夹mysite/mysite：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysite&#x2F;</span><br><span class="line">    manage.py</span><br><span class="line">    mysite&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">        settings.py</span><br><span class="line">        urls.py</span><br><span class="line">        asgi.py</span><br><span class="line">        wsgi.py</span><br></pre></td></tr></table></figure>
<p>我们可以把子文件夹mysite/mysite视为整个项目的配置，其中的settings.py和urls.py这两个文件是我们以后需要经常修改的。</p>
<h2 id="启动服务器server"><strong>启动服务器（server）</strong></h2>
<p>启动服务器，服务器会监测你的代码更新并自动加载，所以无须重启才看到效果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd mysite</span><br><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<p>在settings.py内将语言改成中文，时区改为上海：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LANGUAGE_CODE &#x3D; &#39;zh-hans&#39;</span><br><span class="line"></span><br><span class="line">TIME_ZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39;</span><br></pre></td></tr></table></figure>
<p>刷新浏览器看到中文页面。</p>
<p>我们可以指定服务器的端口和IP地址。比如，把地址设为自己的IP地址（例如192.168.1.110）或0.0.0.0，让联网的其他计算机可见：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py runserver 0.0.0.0:8000</span><br></pre></td></tr></table></figure>
<p>使用Windows的用户用ipconfig命令获取本地网络中的IP 地址，然后复制到setting.py中，比如我是这个：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALLOWED_HOSTS &#x3D; [&#39;192.168.1.110&#39;]</span><br></pre></td></tr></table></figure>
<p>于是，在其他电脑或手机浏览器打开 http://192.168.1.110:8000/ 就可以访问啦！完美！不过网站还在开发中，就不要随便开放共享啦~</p>
<h2 id="创建应用程序app"><strong>创建应用程序（APP）</strong></h2>
<p><strong>项目和应用的区分：</strong></p>
<ul>
<li><strong>应用</strong>是用于执行某项具体操作的程序，<strong>项目</strong>是特定网站的配置和应用程序的集合。</li>
<li><strong>多对多的关系</strong>：一个项目可以包含多个应用程序，一个应用程序可以用在多个项目中。</li>
</ul>
<p>应用放在任意路径都可以，但我们一般放在<strong>manage.py文件相同的目录</strong>中，与mysite子文件夹平行。在这里我创建一个成语检索的app，名为idiom：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py startapp idiom</span><br></pre></td></tr></table></figure>
<p>看看这个应用程序下有哪些文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">idiom&#x2F;</span><br><span class="line">    __init__.py</span><br><span class="line">    admin.py</span><br><span class="line">    apps.py</span><br><span class="line">    migrations&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">    models.py</span><br><span class="line">    tests.py</span><br><span class="line">    views.py</span><br></pre></td></tr></table></figure>
<h2 id="编写视图views"><strong>编写视图（views）</strong></h2>
<p>下面为这个应用程序idiom添砖加瓦，分为三个步骤：</p>
<ol type="1">
<li>创建视图函数</li>
<li>将视图函数映射到APP的urls</li>
<li>将APP中的urls连入网站的根urls</li>
</ol>
<p>这样看逻辑可能更清晰：视图函数 --&gt; APP的urls --&gt; 网站的urls</p>
<h2 id="创建视图函数"><strong>创建视图函数</strong></h2>
<p>打开文件idiom/views.py ，加入以下Python代码，<strong>创建index视图函数</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    return HttpResponse(&quot;Hello, world. You&#39;re at the idiom index.&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="映射到app的urls"><strong>映射到APP的urls</strong></h2>
<p>要调用该视图，我们要将其映射到URL，为此，我们需要添加一个URL配置（URLconf）。<strong>URLconf</strong>相当于网站的目录，也就是<strong>URL模式与视图函数之间的映射表</strong>。</p>
<p>我们在idiom应用的目录下创建一个名为urls.py的文件，windows操作如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type nul&gt;urls.py</span><br></pre></td></tr></table></figure>
<p>看看现在的应用目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">idiom&#x2F;</span><br><span class="line">    __init__.py</span><br><span class="line">    admin.py</span><br><span class="line">    apps.py</span><br><span class="line">    migrations&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">    models.py</span><br><span class="line">    tests.py</span><br><span class="line">    urls.py</span><br><span class="line">    views.py</span><br></pre></td></tr></table></figure>
<p>然后在idiom/urls.py这个空文件中加入以下代码，将index视图映射到APP的url模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.urls import path</span><br><span class="line"></span><br><span class="line">from . import views</span><br><span class="line"></span><br><span class="line">urlpatterns &#x3D; [</span><br><span class="line">    path(&#39;&#39;, views.index, name&#x3D;&#39;index&#39;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="连入网站的urls"><strong>连入网站的urls</strong></h2>
<p>下一步是将根URLconf（mysite/urls.py）指向idiom.urls模块，使得网站域名连接到app的url。我们打开mysite/urls.py，修改为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.contrib import admin</span><br><span class="line">from django.urls import include, path</span><br><span class="line"></span><br><span class="line">urlpatterns &#x3D; [</span><br><span class="line">    path(&#39;idiom&#x2F;&#39;, include(&#39;idiom.urls&#39;)),</span><br><span class="line">    path(&#39;admin&#x2F;&#39;, admin.site.urls),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>该include()功能允许引用其他URLconf，这样我们就将刚刚创建的index视图连接到了网站的URLconf。</p>
<p>也就是说，目前我们可以打开两个网址：</p>
<p><a href="http://127.0.0.1:8000/idiom/">http://example.com/idiom/</a></p>
<p><a href="http://127.0.0.1:8000/idiom/">http://example.com/admin/</a></p>
<p>注意idiom和admin在引用URL模式时的区别：除了admin.site.urls（用于管理后台），我们引用其他URL模式时，都应使用include()。</p>
<p>最后，验证下是否正常运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<p>打开http://127.0.0.1:8000/idiom/ ，可以看到 Hello, world. You're at the idiom index. 这行文字。</p>
<h2 id="urlconf的工作原理"><strong>URLconf的工作原理</strong></h2>
<p>Django允许我们根据需要设计每个应用程序的URL，通过创建<strong>URLconf</strong>（URL配置）。</p>
<p>在 idiom和mysite文件夹下的urls.py中，我们都使用了<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.path">path()</a>函数，这个函数有两个必需的参数 route和view。</p>
<p>path（<em>route</em>，<em>view</em>，<em>kwargs = None</em>，<em>name = None</em>）</p>
<ul>
<li>route是包含URL模式的字符串，比如目前我们有idiom/和admin/。在处理请求时，Django从第一个模式开始，沿列表的顺序，将请求的URL（域名后的部分）与每个模式进行比较，直到找到匹配的URL。这个字符串支持用尖括号匹配和捕获URL的一部分并将其作为关键字参数发送到视图，</li>
<li>view就是指定的视图函数，也可以是一个<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.include">django.urls.include()</a>。kwargs参数允许我们将其他参数传递给视图函数。</li>
<li>name不是必须的，但是命名URL的好处是便于在Django中的其他地方（尤其是在模板内部）明确地引用它。</li>
</ul>
<p>参考：<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.path">django.urls functions for use in URLconfs</a></p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title>有限状态机</title>
    <url>/finite-state-machines/</url>
    <content><![CDATA[<p>有限状态机（Finite-state machine, FSM），是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。主要用来是描述对象在它的生命周期内所经历的状态序列，以及如何响应来自外界的各种事件。 <span id="more"></span></p>
<p>参考：</p>
<p>https://www.huaweicloud.com/articles/ff04e6e5d7386b32d58f9d5015104ce9.html</p>
<p>https://www.cnblogs.com/21207-iHome/p/6085334.html</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title>python批量处理Excel表格</title>
    <url>/excel/</url>
    <content><![CDATA[<p>老爸工作中经常使用Excel，今天用python帮他批量合并Excel表格。</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件路径</span></span><br><span class="line">file_dir = <span class="string">&quot;your path&quot;</span></span><br><span class="line"><span class="comment">#找到文件路径下的所有表格名称，返回列表</span></span><br><span class="line">file_list = os.listdir(file_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个空dataframe</span></span><br><span class="line">df=pd.DataFrame()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> one_file <span class="keyword">in</span> file_list:</span><br><span class="line">    <span class="comment">#重构文件路径</span></span><br><span class="line">    file_path = os.path.join(file_dir,one_file)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#将excel sheet读取到DataFrame，去掉表头和表尾的注释，只留下columns and rows</span></span><br><span class="line">        dataframe = pd.read_excel(file_path, skiprows=<span class="number">1</span>, skipfooter=<span class="number">1</span>, sheet_name=<span class="string">&quot;sheet name&quot;</span>)</span><br><span class="line">        <span class="comment">#去掉空白的行</span></span><br><span class="line">        dataframe = dataframe.dropna(how=<span class="string">&quot;all&quot;</span>)</span><br><span class="line">        <span class="comment">#加入到空df中</span></span><br><span class="line">        df = pd.concat([df, dataframe])</span><br><span class="line">        <span class="comment">#有些文件可能损坏，无法读取</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">&quot;cannot read&quot;</span> + one_file)</span><br><span class="line"></span><br><span class="line"><span class="comment">#写入到一个新excel表中，去掉index</span></span><br><span class="line">df.to_excel(<span class="string">&quot;new_file.excel&quot;</span>,index=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>基础：用flask搭建RESTful API</title>
    <url>/flask-api-1/</url>
    <content><![CDATA[<p><a href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a>是目前发展最迅速的 Python 框架之一，它是一个微型的 Python 开发的 Web 框架，基于<a href="https://www.oschina.net/p/werkzeug">Werkzeug</a> WSGI工具箱和<a href="https://www.oschina.net/p/jinja">Jinja2</a> 模板引擎。以下是用flask来构建 RESTful API 的全过程实录，力求完整、准确、无误地记录每个操作步骤与细节。我用的是windows系统，但我也会提到其他系统的操作。 <span id="more"></span> # 环境搭建与工具安装</p>
<p>首先依然是创建虚拟环境并安装 flask 和 <a href="https://flask-restful.readthedocs.io/en/latest/">flask-restful</a> package。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkvirtualenv flask-api</span><br><span class="line">pip install flask</span><br><span class="line">pip install flask-restful</span><br></pre></td></tr></table></figure>
<h1 id="使用flask搭建api">使用flask搭建API</h1>
<h2 id="最简单的flask应用">最简单的flask应用</h2>
<p>新建一个python脚本，linux用touch，windows操作如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">workon flask-api <span class="comment"># 激活虚拟环境</span></span><br><span class="line">D:</span><br><span class="line"><span class="built_in">type</span> nul&gt;hello.py <span class="comment"># 新建文件</span></span><br><span class="line">atom hello.py <span class="comment"># 用atom编辑器打开</span></span><br></pre></td></tr></table></figure>
<p>粘贴以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)  </span><span class="comment"># 把Flask对象中的route()函数作为一个装饰器，增强该函数的功能</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello, World!&#x27;</span></span><br></pre></td></tr></table></figure>
<p>再打开 Command Prompt 输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set FLASK_APP&#x3D;hello.py</span><br><span class="line">set FLASK_ENV&#x3D;development</span><br><span class="line">flask run</span><br></pre></td></tr></table></figure>
<p>按下快捷键Alt+Shift+=左右分屏，输入：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl http://127.0.0.1:5000/</span><br><span class="line">curl -v http://127.0.0.1:5000/</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">&lt; Content-Type: text&#x2F;html; charset&#x3D;utf-8</span><br><span class="line">&lt; Content-Length: 13</span><br><span class="line">...</span><br><span class="line">&lt;</span><br><span class="line">Hello, World!* Closing connection 0</span><br></pre></td></tr></table></figure>
<p>我们发现返回的是html类型，而不是通常从 RESTful API 获得的 json 文件，于是修改hello.py的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, jsonify</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;about&#x27;</span>: <span class="string">&#x27;Hello, World!&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者：</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(about=<span class="string">&#x27;Hello, World!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>调试模式下，服务器会监测你的代码更新并自动加载，所以无须重启才看到效果。这时curl得到json file：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">&lt; Content-Type: application&#x2F;json</span><br><span class="line">&lt; Content-Length: 31</span><br><span class="line">...</span><br><span class="line">&lt;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;hello&quot;: &quot;Hello, World!&quot;</span><br><span class="line">&#125;</span><br><span class="line">* Closing connection 0</span><br></pre></td></tr></table></figure>
<h2 id="通过url传递参数">通过URL传递参数</h2>
<p>我们可以通过url传递参数给flask api，例如<code>/&lt;int:num&gt;/</code></p>
<p>将hello.py修改为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, jsonify, request</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/multi/&lt;int:num&gt;&#x27;</span>, methods=[<span class="string">&#x27;GET&#x27;</span>]</span>) </span><span class="comment"># methods一般默认为GET</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_multiply10</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;result&#x27;</span>: num*<span class="number">10</span>&#125;) <span class="comment"># 返回数字乘以10后的结果</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run(debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后在cmd输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;multi&#x2F;10</span><br></pre></td></tr></table></figure>
<p>我们得到10乘以10后的结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;result&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然，我们也可以在浏览器内访问以下地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;multi&#x2F;10</span><br></pre></td></tr></table></figure>
<h2 id="使用post方法">使用POST方法</h2>
<p>这一小节我们将使用POST method。</p>
<p>在hello.py中加入以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span>, methods=[<span class="string">&quot;GET&quot;</span>, <span class="string">&quot;POST&quot;</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span>():</span></span><br><span class="line">    <span class="keyword">if</span> (request.method == <span class="string">&quot;POST&quot;</span>):</span><br><span class="line">        some_json = request.get_json()</span><br><span class="line">        <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;You sent&#x27;</span>: some_json&#125;), <span class="number">201</span> <span class="comment"># 如果是post则返回post内容</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> jsonify(&#123;<span class="string">&quot;about&quot;</span>: <span class="string">&quot;Hello World!&quot;</span>&#125;) <span class="comment"># 如果是get则返回hello world</span></span><br></pre></td></tr></table></figure>
<p>现在，我们可以发送如下curl请求并返回POST的内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application&#x2F;json&quot; -X POST -d &#39;&#123;&quot;name&quot;:&quot;Example&quot;,&quot;email&quot;:&quot;example@example.com&quot;&#125;&#39; http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;</span><br></pre></td></tr></table></figure>
<p>注意：Windows的命令行不支持单引号、且需要将双引号转义，所以需要改成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application&#x2F;json&quot; -X POST -d &quot;&#123;\&quot;name\&quot;:\&quot;Example\&quot;,\&quot;email\&quot;:\&quot;example@example.com\&quot;&#125;&quot; http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;</span><br></pre></td></tr></table></figure>
<p>因此，Windows用户建议在git bash上完成以上操作。</p>
<h1 id="使用flask-restful搭建api">使用flask-restful搭建API</h1>
<p>我们把hello.py修改为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"><span class="keyword">from</span> flask_restful <span class="keyword">import</span> Resource, Api</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">api = Api(app)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span>(<span class="params">Resource</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self</span>):</span></span><br><span class="line">        some_json = request.get_json()</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;You sent&#x27;</span>: some_json&#125;, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Multi</span>(<span class="params">Resource</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self, num</span>):</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;result&#x27;</span>: num*<span class="number">10</span>&#125;</span><br><span class="line"></span><br><span class="line">api.add_resource(HelloWorld, <span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">api.add_resource(Multi, <span class="string">&#x27;/multi/&lt;int:num&gt;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>以上代码实现完全相同的功能，不过看起来更简洁和舒服。</p>
<p>参考教程：</p>
<p>https://www.youtube.com/watch?v=s_ht4AKnWZg</p>
<p>参考文献：</p>
<p>https://blog.csdn.net/weixin_41010198/article/details/85230424#API_3</p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>一文读懂Flask Web开发实战！</title>
    <url>/flask-2/</url>
    <content><![CDATA[
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title>部署Flask开发的API到Heroku</title>
    <url>/flask-api-2/</url>
    <content><![CDATA[<p>上篇文章我们介绍了如何用flask开发简单的web api，下面我们把它部署到heroku上，方便更多人使用。 <span id="more"></span></p>
<p><strong>步骤总结：</strong></p>
<ul>
<li><p>注册<a href="https://signup.heroku.com/">Heroku帐号</a></p></li>
<li><p>下载客户端</p></li>
<li><p>在本地命令行登录</p>
<ul>
<li>如果出现IP Address Mismatch，复制并粘贴<code>heroku login -i</code>到终端，用邮箱密码登录</li>
</ul></li>
<li><p>创建应用</p>
<ul>
<li>``` heroku apps:create flask-microblog <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line">- 初始化</span><br><span class="line">  </span><br><span class="line">  - &#96;&#96;&#96;</span><br><span class="line">    mkdir flask-api</span><br><span class="line">    cd flask-api</span><br><span class="line">    git init</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>编写应用代码</p>
<ul>
<li>run.py</li>
<li>requirements.txt</li>
<li><code>echo web: gunicorn run:app &gt; Procfile</code>
<ul>
<li><code>web: gunicorn &lt;filename&gt;:&lt;main method name&gt;</code></li>
</ul></li>
<li>用<code>tree/F</code>检验上述文件是否齐全</li>
</ul></li>
<li><p>部署应用</p>
<ul>
<li><p>关联github，自动部署</p></li>
<li><pre><code>  git add .
  git commit -m &quot;Initialize repo&quot;
  git push -u origin master</code></pre></li>
</ul></li>
<li><p>访问应用地址：https://nlp-ch.herokuapp.com/</p></li>
</ul>
<p><strong>参考文献：</strong></p>
<p>官方教程：</p>
<p>https://devcenter.heroku.com/articles/getting-started-with-python</p>
<p>以及这些博客：</p>
<p>https://noviachen.github.io/posts/b4cb2e1c.html</p>
<p>https://wizardforcel.gitbooks.io/the-flask-mega-tutorial-2017-zh/content/docs/18.html</p>
<p>http://www.bjhee.com/flask-heroku.html</p>
<p>windows上部署参考（建议使用git bash）：https://caijialinxx.github.io/2018/07/25/deploy-on-heroku/</p>
<p>git bash创建并编辑文件参考：https://blog.csdn.net/qq_34289537/article/details/53994070</p>
<p>windows cmd常用命令参考：https://www.jianshu.com/p/80c3ac7bea8f</p>
<p>上传文件到github: https://www.jianshu.com/p/5227f837070b</p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>如何在curl和python中使用API</title>
    <url>/flask-api-3/</url>
    <content><![CDATA[<p>除了自己构建API，我们也可能需要使用其他网站提供的API。 <span id="more"></span></p>
<h1 id="在curl中使用api">在curl中使用API</h1>
<p>我们以学术检索网站<a href="https://api.semanticscholar.org/">semantic scholar</a>为例，查找某论文的信息，并保存为JSON文件<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;api.semanticscholar.org&#x2F;v1&#x2F;paper&#x2F;10.1038&#x2F;nrn3241 &gt; paper1.json</span><br></pre></td></tr></table></figure>
<p>但此时的json数据结构并不整洁，我们用python格式化json字符串并保存在paper2.json<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m json.tool paper1.json paper2.json</span><br></pre></td></tr></table></figure>
<p>或者一步到位<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;api.semanticscholar.org&#x2F;v1&#x2F;paper&#x2F;10.1038&#x2F;nrn3241 | python -mjson.tool &gt; paper3.json</span><br></pre></td></tr></table></figure>
<p>如果仅仅是想显示为Pretty Print打印出来，把后面的filename.json去掉即可。</p>
<h1 id="用python获取api数据">用python获取API数据</h1>
<p>依旧是这个例子，我们将返回的数据转成字典格式<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">&quot;https://api.semanticscholar.org/v1/paper/10.1038/nrn3241&quot;</span>)</span><br><span class="line">response_dic = response.json() <span class="comment"># if the result was written in JSON format, if not it raises an error</span></span><br></pre></td></tr></table></figure>
<p>更多requests相关参考<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>。</p>
<p><strong>参考文献：</strong></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://www.ruanyifeng.com/blog/2019/09/curl-reference.html">curl 的用法指南</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="https://docs.python.org/3/library/json.html">python json官方文档</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="https://skorks.com/2013/04/the-best-way-to-pretty-print-json-on-the-command-line/">在命令行上漂亮地打印JSON的最佳方法</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><a href="https://jzchangmark.wordpress.com/2016/06/12/%E9%80%8F%E9%81%8E-curl%E3%80%81python%E3%80%81postman-%E4%BE%86-request-api/">透过curl、Python、Postman 来Request API</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a href="https://www.w3schools.com/python/ref_requests_response.asp">python请求</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>一个人开发信息检索与抽取网站的全过程</title>
    <url>/flask-web/</url>
    <content><![CDATA[<h1 id="我的网站开发学习">我的网站开发学习</h1>
<p>我不会使用css或js文件，因为我只专注于python后端开发，前端基本都省去，只用bootstrap，但是要精通bootstrap哟！</p>
<p>我的网站是动态呈现，所以一定要用flask或者django，我现在先不考虑部署到服务器，只是本地使用！！！</p>
<p>计划：</p>
<ol type="1">
<li>完成检索功能，包括错误检测。检索成语、句子、词语等等。</li>
<li>其他小插件的完善</li>
<li>重点是毕业论文</li>
</ol>
<span id="more"></span>
<h1 id="搜索框">搜索框</h1>
<p>首先我们把boostrap加入html页面，直接使用<a href="https://www.bootstrapcdn.com/">BootstrapCDN</a>跳过下载，将Bootstrap编译的CSS和JS的缓存版本交付给我们的项目。我们使用最简单的主题即可，也可以使用其他主题（比如<a href="https://bootswatch.com/minty/">minty</a>）。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">title</span>&gt;</span>Document<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">href</span>=<span class="string">&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css&quot;</span> <span class="attr">integrity</span>=<span class="string">&quot;sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u&quot;</span> <span class="attr">crossorigin</span>=<span class="string">&quot;anonymous&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">form</span> <span class="attr">class</span>=<span class="string">&quot;form my-2 my-lg-0&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">class</span>=<span class="string">&quot;form-control mr-sm-2&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text&quot;</span> <span class="attr">placeholder</span>=<span class="string">&quot;Search&quot;</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">button</span> <span class="attr">class</span>=<span class="string">&quot;btn btn-secondary my-2 my-sm-0&quot;</span> <span class="attr">type</span>=<span class="string">&quot;submit&quot;</span>&gt;</span>Search<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js&quot;</span> <span class="attr">integrity</span>=<span class="string">&quot;sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa&quot;</span> <span class="attr">crossorigin</span>=<span class="string">&quot;anonymous&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;idiom.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    idiom_list = json.load(read_file)</span><br><span class="line"></span><br><span class="line">idiom_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> idiom <span class="keyword">in</span> idiom_list:</span><br><span class="line">    idiom_dict[idiom[<span class="string">&#x27;word&#x27;</span>]] = idiom</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;idiom_dict.json&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">    json.dump(idiom_dict, write_file)</span><br><span class="line">print(<span class="string">&#x27;成功将原列表转换为key为成语的字典！&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>查看源代码，</p>
<p>参考：</p>
<p>https://www.bootdey.com/snippets/view/Search-Results#html</p>
<p><a href="https://blog.csdn.net/star_xing123/article/details/101271925">百度搜索框</a></p>
<p><a href="https://www.html.cn/qa/css3/12786.html">怎么在HTML中加入css样式？ - html中文网</a></p>
<p><a href="https://getbootstrap.com/docs/4.3/getting-started/introduction/">Bootstrap Introduction</a></p>
<p><a href="https://bootswatch.com/">Free themes for Bootstrap</a></p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title>flask干货总结</title>
    <url>/flask/</url>
    <content><![CDATA[<h1 id="flask">Flask</h1>
<blockquote>
<p>在一个Web应用里，客户端和服务器上的Flask程序的交互可以简单概括为以下几步：</p>
<p>1）用户在浏览器输入URL访问某个资源。</p>
<p>2）Flask接收用户请求并分析请求的URL。</p>
<p>3）为这个URL找到对应的处理函数。</p>
<p>4）执行函数并生成响应，返回给浏览器。</p>
<p>5）浏览器接收并解析响应，将信息显示在页面中。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span id="more"></span></p>
</blockquote>
<h1 id="术语解释">术语解释</h1>
<ul>
<li>路由：作为动词时，含义是“按某路线发送”，即调用与请求URL对应的视图函数。</li>
<li>视图函数（view function）：处理请求并生成响应的函数。当用户访问URL时会触发视图函数，该函数可执行任意操作，比如从数据库中获取信息，获取请求信息，对用户输入的数据进行计算和处理等。最后，视图函数返回的值将作为响应的主体，一般来说就是HTML页面。</li>
<li>模板：包含程序页面的HTML文件。</li>
<li>静态文件：需要在HTML文件中加载的CSS和Java Script文件，以及图片、字体文件等资源文件。</li>
<li>模板文件存放在项目根目录中的templates文件夹中，静态文件存放在static文件夹下，这两个文件夹需要和包含程序实例的模块处于同一个目录下。</li>
<li>HTTP（Hypertext TransferProtocol，超文本传输协议）定义了服务器和客户端之间信息交流的格式和传递方式，它是万维网（World Wide Web）中数据交换的基础。</li>
<li>WSGI：将HTTP格式的请求数据转换成Flask程序能够使用的Python数据，并把python程序的响应经过WSGI转换生成HTTP响应。</li>
<li>URL中的查询字符串用来向指定的资源传递参数。查询字符串从问号?开始，以键值对的形式写出，多个键值对之间使用&amp;分隔。</li>
<li>这种浏览器与服务器之间交互的数据被称为报文（message），请求时浏览器发送的数据被称为请求报文（request message），而服务器返回的数据被称为响应报文（responsemessage）。</li>
<li>请求报文由请求的方法、URL、协议版本、首部字段（header）以及内容实体组成。</li>
<li>报文由报文首部和报文主体组成，两者由空行分隔，请求报文的主体一般为空。如果URL中包含查询字符串，或是提交了表单，那么报文主体将会是查询字符串和表单数据。</li>
<li>用来映射到数据库表的Python类通常被称为数据库模型（model），一个数据库模型类对应数据库中的一个表。所有的模型类都需要继承Flask-SQLAlchemy提供的db.Model基类。</li>
</ul>
<h1 id="架构">架构</h1>
<p>在MVC架构中，程序被分为三个组件：数据处理（Model）、用户界面（View）、交互逻辑（Controller）。</p>
<p>如果想要使用Flask来编写一个MVC架构的程序，那么视图函数可以作为控制器（Controller），视图（View）则是使用Jinja2渲染的HTML模板，而模型（Model）可以使用其他库来实现。</p>
<p>参考文献：</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://book.douban.com/subject/30310340/">《Flask Web开发实战》</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title>Git和Github常用操作大全</title>
    <url>/git/</url>
    <content><![CDATA[<blockquote>
<p>Git是目前世界上最先进的分布式版本控制系统（没有之一）。Git的功能有版本控制（版本管理、远程仓库、分支协作）。GitHub是一个非常流行的全球代码托管平台.</p>
</blockquote>
<p>参考：<a href="https://blog.csdn.net/Python_Ai_Road/article/details/109476021">30分钟吃掉Git和GitHub常用操作</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">git init</span><br><span class="line">git status</span><br><span class="line"></span><br><span class="line">git add -A </span><br><span class="line">git add . </span><br><span class="line">git add <span class="built_in">next</span>/_config.yml</span><br><span class="line"></span><br><span class="line">git commit -m <span class="string">&quot;comment&quot;</span> </span><br><span class="line"></span><br><span class="line">git remote add origin https://github.com/XX/XX</span><br><span class="line">git push -u origin master</span><br><span class="line"></span><br><span class="line">git clone https://github.com/XX/XX  ../XX</span><br><span class="line">    </span><br><span class="line">git remote -v</span><br><span class="line">git remote rm origin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除git</span></span><br><span class="line">rm -rf .git</span><br><span class="line"></span><br><span class="line">git reset HEAD^ <span class="comment">#可以回退到上一个版本。</span></span><br><span class="line">git reset HEAD^^ <span class="comment">#可以回退到上上个版本。</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
]]></content>
      <categories>
        <category>代码</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Hexo博客Next主题搭建全过程</title>
    <url>/hexo-next/</url>
    <content><![CDATA[<p>经过三次不明觉厉的error崩盘导致我重新初始化博客（不知道哪里出错所以只有推倒重来），2021年3月18日博客终于配置完毕。在此记录全过程。最后一次配置，我只修改了三个文件：_config.yml和next/_config.yml以及custom_file_path（当然还有css/images和source）。其中custom_file_path设置为styles.styl，用于<strong>修改文章内链接文本样式</strong>、<strong>文章内单行代码的样式设置</strong>等等，需要在custom_file_path中加入这个地址。 <span id="more"></span></p>
<h1 id="hexo博客">Hexo博客</h1>
<h2 id="博客的初始化">博客的初始化</h2>
<p>关于怎么部署参考：https://zhuanlan.zhihu.com/p/26625249</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd hexo-final</span><br><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo init</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></figure>
<p>完成以上步骤后，我们看到目前的插件如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-- hexo-generator-archive@1.0.0</span><br><span class="line">+-- hexo-generator-category@1.0.0</span><br><span class="line">+-- hexo-generator-index@2.0.0</span><br><span class="line">+-- hexo-generator-tag@1.0.0</span><br><span class="line">+-- hexo-renderer-ejs@1.0.0</span><br><span class="line">+-- hexo-renderer-marked@4.0.0</span><br><span class="line">+-- hexo-renderer-stylus@2.0.1</span><br><span class="line">+-- hexo-server@2.0.0</span><br><span class="line">+-- hexo-theme-landscape@0.0.3</span><br><span class="line">&#96;-- hexo@5.4.0</span><br></pre></td></tr></table></figure>
<p>我们需要安装其他插件，我使用pandoc渲染markdown，它也用于typora的渲染，可以很好地支持数学公式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-- hexo-browsersync@0.3.0</span><br><span class="line">+-- hexo-deployer-git@3.0</span><br><span class="line">+-- hexo-generator-index-pin-top@0.2.2</span><br><span class="line">+-- hexo-generator-searchdb@1.3.3</span><br><span class="line">+-- hexo-related-popular-posts@5.0.1</span><br><span class="line">+-- hexo-renderer-pandoc@0.3.0</span><br><span class="line">+-- hexo-symbols-count-time@0.7.1</span><br><span class="line">+-- hexo-hide-posts@0.1.1</span><br></pre></td></tr></table></figure>
<p>安装方式： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br><span class="line">npm install 插件 --save</span><br></pre></td></tr></table></figure> 安装完毕后，我们把_config.yml和next/_config.yml以及css/images和source中的文件（注意有_data/styles.styl，这是我的个性化设置）复制粘贴过来即可。</p>
<h2 id="使用hexo还是hugo">使用Hexo还是Hugo</h2>
<p>3月19日，我又尝试了hugo，的确渲染速度很快、有很多漂亮的主题，并且很多主题都有详细的文档，而且hugo是按照文件夹结构生成侧边栏的目录，非常适合做文档和电子书！但缺点是很多主题不能折叠侧边栏，提供沉浸式阅读体验，所以我还是放弃了hugo，可能以后遇到喜欢的主题还是会迁移到hugo吧。</p>
<h2 id="最终我的博客页面外观">最终我的博客页面外观</h2>
<p>我花了很多时间去研究怎样让博客更美观和简洁，其实有很多是无用功，所以3月19日我正式敲定了目前的版本，并不再继续进行博客主题的个性化配置。以下是博客文章的截图：</p>
<p><img src="https://i.loli.net/2021/03/19/MjEKeWoSuidzVxT.png"/></p>
<p>主色调为蓝绿色，辅以紫色，我把最终的配置文件备份在github地址：_config.yml，next/_config.yml，next/source/css/images和source（source中有styles.styl和我写的文章），并不再修改。</p>
<p>但是我经常用typora撰写笔记，因此<code>_post</code>文件夹下经常会更新，所以我写完文章有时会打开server，确认渲染无误，然后保存好写的文章。每隔一天或一周，我就hexo d一下，这样不至于操作太频繁。我可能也会使用<a href="https://novnan.github.io/Hexo/hexo-draft/">draft功能</a>。同时，我将_post文件夹同步在github上，经常push到这个repo。由此，我的博客配置和文章都得以云端保存了。</p>
<h2 id="百度和谷歌搜索">百度和谷歌搜索</h2>
<p>使用dns配置完成了百度认证，使用html完成了谷歌认证，但目前还不能搜到我的网站。不过这不重要，以后再publicize吧。</p>
<h1 id="我的写作工具">我的写作工具</h1>
<p>我用Typora写作，记录零碎的想法、翻译、笔记、代码等，但是<strong>上传网站后访问速度有点慢</strong>，所以一般我会用本地服务器查看笔记。手机的话，我会使用知乎或微信或csdn查看，所以<strong>重要的文章需要同步到各个网站</strong>。可能之后网站优化就好了~</p>
<p>各大同步地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">知乎、微信、csdn、hexo、github</span><br></pre></td></tr></table></figure>
<p>好啦，终于不用浪费时间在网站美工和找工具上啦！专心写文章吧！</p>
]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（上）</title>
    <url>/information-retrieval-1/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<blockquote>
<p><em>I am the very model of a modern Major-General,</em> <em>I’ve information vegetable, animal, and mineral,</em> <em>I know the kings of England, and I quote the fights historical</em> <em>From Marathon to Waterloo, in order categorical...</em></p>
<p>Gilbert and Sullivan, Pirates of Penzance</p>
</blockquote>
<p>假设你是一家跟踪航空公司股票的投资公司分析师。你的任务是确定航空公司机票价格上涨的公告与他们股票第二天的表现之间的关系（如果有的话）。关于股票价格的历史数据很容易得到，但是航空公司的公告呢？你至少需要知道航空公司的名称、提议票价上涨的性质、公告的日期，可能还需要知道其他航空公司的反应。幸运的是，这些都可以在新闻文章中找到，比如这一条。</p>
<span id="more"></span>
<p>美国联合航空公司（United Airlines）周五表示，以高燃油价格为理由，已将飞往一些城市的往返机票价格提高了6美元，而这些航班也由成本较低的航空公司提供服务。发言人蒂姆-瓦格纳（Tim Wagner）表示，AMR Corp.旗下的美国航空（American Airlines）立即配合这一举措。美国联合航空公司（UAL Corp.）旗下的联合航空公司（United）表示，涨价于周四生效，适用于该公司与折扣航空公司竞争的大部分航线，如芝加哥至达拉斯、丹佛至旧金山等。</p>
<p>本章介绍了从文本中提取有限种类的语义内容的技术。这种信息提取过程（IE）将嵌入文本中的非结构化信息提取信息转换为结构化数据，例如用于填充关系数据库以实现进一步处理。关系提取与填充关系数据库有着密切的联系。事实上，知识图谱，即结构化关系数据库的数据集，是搜索引擎向用户展示信息的一种常见方式。</p>
<p>接下来，我们讨论与事件相关的三个任务。事件提取是找到这些实体参与的事件，比如在我们的样本文本中，<em>United</em>（”美联航“）和<em>American</em>（”美航“）的票价上涨以及报道事件<em>said</em>（”说“）和<em>cite</em>（”引用“）。需要指代消歧（第22章）来弄清文本中哪些事件提及是指同一个事件；在我们的运行示例中，<em>increase</em>的两次出现和短语<em>the move</em>都是指同一个事件。</p>
<p>为了弄清文本中事件发生的时间，我们提取时间表达式，比如一周中的几天（周五和周四）；相对表达式，比如从现在起或明年起；以及时刻，比如下午3点半。这些表达式必须归一化到特定的日历日期或时间上，以便定位事件的时间。在我们的示例任务中，这将使我们能够将周五与美联航宣布的时间联系起来，将周四与前一天的票价上涨联系起来，并生成一条时间线，其中美联航的宣布紧随票价上涨，而美航的宣布紧随这两个事件。</p>
<p>最后，许多文本描述了反复出现的模式化事件或情境。模板填充的任务就是在文档中找到这样的情况，并填充到模板槽中。这些槽位填充物可能由直接从文本中提取的文本片段组成，也可能是通过额外处理从文本元素中推断出来的概念，如时间、金额或本体实体。我们的航空公司文本就是这种模式化情境的一个例子，因为航空公司经常会提高票价，然后等着看竞争对手是否跟进。在这种情况下，我们可以将美联航确定为最初提高票价的牵头航空公司，6美元为金额，周四为涨价日期，而美航则为跟风的航空公司，从而得到如下的填充模板。</p>
<figure>
<img src="C:\Users\13607\AppData\Roaming\Typora\typora-user-images\image-20210321212530121.png" alt="image-20210321212530121" /><figcaption aria-hidden="true">image-20210321212530121</figcaption>
</figure>
<h1 id="关系抽取">17.1 关系抽取</h1>
<p>假设我们已经检测到了样本文本中的命名实体（也许使用了第8章的技术），并想识别出检测到的实体之间的关系：</p>
<blockquote>
<p>以高油价为由，[ORG联合航空公司]表示，[时间周五]它已将飞往一些同样由低成本航空公司服务的城市的航班的票价每往返提高了[MONEY 6美元]。发言人[per蒂姆-瓦格纳]表示，[ORG美国航空]是[ORG AMR Corp.]的一个单位，立即配合这一举措。ORG美联航]，[ORG UAL Corp.]的一个单位，说增加生效[时间周四]，适用于它与折扣航空公司竞争的大多数航线，如[LOC芝加哥]到[LOC达拉斯]和[LOC丹佛]到[LOC旧金山]。<em>(机翻结果)</em></p>
</blockquote>
<p><img src="https://i.loli.net/2021/03/18/senloyDcQAGipId.png" width="700"/></p>
<p>例如，这段文本告诉我们，Tim Wagner是American Airlines的发言人，United是UAL Corp.的一个单位，American是AMR的一个单位。这些二元关系是更通用关系的实例，比如part-of或employes，它们在新闻风格的文本中出现得相当频繁。图17.1列出了ACE关系提取评估中使用的17种关系，图17.2显示了一些关系的样例。我们还可以提取更多的特定领域的关系，比如航空路线的概念。例如从这个文本中我们可以得出美联航有到芝加哥、达拉斯、丹佛和旧金山的航线。</p>
<center>
<img src="https://i.loli.net/2021/03/18/B5QEbCoyKd6ZsFi.png"  alt="" width="700" />
</center>
<table>
<caption>Figure 17.2 Semantic relations with examples and the named entity types they involve.</caption>
<thead>
<tr class="header">
<th>Relations</th>
<th>Types Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Physical-Located</td>
<td>PER-GPE He was in Tennessee</td>
</tr>
<tr class="even">
<td>Part-Whole-Subsidiary</td>
<td>ORG-ORG XYZ, the parent company of ABC</td>
</tr>
<tr class="odd">
<td>Person-Social-Family</td>
<td>PER-PER Yoko鈥檚 husband John</td>
</tr>
<tr class="even">
<td>Org-AFF-Founder</td>
<td>PER-ORG Steve Jobs, co-founder of Apple...</td>
</tr>
</tbody>
</table>
<p>这些关系很好地对应了我们在第15章中引入的模型理论概念，为逻辑形式的含义提供了基础。也就是说，一个关系由一系列有序的元组组成，基于一个领域的元素。在大多数标准的信息提取应用中，领域元素对应于文本中出现的命名实体，对应于指代消歧产生的基础实体，或者对应于从领域本体中选择的实体。图 17.3 显示了一个基于模型的视图，可以从我们的运行示例中提取实体和关系的集合。</p>
<table>
<colgroup>
<col style="width: 60%" />
<col style="width: 39%" />
</colgroup>
<thead>
<tr class="header">
<th>Domain领域</th>
<th><span class="math inline">\(D = {a,b, c,d, e, f,g,h,i}\)</span>元素</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>United, UAL, American Airlines, AMR</td>
<td><span class="math inline">\(a,b, c,d\)</span></td>
</tr>
<tr class="even">
<td>Tim Wagner</td>
<td><span class="math inline">\(e\)</span></td>
</tr>
<tr class="odd">
<td>Chicago, Dallas, Denver, and San Francisco</td>
<td><span class="math inline">\(f,g,h,i\)</span></td>
</tr>
<tr class="even">
<td><strong>Classes类别</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>United, UAL, American, and AMR are organizations</td>
<td><span class="math inline">\(Org = {a,b, c,d}\)</span></td>
</tr>
<tr class="even">
<td>Tim Wagner is a person</td>
<td><span class="math inline">\(Pers = {e}\)</span></td>
</tr>
<tr class="odd">
<td>Chicago, Dallas, Denver, and San Francisco are places</td>
<td><span class="math inline">\(Loc = { f,g,h,i}\)</span></td>
</tr>
<tr class="even">
<td><strong>Relations关系</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>United is a unit of UAL</td>
<td><span class="math inline">\(PartOf = {&lt;a,b&gt;,&lt;c,d&gt;}\)</span></td>
</tr>
<tr class="even">
<td>American is a unit of AMR</td>
<td></td>
</tr>
<tr class="odd">
<td>Tim Wagner works for American Airlines</td>
<td><span class="math inline">\(OrgAff = {&lt;c, e&gt;}\)</span></td>
</tr>
<tr class="even">
<td>United serves Chicago, Dallas, Denver, and San Francisco</td>
<td><span class="math inline">\(Serves = {&lt;a, f&gt;,&lt;a,g&gt;,&lt;a,h&gt;,&lt;a,i&gt;}\)</span></td>
</tr>
</tbody>
</table>
<p>请注意这种模型理论的角度是如何将NER任务也包含在内的；命名实体识别对应于一类一元关系的识别。</p>
<p>许多其他领域都定义了关系集。例如美国国家医学图书馆的统一医学语言系统（UMLS）有一个网络，定义了134个大的主题类别、实体类型和54个实体之间的关系，示例如下。</p>
<table>
<thead>
<tr class="header">
<th>Entity</th>
<th>Relation</th>
<th>Entity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Injury</td>
<td>disrupts</td>
<td>Physiological Function</td>
</tr>
<tr class="even">
<td>Bodily Location</td>
<td>location-of</td>
<td>Biologic Function</td>
</tr>
<tr class="odd">
<td>Anatomical Structure</td>
<td>part-of</td>
<td>Organism</td>
</tr>
<tr class="even">
<td>Pharmacologic Substance</td>
<td>causes</td>
<td>Pathological Function</td>
</tr>
<tr class="odd">
<td>Pharmacologic Substance</td>
<td>treats</td>
<td>Pathologic Function</td>
</tr>
</tbody>
</table>
<p>给出这样一个医学句子：</p>
<blockquote>
<p>(17.1) Doppler echocardiography can be used to diagnose left anterior descending artery stenosis in patients with type 2 diabetes</p>
<p>多普勒超声心动图可以用来诊断2型糖尿病患者的左前降支动脉狭窄</p>
</blockquote>
<p>我们可以提取UMLS关系：</p>
<blockquote>
<p>Echocardiography, Doppler Diagnoses Acquired stenosis</p>
<p>超声心动图，多普勒诊断获得性狭窄</p>
</blockquote>
<p>维基百科也提供了大量的关系，这些关系来自于<u>信息框</u>(infoboxes)，即与某些维基百科文章相关的结构化表格。例如，维基百科上斯坦福（Stanford）的信息框包括诸如state = "California"或president = "Marc Tessier-Lavigne"这样的结构化事实。这些事实可以转化为president-of或located-in，或转化为一种称为<u>RDF</u>（Resource Description Framework，资源描述框架）的元语言中的关系。一个RDF triple是实体-关系-实体构成的元组，被称为主-谓-宾（subject-predicate-object）表达式。下面是一个RDF三元组的实例：</p>
<table>
<thead>
<tr class="header">
<th>subject</th>
<th>predicate</th>
<th>object</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Golden Gate Park</td>
<td>location</td>
<td>San Francisco</td>
</tr>
</tbody>
</table>
<p>例如众包的DBpedia (Bizer et al., 2009) 是一个从维基百科衍生出来的本体，包含超过20亿个RDF三元组。另一个来自维基百科信息框的数据集Freebase (Bollacker et al., 2008)，现在是Wikidata的一部分 (Vrandeciˇ c and Kr ´ otzsch, 2014)，涵盖了人员和他们的国籍或位置以及他们出现在的其他位置之间的关系。WordNet 或其他本体提供了有用的本体关系，描述了词或概念之间的层级关系。例如WordNet在类之间有is-a或hypernym的关系：</p>
<blockquote>
<p>Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...</p>
<p>长颈鹿是反刍动物是蹄类动物是哺乳动物是脊椎动物</p>
</blockquote>
<p>WordNet也有个体和类之间的Instance-of关系，例如San Francisco就和city处于Instance-of关系。提取这些关系是扩展或构建本体的重要步骤。</p>
<p>最后，有一些大型的数据集，其中包含了手动标注的句子和它们的关系，用于训练和测试关系提取器。TACRED数据集 (Zhang et al., 2017) 包含106,264个关于特定人或组织的关系三元组的例子，在来自年度TAC知识库人口（TAC KBP）挑战赛的新闻和网络文本的句子中标记得来。TACRED包含41种关系类型（如per:出生城市、org:子公司、org:成员、per:配偶），以及一个无关系标签；如图17.4所示。大约80%的例子都被标注为无关系；拥有足够的负数据对于训练监督分类器非常重要。</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="header">
<th>Example</th>
<th>Entity Types Label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Carey will succeed <strong>Cathleen P. Black</strong>, who held the position for 15 years and will take on a new role as <u>chairwoman</u> of Hearst Magazines, the company said.</td>
<td><strong>PERSON</strong>/<u>TITLE</u><br/>Relation: <em>per:title</em></td>
</tr>
<tr class="even">
<td><strong>Irene Morgan Kirkaldy</strong>, who was born and reared in <u>Baltimore</u>, lived on Long Island and ran a child-care center in Queens with her second husband, Stanley Kirkaldy.</td>
<td><strong>PERSON</strong>/<u>CITY</u><br/>Relation: <em>per:city of birth</em></td>
</tr>
<tr class="odd">
<td><strong>Baldwin</strong> declined further comment, and said JetBlue chief <u>executive</u> Dave Barger was unavailable.</td>
<td>Types: <strong>PERSON</strong>/<u>TITLE</u><br/>Relation: <em>no relation</em></td>
</tr>
</tbody>
</table>
<p>SemEval 2010任务8也推出了一个标准数据集，检测命名词（nominal）之间的关系 (Hendrickx et al., 2009)。该数据集有10,717个例子，每个例子都有一个命名词对（未分类），手工标注为9个定向关系之一，比如产品-生产者<em>product-producer</em> (一家工厂生产西装 <em>a factory manufactures suits</em>) ，或者成分-整体<em>component-whole</em> (我的公寓有一个大厨房 <em>my apartment has a large kitchen</em>)。</p>
<hr />
<p><strong>本章剩余内容见：<a href="http://nlpcourse.cn/information-retrieval-2/">《自然语言处理综论》第17章-信息抽取（中）</a></strong></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（下）</title>
    <url>/information-retrieval-3/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<h1 id="抽取时间">17.3 抽取时间</h1>
<p>时间和日期是一种特别重要的命名实体，它在自动问答、日历和私人助理应用中起着重要的作用。为了对时间和日期进行推理，在我们提取了这些时间表达式后，必须对它们进行归一化处理——将其转换为标准格式，这样我们才能对它们进行推理。在本节中，我们将同时考虑时间表达式的提取和归一化。</p>
<span id="more"></span>
<h2 id="时间表达式的提取">17.3.1 时间表达式的提取</h2>
<p>时间表达式是指绝对时间点、相对时间、持续时间以及这些的集合。<strong>绝对</strong>（absolute）时间表达式是指那些可以直接相对映射到日历日期、一天中的时间或两者都有的表达式。<strong>相对</strong>（Relative）时间表达式通过其他一些参考点映射到特定的时间（如从上周二开始一周的持续时间）。最后，<strong>持续时间</strong>（durations）表示不同粒度的时间跨度（秒、分、天、周、世纪等）。图17.11列出了这些类别中的一些时间表达式样本。</p>
<table>
<caption>Figure 17.11 Examples of absolute, relational and durational temporal expressions.</caption>
<thead>
<tr class="header">
<th>Absolute</th>
<th>Relative</th>
<th>Durations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>April 24, 1916</td>
<td>yesterday</td>
<td>four hours</td>
</tr>
<tr class="even">
<td>The summer of ’77</td>
<td>next semester</td>
<td>three weeks</td>
</tr>
<tr class="odd">
<td>10:15 AM</td>
<td>two weeks from yesterday</td>
<td>six days</td>
</tr>
<tr class="even">
<td>The 3rd quarter of 2006</td>
<td>last quarter</td>
<td>the last three quarters</td>
</tr>
</tbody>
</table>
<p>时间表达式是以时间词汇触发器<u>lexical triggers</u>为中心的语法结构。词汇触发器可以是名词、专有名词、形容词和副词；完整的时间表达式由它们的短语投射组成：名词短语、形容词短语和副词短语。图17.12提供了示例。</p>
<table>
<caption>Figure 17.12 Examples of temporal lexical triggers.</caption>
<thead>
<tr class="header">
<th>Category</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Noun</td>
<td>morning, noon, night, winter, dusk, dawn</td>
</tr>
<tr class="even">
<td>Proper Noun</td>
<td>January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet</td>
</tr>
<tr class="odd">
<td>Adjective</td>
<td>recent, past, annual, former</td>
</tr>
<tr class="even">
<td>Adverb</td>
<td>hourly, daily, monthly, yearly</td>
</tr>
</tbody>
</table>
<p>让我们看看TimeML标注方案，其中时间表达式用XML标签、TIMEX3和该标签的各种属性进行标注（Pustejovsky et al.2005，Ferro et al.2005）。下面的示例说明了此方案的基本用法（我们将对属性的讨论推迟到第17.3.2节）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A fare increase initiated &lt;TIMEX3&gt;last week&lt;&#x2F;TIMEX3&gt; by UAL Corp’s United Airlines was matched by competitors over &lt;TIMEX3&gt;the weekend&lt;&#x2F;TIMEX3&gt;, marking the second successful fare increase in &lt;TIMEX3&gt;two weeks&lt;&#x2F;TIMEX3&gt;.</span><br></pre></td></tr></table></figure>
<p>时间表达式识别任务包括查找与这些时间表达式对应的所有文本跨度的开始和结束。基于规则的时间表达式识别方法使用级联自动机来识别复杂度不断提高的模式。首先标记词性，然后根据包含触发词（如二月）或类（如月份）的模式，从前一阶段的结果中识别出越来越大的语块。图17.13给出了一个基于规则的系统的片段。</p>
<figure class="highlight perl"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yesterday/today/tomorrow</span></span><br><span class="line">$string =˜ s/((($OT+the$CT+\s+)?$OT+day$CT+\s+$OT+(before|after)$CT+\s+)?$OT+$TERelDayExpr$CT+</span><br><span class="line">(\s+$OT+(morning|afternoon|evening|night)$CT+)?)/&lt;TIMEX$tever TYPE=\<span class="string">&quot;DATE\&quot;&gt;$1</span></span><br><span class="line"><span class="string">&lt;\/TIMEX$tever&gt;/gio;</span></span><br><span class="line"><span class="string">$string =˜ s/($OT+\w+$CT+\s+)&lt;TIMEX$tever TYPE=\&quot;DATE\&quot;[ˆ&gt;]*&gt;($OT+(Today|Tonight)$CT+)</span></span><br><span class="line"><span class="string">&lt;\/TIMEX$tever&gt;/$1$4/gso;</span></span><br><span class="line"><span class="string"># this (morning/afternoon/evening)</span></span><br><span class="line"><span class="string">$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+this$CT+\s*$OT+(morning|afternoon|evening)$CT+)/</span></span><br><span class="line"><span class="string">&lt;TIMEX$tever TYPE=\&quot;DATE\&quot;&gt;$1&lt;\/TIMEX$tever&gt;/gosi;</span></span><br><span class="line"><span class="string">$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+last$CT+\s*$OT+night$CT+)/&lt;TIMEX$tever</span></span><br><span class="line"><span class="string">TYPE=\&quot;DATE\&quot;&gt;$1&lt;\/TIMEX$tever&gt;/gsio;</span></span><br></pre></td></tr></table></figure>
<p>: Figure 17.13 Perl fragment from the GUTime temporal tagging system in Tarsqi (Verhagen et al., 2005).</p>
<p>序列标记方法遵循与命名实体标记相同的IOB方案，使用I、O和B标签标记TIMEX3分隔的时间表达式的内部、外部或开头的单词，如下所示：</p>
<p>A O fare O increase O initiated O last B week I by O UAL O Corp’s... O</p>
<p>从标记（token）及其上下文中提取特征，并训练统计序列标注器（可以使用任何序列模型）。图17.14列出了时间标注中使用的标准特征。时间表达式识别器使用通常的recall、precision和F-measure进行评估。所有这些非常词汇化的方法的一个主要困难是避免触发假阳性的表达式：</p>
<blockquote>
<p>(17.15) 1984 tells the story of Winston Smith...</p>
<p>(17.16) ...U2’s classic Sunday Bloody Sunday</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>Feature</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Token</td>
<td>The target token to be labeled</td>
</tr>
<tr class="even">
<td>Tokens in window</td>
<td>Bag of tokens in the window around a target</td>
</tr>
<tr class="odd">
<td>Shape</td>
<td>Character shape features</td>
</tr>
<tr class="even">
<td>POS</td>
<td>Parts of speech of target and window words</td>
</tr>
<tr class="odd">
<td>Chunk tags</td>
<td>Base phrase chunk tag for target and words in a window</td>
</tr>
<tr class="even">
<td>Lexical triggers</td>
<td>Presence in a list of temporal terms</td>
</tr>
</tbody>
</table>
<hr />
<p>: Figure 17.14 Typical features used to train IOB-style temporal expression taggers.</p>
<h2 id="时间归一化">17.3.2 时间归一化</h2>
<p>时间归一化是将时间表达式映射到特定时间点或持续时间的时间规范化的过程。时间点与日历日期、一天中的时间或两者都对应。持续时间主要由时间长度组成，但也可能包括关于起点和终点的信息。标准化时间用ISO 8601标准中的值属性表示，该标准用于编码时间值（ISO86012004）。图17.15再现了我们前面的示例，其中添加了值属性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;TIMEX3 i d &#x3D; ’ ’ t 1 ’ ’ t y p e &#x3D;”DATE” v a l u e &#x3D;” 2007 −07 −02 ” f u n cti o n I n D o c u m e nt &#x3D;”CREATION TIME”</span><br><span class="line">&gt; J u l y 2 , 2007 &lt;&#x2F;TIMEX3&gt; A f a r e i n c r e a s e i n i t i a t e d &lt;TIMEX3 i d &#x3D;” t 2 ” t y p e &#x3D;”DATE”</span><br><span class="line">v a l u e &#x3D;” 2007 −W26” a nc h o rTime ID &#x3D;” t 1 ”&gt;l a s t week&lt;&#x2F;TIMEX3&gt; by U nit e d A i r l i n e s was</span><br><span class="line">matc he d by c o m p e t i t o r s o v e r &lt;TIMEX3 i d &#x3D;” t 3 ” t y p e &#x3D;”DURATION” v a l u e &#x3D;”P1WE”</span><br><span class="line">a nc h o rTime ID &#x3D;” t 1 ”&gt; t h e weekend &lt;&#x2F;TIMEX3&gt; , ma r ki n g t h e s e c o n d s u c c e s s f u l f a r e</span><br><span class="line">i n c r e a s e i n &lt;TIMEX3 i d &#x3D;” t 4 ” t y p e &#x3D;”DURATION” v a l u e &#x3D;”P2W” a nc h o rTime ID &#x3D;” t 1 ”&gt; two</span><br><span class="line">weeks &lt;&#x2F;TIMEX3&gt;.</span><br></pre></td></tr></table></figure>
<p>: Figure 17.15 TimeML markup including normalized values for temporal expressions.</p>
<p>此文本的日期行或文档日期为2007年7月2日。这种表达式的ISO表示为YYYY-MM-DD，在本例中为2007-07-02。我们示例文本中的时间表达式的编码都从这个日期开始，这里显示为VALUE属性的值。</p>
<p>正文中的第一个时间表达式是指一年中的某个特定星期。在ISO标准中，周数从01到53，一年中的第一周是一年中第一个星期四。这些周用模板yyyywnn表示。我们的文件日期的ISO周是第27周；因此上周的值表示为 “2007-W26”。</p>
<p>下一个时间表达式是周末。ISO周从周一开始；因此，周末发生在一周的末尾，并且完全包含在一周内。周末被视为持续时间，因此value属性的值必须是一个长度。持续时间根据模式Pnx表示，其中n是表示长度的整数，x表示单位，如P3Y表示三年，P2D表示两天。在本例中，一个周末被捕获为P1WE。在这种情况下，也有足够的信息锚定这个特定的周末作为一个特定的一周的一部分。这些信息编码在ANCHORTIMEID属性中。最后，短语two weeks还表示作为P2W捕获的持续时间。图17.16描述了表示其他时间和持续时间的一些基本方法。更多细节请参考ISO8601（2004）、Ferro等人（2005）和Pustejovsky等人（2005）。当前大多数时间标准化方法都是基于规则的（Chang和Manning 2012，Strotgen和Gertz 2013）。匹配时态表达式的模式与语义分析过程相关联。就像构图一样</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>信息抽取技术综述</title>
    <url>/information-retrieval/</url>
    <content><![CDATA[<p>信息抽取是指从非结构化或半结构化文本中寻找结构化信息的任务。它是文本挖掘的一项重要任务，在自然语言处理、信息检索和网络挖掘等各个领域得到了广泛的研究。 <span id="more"></span> <strong>信息抽取(Information Extraction, IE)的两个基本任务：</strong></p>
<ul>
<li><p><strong>命名实体识别</strong>：识别实体的名称，如人、组织和地点。</p></li>
<li><p><strong>关系抽取</strong>：提取实体之间的语义关系，如FounderOf和HeadquarteredIn。</p></li>
</ul>
<p>本文，我们将对过去几十年命名实体识别和关系抽取方面的主要工作进行综述。</p>
<h1 id="信息抽取系统">信息抽取系统</h1>
<p>命名实体识别、指代消歧和关系抽取等任务，是成熟的、特定领域的信息提取系统的基本支持组件。</p>
<p>例如，给定下面的英文句子，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In 1998, Larry Page and Sergey Brin founded Google Inc.</span><br><span class="line">1998年，Larry Page和Sergey Brin创立了Google公司。</span><br></pre></td></tr></table></figure>
<p>我们可以提取以下信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">FounderOf(Larry Page, Google Inc.),</span><br><span class="line">FounderOf(Sergey Brin, Google Inc.),</span><br><span class="line">FoundedIn(Google Inc., <span class="number">1998</span>).</span><br></pre></td></tr></table></figure>
<p>提取的信息可以被其他计算机系统（如搜索引擎和数据库管理系统）利用，为最终用户提供更好的服务。具体提取的信息类型和结构取决于特定应用的需要。下面给出一些信息提取的应用实例：</p>
<blockquote>
<ul>
<li>生物医学研究人员经常需要从大量的科学出版物中筛选出与特定基因、蛋白质或其他生物医学实体相关的发现。为了协助这项工作，基于关键词匹配的简单搜索可能并不足够，因为生物医学实体通常具有同义词和模糊的名称，因此很难准确地检索相关文档。因此，生物医学文献挖掘的一项关键任务是从文本中自动识别生物医学实体的提及，并将其与现有知识库（如FlyBase）中的相应条目链接起来。</li>
<li>金融专业人员经常需要从新闻文章中寻找特定的信息，以帮助他们进行日常决策。例如，一家金融公司可能需要知道在某个时间跨度内发生的所有公司收购，以及每次收购的细节。从文本中自动查找此类信息需要标准的信息提取技术，如命名实体识别和关系提取。</li>
<li>情报分析员审查大量文本，以搜索参与恐怖主义事件的人员、使用的武器和攻击目标等信息。虽然信息检索技术可以用来快速查找描述恐怖主义事件的文件，但需要信息提取技术来进一步确定这些文件中的具体信息单元。</li>
<li>随着网络的快速发展，搜索引擎已经成为人们日常生活中不可缺少的一部分，现在用户的搜索行为也更加了解。基于文档的词袋表示的搜索已经不能提供满意的结果。更高级的搜索问题，如实体搜索、结构化搜索和问题解答等，可以为用户提供更好的搜索体验。为了方便这些搜索功能，通常需要将信息提取作为一个预处理步骤，以丰富文档表示或填充基础数据库。</li>
</ul>
</blockquote>
<p>早期的信息提取系统，如参与MUCs的系统，通常是基于规则的系统（如[32，42]）。它们使用人类开发的语言提取模式来匹配文本和定位信息单元。它们可以在特定的目标域上取得很好的性能，但是设计好的提取规则需要耗费大量的人力，而且开发的规则对领域的依赖性很强。意识到这些人工开发系统的局限性，研究人员转而采用统计机器学习的方法。而随着信息提取系统被分解为命名实体识别等组件，许多信息提取子任务可以转化为分类问题，这些问题可以通过标准的监督学习算法，如支持向量机和最大熵模型来解决。由于信息提取涉及到识别扮演不同角色的文本片段，序列标签方法，如隐藏马尔科夫模型和条件随机场也得到了广泛的应用。</p>
<p>在本章中，我们将重点关注信息提取中最基本的两个任务，即命名实体识别和关系提取。这两个任务的最先进的解决方案都依赖于统计机器学习方法。我们还讨论了传统上没有引起太多关注的无监督信息提取。本章的其余部分组织如下。第2节讨论了当前命名实体识别的方法，包括基于规则的方法和统计学习方法。第3节讨论了完全监督环境和弱监督环境下的关系提取。然后，我们在第4节讨论了无监督的关系发现和开放的信息提取。在第5节中，我们讨论了信息提取系统的评估。最后我们在第6节中总结。</p>
<h2 id="模板填充"><strong>模板填充</strong></h2>
<p>如图的恐怖主义模板中，左边是槽位子集，右边是槽位填充值。</p>
<p>其中一些槽位填充值，如"Enrique Ormazabal Ormazabal"和 "商人"是直接从文本中提取的，而其他的槽位填充值，如抢劫、完成、枪支等则是根据文档从对应槽位的预定义值集中选择的。</p>
<p><img width=500 src="https://i.loli.net/2021/03/09/An4ryIzRKLDSTsg.png"/></p>
<h1 id="有监督的方法">有监督的方法</h1>
<p>传统的信息提取任务对提取信息的结构有明确的定义，例如命名实体的类型、关系的类型、或者模板槽。在某些场景下，我们事先并不知道我们想要提取的信息结构，而希望从大型语料库中挖掘这样的结构。例如，从一组地震新闻文章中，我们可能希望自动发现地震的日期、时间、震中、震级和伤亡是新闻文章中报道的最重要的信息。</p>
<p>最近已经有一些关于这类无监督信息提取问题的研究，但总体上沿着这个方向的工作仍然有限。另一个新的方向是开放信息提取，系统要从Web这样一个庞大的、多样化的语料库中提取所有有用的实体关系。这种系统的输出不仅包括关系中涉及的论据，还包括从文本中提取的关系的描述。最近在这个方向上取得的进展包括TextRunner[6]、Woe[66]和ReVerb[29]等系统。</p>
<h1 id="无监督的方法">无监督的方法</h1>
<h2 id="现存系统">现存系统</h2>
<p>TextRunner</p>
<p>Woe</p>
<p>ReVerb</p>
<h2 id="术语解释">术语解释</h2>
<p>国防高级研究计划局, Defense Advanced Research Projects Agency, DARPA</p>
<p>消息理解会议, Message Understanding Conferences, MUC (DARPA发起并资助)</p>
<p>DeJong的FRUMP计划</p>
<h2 id="监督学习算法">监督学习算法</h2>
<h2 id="序列标注方法">序列标注方法</h2>
<h1 id="命名实体识别">命名实体识别</h1>
<p><strong>命名实体</strong>：指代某种现实世界实体的词语序列，例如“California,” “Steve Jobs” and “Apple Inc.”。</p>
<p><strong>命名实体识别（NER）</strong>：从自由形式的文本中识别出命名实体，并将其分类为一组预定义的类型，如人名、组织和地点。</p>
<p>命名实体识别是信息提取中最基本的任务。关系和事件提取等更复杂的任务，需要准确的命名实体识别作为基础。</p>
<p>通常情况下，这项任务不能简单地通过与预先编译的地名录进行字符串匹配来完成，因为给定实体类型的命名实体通常不会形成一个封闭的集合，任何地名录都是不完整的。另一个原因是，命名实体的类型可能取决于上下文。例如，"JFK "可能指的是 "John F. Kennedy"这个人或者"JFK International Airport "这个地点，或任何其他具有相同缩写的实体。为了确定在特定文档中出现的 "JFK "的实体类型，必须考虑其上下文。</p>
<p>NER已有多个评估项目，包括自动内容提取(ACE)项目、2002年和2003年自然语言学习会议(CoNLL)的共享任务[63]、BioCreAtIvE(Critical Assessment of Information Extraction Systems in Biology)挑战评估[2]。</p>
<p>最常研究的命名实体类型是人名、组织和地点，这是由MUC-6首次定义的。这些类型足够通用，对许多应用领域都有用。日期、时间、货币值和百分比等表达式的提取，也是由MUC-6引入的，通常也是在NER下研究的，尽管严格来说这些表达式不是命名实体。除了这些一般的实体类型外，其他类型的实体通常是针对特定领域和应用而定义的。例如，GENIA语料库使用细粒度的本体对生物实体进行分类[52]。在在线搜索和广告中，产品名称的提取是一项有用的任务。</p>
<p>参考文献：</p>
<p>https://zhuanlan.zhihu.com/p/266056681</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>信息论知识与资源汇总</title>
    <url>/information-theory-1/</url>
    <content><![CDATA[<h1 id="信息">信息</h1>
<p>信息是事物运动状态或存在方式不确定性的描述。不确定性大小 <span class="math inline">\(f(p(x))\)</span> 应该满足以下3个条件：</p>
<ol type="1">
<li><p><span class="math inline">\(f(1)=0\)</span></p></li>
<li><p><span class="math inline">\(f(p(x))\)</span> 是单调减函数</p></li>
<li><p>独立可加性，独立事件应具有增量的信息：<span class="math inline">\(f(p(x) p(y))=f(p(x))+f(p(y))\)</span></p></li>
</ol>
<p>因此，我们定义一个事件 X = x 的 <strong>自信息(self-information)</strong> 为： <span class="math display">\[
I(x) = -logP(x)
\]</span></p>
<p>使用底数为 2 的对数，单位是 <strong>比特(bit)</strong> 或者 <strong>香农(shannons)</strong>。</p>
<p>我们主要使用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。我们可以用 <strong>香农熵(Shannon entropy)</strong> 来对整个概率分布中的不确定性总量进行量化:</p>
<p><span class="math display">\[
H(x) = E_{x \sim P}[I(x)] = -E_{x \sim P}[logP(x)]
\]</span></p>
<p>二值随机变量的熵由 $(p − 1) log(1 − p) − p log p $给出。</p>
<span id="more"></span>
<h2 id="课程">课程</h2>
<p>信息论与编码理论：https://www.icourse163.org/course/XIDIAN-1002199004</p>
<h2 id="书目">书目</h2>
<p>《信息简史》</p>
<p>《信息论与编码理论》</p>
<h2 id="参考">参考</h2>
<p>https://blog.csdn.net/lyxleft/article/details/84867306</p>
<p>https://www.icourse163.org/learn/XDU-1002199004?tid=1463141462#/learn/content?type=detail&amp;id=1240328941&amp;sm=1</p>
]]></content>
      <categories>
        <category>数学</category>
        <category>信息论</category>
      </categories>
      <tags>
        <tag>information theory</tag>
      </tags>
  </entry>
  <entry>
    <title>信息抽取-Pilot Study</title>
    <url>/ir-project-0/</url>
    <content><![CDATA[<p>这个周末要把项目雏形做出来，在这里记录实验全过程、用到的资料和数据等，代码我放到了github。</p>
<h1 id="第一步建立学术文献语料库">第一步：建立学术文献语料库</h1>
<h2 id="下载pdf文件">1.1 下载pdf文件</h2>
<p>在pilot study中我们只选取ACL中前100篇top cited papers，获取文章列表及pdf的<a href="https://github.com/MissFreak/academic-information-retrieval/blob/main/corpus/get-papers.py">完整代码</a>。</p>
<table>
<colgroup>
<col style="width: 88%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="header">
<th>title</th>
<th>citation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Neural Machine Translation of Rare Words with Subword Units</td>
<td>3461</td>
</tr>
<tr class="even">
<td>Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.</td>
<td>2405</td>
</tr>
<tr class="odd">
<td>End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.</td>
<td>1823</td>
</tr>
<tr class="even">
<td>Get To The Point: Summarization with Pointer-Generator Networks.</td>
<td>1351</td>
</tr>
<tr class="odd">
<td>OpenNMT: Open-Source Toolkit for Neural Machine Translation.</td>
<td>1286</td>
</tr>
</tbody>
</table>
<span id="more"></span>
<h2 id="格式转化与数据清洗">1.2 格式转化与数据清洗</h2>
<p>把pdf转化为txt，去掉hyphen和newline。</p>
<p>然后切分句子，把这些句子保存在csv中。</p>
<h1 id="第二步抽取术语">第二步：抽取术语</h1>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>爬虫构建语料库的方法</title>
    <url>/ir-project-1/</url>
    <content><![CDATA[<blockquote>
<p>Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。</p>
<p>Scrapy 常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
<p>通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。</p>
</blockquote>
<p>由于<code>scrapy</code>在结构化数据提取中的重要作用，我将用它实现一个功能全面的学术文章信息采集的工具，并搭建为语料库。事实上，整套程序完全可以利用requests、beautifulsoup等实现（我会提供相关代码），但是利用scrapy框架会更高效、简洁，不用重复造轮子。而且，对于下载失败的URL，Scrapy也会重新下载。</p>
<p>预计2~3天完成这个项目，除了用于毕业论文的语料库，还将帮助我更好地巩固Python基础。2021年暑假，我将花大量时间精力用于开展毕业论文相关的项目，并研究算法，预计暑假结束前完成数学模型的搭建。这是我的第一个项目，所有项目都在<code>pycharm</code>进行（<a href="https://zhuanlan.zhihu.com/p/36147819">快捷键</a>）。</p>
<p>关于scrapy的详细介绍和架构图见<a href="https://www.runoob.com/w3cnote/scrapy-detail.html">菜鸟教程</a>。我也会按照这个教程描述的步骤完成整个项目。</p>
<h1 id="一新建项目">一、新建项目</h1>
<p>我们在pycharm的terminal中<code>pip install scrapy</code>并新建项目：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy startproject paperscraper</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/weixin_44322399/article/details/104439555">Bash指令：windows下使用tree指令快速查看文件目录树_10 DAY'S-CSDN博客</a></p>
<p>使用上述方式得到项目的文件结构，其中settings.py和spiders比较重要：</p>
<ul>
<li><strong>settings.py –</strong>该文件包含项目设置。</li>
<li><strong>Spiders/ –</strong>此文件夹将存储自定义spider，每次scrapy 运行一个spider，它都会在这个文件夹中寻找它。</li>
</ul>
<h2 id="如何防止反爬虫">1.1 如何防止反爬虫？</h2>
<p>由于可能出现403错误（无权限访问），如何避免爬虫被网站识别出来导致被禁呢？我们需要伪装浏览器，设置request headers，在settings.py中进行如下修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;User-Agent&#x27;</span>: ua.random</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同时要保证：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="创建paperbot-spider">1.2 创建paperbot spider</h2>
<p>我们使用基本模板在<strong>spiders/</strong>文件夹中创建一个新的spider“paperbot.py”：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy genspider paperbot www.sciencedirect.com&#x2F;journal&#x2F;journal-of-memory-and-language</span><br></pre></td></tr></table></figure>
<p>生成内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaperbotSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;paperbot&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.sciencedirect.com/journal/journal-of-memory-and-language&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.sciencedirect.com/journal/journal-of-memory-and-language/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li>allowed_domains是是过滤爬取的域名，限制爬虫爬取当前域名下的网页。</li>
<li>start_urls是起始爬取页面。</li>
</ul>
<h2 id="背后的逻辑">1.3 背后的逻辑</h2>
<p>这段代码其实是简化版的，我们可以看下面的代码实例来弄清楚背后的逻辑：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span>(<span class="params">self</span>):</span></span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">&#x27;http://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;http://quotes.toscrape.com/page/2/&#x27;</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        page = response.url.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">2</span>]</span><br><span class="line">        filename = <span class="string">f&#x27;quotes-<span class="subst">&#123;page&#125;</span>.html&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">f&#x27;Saved file <span class="subst">&#123;filename&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>start_requests()：必须返回一个请求的迭代器 (iterable) （您可以返回一个请求列表或编写一个生成器函数），爬行器将从中开始爬行。随后的请求将从这些初始请求依次生成。</li>
<li>parse()：将调用该方法来处理为每个请求下载的响应。response参数是TextResponse的一个实例，它保存页面内容，并有更多有用的方法来处理它。并查找要跟踪的新url并从中创建新请求（Request）。</li>
</ul>
<p>可以简化成：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;http://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://quotes.toscrape.com/page/2/&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        page = response.url.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">2</span>]</span><br><span class="line">        filename = <span class="string">f&#x27;quotes-<span class="subst">&#123;page&#125;</span>.html&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br></pre></td></tr></table></figure>
<h1 id="二明确目标">二、明确目标</h1>
<p>我们打算抓取https://www.sciencedirect.com/journal/journal-of-memory-and-language/issues期刊2010-2021年间的所有文章的题目、年份、doi、作者、Highlights、摘要和关键词（可能的话还有全文）。</p>
<ol type="1">
<li>打开 mySpider 目录下的 items.py。</li>
<li>创建一个 ItcastItem 类，定义类型为 scrapy.Field 的类属性</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaperscraperItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    year = scrapy.Field()</span><br><span class="line">    doi = scrapy.Field()</span><br><span class="line">    author = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>这个我们稍后修改，</p>
<h1 id="三搭建spider">三、搭建spider</h1>
<p>下面是项目的关键部分——spider的搭建，负责提取信息。</p>
<blockquote>
<p>Spider（爬虫）：它负责处理所有Responses，从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)。</p>
</blockquote>
<h2 id="使用scrapy-shell调试选择器">3.1 使用scrapy shell调试选择器</h2>
<p>Scrapy提取数据有自己的一套机制，被称做选择器（Selector类），它能够自由“选择”由XPath或CSS表达式指定的HTML文档的某些部分。</p>
<p>为了获得每期的文章ID，我们打开scrapy shell调试和验证选择器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scrapy shell</span><br><span class="line">fetch(<span class="string">&quot;https://www.sciencedirect.com/journal/journal-of-memory-and-language/vol/120/suppl/C&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在调试过程中，我们发现可以在该页面确定发表时间，并找到所有文章标题，由于标题可能存在内部标签，所以用正则表达式提取；以及作者（editorial board没有作者）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response.css(<span class="string">&#x27;.js-issue-status.text-s::text&#x27;</span>).get()</span><br><span class="line">response.css(<span class="string">&#x27;span.js-article-title&#x27;</span>).re(<span class="string">r&#x27;&lt;span[^&gt;]*&gt;([\s\S]*?)&lt;/span&gt;&#x27;</span>)</span><br><span class="line"><span class="comment"># &#x27;Revealing pragmatic processes through a one-word answer: When the French reply &lt;em&gt;Si&lt;/em&gt;&#x27;，</span></span><br><span class="line"><span class="comment"># 得到12条类似的标题，其中第一个Editorial Board要去掉</span></span><br><span class="line">response.css(<span class="string">&#x27;div.u-clr-grey8.text-s::text&#x27;</span>).getall()</span><br></pre></td></tr></table></figure>
<p>但要模仿浏览器点击才能获取摘要，因此我们换种方式，首先获取每篇文章的doi，然后去每篇文章的页面爬取信息。 ## 3.2 爬虫逻辑</p>
<p>如果用doi来构建每篇文章的url，还是会出现版本不同时url规则不同的情况：</p>
<p><img src="https://i.loli.net/2021/07/02/l8Vqe4Uv2ZBMkAO.png"/></p>
<p>因此，我们发现最好的方式是模拟浏览器进行操作，不断爬取链接并下载内容。在这里我们重新创建一个<a href="https://www.jianshu.com/p/a6a08b4f7c04">crawlspider</a>，来抓取链接，我们取名为paperbot2。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy genspider -t crawl paperbot2 www.sciencedirect.com</span><br></pre></td></tr></table></figure>
<p>生成新的文件paperbot2.py，我们继续调试代码，这时可能出现403错误，<a href="https://blog.csdn.net/u011781521/article/details/70211474">我们使用这篇文章的第二个办法</a>，把scrapy package里面的user-agent换掉。 ## 3.3 代码</p>
<p>在paperbot2.py写入如下内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> paperscraper.items <span class="keyword">import</span> AbstractItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Paperbot2Spider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;paperbot2&#x27;</span></span><br><span class="line">    issue_urls = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">70</span>, <span class="number">121</span>):</span><br><span class="line">        issue_urls.append(<span class="string">&#x27;https://www.sciencedirect.com/journal/journal-of-memory-and-language/vol/&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;/suppl/C&#x27;</span>)</span><br><span class="line">    start_urls = issue_urls</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(restrict_css=<span class="string">&#x27;.article-content-title&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        self.logger.info(<span class="string">&#x27;Hi, this is an item page! %s&#x27;</span>, response.url)</span><br><span class="line">        item = AbstractItem()</span><br><span class="line">        item[<span class="string">&#x27;title&#x27;</span>] = response.css(<span class="string">&#x27;.title-text&#x27;</span>).re(<span class="string">r&#x27;&lt;span[^&gt;]*&gt;([\s\S]*?)&lt;/span&gt;&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">&#x27;year&#x27;</span>] = response.css(<span class="string">&#x27;.publication-volume .text-xs&#x27;</span>).re(<span class="string">r&#x27;[^\d](\d&#123;4&#125;)[^\d]&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">&#x27;link&#x27;</span>] = response.url</span><br><span class="line">        item[<span class="string">&#x27;highlights&#x27;</span>] = <span class="string">&#x27;\n&#x27;</span>.join(response.css(<span class="string">&#x27;.author-highlights .list p::text&#x27;</span>).getall()) <span class="comment"># 这里用\n分割每一句话</span></span><br><span class="line">        item[<span class="string">&#x27;abstract&#x27;</span>] = <span class="string">&#x27;\n&#x27;</span>.join(response.css(<span class="string">&#x27;#abstracts p::text&#x27;</span>).getall())</span><br><span class="line">        item[<span class="string">&#x27;keywords&#x27;</span>] = <span class="string">&#x27;\n&#x27;</span>.join(response.css(<span class="string">&#x27;.keyword span::text&#x27;</span>).getall()</span><br><span class="line">)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>在settings.py中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BOT_NAME = <span class="string">&#x27;paperscraper&#x27;</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;paperscraper.spiders&#x27;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;paperscraper.spiders&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;User-Agent&#x27;</span>: ua.random</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在items.py：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AbstractItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    year = scrapy.Field()</span><br><span class="line">    link = scrapy.Field()</span><br><span class="line">    highlights = scrapy.Field()</span><br><span class="line">    abstract = scrapy.Field()</span><br><span class="line">    keywords = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>由于我们定义了两个items，需要定制写入不同CSV文件的<a href="https://blog.csdn.net/sc_lilei/article/details/79590696">Pipeline</a>，并且在setting中激活该pipeline。</p>
<p>参照<a href="https://zhuanlan.zhihu.com/p/58944212">这篇文章</a>，我们也可以直接在输出的时候写入csv：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy crawl paperbot2 -o jml.csv </span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p>https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/</p>
<p>https://www.bookstack.cn/read/piaosanlang-spiders/fbe165cd1d0be224.md</p>
<p>https://docs.scrapy.org/en/latest/intro/tutorial.html</p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>根据POS生成候选词</title>
    <url>/ir-project-2/</url>
    <content><![CDATA[<h1 id="候选词满足的要求">候选词满足的要求</h1>
<p>我们先查看keywords的特点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml-dataset.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">keyword_list = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df[<span class="string">&#x27;keywords&#x27;</span>]:</span><br><span class="line">    keyword_list.extend(<span class="built_in">str</span>(row).split(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line">sorted_keyword_list = Counter(keyword_list).most_common()</span><br><span class="line"></span><br><span class="line">gram_count = [<span class="built_in">len</span>(w.split()) <span class="keyword">for</span> w <span class="keyword">in</span> keyword_list]</span><br><span class="line">sorted_gram_count = Counter(gram_count).most_common()</span><br><span class="line"></span><br><span class="line">pos_tags = [<span class="string">&#x27; &#x27;</span>.join(tok.tag_ <span class="keyword">for</span> tok <span class="keyword">in</span> nlp(keyword[<span class="number">0</span>])) <span class="keyword">for</span> keyword <span class="keyword">in</span> sorted_keyword_list]</span><br><span class="line">sorted_pos_tags = Counter(pos_tags)</span><br><span class="line">print(sorted_pos_tags)</span><br></pre></td></tr></table></figure>
<p>然后把数据集分开保存为txt文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml-dataset.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    os.mkdir(<span class="string">r&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> FileExistsError <span class="keyword">as</span> error:</span><br><span class="line">    print(error)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> abstract <span class="keyword">in</span> df[<span class="string">&#x27;abstract&#x27;</span>]:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml/&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(abstract.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    i += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>从中抽取出pos满足规则的词组。要注意的是，这里的词组不影响后续的ranking 算法所以先把这一步做出来就行。要注意抽取的是meaningful的词组。相关的是text chunking (syntactic constituents), collocation extraction. 事实上这一步要做的就是分块，我们抽取出所有的名词词组！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml/0.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line">chunks = <span class="built_in">list</span>(doc.noun_chunks)</span><br><span class="line">print(chunks)</span><br><span class="line"><span class="comment"># [a focused element, a focus particle, recall, alternatives, Recall benefit, both inclusive (even) and exclusive (only) particles, Particles, the relevance, alternatives, discourse interpretation, Higher discourse relevance, improved memory encoding, the alternatives, Focus sensitive particles, the relevance, contextual alternatives, the interpretation, a sentence, Two experiments, better encoding, focus alternatives, Participants, auditory stimuli, a set, elements, three different versions, the critical sentences, the inclusive particle, no particle, (control condition, blocks, ten trials, participants, the elements, the context sentence, The results, both particles, memory performance, the alternatives, the focused element, the control condition, The results, the assumption, information-structural alternatives, memory, the presence, a focus sensitive particle]</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>用python处理json数据</title>
    <url>/json/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://baike.baidu.com/item/JSON">JSON</a>(<a href="https://baike.baidu.com/item/JavaScript">JavaScript</a> Object Notation, JS 对象简谱) 是一种轻量级的数据交换格式。它基于 <a href="https://baike.baidu.com/item/ECMAScript">ECMAScript</a> (欧洲计算机协会制定的js规范)的一个子集，采用完全独立于编程语言的文本格式来存储和表示数据。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。 易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。——百度百科</p>
</blockquote>
<p>编码和解码JSON数据的过程相当于把同一样东西翻译成中文和日语。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_str = &#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;example&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">18</span>&#125;</span><br><span class="line"><span class="comment"># 序列化json</span></span><br><span class="line">json_str = json.dumps(params, sort_keys=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 反序列化json</span></span><br><span class="line">dict_json = json.loads(json_str)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="序列化json">序列化JSON</h1>
<h2 id="将python对象转换为json">将Python对象转换为JSON</h2>
<table>
<thead>
<tr class="header">
<th>Python</th>
<th>JSON格式</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>dict</code></td>
<td><code>object</code></td>
</tr>
<tr class="even">
<td><code>list</code>， <code>tuple</code></td>
<td><code>array</code></td>
</tr>
<tr class="odd">
<td><code>str</code></td>
<td><code>string</code></td>
</tr>
<tr class="even">
<td><code>int</code>，<code>long</code>，<code>float</code></td>
<td><code>number</code></td>
</tr>
<tr class="odd">
<td><code>True</code></td>
<td><code>true</code></td>
</tr>
<tr class="even">
<td><code>False</code></td>
<td><code>false</code></td>
</tr>
<tr class="odd">
<td><code>None</code></td>
<td><code>null</code></td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;president&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Zaphod Beeblebrox&quot;</span>,</span><br><span class="line">        <span class="string">&quot;species&quot;</span>: <span class="string">&quot;Betelgeusian&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">type</span>(data)</span><br><span class="line"><span class="comment">#&lt;class &#x27;dict&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存为json文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data_file.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">    json.dump(data, write_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者将序列化的JSON数据写入python字符串对象</span></span><br><span class="line">json_string = json.dumps(data)</span><br><span class="line"><span class="comment"># &lt;class &#x27;str&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="一些有用的关键字参数">一些有用的关键字参数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json.dumps(data, indent=<span class="number">4</span>, sort_keys = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>indent定义缩进的级别；如果<em>sort_keys</em>为true，则字典的输出将按key排序。</p>
<h1 id="反序列化json读取json数据">反序列化JSON：读取JSON数据</h1>
<h2 id="将json编码的数据转换为python对象">将JSON编码的数据转换为Python对象</h2>
<table>
<thead>
<tr class="header">
<th>JSON</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>object</code></td>
<td><code>dict</code></td>
</tr>
<tr class="even">
<td><code>array</code></td>
<td><code>list</code></td>
</tr>
<tr class="odd">
<td><code>string</code></td>
<td><code>str</code></td>
</tr>
<tr class="even">
<td><code>number</code> (int)</td>
<td><code>int</code></td>
</tr>
<tr class="odd">
<td><code>number</code> (real)</td>
<td><code>float</code></td>
</tr>
<tr class="even">
<td><code>true</code></td>
<td><code>True</code></td>
</tr>
<tr class="odd">
<td><code>false</code></td>
<td><code>False</code></td>
</tr>
<tr class="even">
<td><code>null</code></td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<h2 id="从文件中读取">从文件中读取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data_file.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    data = json.load(read_file)</span><br><span class="line"><span class="built_in">type</span>(data)</span><br><span class="line"><span class="comment"># &lt;class &#x27;list&#x27;&gt;</span></span><br><span class="line"><span class="built_in">type</span>(data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="从字符串创建">从字符串创建</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_string = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    &quot;researcher&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;name&quot;: &quot;Ford Prefect&quot;,</span></span><br><span class="line"><span class="string">        &quot;species&quot;: &quot;Betelgeusian&quot;,</span></span><br><span class="line"><span class="string">        &quot;relatives&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                &quot;name&quot;: &quot;Zaphod Beeblebrox&quot;,</span></span><br><span class="line"><span class="string">                &quot;species&quot;: &quot;Betelgeusian&quot;</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">data = json.loads(json_string)</span><br></pre></td></tr></table></figure>
<h2 id="从api获取">从API获取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&quot;https://jsonplaceholder.typicode.com/todos&quot;</span>)</span><br><span class="line">todos = json.loads(response.text)</span><br><span class="line">todos == response.json()</span><br></pre></td></tr></table></figure>
<p>这里的伪json数据适合用来练习。</p>
<h1 id="遍历json字符串">遍历JSON字符串</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">items = data.items()</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> items:</span><br><span class="line">    print(<span class="built_in">str</span>(key) + <span class="string">&#x27;=&#x27;</span> + <span class="built_in">str</span>(value))</span><br></pre></td></tr></table></figure>
<h1 id="从网上下载的json文件">从网上下载的json文件</h1>
<p>有时候网上的json文件会有错误，比如某些字符串没有加双引号等等。我们要注意JSON中的名称-值对列表被花括号包裹，<strong>名称必须被双引号包裹，而值并不是总是需要被双引号包裹。当值是字符串时，必须使用双引号。</strong>而在json中，还有数字、布尔值（true false）、数组、对象、null等其他数据类型，而这些都不应该被双引号包裹。</p>
<p><strong>参考文献：</strong></p>
<p>《JSON必知必会》</p>
<p>https://www.jianshu.com/p/63dd4c77ad29</p>
<p>https://realpython.com/python-json/</p>
<p>https://blog.csdn.net/qq_33017925/article/details/87488200</p>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/links/</url>
    <content><![CDATA[<p>伪代码书写规范 https://www.cnblogs.com/betterrong/p/14316149.html</p>
<p>33种经典图表类型总结，轻松玩转数据可视化 https://www.jianshu.com/p/28c4b43c396d</p>
<p>数据可视化案例——力导向图，网络图，关系图（使用pyecharts，networkx，echarts，js）</p>
<p>https://www.cnblogs.com/caiyishuai/p/12323598.htmlpython</p>
<p>【绘制关系网络图】Gephi 入门使用</p>
<p>https://blog.csdn.net/qq_42374697/article/details/112849248</p>
<p>Python中带有NetworkX的字符共现网络图 http://andrewtrick.com/stormlight_network.html</p>
<p>https://rainynotes.net/co-occurrence-matrix-visualization/</p>
<p>https://www.manongdao.com/article-2031327.html</p>
]]></content>
  </entry>
  <entry>
    <title>机器学习驱动的语言评估</title>
    <url>/language-testing/</url>
    <content><![CDATA[<ul>
<li>一种快速创建language proficiency assessments的方法</li>
<li>并提供实验证据证明such tests can be valid, reliable, and secure</li>
<li>根据给定的标准归纳出proficiency scales</li>
<li>使用语言模型直接估计项目难度以进行计算机自适应测试computer-adaptive testing</li>
<li>这减轻了对人类受试者进行昂贵的pilot testing with human subjects。我们使用这些方法开发了一项名为 Duolingo 英语测试的在线能力考试，并证明其分数与其他high-stakes英语评估显着一致。此外，我们的方法产生highly reliable高度可靠的测试分数，同时生成足够大的项目库 item banks以满足安全要求security requirements。</li>
</ul>
<span id="more"></span>
<p>标准化测试的创建和维护很麻烦。莱恩等人。（<a href="javascript:;">2016 年</a>）和<em>教育和心理测试标准</em>（AERA 等，<a href="javascript:;">2014 年</a>）描述了规划、创建、修订、管理、分析和报告高风险测试及其开发的许多程序和要求。</p>
<p>项目数量过少 . 只有当项目库足够大以减少项目暴露时，这些安全优势才成立 (Way, <a href="javascript:;">1998</a> )。这进一步增加了项目编写者的负担，并且还需要大量的项目试点测试。</p>
<p>使用 ML/NLP 技术自动创建、评分和心理测量分析的 test item formats 测试项目格式。这解决了语言测试开发中的“冷启动”问题“cold start” ，通过放宽手动创建项目的要求并完全减少对人工试点测试的需求。</p>
<ul>
<li>语言测试和psychometrics 心理测量学 (§ <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00310/96485/Machine-Learning-Driven-Language-Assessment#sec1">2</a> ) 中的重要概念</li>
<li>ML/NLP 方法来学习单词 (§ <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00310/96485/Machine-Learning-Driven-Language-Assessment#sec4">3</a> ) 和长篇文章 (§ <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00310/96485/Machine-Learning-Driven-Language-Assessment#sec5">4</a> ) 的熟练度量表.</li>
<li>然后，我们使用 Duolingo English Test 的结果证明我们的方法的有效性、可靠性和安全性，这是一种使用这些方法开发的在线、可操作的英语水平评估 (§ <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00310/96485/Machine-Learning-Driven-Language-Assessment#sec9">5</a> )。</li>
</ul>
<h2 id="项目反应理论irt">项目反应理论（IRT）</h2>
<p>在心理测量学中，item response theory (IRT)是一种designing and scoring measures of ability and other cognitive variables的范式 (Lord, <a href="javascript:;">1980</a> )。IRT 构成了大多数现代高风险标准化测试的基础，并且通常假设：</p>
<ol type="1">
<li>考生对测试项目的反应由<em>项目反应函数 (IRF)</em>建模；</li>
<li>每个考生都有一个一维的潜在<em>能力</em>，记为<em>θ</em>；</li>
<li>测试项目是本地独立的。</li>
</ol>
<p>在这项工作中，我们使用了一个简单的逻辑 IRF，也称为<em>Rasch 模型</em>（Rasch，<a href="javascript:;">1993 年</a>）。这表示对测试项目<em>i</em>的正确响应概率correct response to test item<em>p</em> <em>i</em> ( <em>θ</em> )作为item <em>difficulty</em> parameter<em>δ</em> <em>i</em>和考生<em>能力</em>参数<em>θ</em> the examinee’s <em>ability</em> parameter之间差异的函数：</p>
<p>等式<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00310/96485/Machine-Learning-Driven-Language-Assessment#sec1">1</a>的响应模式<a href="javascript:;">如图 1</a>所示。如同大多数IRF，这<em>p<strong>我<em>（</em>θ<em>与受检能力）单调增加</em>θ<em>，并与项目的难度减小</em>δ</strong>我</em>。</p>
<p>Rasch 模型 IRF，显示了跨考生能力水平<em>θ</em>对三个测试项目难度<em>δ</em> <em>i</em>正确响应<em>p</em> <em>i</em> ( <em>θ</em> )的概率。</p>
<p>在典型的标准化测试开发中，首先创建项目，然后对人类受试者进行“试点测试”。这些试点测试产生了许多评分正确或不正确的 &lt;examinee, item&gt; 对，下一步是从这些评分中凭经验估计<em>θ</em>和<em>δ</em> <em>i</em>参数。读者可能认为 Rasch 模型等效于二元逻辑回归，用于预测考生是否会正确回答项目<em>i</em>（其中<em>θ</em>表示“考生特征”的权重，<em>-δ</em> <em>i</em>表示“项目特征”的权重，并且偏差/拦截权重为零）。一旦估计了参数，就可以丢弃导频种群的<em>θ</em> s，并且<em>δ</em> <em>i</em> s 用于估计未来考生的<em>θ</em>，最终决定他或她的考试成绩。</p>
<h1 id="测试">测试</h1>
<p>停止和测试 "的评估不能严格地评估学生对某一主题的理解。基于人工智能的评估为教师、学生和家长提供持续的反馈，让他们了解 学生如何学习，他们需要什么支持，以及他们在实现学习目标方面取得的进展。</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>语言测试</category>
      </categories>
  </entry>
  <entry>
    <title>英文文献的命名实体识别（上）</title>
    <url>/named-entity-recognition/</url>
    <content><![CDATA[<p>命名实体识别（NER）是指自动识别文本中的命名实体并将其分类为预定义的类别。实体可以是人员、组织、位置、时间、数量、货币价值、百分比等的名称。作为信息抽取的基础步骤，NER从非结构化文本中提取关键元素，因此它是一项至关重要的技术。</p>
<p>我们可以创建自己的实体类别以适应不同的任务。现在有许多出色的开源库，包括<a href="https://www.nltk.org/">NLTK</a>，<a href="https://spacy.io/">SpaCy</a>和<a href="https://nlp.stanford.edu/software/CRF-NER.shtml">Stanford NER</a>。如何用这些工具实现，参考<a href="https://www.kdnuggets.com/2018/08/named-entity-recognition-practitioners-guide-nlp-4.html">命名实体识别：NLP从业人员指南</a>和<a href="https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da">NLTK和SpaCy命名实体识别</a>。也可以参考<a href="https://monkeylearn.com/blog/named-entity-recognition/">这篇</a>文章。</p>
<span id="more"></span>
<h2 id="学术文献">学术文献</h2>
<p><a href="https://www.frontiersin.org/articles/10.3389/fcell.2020.00673/full">用于生物医学信息提取的命名实体识别和关系检测</a></p>
<p><a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3321-4">使用具有上下文信息的深度神经网络进行生物医学命名实体识别</a></p>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0885230815300504">用于命名实体识别工具的多领域评估框架</a></p>
<p><a href="https://devopedia.org/named-entity-recognition#Wang-et-al.-2019">命名实体识别</a></p>
<p><a href="https://paperswithcode.com/task/named-entity-recognition-ner">带代码的文献</a></p>
<h1 id="如何训练ner分类器">如何训练NER分类器</h1>
<p>通常使用BIO表示法，该表示法区分实体的开始（B）和内部（I），O标记非实体。NER往往需要特定领域的训练，尤其是更细粒度的NER。</p>
<h2 id="评估">评估</h2>
<p><em>F-Score</em>是用于评估NER的一种常用度量，它是Precision和Recall的组合。Precision, recall, and F-Score are defined as follows (<a href="https://www.frontiersin.org/articles/10.3389/fcell.2020.00673/full#B21">Campos et al., 2012</a>):</p>
<p><span class="math display">\[
\begin{array}{c}
\text { Precision }=\frac{\text { Relevant Names Recognized }}{\text { Total Names Recognized }} \\
=\frac{\text { True Positives }}{\text { True Positives+False Positives }} \\
\text { Recall }=\frac{\text { Relevant Names Recognized }}{\text { Relevant Names in Corpus }} \\
=\frac{\text { True Positives }}{\text { True Positives+False Negatives }} \\
\text { F-score }=2 \times \frac{\text { Precision } \times \text { Recall }}{\text { Precision+Recáll }}
\end{array}
\]</span></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>名词短语抽取方法汇总和比较</title>
    <url>/noun-chunks/</url>
    <content><![CDATA[<p>根据以往文献，有以下模式：</p>
<p><span class="math inline">\(((A|N)+|(A|N)∗(NP)?(A|N)∗)N\)</span></p>
<p><span class="math inline">\((A | N) ∗ N(P D ∗ (A | N) ∗ N)∗\)</span>​</p>
<span id="more"></span>
<h1 id="测试数据">测试数据</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\pythonProject\result\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line">doc = nlp(papers_dict[<span class="number">0</span>][<span class="string">&#x27;abstract&#x27;</span>])</span><br><span class="line"><span class="comment"># from spacy import displacy</span></span><br><span class="line"><span class="comment"># displacy.serve(doc, style=&quot;dep&quot;)</span></span><br></pre></td></tr></table></figure>
<p>加载candidates：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\pythonProject\result\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line">doc = papers_dict[<span class="number">0</span>][<span class="string">&#x27;abstract&#x27;</span>]</span><br><span class="line">print(doc)</span><br><span class="line">keyword = doc[lower:upper]</span><br></pre></td></tr></table></figure>
<h1 id="最终版">最终版</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time, json, re</span><br><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\pythonProject\result\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chunking</span>(<span class="params">paper</span>):</span></span><br><span class="line">    doc = nlp(paper[<span class="string">&#x27;abstract&#x27;</span>])</span><br><span class="line">    index = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">        list_of_deps = [<span class="string">&#x27;agent&#x27;</span>, <span class="string">&#x27;dobj&#x27;</span>, <span class="string">&#x27;pobj&#x27;</span>, <span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;nsubjpass&#x27;</span>, <span class="string">&#x27;csubj&#x27;</span>, <span class="string">&#x27;csubjpass&#x27;</span>, <span class="string">&#x27;conj&#x27;</span>, <span class="string">&#x27;attr&#x27;</span>, <span class="string">&#x27;ccomp&#x27;</span>, <span class="string">&#x27;pcomp&#x27;</span>, <span class="string">&#x27;xcomp&#x27;</span>, <span class="string">&#x27;appos&#x27;</span>, <span class="string">&#x27;dative&#x27;</span>, <span class="string">&#x27;dep&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> token.pos_ <span class="keyword">in</span> [<span class="string">&#x27;NOUN&#x27;</span>, <span class="string">&#x27;PROPN&#x27;</span>, <span class="string">&#x27;VBG&#x27;</span>] <span class="keyword">and</span> token.dep_ <span class="keyword">in</span> list_of_deps:</span><br><span class="line">            subtree_dict = &#123;child.text: child.i <span class="keyword">for</span> child <span class="keyword">in</span> token.subtree <span class="keyword">if</span> child.i &lt; token.i&#125;</span><br><span class="line">            upper = token.i + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> subtree_dict:</span><br><span class="line">                lower = <span class="built_in">min</span>(subtree_dict.values())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                lower = token.i</span><br><span class="line">            index.append((lower, upper))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> index:</span><br><span class="line">        new_index = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(index)):</span><br><span class="line">            <span class="keyword">if</span> index[i][<span class="number">0</span>] &gt;= index[i-<span class="number">1</span>][<span class="number">1</span>]:</span><br><span class="line">                new_index.append(index[i-<span class="number">1</span>])</span><br><span class="line">        new_index.append(index[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        index = new_index</span><br><span class="line"></span><br><span class="line">        text = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        tag_1 = <span class="string">&#x27;&lt;span class=&quot;noun&quot;&gt;&#x27;</span></span><br><span class="line">        tag_2 = <span class="string">&#x27;&lt;/span&gt;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(doc)):</span><br><span class="line">            <span class="keyword">if</span> index:</span><br><span class="line">                <span class="keyword">if</span> i == index[<span class="number">0</span>][<span class="number">0</span>]:</span><br><span class="line">                    text += tag_1</span><br><span class="line">                <span class="keyword">elif</span> i == index[<span class="number">0</span>][<span class="number">1</span>]:</span><br><span class="line">                    text += tag_2</span><br><span class="line">                    index.pop(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> doc[i].is_punct:</span><br><span class="line">                text += doc[i].text</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                text += <span class="string">&#x27; &#x27;</span> + doc[i].text</span><br><span class="line">        text = text.replace(<span class="string">&#x27;&lt;span class=&quot;noun&quot;&gt; &#x27;</span>, <span class="string">&#x27; &lt;span class=&quot;noun&quot;&gt;&#x27;</span>).replace(<span class="string">r&#x27;- &#x27;</span>, <span class="string">&#x27;-&#x27;</span>).replace(<span class="string">r&#x27; -&#x27;</span>, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import pickle</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    list_of_abstracts = []</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> executor.<span class="built_in">map</span>(chunking, papers_dict):</span><br><span class="line">            list_of_abstracts.append(text)</span><br><span class="line">    t2 = time.perf_counter()</span><br><span class="line">    print(<span class="string">f&#x27;Finished in <span class="subst">&#123;t2-t1&#125;</span> seconds&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    list_papers_dict = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(list_of_abstracts)):</span><br><span class="line">        new_papers_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;author&quot;</span> <span class="keyword">in</span> papers_dict[i]:</span><br><span class="line">            new_papers_dict[<span class="string">&#x27;author&#x27;</span>] = papers_dict[i][<span class="string">&#x27;author&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        new_papers_dict[<span class="string">&#x27;title&#x27;</span>] = papers_dict[i][<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">        new_papers_dict[<span class="string">&#x27;abstract&#x27;</span>] = list_of_abstracts[i]</span><br><span class="line">        list_papers_dict.append(new_papers_dict)</span><br><span class="line"></span><br><span class="line">    print(<span class="built_in">len</span>(list_papers_dict))</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\毕业论文\visualization\data\papers.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">        json.dump(list_papers_dict, write_file, indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="方法一nested-noun-phrases">方法一：Nested Noun Phrases</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keywords = []</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="comment"># 找到名词短语的head</span></span><br><span class="line">    <span class="keyword">if</span> token.pos_ == <span class="string">&#x27;NOUN&#x27;</span> <span class="keyword">and</span> token.dep_ <span class="keyword">in</span> [<span class="string">&#x27;dobj&#x27;</span>, <span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;pobj&#x27;</span>, <span class="string">&#x27;conj&#x27;</span>, <span class="string">&#x27;attr&#x27;</span>, <span class="string">&#x27;pcomp&#x27;</span>]:</span><br><span class="line">        subtree_list = []</span><br><span class="line">        <span class="comment"># 找到名词短语head的子树</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> token.subtree:</span><br><span class="line">            <span class="comment"># 名词短语head的子树中不能存在&quot;,&quot;</span></span><br><span class="line">            <span class="keyword">if</span> t.is_punct <span class="keyword">and</span> t.text != <span class="string">&quot;-&quot;</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            subtree_list.append(t)</span><br><span class="line"></span><br><span class="line">        subtree_list_new = <span class="string">&#x27; &#x27;</span>.join(t.text <span class="keyword">for</span> t <span class="keyword">in</span> subtree_list).replace(<span class="string">r&#x27; - &#x27;</span>, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">        keywords.append(subtree_list_new)</span><br><span class="line">print(keywords)</span><br><span class="line"><span class="comment"># 24个：[&#x27;a linguistic expression often used to communicate the opposite of what is said&#x27;, &#x27;the opposite of what is said&#x27;, &#x27;an intention to insult or ridicule&#x27;, &#x27;ridicule&#x27;, &#x27;Inherent ambiguity in sarcastic expressions&#x27;, &#x27;sarcastic expressions&#x27;, &#x27;sarcasm detection&#x27;, &#x27;this work&#x27;, &#x27;sarcasm&#x27;, &#x27;textual conversations&#x27;, &#x27;various social networking platforms and online media&#x27;, &#x27;online media&#x27;, &#x27;this end&#x27;, &#x27;an interpretable deep learning model using multi-head self-attention and gated recurrent units&#x27;, &#x27;multi-head self-attention and gated recurrent units&#x27;, &#x27;the effectiveness and interpretability of our approach&#x27;, &#x27;interpretability&#x27;, &#x27;our approach&#x27;, &#x27;the-art&#x27;, &#x27;state-of-the-art results on datasets from social networking platforms&#x27;, &#x27;datasets from social networking platforms&#x27;, &#x27;social networking platforms&#x27;, &#x27;online discussion forums&#x27;, &#x27;political dialogues&#x27;]</span></span><br></pre></td></tr></table></figure>
<h1 id="方法二noun-chunks">方法二：Noun Chunks</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">chunks = <span class="built_in">list</span>(doc.noun_chunks)</span><br><span class="line">print(chunks)</span><br><span class="line"><span class="comment"># 一共29个：[Sarcasm, a linguistic expression, the opposite, what, usually something, an intention, Inherent ambiguity, sarcastic expressions, sarcasm detection, this work, we, sarcasm, textual conversations, English, various social networking platforms, online media, this end, we, an interpretable deep learning model, multi-head self-attention and gated recurrent units, We, the effectiveness, interpretability, our approach, the-art, datasets, social networking platforms, online discussion forums, political dialogues]</span></span><br></pre></td></tr></table></figure>
<h1 id="方法三">方法三：</h1>
<p>根据<a href="https://www.cnblogs.com/ljhdo/p/5076717.html">这篇文章</a>提到的公理：</p>
<blockquote>
<p>如果A成分直接依存于B成分，而C成分在句中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分；</p>
</blockquote>
<p>也就是说每个名词短语都位于弧线内。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keywords = []</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="keyword">if</span> token.pos_ <span class="keyword">in</span> [<span class="string">&#x27;NOUN&#x27;</span>, <span class="string">&#x27;PROPN&#x27;</span>] <span class="keyword">and</span> token.dep_ <span class="keyword">in</span> [<span class="string">&#x27;dobj&#x27;</span>, <span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;pobj&#x27;</span>, <span class="string">&#x27;conj&#x27;</span>, <span class="string">&#x27;attr&#x27;</span>, <span class="string">&#x27;pcomp&#x27;</span>]:</span><br><span class="line">        subtree_dict = &#123;child.text: child.i <span class="keyword">for</span> child <span class="keyword">in</span> token.subtree <span class="keyword">if</span> child.i &lt; token.i&#125;</span><br><span class="line">        <span class="keyword">if</span> subtree_dict:</span><br><span class="line">            lower = <span class="built_in">min</span>(subtree_dict.values())</span><br><span class="line">            upper = token.i + <span class="number">1</span></span><br><span class="line">            keyword = doc[lower:upper]</span><br><span class="line">            print(keyword)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            keyword = token</span><br><span class="line">        keywords.append(keyword)</span><br><span class="line"></span><br><span class="line">print(keywords)</span><br><span class="line">print(<span class="built_in">len</span>(keywords)) <span class="comment"># 26</span></span><br><span class="line"><span class="comment"># 问题是有 and 的地方还要再处理</span></span><br><span class="line"><span class="comment"># [Sarcasm, a linguistic expression, the opposite, an intention, ridicule, Inherent ambiguity, sarcastic expressions, sarcasm detection, this work, sarcasm, textual conversations, English, various social networking platforms, online media, this end, an interpretable deep learning model, multi-head self-attention and gated recurrent units, the effectiveness, interpretability, our approach, the-art, state-of-the-art results, datasets, social networking platforms, online discussion forums, political dialogues]</span></span><br></pre></td></tr></table></figure>
<p>此时基本抽出的都是名词，但还需要：</p>
<ol type="1">
<li>抽取每个名词短语的模式，以便进行迭代筛选。</li>
<li>还要去掉每个名词短语中的stopwords：the, our, that, these, those等</li>
</ol>
<p>在这之前，我们先根据现在的结果可视化展示一下。我们根据下面的代码修改原来的abstracts：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_of_index = []</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="keyword">if</span> token.pos_ <span class="keyword">in</span> [<span class="string">&#x27;NOUN&#x27;</span>, <span class="string">&#x27;PROPN&#x27;</span>] <span class="keyword">and</span> token.dep_ <span class="keyword">in</span> [<span class="string">&#x27;dobj&#x27;</span>, <span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;pobj&#x27;</span>, <span class="string">&#x27;conj&#x27;</span>, <span class="string">&#x27;attr&#x27;</span>, <span class="string">&#x27;pcomp&#x27;</span>]:</span><br><span class="line">        subtree_dict = &#123;child.text: child.i <span class="keyword">for</span> child <span class="keyword">in</span> token.subtree <span class="keyword">if</span> child.i &lt; token.i&#125;</span><br><span class="line">        upper = token.i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> subtree_dict:</span><br><span class="line">            lower = <span class="built_in">min</span>(subtree_dict.values())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lower = token.i</span><br><span class="line">        list_of_index.append((lower, upper))</span><br><span class="line"></span><br><span class="line">print(list_of_index)</span><br></pre></td></tr></table></figure>
<h1 id="完整代码">完整代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time, json, re</span><br><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\pythonProject\result\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chunking</span>(<span class="params">paper</span>):</span></span><br><span class="line">    doc = nlp(paper[<span class="string">&#x27;abstract&#x27;</span>])</span><br><span class="line">    index = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">        <span class="keyword">if</span> token.pos_ <span class="keyword">in</span> [<span class="string">&#x27;NOUN&#x27;</span>, <span class="string">&#x27;PROPN&#x27;</span>] <span class="keyword">and</span> token.dep_ <span class="keyword">in</span> [<span class="string">&#x27;dobj&#x27;</span>, <span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;pobj&#x27;</span>, <span class="string">&#x27;conj&#x27;</span>, <span class="string">&#x27;attr&#x27;</span>, <span class="string">&#x27;pcomp&#x27;</span>]:</span><br><span class="line">            subtree_dict = &#123;child.text: child.i <span class="keyword">for</span> child <span class="keyword">in</span> token.subtree <span class="keyword">if</span> child.i &lt; token.i&#125;</span><br><span class="line">            upper = token.i + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> subtree_dict:</span><br><span class="line">                lower = <span class="built_in">min</span>(subtree_dict.values())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                lower = token.i</span><br><span class="line">            index.append((lower, upper))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> index:</span><br><span class="line">        new_index = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(index)):</span><br><span class="line">            <span class="keyword">if</span> index[i][<span class="number">0</span>] &gt; index[i-<span class="number">1</span>][<span class="number">1</span>]:</span><br><span class="line">                new_index.append(index[i-<span class="number">1</span>])</span><br><span class="line">        new_index.append(index[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        index = new_index</span><br><span class="line"></span><br><span class="line">        text = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        tag_1 = <span class="string">&#x27;&lt;span class=&quot;noun&quot;&gt;&#x27;</span></span><br><span class="line">        tag_2 = <span class="string">&#x27;&lt;/span&gt;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(doc)):</span><br><span class="line">            <span class="keyword">if</span> index:</span><br><span class="line">                <span class="keyword">if</span> i == index[<span class="number">0</span>][<span class="number">0</span>]:</span><br><span class="line">                    text += tag_1</span><br><span class="line">                <span class="keyword">elif</span> i == index[<span class="number">0</span>][<span class="number">1</span>]:</span><br><span class="line">                    text += tag_2</span><br><span class="line">                    index.pop(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> doc[i].is_punct:</span><br><span class="line">                text += doc[i].text</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                text += <span class="string">&#x27; &#x27;</span> + doc[i].text</span><br><span class="line">        text = text.replace(<span class="string">&#x27;&lt;span class=&quot;noun&quot;&gt; &#x27;</span>, <span class="string">&#x27; &lt;span class=&quot;noun&quot;&gt;&#x27;</span>).replace(<span class="string">r&#x27;- &#x27;</span>, <span class="string">&#x27;-&#x27;</span>).replace(<span class="string">r&#x27; -&#x27;</span>, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    list_of_abstracts = []</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> executor.<span class="built_in">map</span>(chunking, papers_dict):</span><br><span class="line">            list_of_abstracts.append(text)</span><br><span class="line">    fw = <span class="built_in">open</span>(<span class="string">&#x27;../result/list_of_abstracts_pickle&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">    pickle.dump(list_of_abstracts, fw)</span><br><span class="line">    fw.close()</span><br><span class="line">    t2 = time.perf_counter()</span><br><span class="line">    print(<span class="string">f&#x27;Finished in <span class="subst">&#123;t2-t1&#125;</span> seconds&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\pythonProject\result\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line">list_papers_dict = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(list_of_abstracts)):</span><br><span class="line">    new_papers_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;author&quot;</span> <span class="keyword">in</span> papers_dict[i]:</span><br><span class="line">        new_papers_dict[<span class="string">&#x27;author&#x27;</span>] = papers_dict[i][<span class="string">&#x27;author&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    new_papers_dict[<span class="string">&#x27;title&#x27;</span>] = papers_dict[i][<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">    new_papers_dict[<span class="string">&#x27;abstract&#x27;</span>] = list_of_abstracts[i]</span><br><span class="line">    list_papers_dict.append(new_papers_dict)</span><br><span class="line"></span><br><span class="line">print(<span class="built_in">len</span>(list_papers_dict))</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\毕业论文\visualization\data\papers.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">    json.dump(list_papers_dict, write_file, indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>但是还需要解决两种情况：</p>
<ol type="1">
<li><p>propoer noun</p></li>
<li><p>动名词</p>
<p>这两种情况还是可能遗漏，导致召回率不高。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_of_pos = [<span class="string">&#x27;NOUN&#x27;</span>, <span class="string">&#x27;PROPN&#x27;</span>]</span><br><span class="line">list_of_deps = [<span class="string">&#x27;agent&#x27;</span>, <span class="string">&#x27;dobj&#x27;</span>, <span class="string">&#x27;pobj&#x27;</span>, <span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;nsubjpass&#x27;</span>, <span class="string">&#x27;csubj&#x27;</span>, <span class="string">&#x27;csubjpass&#x27;</span>, <span class="string">&#x27;conj&#x27;</span>, <span class="string">&#x27;attr&#x27;</span>, <span class="string">&#x27;ccomp&#x27;</span>, <span class="string">&#x27;pcomp&#x27;</span>, <span class="string">&#x27;xcomp&#x27;</span>, <span class="string">&#x27;appos&#x27;</span>, <span class="string">&#x27;dative&#x27;</span>, <span class="string">&#x27;dep&#x27;</span>]</span><br></pre></td></tr></table></figure></p></li>
</ol>
<h2 id="错误收集">错误收集</h2>
<p>还有很多问题，我们通过观察pos diagram来解决：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\pythonProject\result\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_pos_diagram</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="comment">#对于每个摘要，把所有keywords的pattern保存在列表</span></span><br><span class="line">    doc = nlp(papers_dict[i][<span class="string">&#x27;abstract&#x27;</span>])</span><br><span class="line">    displacy.serve(doc, style=<span class="string">&quot;dep&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">draw_pos_diagram(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>另外，需要获取pos解释的话可以：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> nlp.get_pipe(<span class="string">&quot;parser&quot;</span>).labels:</span><br><span class="line">    print(<span class="string">&#x27;`&#x27;</span>+label+<span class="string">&#x27;`: &#x27;</span>+<span class="built_in">str</span>(spacy.explain(label)))</span><br></pre></td></tr></table></figure>
<p>或者具体的查看<a href="https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf">dependency manual</a></p>
<p>2 Curriculum Learning (CL)</p>
<figure>
<img src="C:\Users\13607\AppData\Roaming\Typora\typora-user-images\image-20210729231504878.png" alt="image-20210729231504878" /><figcaption aria-hidden="true">image-20210729231504878</figcaption>
</figure>
<p><a href="https://aclanthology.org/W16-5615.pdf">参考</a></p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>Pandas的一些常用操作</title>
    <url>/pandas/</url>
    <content><![CDATA[<h1 id="代码">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.set_option(<span class="string">&#x27;display.max_colwidth&#x27;</span>,<span class="literal">None</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>具有分布式语义的关系提取器的半监督性引导</title>
    <url>/paper-2/</url>
    <content><![CDATA[<p>用于从文本中提取关系的半监督性引导技术，在限制语义漂移的同时，反复扩展一组初始种子关系。我们研究了使用词嵌入来寻找类似关系的关系提取的引导技术。实验结果表明，与使用TFIDF寻找相似关系的基线相比，依靠词嵌入在从新闻网文档集合中提取四种关系的任务上取得了更好的性能。</p>
<span id="more"></span>
<h1 id="用词嵌入引导关系提取器">3 用词嵌入引导关系提取器</h1>
<p>BREDS遵循Snowball的架构，具有相同的处理阶段：寻找种子匹配，生成提取模式，寻找关系实例，以及检测语义漂移。然而，它的不同之处在于，它试图使用词嵌入来寻找类似的关系，而不是依赖TF-IDF表示。</p>
<h2 id="寻找种子匹配">3.1 寻找种子匹配</h2>
<p>BREDS扫描文档集，如果一个种子实例的两个实体在一个句子中的文本片段中共同出现，那么该片段就会被考虑，BREDS会像Snowball一样提取三个文本语境。BEF, BET, 和AFT。在BET语境中，BREDS试图根据最初在ReVerb（Fader等人，2011）中提出的浅层启发式方法来识别一个关系模式。<strong>该模式将关系语境限制在一个动词（如invented），一个动词后跟一个介词（如located in），或一个动词后跟名词、形容词或以介词结尾的副词（如has atomic weight of）</strong>。尽管如此，这些模式将只考虑动词介导的关系 (verb mediated relationships) 。<strong>如果两个实体之间不存在动词，BREDS会提取两个实体之间的所有词，以建立BET语境的表示。</strong></p>
<p>每个上下文都被一个简单的合成函数转化为一个单一的向量，该函数首先去除停止词和形容词，然后对每个单独的词的词嵌入向量进行求和。<strong>通过对每个单独的词的嵌入相加来表示小短语</strong>，可以很好地表示小短语中的语义（Mikolov等人，2013b）。</p>
<p>一个关系实例<span class="math inline">\(i\)</span>由三个嵌入向量表示：<span class="math inline">\(V_{BEF}\)</span> , $V_{BET} $, 和 <span class="math inline">\(V_{AFT}\)</span> 。考虑到这个句子：</p>
<p>The tech company Soundcloud is based in Berlin, capital of Germany.</p>
<p>BREDS生成的关系实例有：</p>
<p><span class="math display">\[V_{BEF}  = E(&quot;tech&quot;) + E(&quot;company&quot;) \]</span></p>
<p><span class="math display">\[V_{BET}  = E(&quot;is&quot;) + E(&quot;based&quot;) \]</span></p>
<p><span class="math display">\[V_{AFT}  = E(&quot;capital&quot;) \]</span></p>
<p>其中，<span class="math inline">\(E(x)\)</span>是词<span class="math inline">\(x\)</span>的词嵌入。</p>
<p>BREDS还尝试使用词性标记（PoS）来识别被动语态，这可以帮助检测关系三元组中实体的正确顺序。BREDS通过考虑任何形式的动词be，后面跟一个过去式或过去分词的动词，并以单词by结尾，来识别被动语态的存在。</p>
<p>例如，种子<span class="math inline">\(&lt;Google, owns, DoubleClick&gt;\)</span>指出，Google拥有DoubleClick这个组织。使用这个种子，如果BREDS检测到类似 "agreed to be acquired by " 这样的模式，它将在产生一个关系三元组时调换实体的顺序，输出三元组<span class="math inline">\(&lt;ORG2, owns, ORG1&gt;\)</span>，而不是三元组<span class="math inline">\(&lt;ORG1, owns, ORG2&gt;\)</span>。</p>
<h2 id="提取模式的生成">3.2 提取模式的生成</h2>
<p>与Snowball一样，BREDS通过对上一步收集到的关系实例应用单程聚类算法来生成提取模式。每个产生的聚类包含一组关系实例，由它们的三个上下文向量表示。</p>
<p>算法1描述了BREDS采取的聚类方法，它将关系实例的列表作为输入，并将第一个实例分配到一个新的空簇中。接下来，它在实例列表中进行迭代，计算实例<span class="math inline">\(i_n\)</span>和每个簇<span class="math inline">\(Cl_j\)</span>之间的相似度。该实例被分配到第一个相似度高于或等于阈值<span class="math inline">\(τ_{sim}\)</span>的聚类。如果所有聚类的相似度都低于阈值<span class="math inline">\(τ_{sim}\)</span> ，那么就会创建一个新的聚类C<span class="math inline">\(_m\)</span>，其中包含实例$i_n $。</p>
<p>算法1： <img src="https://i.loli.net/2021/07/18/hGfrd9seWMLuaAQ.png" width="400"/></p>
<p>在实例$i_n <span class="math inline">\(和聚类\)</span>Cl_j <span class="math inline">\(之间的相似性函数\)</span>Sim(i_n, Cl_j )<span class="math inline">\(，如果大多数相似性分数高于阈值\)</span>τ_{sim}$ ，则返回实例$i_n <span class="math inline">\(和聚类\)</span>Cl_j $ 中的任何实例之间的最大相似性。否则就会返回一个0的值。两个实例之间的相似性是根据公式（1）计算的。因此，算法1中的聚类与原来的Snowball方法不同，它是朝着聚类中心点计算相似度的。</p>
<h2 id="查找关系实例">3.3 查找关系实例</h2>
<p>在提取模式生成之后，BREDS用算法2找到关系实例。它再次扫描文档，收集所有包含实体对的文本片段，这些实体对的语义类型与种子实例中的相同。对于每个片段，按照第3.1节所述，生成一个实例i，<strong>并计算与所有先前生成的提取模式（即集群）的相似度</strong>。如果<span class="math inline">\(i\)</span>和一个模式<span class="math inline">\(Cl_j\)</span>之间的相似度等于或高于<span class="math inline">\(τ_{sim}\)</span> ，那么<span class="math inline">\(i\)</span>就被认为是一个候选实例，并且根据公式（2）更新该模式的置信度分数。具有最高相似度的模式（patternbest）和相应的相似度分数（simbest）一起与<span class="math inline">\(i\)</span>相关联。这些信息被保存在候选者的历史中。请注意，"候选人 "和 "关注点 "的历史在所有的自举迭代中都被保留下来，新的模式或实例可以被添加，或者现有模式或实例的分数可以改变。</p>
<p>算法2：<img src="https://i.loli.net/2021/07/18/p39NkE5SMrHYgCb.png" width="400"/></p>
<h2 id="语义漂移检测">3.4 语义漂移检测</h2>
<p>与Snowball一样，BREDS在每次迭代结束时，根据用公式（3）计算的分数对候选实例进行排名。得分等于或高于阈值<span class="math inline">\(τ_t\)</span>的实例被添加到种子集，用于引导算法的下一次迭代。</p>
<h1 id="引言">1 引言</h1>
<p>关系提取（RE）将非结构化文本转化为关系三要素，每个三要素代表两个命名实体之间的关系。一个关系提取的引导系统从一系列的文档和一些种子实例开始。该系统扫描文件集，收集种子实例的发生语境。然后基于这些语境，系统生成提取模式。文件再次被扫描，使用这些模式来匹配新的关系实例。这些新提取的实例 然后被添加到种子集中，这个过程被重复进行 重复这个过程，直到达到一定的停止标准。</p>
<p>因此，引导的目标是用新的关系实例扩大种子集，同时限制语义漂移，即提取的关系的语义逐渐偏离种子关系的语义。最先进的方法依赖于具有TF-IDF权重的词向量表示（Salton and Buckley, 1988）。然而，依靠TF-IDF权重扩大种子 依靠TF-IDF表示法来扩大种子集，以找到 类似的实例有局限性，因为任何两个关系实例向量之间的相似性 只有当实例至少共享一个术语时，TF-IDF权重的任何两个关系实例向量之间的相似性才是正的。例如， 短语是由和是联合创始人的 没有任何共同的词，但它们有 相同的语义。词根技术可以帮助解决 这些情况，但只适用于同一词根的变体（Porter, 1980）。(Porter, 1980)。</p>
<p>我们建议用一种基于词嵌入的方法来解决这一难题（Mikolov et al., 2013a）。依靠词的嵌入，两个短语的相似性可以被发现。两个短语的相似性可以被捕捉到，即使不存在共同的词。即使没有共同的词汇存在。联合创始人和创立者的词嵌入应该是相似的，因为这些词往往出现在相同的 语境中出现。然而，词嵌入也可能引入语义漂移。当使用词嵌入时，像在大学学习历史这样的短语有很高的相似性，例如 历史教授在。在我们的方法中，我们通过对提取的关系实例进行排序，以及对生成的提取模式进行评分来控制语义漂移。</p>
<p>我们在BREDS中实现了这些想法，这是一个基于词嵌入的RE的引导系统。我们对BREDS进行了评估，该系统收集了120万条来自新闻的句子。对BREDS进行了评估，该系统收集了120万篇新闻文章的句子。实验结果表明，我们的方法优于基于Agichtein和Jonathan的观点的基线引导系统。的想法，该系统依赖于TF-IDF表示。实验结果表明，我们的方法优于基于Agichtein和Gravano(2000)思想的基准引导系统，该系统依赖于TF-IDF表示。</p>
<h1 id="引导关系提取器">2 引导关系提取器</h1>
<p>Brin(1999)开发了DIPRE，这是第一个将引导法用于RE的系统，该系统将种子的出现表现为三个上下文的字符串：在第一个实体之前的词(BEF)，两个实体之间的词（BET），以及第二个实体之后的词（AFT）。实体(AFT)。DIPRE通过在字符串匹配的基础上对上下文进行分组来生成提取模式，并通过限制一个模式可以提取的实例数量来控制语义漂移。Agichtein和Gravano（2000）开发了 Snowball，该方法的灵感来自于DIPRE的方法，即为每个出现的情况收集三个语境，但对每个上下文计算一个TF-IDF表示。种子语境的聚类是通过一个基于三个向量表示的上下文之间的余弦相似度的单程算法：</p>
<p><span class="math display">\[ \begin{aligned} \operatorname{Sim}\left(S_{n}, S_{j}\right)=&amp; \alpha \cdot \cos \left(B E F_{i}, B E F_{j}\right) +\beta \cdot \cos \left(B E T_{i}, B E T_{j}\right) +\gamma \cdot \cos \left(A F T_{i}, A F T_{j}\right) \end{aligned} \]</span> (1)</p>
<p>在该公式中，参数α、β和γ对每个向量进行加权<strong>。一个提取模式由形成一个聚类的向量的中心点来表示。</strong>这些模式被用来再次扫描文本，对于任何一对与种子具有相同语义类型的实体共同出现的每个文本片段，都会产生三个向量。如果上下文向量与提取模式的相似度大于阈值<span class="math inline">\(τ_{sim}\)</span>，则该实例被提取。</p>
<p>Snowball对模式进行评分，并对提取的实例进行排名，以控制语义漂移。<strong>一个模式的评分是基于它所提取的实例，这些实例可以包括在三个集合中</strong>：如果一个提取的实例包含一个实体e1，而这个实体是种子的一部分，并且如果实例中的相关实体e2与种子中的实体相同，那么这个提取被认为是积极的（包括在集合<span class="math inline">\(P\)</span>中）。如果该关系与种子集中的关系相矛盾（即e2不匹配），那么该提取被认为是负面的（包含在一个集合<span class="math inline">\(N\)</span>中）。如果这个关系不是种子集的一部分，那么这个提取就被认为是未知的（包括在一个<span class="math inline">\(U\)</span>集里）。<strong>根据以下情况给每个模式<span class="math inline">\(p\)</span>分配一个分数：</strong> <span class="math display">\[
\operatorname{Conf}_{\rho}(p)=\frac{|P|}{|P|+W_{n g t} \cdot|N|+W_{u n k} \cdot|U|} (2)
\]</span> <span class="math inline">\(W_{ngt}\)</span>和<span class="math inline">\(W_{unk}\)</span>分别是与消极提取和未知提取相关的权重。<strong>一个实例的置信度是根据对提取它的模式的相似度得分计算的，并以模式的置信度为权重：</strong> <span class="math display">\[
\operatorname{Conf}_{\iota}(i)=1-\prod_{j=0}^{|\xi|}\left(1-\operatorname{Conf}_{\rho}\left(\xi_{j}\right) \times \operatorname{Sim}\left(C_{i}, \xi_{j}\right)\right) (3)
\]</span> 其中，<span class="math inline">\(ξ\)</span>是提取了i的模式集，<span class="math inline">\(C_i\)</span>是i发生的文本语境。信度高于阈值<span class="math inline">\(τ_t\)</span>的实例被用作下一次迭代的种子。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>期刊翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>数值开放信息抽取的自助法</title>
    <url>/paper-1/</url>
    <content><![CDATA[<p>我们设计并发布了第一个开放的数字关系提取器BONIE，用于提取其中一个参数是数字或数量单位短语的开放IE元组。BONIE使用自助法来学习表达句子中数字关系的特定依存模式。BONIE的创新之处在于特定任务的定制，例如推断隐性关系，这种隐性关系由于单位等上下文而清晰可见（例如，"平方公里 "表明面积，即使句子中缺少 "面积 "一词）。与最先进的开放式IE系统相比，BONIE在数字事实上获得了1.5倍的产量和15分的精度提升。</p>
<span id="more"></span>
<p><img src="https://i.loli.net/2021/07/18/AX5zfpFHZYqWSbG.png"/></p>
<p>https://homes.cs.washington.edu/~mausam/papers/acl17.pdf</p>
<h1 id="开放式数字关系提取">3 开放式数字关系提取</h1>
<p>开放式数字关系提取的目标是处理一个在其中提到数值的句子，并提取任何形式的元组（Arg1，关系短语，Arg2），其中Arg2（或Arg1）是一个数值。作为第一步，BONIE学习Arg2是数值 的模式，因为大多数英语句子倾向于用主动语态表达数字事实。</p>
<p>图1概述了BONIE的算法，它分两个阶段运行：训练和提取。BONIE的训练包括创建种子事实，通过引导生成训练数据，以及通过依赖性分析进行模式学习。在提取阶段，BONIE执行模式匹配和基于解析的扩展来构建数字图元。这些数字图元通过一个新的关系构建步骤变得更加连贯。</p>
<p>作为一个例子，"印度有12亿人口 "这个句子与种子模式#2（来自图2）相匹配，创造了一个种子事实（印度；人口；12亿；空）。这个 "null "代表这个数量不需要任何单位。在引导过程中，这个种子事实可能会与语料库中的一个句子 "印度是世界上人口第二多的国家，人口为12.5亿。"相匹配。这个训练例子将有助于学习一个新的模式。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>这个模式，当应用于 "Microsoft Windows是最流行的操作系统，有3亿用户的客户群 "这个句子时，将提取（Microsoft Windows；有客户群；3亿用户）。</p>
<p><img src="https://i.loli.net/2021/07/18/4e6uBdSXLjD9Obm.png"/></p>
<p>图2：BONIE中的种子依赖模式</p>
<p>虽然BONIE的骨架与OLLIE的骨架（Mausam等人，2012）大致相似，但它带来了针对数字提取问题的定制，比如修改过的模式语言、生成高质量种子集和训练数据的启发式方法、对非名词关系的特殊处理，以及一个新的关系构建步骤。我们现在详细描述BONIE的算法。</p>
<h2 id="种子事实的生成">3.1 种子事实的生成</h2>
<p>由于公开的数字事实不是现成的，我们首先编写了一些高精度的依赖模式（见图2的列表）。每个依赖模式都编码了连接该句中的关系、数量和参数的依赖解析的最小子树。BONIE通过<code>'&lt;depLabel&gt;#&lt;word&gt;#&lt;POSTag&gt;'</code>对模式中的节点进行编码，其中'depLabel'是连接该节点和它的父节点的边，'word'是该节点的词，'POSTag'是它的语篇标签；'#'是分隔它们的分界符。模式中的{rel}、{arg}和{quantity}分别是关系、参数和数量标题词的占位符。BONIE通过解析语料库并将种子模式与解析结果相匹配来生成种子事实。如果匹配成功，就会生成一个形式为（arg headword; relation headword; quantity; unit）的种子事实。论据和关系头词直接从解析中提取。对于其他两个，它使用Illinois Quantifier（Roy等人，2015），它分别返回数量和单位。由于种子事实是我们训练任务的基础，它们必须尽可能干净--BONIE 3&lt;(#is#verb)&lt;(nsubj#{arg}#nnp|nn)(prep#with#in)&lt;(pobj#{rel}#nnp|nn)&lt;(prep#of#in)&lt;(pobj#{quantity}#.+) &gt;&gt;&gt;&gt;&gt; 添加几个过滤器以减少噪音。它认为只有当模式中的数量节点在伊利诺伊量词给出的某个数量跨度内时，种子事实才是有效的。它还拒绝任何参数不是专有名词的事实。经过这些过滤，它得到了高精度的提取结果，但不一定是好的种子--许多种子是通用的，可能很容易与不相关的句子相匹配。例如，(Michael; drove; 20; kms)并不是一个好的种子，因为 "Michael "并不具体，它可能会错误地将提到另一个Michael的句子与一些不相关的开车20公里的引用相匹配。为了改进这个集合，BONIE检查Yago KB（Hoffart等人，2013）中是否存在一个种子事实，并只保留那些常见的事实。由于Yago有许多关于高度、面积、纬度、GDP等的数字事实，这给了BONIE一个多样化的干净事实集，用于进一步训练。最后，一些数字事实可以不使用名词关系词来表达。BONIE使用WordNet（Miller，1995）从这种种子事实中使用关系头词的派生相关名词形式来生成新的种子。例如，(Brown ; tall ; 13 ; inches)被转化为(Brown; height; 13; inches)，它被添加为一个种子事实。3.2 Bootstrapping 与OLLIE类似，BONIE找到包含种子事实中所有单词的句子，并生成（句子，事实）对。但与OLLIE不同的是，BONIE有数量和单位，将它们作为词来匹配并不合适。Illinois Quantifier对两者都进行了内部规范化处理，例如，将 "美元 "和"<span class="math inline">\(&quot;改为 &quot;US\)</span>"，将"%"改为"%"。由于种子事实也有规范化的单位，我们在候选句子上运行Illinois量化器，直接匹配规范化的单位。此外，BONIE保持了一个百分比阈值δ，以控制句子中的数量与种子事实之间的允许差异量。一旦一个事实的所有成分与一个句子相匹配，BONIE就会生成（句子，事实）对。3.3 开放式模式学习 对于每个（句子，事实）对，BONIE解析句子，用'{arg}'和'{rel}'占位符替换事实的参数和关系词。对于数量和单位词，BONIE用'{quantity}'替换解析中更高层次的那个。包含'{arg}'、'{rel}'和'{quantity}'的最小路径被作为一个模式学习。由于数量和单位在句子中通常会保持接近，BONIE会拒绝所有它们之间的距离超过某个阈值的模式。有些模式是用特定的词来学习的，比如例子（部分）中的'包含'&lt;(#包含#verb)&lt;(dobj#{quantity}#.+)&lt;... 我们认为这个模式应该适用于'包含'的所有转折词和同义词，BONIE使用WordNet来扩展这个模式，包括所有的转折词和同义词。每个模式都是根据它在数据中被学习的次数来评分的。3.4 构建提取物 在将一个模式与一个新的句子相匹配之后，与OLLIE类似，arg/rel短语通过在pos、det、num、neg、amod、quantmod、nn和pobj边上扩展提取的头名来完成。如果参数头词的一个子项是prep、rcmod或partmod边，则该边下的整个子树被提取出来。数量短语是由伊利诺伊量词提取的，但是如果数量节点的任何一个兄弟节点是由prep边连接的，有'of'字样，BONIE就会展开它下面的整个子树。这使得 "100美元的10%"可以被包含在数量短语中。关系短语的构造。只要关系头词是一个形容词或副词，BONIE就会使用WordNet来获得其衍生相关的名词形式，并成为新的关系。</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>&lt;(#is#verb)&lt;(nsubj#{arg}#nnp|nn)(prep#with#in)&lt; (pobj#{rel}#nnp|nn)&lt;(prep#of#in)&lt;(pobj#{quantity}#.+) &gt;&gt;&gt;&gt;&gt;<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>期刊翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>用分布式语义bootstrap关系</title>
    <url>/paper-3/</url>
    <content><![CDATA[<p>BREDS（Bootstrapping Relationship Extraction with Distributional Semantics）是本论文提出的一个新的半监督引导系统，用于依存分布式语义的关系提取。BREDS依靠词向量表示（即词嵌入）和一个简单的组合函数来引导关系。本章介绍了BREDS，描述了它的结构和工作流程，并报告了一个验证性实验的结果。</p>
<span id="more"></span>
<h1 id="breds">5.1 BREDS</h1>
<p>一个用于关系提取的引导系统从一系列的文档和一些种子实例开始。该系统扫描文档，收集包含种子实例出现的文本片段。然后，基于这些语境，系统生成提取模式。使用提取模式再次扫描文件集，以匹配新的关系实例。然后，这些新提取的实例被添加到种子集中，这个过程不断重复，直到满足一定的停止标准为止。</p>
<p>引导过程中的<strong>一个关键环节是用新的关系实例扩展种子集，同时限制语义漂移</strong>，即提取的关系的语义逐渐偏离种子关系的语义。BREDS以一种基于词嵌入的新方法来解决这一挑战。</p>
<p>目前最先进的引导方法依存于具有TF-IDF权重的词向量表示，例如Agichtein和Gravano（2000）的Snowball。然而，通过依靠TF-IDF表示来扩大种子集来寻找相似的实例有其局限性，因为<strong>任何两个关系实例向量的TF-IDF权重的相似性只有在实例至少共享一个词时才是正数</strong>。例如，这两个关系短语：</p>
<blockquote>
<p>Microsoft was founded by Bill Gates.</p>
<p>微软是由比尔-盖茨创立的。</p>
<p>Bill Gates is the co-founder of Microsoft.</p>
<p>比尔-盖茨是微软的联合创始人。</p>
</blockquote>
<p>没有任何共同的词，但两者都代表相同的语义，即一个人是一个组织的创始人。词根技术 (Stemming ) 可以在这些情况下提供帮助（Porter, 1997）。然而，这种技术只对同一词根的变化起作用。<strong>依靠词嵌入，即使没有共同的词存在，两个关系短语的相似性也可以被捕捉到。</strong>例如，联合创始人和创立者的词嵌入应该是相似的，因为这些词往往出现在相同的语境中。</p>
<p>然而，词嵌入也可能引入语义漂移。例如，当依存词嵌入时，关系短语如：</p>
<blockquote>
<p>John <em>studied history at</em> Lisbon University.</p>
<p>约翰在里斯本大学学习历史。</p>
<p>Mary <em>is an history professor at</em> Lisbon University.</p>
<p>玛丽是里斯本大学的历史教授。</p>
</blockquote>
<p>两者的相似度很高。<strong>BREDS通过对提取的关系实例进行排序和对生成的提取模式进行评分来控制语义漂移。</strong></p>
<p>请注意，TF-IDF方法将一个句子或一个关系短语表示为一个向量，而在词嵌入方法中，每个词都由一个向量表示。BREDS将关系短语的嵌入向量合并为一个向量，并根据这个单一向量计算关系实例的相似性。</p>
<p>BREDS的处理阶段与Snowball相同（见图5.1）：</p>
<ul>
<li><p><strong>1) 寻找种子匹配</strong></p></li>
<li><p><strong>2) 生成提取模式</strong></p></li>
<li><p><strong>3) 寻找关系实例</strong></p></li>
<li><p><strong>4) 处理语义漂移</strong></p>
<p><img src="https://i.loli.net/2021/07/18/Va759Zqi8CkhQ2B.png"/></p></li>
</ul>
<p>图 5.1：BREDS 的一般工作流程。</p>
<p>然而，它的不同之处在于试图使用词嵌入来寻找类似的关系，而不是依存TF-IDF表示。本节的其余部分将详细介绍这四个处理阶段的每一个。</p>
<h2 id="寻找种子匹配">5.1.1 寻找种子匹配</h2>
<p>就像其他的引导系统一样，BREDS用特定关系类型的种子实例进行初始化。然后，BREDS扫描文档集，如果一个种子实例的两个实体都在一个句子的文本段中共同出现，那么该段就会被考虑，BREDS会像Snowball一样提取文本语境：</p>
<ul>
<li>BEF：第一个实体之前的词；</li>
<li>BET：两个实体之间的词；</li>
<li>AFT：第二个实体之后的词。</li>
</ul>
<p>例如，在这个句子中：</p>
<blockquote>
<p>The tech company Soundcloud is based in Berlin, capital of Germany.</p>
<p>科技公司Soundcloud的总部设在德国首都柏林。</p>
</blockquote>
<p>这三个文本语境对应于：</p>
<blockquote>
<p>BEF: The tech company</p>
<p>BET: is based in</p>
<p>AFT: capital of Germany</p>
</blockquote>
<p>在BET语境中，BREDS试图根据Fader等人（2011）在ReVerb OpenIE系统中最初提出的启发式方法来识别一种关系模式。该关系模式将关系语境限定为：</p>
<ul>
<li>动词（例如，invented ）；</li>
<li>动词后接介词（例如，located in）；</li>
<li>动词后接以介词结尾的名词、形容词或副词（例如， has atomic weight of ）。</li>
</ul>
<p>然而，这些模式将只考虑动词介导的关系。如果两个实体之间不存在动词，BREDS会提取两个实体之间的所有词，以建立BET语境的表征。例如，给定一个句子：</p>
<p>谷歌总部在山景城。</p>
<p>关系模式将对应于：是基于，而在句子中：谷歌总部在山景城，该模式将是：总部在。</p>
<p>每个上下文都被一个简单的合成函数转化为一个单一的向量，该函数首先去除停顿词和形容词，然后对每个单独的词的词嵌入向量进行求和。Mikolov等人(2013a,b)表明，通过对每个单独的词的嵌入相加来表示小短语，可以很好地表示短语中的语义。</p>
<h3 id="关系表示法">关系表示法</h3>
<p>一个关系实例<span class="math inline">\(i\)</span>因此由三个嵌入向量表示：VBEF, VBET, VAFT。例如，在这个句子中。科技公司Soundcloud的总部设在德国首都柏林。该句子所表达的关系实例将由以下嵌入向量表示：</p>
<p><span class="math display">\[ VBEF = embedding(&quot;tech&quot;) + embedding(&quot;company&quot;) - VBET = embedding(&quot;is&quot;) + \\embedding(&quot;based&quot;) - VAFT = embedding(&quot;capital&quot;) \]</span></p>
<p>其中，<span class="math inline">\(embedding(x)\)</span>是一个函数，表示单词<span class="math inline">\(x\)</span>的嵌入向量。对于BET语境，BREDS还尝试使用部分语音（PoS）标签来识别被动语态，这可以帮助检测关系三元组中实体的正确顺序。例如，使用种子<span class="math inline">\(&lt;Google, DoubleClick&gt;\)</span>表达组织Google拥有组织DoubleClick的关系，如果BREDS提取两个组织之间的关系实例，并检测出这样的模式。ORG1同意被ORG2收购 ORG1被ORG2收购，当从短语中表达的实例中产生一个关系三重时，它将调换实体的顺序。因此，它将输出<span class="math inline">\(&lt;ORG2, owns, ORG1&gt;\)</span>这个三重词，而不是<span class="math inline">\(&lt;ORG1, owns, ORG2&gt;\)</span>。BREDS通过考虑任何形式的动词be，后面跟一个过去式或过去分词的动词，并以单词by结尾，后面跟一个命名的实体，来识别被动语态的存在。这种限制被放宽了，允许在两个动词和介词by之间出现副词、形容词或名词。</p>
<h2 id="提取模式">5.1.2 提取模式</h2>
<p>在从文档集合中收集所有的种子语境并生成实例之后，BREDS通过对上一步收集的关系实例应用单程聚类算法来生成提取模式。每个产生的聚类包含一组关系实例，其中每个实例由三个嵌入向量表示。算法1描述了聚类方法，它将关系实例的列表作为输入，并将第一个实例分配给一个新的空聚类。接下来，它通过实例列表进行迭代，计算一个实例<span class="math inline">\(x\)</span>和每个聚类<span class="math inline">\(C_l\)</span>之间的相似性。实例<span class="math inline">\(x\)</span>被分配到第一个相似度高于或等于<span class="math inline">\(τ_{sim}\)</span>的聚类。如果所有聚类的相似度都低于<span class="math inline">\(τ_{sim}\)</span>，就会创建一个新的聚类Clnew，其中包含实例x。</p>
<p>实例in和聚类Clj之间的相似度函数<span class="math inline">\(Sim(i_n, Cl_j )\)</span>，如果大多数相似度分数高于<span class="math inline">\(τ_{sim}\)</span>，则返回$i_n <span class="math inline">\(和聚类\)</span>Cl_j $ 中任何一个实例之间相似度的最大值。否则就会返回一个0的值。因此，算法1中的聚类与原来的Snowball方法不同，它计算的是聚类中心点的相似度。图5.2说明了相似性的计算，一个实例与集群内的其他每个实例进行比较，分数的多数决定了该实例是否被添加到集群中。</p>
<p>图5.2：一个实例和一个实例集群的比较。</p>
<p>任何两个关系实例之间的相似性是通过测量每个实例的上下文嵌入向量之间的余弦相似度来计算的。</p>
<p><span class="math display">\[Sim(Sn, Sj ) = α - cos(BEFi , BEFj ) (5.1) + β - cos(BETi , BETj ) + γ - cos(AF Ti , AF Tj ) \]</span></p>
<p>其中参数α、β和γ定义了与每个上下文的嵌入向量相关的权重。</p>
<h2 id="寻找关系实例">5.1.3 寻找关系实例</h2>
<p>在提取模式生成之后，BREDS再次扫描文档，收集所有包含实体对的文本片段，这些实体对的语义类型与种子实例的语义类型相匹配。例如，对于种子&lt;Google, DoubleClick&gt;，BREDS收集了所有包含一对被标记为组织的命名实体的文本片段。对于每一个收集到的语段，都会提取BEF、BET、AFT这三个语境，并生成一个实例x，如5.1.1所述。然后，计算与所有先前生成的提取模式（即聚类）的相似性。如果<span class="math inline">\(x\)</span>和一个模式<span class="math inline">\(C_l\)</span>之间的相似度等于或高于<span class="math inline">\(τ_{sim}\)</span>，那么x就被认为是一个候选实例，并且模式Cl的置信度分数被更新。</p>
<p>一个模式的置信度分数是根据提取的关系实例来计算的。如果一个提取的关系实例包含一个实体<span class="math inline">\(e_1\)</span>，它是种子集中实例的一部分，并且相关的实体<span class="math inline">\(e_2\)</span>与种子集中的实体相同，那么这个提取被认为是肯定的（即包含在集合P中）。如果该关系与种子集中的关系相矛盾（即<span class="math inline">\(e_2\)</span>不匹配），则该提取被认为是负面的（即包括在集合N中）；如果该关系不是种子集的一部分，则该提取被认为是未知的（即包括在集合U中）。如公式5.2所定义的，根据每个模式p的提取情况，给每个模式分配一个置信度分数。</p>
<p>在该等式中，Wngt和Wunk分别是与负数和未知数提取相关的权重。具有最高相似度的模式（即patternbest）与i相关联，同时还有相似度得分（即simbest）。这些信息被保存在候选者的历史中。算法2详细描述了这个过程。请注意，由于 "候选人 "和 "关注点 "的历史在所有的自举迭代中被保留下来，新的模式或实例可以被添加，或者现有模式或实例的分数可以改变。</p>
<h2 id="处理语义漂移">5.1.4 处理语义漂移</h2>
<p>为了控制语义漂移，BREDS遵循 "滚雪球 "的框架，对提取的实例进行排名，并抛弃排名最低的实例。在每个迭代结束时，候选的所有实例都根据它们当前的置信度分数进行排名。一个实例的置信度分数是基于对提取它的模式的所有分数的相似性，并以该模式的置信度分数加权。<span class="math inline">\(Confι(i) = 1 - Y |ξ| j=0 (1 - Confρ(ξj ) × Sim(Ci , ξj ))\)</span> (5.3)</p>
<p>在上述公式中，ξ是提取了i的模式集，Ci是i发生的文本语境。只有置信度等于或高于τmin的关系实例被添加到种子集，并随后用于下一次引导迭代。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>期刊翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>文献整理</title>
    <url>/papers/</url>
    <content><![CDATA[<h1 id="information-extraction">Information Extraction</h1>
<table>
<colgroup>
<col style="width: 78%" />
<col style="width: 5%" />
<col style="width: 10%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th>Title</th>
<th>Year</th>
<th>Author</th>
<th>Mark</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://research.google/pubs/pub49122/">Representation Learning for Information Extraction from Form-like Documents</a></td>
<td></td>
<td><code>Google</code></td>
<td>5</td>
</tr>
<tr class="even">
<td><a href="https://ai.googleblog.com/2020/06/extracting-structured-data-from.html">Extracting Structured Data from Templatic Documents</a></td>
<td></td>
<td><code>Google</code></td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://aclanthology.org/E17-2074.pdf">Real-Time Keyword Extraction from Conversations</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://aclanthology.org/P17-1102.pdf">PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://aclanthology.org/N13-1109.pdf">A Just-In-Time Keyword Extraction from Meeting Transcripts</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>A Graph Degeneracy-based Approach to Keyword Extraction</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="organizations-conferences">Organizations &amp; Conferences</h1>
<p><a href="https://research.google/pubs/">Google Research Publications</a></p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>用模式匹配的方法抽取关键词</title>
    <url>/pattern_match/</url>
    <content><![CDATA[<p>按照文献：</p>
<p><img src="https://i.loli.net/2021/07/17/BrqQVJfDI8yaU5K.png" width="400"/></p>
<span id="more"></span>
<h1 id="第一种方式-dependency-matcher">第一种方式: dependency matcher</h1>
<p>我们使用dependency matcher，先看看<a href="https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/02_pattern_matching.html">示例</a>和<a href="https://spacy.io/usage/rule-based-matching#dependencymatcher">官方</a>，然后修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;We proposed a new SVM algorithm. John proposed an algorithm.&quot;</span></span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from spacy import displacy</span></span><br><span class="line"><span class="comment"># displacy.serve(doc, style=&quot;dep&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> DependencyMatcher</span><br><span class="line">dep_matcher = DependencyMatcher(vocab=nlp.vocab)</span><br><span class="line"></span><br><span class="line">dep_pattern = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;RIGHT_ID&quot;</span>: <span class="string">&quot;verb&quot;</span>,</span><br><span class="line">        <span class="string">&quot;RIGHT_ATTRS&quot;</span>: &#123;<span class="string">&quot;lemma&quot;</span>: <span class="string">&quot;propose&quot;</span>&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;LEFT_ID&quot;</span>: <span class="string">&quot;verb&quot;</span>,</span><br><span class="line">        <span class="string">&quot;REL_OP&quot;</span>: <span class="string">&quot;&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;RIGHT_ID&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">        <span class="string">&quot;RIGHT_ATTRS&quot;</span>: &#123;<span class="string">&quot;DEP&quot;</span>: <span class="string">&quot;dobj&quot;</span>&#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;LEFT_ID&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">        <span class="string">&quot;REL_OP&quot;</span>: <span class="string">&quot;&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;RIGHT_ID&quot;</span>: <span class="string">&quot;object_modifier&quot;</span>,</span><br><span class="line">        <span class="string">&quot;RIGHT_ATTRS&quot;</span>: &#123;<span class="string">&quot;DEP&quot;</span>: &#123;<span class="string">&quot;IN&quot;</span>: [<span class="string">&quot;amod&quot;</span>, <span class="string">&quot;compound&quot;</span>]&#125;&#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">dep_matcher.add(<span class="string">&#x27;find_object&#x27;</span>, patterns=[dep_pattern])</span><br><span class="line">dep_matches = dep_matcher(doc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call the variable to examine the output</span></span><br><span class="line">print(dep_matches)</span><br><span class="line"></span><br><span class="line">match_id, token_ids = dep_matches[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(token_ids)):</span><br><span class="line">    print(dep_pattern[i][<span class="string">&quot;RIGHT_ID&quot;</span>] + <span class="string">&quot;:&quot;</span>, doc[token_ids[i]].text)</span><br><span class="line"></span><br><span class="line">[(<span class="number">12648607455381628407</span>, [<span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>]), (<span class="number">12648607455381628407</span>, [<span class="number">1</span>, <span class="number">5</span>, <span class="number">4</span>])]</span><br><span class="line">verb: proposed</span><br><span class="line"><span class="built_in">object</span>: algorithm</span><br><span class="line">object_modifier: new</span><br></pre></td></tr></table></figure>
<p>由于dependency matcher不能使用操作符，更适用于抽取主语宾语，而不是抽取前面的修饰语，非常不灵活。所以我们试试matcher。</p>
<h1 id="第二种方式-matcher">第二种方式: Matcher</h1>
<p>我们修改pattern为以下代码。但是这个方式的缺陷在于不能捕获复杂的句法，因此也不太实用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">matcher = Matcher(nlp.vocab)</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;We proposed a new SVM algorithm. John proposed an algorithm.&quot;</span></span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line">pattern = [</span><br><span class="line">    &#123;<span class="string">&quot;LEMMA&quot;</span>: <span class="string">&quot;propose&quot;</span>, <span class="string">&quot;POS&quot;</span>: <span class="string">&quot;VERB&quot;</span>, <span class="string">&quot;DEP&quot;</span>: <span class="string">&quot;ROOT&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;POS&quot;</span>: <span class="string">&quot;DET&quot;</span>, <span class="string">&quot;OP&quot;</span>: <span class="string">&quot;?&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;DEP&quot;</span>: &#123;<span class="string">&quot;IN&quot;</span>: [<span class="string">&quot;amod&quot;</span>, <span class="string">&quot;compound&quot;</span>]&#125;, <span class="string">&quot;OP&quot;</span>: <span class="string">&quot;*&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;DEP&quot;</span>: <span class="string">&quot;dobj&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line">matcher.add(<span class="string">&quot;focus&quot;</span>, [pattern])</span><br><span class="line"></span><br><span class="line">matches = matcher(doc, as_spans=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> span <span class="keyword">in</span> matches:</span><br><span class="line">    print(<span class="string">&#x27;&#123;&#125;: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(span.label_, span.text))</span><br></pre></td></tr></table></figure>
<p>我们可以拿txt文件试验：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./corpus/abstracts.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    corpus = f.read()</span><br></pre></td></tr></table></figure>
<p>由于速度还是很慢，要用并行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./corpus/abstracts.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    corpus = f.read().splitlines()</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">matcher = Matcher(nlp.vocab)</span><br><span class="line"></span><br><span class="line">pattern = [</span><br><span class="line">    &#123;<span class="string">&quot;LEMMA&quot;</span>: <span class="string">&quot;propose&quot;</span>, <span class="string">&quot;POS&quot;</span>: <span class="string">&quot;VERB&quot;</span>, <span class="string">&quot;DEP&quot;</span>: <span class="string">&quot;ROOT&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;POS&quot;</span>: <span class="string">&quot;DET&quot;</span>, <span class="string">&quot;OP&quot;</span>: <span class="string">&quot;?&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;DEP&quot;</span>: &#123;<span class="string">&quot;IN&quot;</span>: [<span class="string">&quot;amod&quot;</span>, <span class="string">&quot;compound&quot;</span>]&#125;, <span class="string">&quot;OP&quot;</span>: <span class="string">&quot;*&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;DEP&quot;</span>: <span class="string">&quot;dobj&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line">pattern_name = <span class="string">&#x27;focus&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pos_match</span>(<span class="params">corpus</span>):</span></span><br><span class="line">    doc = nlp(corpus)</span><br><span class="line">    matcher.add(pattern_name, [pattern])</span><br><span class="line">    matches = matcher(doc, as_spans=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;\n&#x27;</span>.join([span.text <span class="keyword">for</span> span <span class="keyword">in</span> matches])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> executor.<span class="built_in">map</span>(pos_match, corpus):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(s) &gt;<span class="number">1</span>:</span><br><span class="line">                print(s)</span><br><span class="line"></span><br><span class="line">    t2 = time.perf_counter()</span><br><span class="line">    print(<span class="string">f&#x27;Finished in <span class="subst">&#123;t2-t1&#125;</span> seconds&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p><a href="https://github.com/ines/spacy-course/blob/master/chapters/en/slides/chapter1_03_rule-based-matching.md">spacy course代码样例</a></p>
<p><a href="https://spacy.io/api/matcher">官网matcher</a></p>
<p><a href="https://spacy.io/usage/rule-based-matching">rule-based-matching</a></p>
<p><a href="https://blog.csdn.net/m0_51732188/article/details/109479891">Harry Porter Spacy分析</a></p>
<p>文档用https://www.mkdocs.org/getting-started/发布</p>
<p>http://markneumann.xyz/blog/dependency_matcher/</p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>PDF相关知识大全</title>
    <url>/pdf-explained/</url>
    <content><![CDATA[<h1 id="pdf是什么">Pdf是什么</h1>
<p>Portable Document Format，由Adobe 公司于1993 年提出的一种文件规范，主要是为了让同一份文件在不同的装置、硬体或作业系统上皆有相同的呈现效果，达到跨平台皆可使用的目的。</p>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title>PDF文件处理大全</title>
    <url>/pdf/</url>
    <content><![CDATA[<h1 id="pdfplumberpdf文件预处理">Pdfplumber：PDF文件预处理</h1>
<h2 id="用pdfminer把pdf转为txt">用pdfminer把pdf转为txt</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pdfminer.high_level <span class="keyword">import</span> extract_text</span><br><span class="line">file_path = <span class="string">r&#x27;D:\pdf-file\Psychology_of_Language.pdf&#x27;</span></span><br><span class="line">text = extract_text(file_path, page_numbers=<span class="built_in">range</span>(<span class="number">33</span>,<span class="number">35</span>))</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p><code>pdfminer.high_level.``extract_text</code>（<em>pdf_file</em>，<em>password =''</em>，<em>page_numbers = None</em>，<em>maxpages = 0</em>，<em>caching = True</em>，<em>codec ='utf-8'</em>，<em>laparams = None</em> ）</p>
<p>参考：https://pdfminersix.readthedocs.io/en/latest/reference/highlevel.html#api-extract-text</p>
<h2 id="更精确的转换除去换行符">更精确的转换（除去换行符）</h2>
<h3 id="少量文件转换">少量文件转换</h3>
<h4 id="先把pdf转换为word">先把pdf转换为word</h4>
<p>转换工具：https://pdf2doc.com/zh/</p>
<h4 id="然后把word转为txt">然后把word转为txt</h4>
<p>文件多的话用多线程PDF转Word：https://github.com/python-fan/pdf2word</p>
<h3 id="批量转换">批量转换</h3>
<p>还不知道，参考：https://www.zhihu.com/question/357994254</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdfplumber</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">file_path = <span class="string">&#x27;book.pdf&#x27;</span></span><br><span class="line">pdf = pdfplumber.<span class="built_in">open</span>(file_path)</span><br><span class="line">start_page = <span class="number">34</span></span><br><span class="line">end_page = <span class="number">35</span></span><br><span class="line">text_string = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> page_num <span class="keyword">in</span> <span class="built_in">range</span>(start_page-<span class="number">1</span>, end_page):</span><br><span class="line">    text_string += pdf.pages[page_num].extract_text()+<span class="string">&#x27; &#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;book.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text_string)</span><br></pre></td></tr></table></figure>
<h2 id="把表格保存为csv">把表格保存为csv</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdfplumber</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">file_path = <span class="string">&#x27;book.pdf&#x27;</span></span><br><span class="line">pdf = pdfplumber.<span class="built_in">open</span>(file_path)</span><br><span class="line"></span><br><span class="line">chars = []</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> pdf.pages[<span class="number">7</span>:<span class="number">166</span>]:</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> page.chars:</span><br><span class="line">        chars.append(char)</span><br><span class="line">width_unique = <span class="built_in">set</span>([char[<span class="string">&#x27;height&#x27;</span>] <span class="keyword">for</span> char <span class="keyword">in</span> chars])</span><br><span class="line">print(<span class="string">&#x27;The unique heights: &#x27;</span>+<span class="built_in">str</span>(width_unique))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title>英文词性标记（POS Tagging）</title>
    <url>/pos-tagging/</url>
    <content><![CDATA[<h1 id="词性标记pos-tagging">词性标记（POS Tagging）</h1>
<p>POS标签大致分为两种：通用POS标签和细粒度POS标签。</p>
<h2 id="跨语言的通用pos标签">跨语言的通用POS标签</h2>
<p>通用依存关系（<a href="https://universaldependencies.org/">UD</a>）是一个致力于开发跨语言树库标注的项目，为不同语言标注一致的语法<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。 <span id="more"></span> 该标注方案基于（通用的）斯坦福依存关系（de Marneffe et al., 2006, 2008, 2014）、Google通用词性标签（Petrov et al., 2012）和Interset interlingua的形态语义标签集（Zeman, 2008）的发展进化。<a href="https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf">dependency manual</a>。</p>
<p>该<a href="https://universaldependencies.org/u/pos/index.html">网站</a>列举了各个通用POS标签的详细解释。</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 14%" />
<col style="width: 34%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th>Tag</th>
<th>Category</th>
<th>Explanation</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/ADJ.html">ADJ</a></td>
<td>adjective</td>
<td>形容词，修饰名词或充当谓语</td>
<td>big, African, first</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/ADP.html">ADP</a></td>
<td>adposition</td>
<td>介词，包括前置词和后置词（prepositions and postpositions）</td>
<td>in, during</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/ADV.html">ADV</a></td>
<td>adverb</td>
<td>副词，修饰动词、形容词和副词本身，表示时间、地点、方向或方式</td>
<td>very, up, tomorrow, where, never</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/AUX_.html">AUX</a></td>
<td>auxiliary</td>
<td>助词，包括Tense auxiliaries, Passive auxiliaries, Modal auxiliaries, Verbal copulas</td>
<td>has, was, should</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/CCONJ.html">CCONJ</a></td>
<td>coordinating conjunction</td>
<td>并列连词</td>
<td>and, or, but</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/DET.html">DET</a></td>
<td>determiner</td>
<td>限定词，修饰名词</td>
<td>the, a, an</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/INTJ.html">INTJ</a></td>
<td>interjection</td>
<td>感叹词</td>
<td>ouch, yes</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/NOUN.html">NOUN</a></td>
<td>noun</td>
<td>普通名词</td>
<td>girl, tree, air</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/NUM.html">NUM</a></td>
<td>numeral</td>
<td>数词</td>
<td>2014, one, II</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/PART.html">PART</a></td>
<td>particle</td>
<td>小品词，与其他词语结合</td>
<td>'s, not</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/PRON.html">PRON</a></td>
<td>pronoun</td>
<td>代词，代替名词或名词短语</td>
<td>you, who, somebody, it</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/PROPN.html">PROPN</a></td>
<td>proper noun</td>
<td>专有名词，特定任务、地点或物体的名称</td>
<td>Mary, London, NATO</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/PUNCT.html">PUNCT</a></td>
<td>punctuation</td>
<td>标点符号是非字母字符和字符组</td>
<td>. , ()</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/SCONJ.html">SCONJ</a></td>
<td>subordinating conjunction</td>
<td>从属连词，引入从句的连词</td>
<td>since, that, who, if, while</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/SYM.html">SYM</a></td>
<td>symbol</td>
<td>符号，可以用普通单词代替（比如$换作美元）</td>
<td>$, %, ♥‿♥, 😝</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/VERB.html">VERB</a></td>
<td>verb</td>
<td>动词，表示事件和动作</td>
<td>run, runs, running</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/X.html">X</a></td>
<td>other</td>
<td>其他</td>
<td>xfgh</td>
</tr>
</tbody>
</table>
<p>该项目的<a href="https://github.com/UniversalDependencies/docs">Github地址</a>涵盖了不同语言。</p>
<h2 id="特定语言的细粒度pos标签">特定语言的细粒度POS标签</h2>
<p>我们可以将通用POS标签细化，例如将英语中的NOUN（普通名词）进一步划分为复数普通名词（NNS），NN（单数普通名词），但这些标签是基于特定语言的。</p>
<p>我们可以打开<a href="https://spacy.io/usage/linguistic-features">spacy</a>，线上运行以下代码。其中pos表示<a href="https://universaldependencies.org/docs/u/pos/">通用POS标签集</a>的粗粒度<a href="https://universaldependencies.org/docs/u/pos/">词性</a>，而tag表示细粒度的词性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">doc = nlp(<span class="string">&quot;It took me more than two hours to translate a few pages of English.&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">	print(token.text, <span class="string">&#x27;=&gt;&#x27;</span>,token.pos_,<span class="string">&#x27;=&gt;&#x27;</span>,token.tag_)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">It &#x3D;&gt; PRON &#x3D;&gt; PRP</span><br><span class="line">took &#x3D;&gt; VERB &#x3D;&gt; VBD</span><br><span class="line">me &#x3D;&gt; PRON &#x3D;&gt; PRP</span><br><span class="line">more &#x3D;&gt; ADJ &#x3D;&gt; JJR</span><br><span class="line">than &#x3D;&gt; SCONJ &#x3D;&gt; IN</span><br><span class="line">two &#x3D;&gt; NUM &#x3D;&gt; CD</span><br><span class="line">hours &#x3D;&gt; NOUN &#x3D;&gt; NNS</span><br><span class="line">to &#x3D;&gt; PART &#x3D;&gt; TO</span><br><span class="line">translate &#x3D;&gt; VERB &#x3D;&gt; VB</span><br><span class="line">a &#x3D;&gt; DET &#x3D;&gt; DT</span><br><span class="line">few &#x3D;&gt; ADJ &#x3D;&gt; JJ</span><br><span class="line">pages &#x3D;&gt; NOUN &#x3D;&gt; NNS</span><br><span class="line">of &#x3D;&gt; ADP &#x3D;&gt; IN</span><br><span class="line">English &#x3D;&gt; PROPN &#x3D;&gt; NNP</span><br><span class="line">. &#x3D;&gt; PUNCT &#x3D;&gt; .</span><br></pre></td></tr></table></figure>
<p>有关spaCy模型在不同语言中分配的细粒度和粗粒度词性标签的列表，请参阅<a href="https://spacy.io/models">models目录中</a>记录的标签方案。</p>
<p>比如英语的所有细粒度<a href="https://spacy.io/models/en">标签</a>如下：</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="header">
<th>A</th>
<th>B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>$</code></td>
<td>symbol, currency</td>
</tr>
<tr class="even">
<td><code>''</code></td>
<td>closing quotation mark</td>
</tr>
<tr class="odd">
<td><code>,</code></td>
<td>punctuation mark, comma</td>
</tr>
<tr class="even">
<td>`<code>| opening quotation mark                    | |</code>-LRB-<code>| left round bracket                        | |</code>-RRB-<code>| right round bracket                       | |</code>.<code>| punctuation mark, sentence closer         | |</code>:<code>| punctuation mark, colon or ellipsis       | |</code>ADD<code>| email                                     | |</code>AFX<code>| affix                                     | |</code>CC<code>| conjunction, coordinating                 | |</code>CD<code>| cardinal number                           | |</code>DT<code>| determiner                                | |</code>EX<code>| existential there                         | |</code>FW<code>| foreign word                              | |</code>HYPH<code>| punctuation mark, hyphen                  | |</code>IN<code>| conjunction, subordinating or preposition | |</code>JJ<code>| adjective                                 | |</code>JJR<code>| adjective, comparative                    | |</code>JJS<code>| adjective, superlative                    | |</code>LS<code>| list item marker                          | |</code>MD<code>| verb, modal auxiliary                     | |</code>NFP<code>| superfluous punctuation                   | |</code>NN<code>| noun, singular or mass                    | |</code>NNP<code>| noun, proper singular                     | |</code>NNPS<code>| noun, proper plural                       | |</code>NNS<code>| noun, plural                              | |</code>PDT<code>| predeterminer                             | |</code>POS<code>| possessive ending                         | |</code>PRP<code>| pronoun, personal                         | |</code>PRP<span class="math inline">\(` | pronoun, possessive | | `RB` | adverb | | `RBR` | adverb, comparative | | `RBS` | adverb, superlative | | `RP` | adverb, particle | | `SYM` | symbol | | `TO` | infinitival &quot;to | | `UH` | interjection | | `VB` | verb, base form | | `VBD` | verb, past tense | | `VBG` | verb, gerund or present participle | | `VBN` | verb, past participle | | `VBP` | verb, non-3rd person singular present | | `VBZ` | verb, 3rd person singular present | | `WDT` | wh-determiner | | `WP` | wh-pronoun, personal | | `WP\)</span><code>| wh-pronoun, possessive                    | |</code>WRB<code>| wh-adverb                                 | |</code>XX`</td>
<td>unknown</td>
</tr>
</tbody>
</table>
<p>所有dep标签如下：</p>
<p><code>ROOT</code>: None <code>acl</code>: clausal modifier of noun (adjectival clause) <code>acomp</code>: adjectival complement <code>advcl</code>: adverbial clause modifier <code>advmod</code>: adverbial modifier <code>agent</code>: agent <code>amod</code>: adjectival modifier <code>appos</code>: appositional modifier <code>attr</code>: attribute <code>aux</code>: auxiliary <code>auxpass</code>: auxiliary (passive) <code>case</code>: case marking <code>cc</code>: coordinating conjunction <code>ccomp</code>: clausal complement <code>compound</code>: compound <code>conj</code>: conjunct <code>csubj</code>: clausal subject <code>csubjpass</code>: clausal subject (passive) <code>dative</code>: dative <code>dep</code>: unclassified dependent <code>det</code>: determiner <code>dobj</code>: direct object <code>expl</code>: expletive <code>intj</code>: interjection <code>mark</code>: marker <code>meta</code>: meta modifier <code>neg</code>: negation modifier <code>nmod</code>: modifier of nominal <code>npadvmod</code>: noun phrase as adverbial modifier <code>nsubj</code>: nominal subject <code>nsubjpass</code>: nominal subject (passive) <code>nummod</code>: numeric modifier <code>oprd</code>: object predicate <code>parataxis</code>: parataxis <code>pcomp</code>: complement of preposition <code>pobj</code>: object of preposition <code>poss</code>: possession modifier <code>preconj</code>: pre-correlative conjunction <code>predet</code>: None <code>prep</code>: prepositional modifier <code>prt</code>: particle <code>punct</code>: punctuation <code>quantmod</code>: modifier of quantifier <code>relcl</code>: relative clause modifier <code>xcomp</code>: open clausal complement</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/#:~:text=Dependency%20parsing%20is%20the%20process,tags%20are%20the%20dependency%20tags.">词性标记和依存关系分析</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>pos</tag>
      </tags>
  </entry>
  <entry>
    <title>用bootstrapping迭代的方式发掘新的模式</title>
    <url>/pattern_mining/</url>
    <content><![CDATA[<p>阅读david sbatista的<a href="https://github.com/davidsbatista/Snowball/blob/master/Snowball/">代码</a>、<a href="http://www.davidsbatista.net/assets/documents/publications/breds-emnlp_15.pdf">文章</a>和<a href="http://www.davidsbatista.net/assets/documents/publications/dsbatista-phd-thesis-2016.pdf">博士论文</a></p>
<span id="more"></span>
<p>参考：</p>
<p>https://zhuanlan.zhihu.com/p/139485679</p>
<p>https://github.com/davidsbatista/BREDS</p>
<p>https://zhuanlan.zhihu.com/p/101058270</p>
<p>https://www.eecis.udel.edu/~vijay/fall13/snlp/lit-survey/Bootstrapping.pdf</p>
<p>https://www.codenong.com/55393087/</p>
<p>http://www.davidsbatista.net/posts/</p>
<p>https://www.youtube.com/watch?v=Ra15lX-wojg</p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>预训练模型</title>
    <url>/pre-training/</url>
    <content><![CDATA[<p>预训练模型分为自编码、自回归、编码解码。</p>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/80377698">【全】一文带你了解自编码器（AutoEncoder）</a>(这篇文章很赞！)</p>
<h1 id="自编码器">自编码器</h1>
<p>自编码器可以理解为一个试图去还原其原始输入的系统。</p>
<p>https://zhuanlan.zhihu.com/p/75580708</p>
<p>自编码器模型主要由编码器（Encoder）和解码器（Decoder）组成，其主要目的是将输入x转换成中间变量y，然后再将y转换成 <span class="math inline">\(\bar{x}\)</span>，然后对比输入x和输出 <span class="math inline">\(\bar{x}\)</span> 使得他们两个无限接近。</p>
<p>自编码器是由nathan hubens提出的一种输入等于输出的神经网络模型，可能大家会疑惑为什么要训练一个这样的模型，毕竟输入等于输出在大家看来就是一件多此一举的事情。这里博主先给大家上一个模型图让大家感受一下。</p>
<p>上图是由BP神经网络组成的最简单的自编码器，只有三层结构，中间的隐藏层才是我们所需要关注的地方，以隐藏层为界限，左边为编码器（encoder）， 右边为解码器（decoder），所以在训练过程中，输入才能在经过编码后再解码，还原成原来的模样</p>
<h2 id="神经网络自编码模型无监督">神经网络自编码模型：无监督</h2>
<p>在深度学习中，自动编码器是一种无监督的神经网络模型，它可以学习到输入数据的隐含特征，这称为编码(coding)，同时用学习到的新特征可以重构出原始输入数据，称之为解码(decoding)。从直观上来看，自动编码器可以用于特征降维，类似主成分分析PCA，但是其相比PCA其性能更强，这是由于神经网络模型可以提取更有效的新特征。除了进行特征降维，自动编码器学习到的新特征可以送入有监督学习模型中，所以自动编码器可以起到特征提取器的作用。举个例子，我有一张清晰图片，首先我通过编码器压缩这张图片的大小（如果展现出来可能比较模型），然后在需要解码的时候将其还原成清晰的图片。具体过程如下图所示。</p>
<p>到了真正使用自编码的时候. 通常只会用到自编码前半部分。</p>
<h2 id="降噪自动编码器daedenoising-autoencoder">降噪自动编码器DAE（Denoising Autoencoder）</h2>
<p><a href="https://blog.csdn.net/wydbyxr/article/details/83378372">生成模型--降噪自编码器</a></p>
<p><a href="https://www.shuzhiduo.com/R/lk5alpR2J1/">降噪自动编码器</a></p>
<p><a href="https://medium.com/@falconives/day-32-denoising-autoencoder-dae-77f5b4cfe0b4">Day 32 — Denoising Autoencoder (DAE)</a></p>
<p><a href="https://www.zhihu.com/column/yuchiliu">AI机动队</a></p>
<p>自编码器还有降噪的功能。把<strong>加噪后的数据集</strong>当成输入，<strong>原本的数据集</strong>当做<strong>输出</strong>，训练一个自编码器，让它在训练过程中学习数据的规律，从而把噪声去掉。这就是博主所说的去噪功能。</p>
<p>降噪自编码器：降噪自编码器就是输入换成了加噪的数据集，输出用原数据集去训练的自编码器，目的是习得降噪功能。</p>
<p>自回归语言模型只能根据上文预测下一个单词，或者反过来，只能根据下文预测前面一个单词。相反，把句子中随机一个单词用[mask]替换掉，是不是就能同时根据该单词的上下文来预测该单词。我们都知道Bert在预训练阶段使用[mask]标记对句子中15%的单词进行随机屏蔽，然后根据被mask单词的上下文来预测该单词，这就是自编码语言模型的典型应用。 链接：https://www.jianshu.com/p/38d8318890f9</p>
<p>自编码语言模型能很自然的把上下文信息融合到模型中（Bert中的每个Transformer都能看到整句话的所有单词，等价于双向语言模型）</p>
<h1 id="自回归高时延问题">自回归：高时延问题</h1>
<p><a href="https://www.zhihu.com/question/404858266">深度学习中自回归模型与非自回归模型相比，inference慢很多，那它的优点是什么</a>？</p>
<p>预训练工作往往针对自回归语言生成模型设计，自回归每次会使用已生成的序列作为已知信息预测未来的一个单词，最终再把每个时间步生成的单词拼成一个完整的序列输出。这其中的时延成为了线上使用或者实时使用这些预训练的自然语言生成模型的瓶颈。</p>
<p><strong>非自回归模型的提出缓解了自回归模型的高时延问题</strong>。在非自回归模型中，每个单词之间没有依赖关系，整个输出序列的每个单词被并行地同步预测。虽然其推断速度得到了很大改善，<strong>但是生成质量却往往弱于自回归模型</strong>。</p>
<p>一般的语言模型都是从左到右计算某个词出现的概率，但是当我们做完型填空或者阅读理解这一类NLP任务的时候词的上下文信息都是需要考虑的，而这个时候只考虑了该词的上文信息而没有考虑到下文信息。所以，反向的语言模型出现了，就是从右到左计算某个词出现的概率，这一类语言模型称之为<strong>自回归语言模型</strong>。</p>
<p>GPT：坚持只用单向Transformer</p>
<p>ELMo：拼接两个上文和下文LSTM的变形自回归语言模型</p>
<p><strong>生成类NLP任务有优势，比如生成类NLP任务有优势，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。</strong></p>
<h2 id="编码解码">编码解码</h2>
<p><a href="https://zhuanlan.zhihu.com/p/136597401">Seq2Seq 编码器-解码器模型与注意力机制</a></p>
<p>对于循环神经网络，它的输入是一段不定长的序列，输出却是定长的。当输入输出都是不定长序列时，我们可以使用编码器-解码器 (encoder-decoder) 来对其进行建模。</p>
<p>Encoder-Decoder 模型主要是 NLP 领域里的概念。它并不特值某种具体的算法，而是一类算法的统称。Encoder-Decoder 算是一个通用的框架，在这个框架下可以使用不同的算法来解决不同的任务。</p>
<p>Encoder-Decoder 这个框架很好的诠释了机器学习的核心思路：</p>
<blockquote>
<p><em>将现实问题转化为数学问题，通过求解数学问题，从而解决现实问题。</em></p>
</blockquote>
<p>Encoder 又称作编码器。它的作用就是「将现实问题转化为数学问题」</p>
<p>Decoder 又称作解码器，他的作用是「求解数学问题，并转化为现实世界的解决方案」</p>
<p><a href="https://easyaitech.medium.com/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82-nlp-%E9%87%8C%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6-encoder-decoder-%E5%92%8C-seq2seq-1012abf88572">一文看懂 NLP 里的模型框架 Encoder-Decoder 和 Seq2Seq</a></p>
<p>关于 Encoder-Decoder，有2 点需要说明：</p>
<ol type="1">
<li>不论输入和输出的长度是什么，中间的「向量 c」 长度都是固定的（这也是它的缺陷，下文会详细说明）</li>
<li>根据不同的任务可以选择不同的编码器和解码器（可以是一个 RNN ，但通常是其变种 LSTM 或者 GRU ）</li>
</ol>
<p>Encoder-Decoder 的问题在于：由于只有一个「向量 c」来传递信息，且 c 的长度固定。当输入信息太长时，会丢失掉一些信息。</p>
<p><strong>Attention 机制就是为了解决「信息过长，信息丢失」的问题。</strong></p>
<p><em>Ａttention</em> 模型的特点是 Eecoder 不再将整个输入序列编码为固定长度的「中间向量 Ｃ」 ，而是编码成一个向量的序列。这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-4b37c57bbd456c29a2cb691a6ed9a8e9_720w.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><a href="https://zhuanlan.zhihu.com/p/57839295">机器学习中的编码器-解码器结构哲学</a></p>
<h2 id="beam-search集束搜索"><a href="https://zhuanlan.zhihu.com/p/391848454">beam search集束搜索</a></h2>
<p><a href="https://blog.csdn.net/sdgihshdv/article/details/76737537">集束搜索（Beam Search Algorithm ）</a></p>
<p>https://blog.csdn.net/weixin_39871378/article/details/118332258</p>
<p>显然集束搜索属于贪心算法，不能保证一定能够找到全局最优解，因为考虑到搜索空间太大，而采用一个相对的较优解。而维特比算法在字典大小较小时能够快速找到全局最优解，贪心搜索可以认为beam size为1时的集束搜索特例。</p>
<p><a href="https://www.cnblogs.com/mantch/p/11433829.html">seq2seq通俗理解----编码器和解码器(TensorFlow实现)</a></p>
<h1 id="预训练模型的微调">预训练模型的微调</h1>
<p>如何“微调”（fine-tune）基于Transformers的“预训练“模型（比如BERT、Roberta），在自己的任务上（比如阅读理解、实体识别、情感分类）取得新的SOTA？</p>
<p>“微调”/fine-tune通常指：<strong>一种深度学习模型的训练方式/步骤。</strong></p>
<h1 id="零次学习zero-shot-learning"><a href="https://zhuanlan.zhihu.com/p/34656727">零次学习（Zero-Shot Learning）</a></h1>
<p><em>Zero</em>-<em>shot</em> learning 指的是我们之前没有这个类别的训练样本，但是我们可以学习到一个映射X-&gt;Y。如果这个映射足够好的话，我们就可以处理没有看到的类了。</p>
<h1 id="强化学习">强化学习</h1>
<p>强化学习是除了<a href="https://zh.wikipedia.org/wiki/监督学习">监督学习</a>和<a href="https://zh.wikipedia.org/w/index.php?title=非监督学习&amp;action=edit&amp;redlink=1">非监督学习</a>之外的第三种基本的机器学习方法。<em>强化学习</em>的目的是最大化长期未来奖励，即寻找最大的U。</p>
<h1 id="回归和非回归">回归和非回归</h1>
<p>文本生成分为<strong>回归模型</strong>和<strong>非回归模型</strong>两种形式。</p>
<p>编码，其实就是主特征提取，而且是自动的不需要人工设计的主特征提取任务。回归等价于函数拟合，值是连续的。</p>
<p>回归更多是指代一种“方法”，即研究两个或两个以上变量相关关系的方法。</p>
<h1 id="生成对抗网络"><a href="https://zhuanlan.zhihu.com/p/33752313">生成对抗网络</a></h1>
<h1 id="预训练模型"><a href="https://zhuanlan.zhihu.com/p/115014536">预训练模型</a></h1>
<h1 id="单向模型"><a href="https://zhuanlan.zhihu.com/p/76912493">单向模型</a></h1>
<ul>
<li><p><strong>单向</strong>特征表示的<strong>自回归</strong>预训练语言模型，统称为<strong>单向模型</strong>：</p></li>
<li><ul>
<li>ELMO/ULMFiT/SiATL/GPT1.0/GPT2.0；</li>
</ul></li>
<li><p><strong>双向</strong>特征表示的<strong>自编码</strong>预训练语言模型，统称为<strong>BERT系列模型：</strong></p></li>
<li><ul>
<li>(BERT/MASS/UNILM/ERNIE1.0/ERNIE(THU)/MTDNN/ERNIE2.0/SpanBERT/RoBERTa)</li>
</ul></li>
<li><p><strong>双向</strong>特征表示的<strong>自回归</strong>预训练语言模型：<strong>XLNet</strong>；</p></li>
</ul>
<h1 id="无监督学习">无监督学习</h1>
<p>无监督学习不依赖任何标签值，通过对数据内在特征的挖掘，找到样本间的关系</p>
<p>无监督学习中被广泛采用的方式是自动编码器（autoencoder）</p>
<h1 id="自监督学习">自监督学习</h1>
<p><a href="https://zhuanlan.zhihu.com/p/108906502">Self-supervised Learning 再次入门</a></p>
<h1 id="变分自编码器">变分自编码器</h1>
<p>VAE一经提出就迅速获得了深度生成模型领域广泛的关注，并和生成对抗网络（Generative Adversarial Networks，GAN）被视为无监督式学习领域最具研究价值的方法之一，在深度生成模型领域得到越来越多的应用。</p>
<h1 id="生成建模">生成建模</h1>
<p>https://www.cnblogs.com/love6tao/p/11274845.html</p>
<h1 id="bert学习">BERT学习</h1>
<p><a href="https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=GEgLpFVlo1Z-">BERT Fine-Tuning Sentence Classification</a></p>
<p>把bert换成scibert</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语言模型</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>编程方法论</title>
    <url>/programming-methodology/</url>
    <content><![CDATA[<h1 id="编程方法总结">编程方法总结</h1>
<p>首先，本着大道至简的原理，使用sublime text和terminal来查看和运行最终脚本。</p>
<p>其次，由于缺乏自动提示等插件，可以使用jupyter notebook作为打草稿的工具。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>代码</category>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>methodology</tag>
      </tags>
  </entry>
  <entry>
    <title>候选词过滤停用词</title>
    <url>/project-3/</url>
    <content><![CDATA[<h1 id="section"></h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\毕业论文\visualization\data\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line">doc = papers_dict[<span class="number">0</span>][<span class="string">&#x27;abstract&#x27;</span>]</span><br><span class="line">print(doc)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">candidates = re.findall(<span class="string">r&#x27;&lt;span class=&quot;noun&quot;&gt;.*?&lt;/span&gt;&#x27;</span>, doc, re.M)</span><br><span class="line">candidates = [keyword[<span class="number">19</span>:-<span class="number">7</span>] <span class="keyword">for</span> keyword <span class="keyword">in</span> candidates]</span><br><span class="line">print(candidates)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>画关键词共现图谱</title>
    <url>/project-V2/</url>
    <content><![CDATA[<h1 id="加载数据">加载数据</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\毕业论文\visualization\data\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="comment"># set(.remove(&#x27;-&#x27;))</span></span><br><span class="line">punctuation = <span class="built_in">list</span>(string.punctuation)</span><br><span class="line">punctuation.remove(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">print(punctuation)</span><br><span class="line"></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">candidates_list = []</span><br><span class="line"><span class="keyword">for</span> paper <span class="keyword">in</span> papers_dict:</span><br><span class="line">    doc = paper[<span class="string">&#x27;abstract&#x27;</span>]</span><br><span class="line">    candidates = re.findall(<span class="string">r&#x27;&lt;span class=&quot;noun&quot;&gt;.*?&lt;/span&gt;&#x27;</span>, doc, re.M)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        candidates = <span class="built_in">list</span>(<span class="built_in">set</span>([keyword[<span class="number">19</span>:-<span class="number">7</span>] <span class="keyword">for</span> keyword <span class="keyword">in</span> candidates]))</span><br><span class="line">        candidates_list.append(candidates)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">&#x27;Paper &#x27;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;does not have candidates&#x27;</span>)</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">stopwords = [<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;et&#x27;</span>, <span class="string">&#x27;cases&#x27;</span>, <span class="string">&#x27;kinds&#x27;</span>, <span class="string">&#x27;such&#x27;</span>, <span class="string">&#x27;systems&#x27;</span>, <span class="string">&#x27;that&#x27;</span>, <span class="string">&#x27;all&#x27;</span>, <span class="string">&#x27;many&#x27;</span>, <span class="string">&#x27;other&#x27;</span>, <span class="string">&#x27;al&#x27;</span>, <span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;our&#x27;</span>, <span class="string">&#x27;The&#x27;</span>, <span class="string">&#x27;Our&#x27;</span>, <span class="string">&#x27;these&#x27;</span>, <span class="string">&#x27;their&#x27;</span>, <span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;its&#x27;</span>, <span class="string">&#x27;model&#x27;</span>, <span class="string">&#x27;models&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;paper&#x27;</span>, <span class="string">&#x27;task&#x27;</span>, <span class="string">&#x27;performance&#x27;</span>, <span class="string">&#x27;results&#x27;</span>, <span class="string">&#x27;Results&#x27;</span>, <span class="string">&#x27;method&#x27;</span>, <span class="string">&#x27;approach&#x27;</span>, <span class="string">&#x27;methods&#x27;</span>, <span class="string">&#x27;tasks&#x27;</span>, <span class="string">&#x27;training&#x27;</span>, <span class="string">&#x27;work&#x27;</span>, <span class="string">&#x27;state-of-the-art&#x27;</span>, <span class="string">&#x27;system&#x27;</span>, <span class="string">&#x27;evaluation&#x27;</span>, <span class="string">&#x27;problem&#x27;</span>, <span class="string">&#x27;approaches&#x27;</span>, <span class="string">&#x27;human&#x27;</span>, <span class="string">&#x27;experiments&#x27;</span>, <span class="string">&#x27;features&#x27;</span>, <span class="string">&#x27;analysis&#x27;</span>, <span class="string">&#x27;framework&#x27;</span>, <span class="string">&#x27;research&#x27;</span>, <span class="string">&#x27;works&#x27;</span>, <span class="string">&#x27;paper&#x27;</span>,  <span class="string">&#x27;what&#x27;</span>, <span class="string">&#x27;success&#x27;</span>, <span class="string">&#x27;another&#x27;</span>, <span class="string">&#x27;Research&#x27;</span>, <span class="string">&#x27;much&#x27;</span>, <span class="string">&#x27;questions&#x27;</span>, <span class="string">&#x27;effectiveness&#x27;</span>, <span class="string">&#x27;study&#x27;</span>, <span class="string">&#x27;Experimental&#x27;</span>, <span class="string">&#x27;Experiments&#x27;</span>, <span class="string">&#x27;Its&#x27;</span>, <span class="string">&#x27;Experiment&#x27;</span>, <span class="string">&#x27;process&#x27;</span>, <span class="string">&#x27;improvements&#x27;</span>, <span class="string">&#x27;techniques&#x27;</span>, <span class="string">&#x27;applications&#x27;</span>, <span class="string">&#x27;studies&#x27;</span>, <span class="string">&#x27;future&#x27;</span>, <span class="string">&#x27;addition&#x27;</span>, <span class="string">&#x27;problems&#x27;</span>, <span class="string">&#x27;examples&#x27;</span>, <span class="string">&#x27;resources&#x27;</span>, <span class="string">&#x27;scores&#x27;</span>, <span class="string">&#x27;challenges&#x27;</span>, <span class="string">&#x27;structures&#x27;</span>]</span><br><span class="line">all_candidates = []</span><br><span class="line"><span class="keyword">for</span> candidate <span class="keyword">in</span> candidates_list:</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> candidate:</span><br><span class="line">        words = c.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">list</span>(<span class="built_in">set</span>(words) &amp; <span class="built_in">set</span>(stopwords)):</span><br><span class="line">            all_candidates.append(c)</span><br><span class="line"></span><br><span class="line">print(<span class="built_in">len</span>(all_candidates))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">sorted_gram_count = Counter(all_candidates).most_common()</span><br><span class="line">sorted_gram_count = [i <span class="keyword">for</span> i <span class="keyword">in</span> sorted_gram_count <span class="keyword">if</span> <span class="string">&#x27; &#x27;</span> <span class="keyword">in</span> i[<span class="number">0</span>] <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">list</span>(<span class="built_in">set</span>(<span class="built_in">list</span>(i[<span class="number">0</span>])) &amp; <span class="built_in">set</span>(punctuation)) <span class="keyword">or</span> <span class="keyword">not</span> i[<span class="number">0</span>].islower()]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(sorted_gram_count)</span><br><span class="line">df.columns = [<span class="string">&#x27;candidate&#x27;</span>, <span class="string">&#x27;count&#x27;</span>]</span><br><span class="line">df.to_csv(<span class="string">&#x27;candidates.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">terms = df[df[<span class="string">&#x27;count&#x27;</span>] &gt;= <span class="number">20</span>][<span class="string">&#x27;candidate&#x27;</span>].to_list()</span><br><span class="line">print(<span class="built_in">len</span>(terms))</span><br></pre></td></tr></table></figure>
<h1 id="关键词共现图谱">关键词共现图谱</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Edge = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(terms)):</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(terms) - i):</span><br><span class="line">        word1 = terms[i]</span><br><span class="line">        word2 = terms[i + n]</span><br><span class="line">        weight = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> each_abstract <span class="keyword">in</span> candidates_list:</span><br><span class="line">            <span class="keyword">if</span> ((word1 <span class="keyword">in</span> each_abstract) <span class="keyword">and</span> (word2 <span class="keyword">in</span> each_abstract)):</span><br><span class="line">                weight += <span class="number">1</span></span><br><span class="line">        Edge.append((word1, word2, weight))</span><br><span class="line"></span><br><span class="line">df_Edge = pd.DataFrame(Edge)</span><br><span class="line">df_Edge.columns = [<span class="string">&#x27;Source&#x27;</span>, <span class="string">&#x27;Target&#x27;</span>, <span class="string">&#x27;Weight&#x27;</span>]</span><br><span class="line">df_Edge = df_Edge[df_Edge[<span class="string">&#x27;Weight&#x27;</span>] &gt; <span class="number">1</span>]</span><br><span class="line">df_Edge.to_csv(<span class="string">&#x27;Edge.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">Node = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sorted_gram_count[:<span class="number">76</span>]:</span><br><span class="line">    Node.append((i[<span class="number">0</span>], i[<span class="number">0</span>], i[<span class="number">1</span>]))</span><br><span class="line">df_Node = pd.DataFrame(Node)</span><br><span class="line">df_Node.columns = [<span class="string">&#x27;Source&#x27;</span>, <span class="string">&#x27;Target&#x27;</span>, <span class="string">&#x27;Weight&#x27;</span>]</span><br><span class="line">df_Node.to_csv(<span class="string">&#x27;Node.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line">G = nx.Graph()</span><br><span class="line">G.add_weighted_edges_from(Edge)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sorted_gram_count[:<span class="number">76</span>]:</span><br><span class="line">    G.add_node(i[<span class="number">0</span>], weight=i[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># saving graph created above in gexf format</span></span><br><span class="line">nx.write_gexf(G, <span class="string">&quot;keywords.gexf&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="参考资料">参考资料</h1>
<h2 id="创建networkx-图">创建NetworkX 图</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line">G = nx.Graph()</span><br><span class="line">G.add_weighted_edges_from(Edge)</span><br></pre></td></tr></table></figure>
<h2 id="以-gexf-格式保存-networkx-图">以 gexf 格式保存 NetworkX 图</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># importing the required module</span></span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">G = nx.from_pandas_dataframe(df_new, <span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;target&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment"># saving graph created above in gexf format</span></span><br><span class="line">nx.write_gexf(G, <span class="string">&quot;geeksforgeeks.gexf&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="不同任务又分为哪些子任务">不同任务又分为哪些子任务</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">term = <span class="string">&quot;extraction&quot;</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;candidates.csv&#x27;</span>)</span><br><span class="line">df.columns = [<span class="string">&#x27;Label&#x27;</span>, <span class="string">&#x27;count&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;Label&#x27;</span>] = df[<span class="string">&quot;Label&quot;</span>].<span class="built_in">str</span>.lower()</span><br><span class="line">df_new = df[df[<span class="string">&#x27;Label&#x27;</span>].<span class="built_in">str</span>.contains(<span class="string">&quot; &quot;</span>+term)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">aggregation_functions = &#123;<span class="string">&#x27;count&#x27;</span>: <span class="string">&#x27;sum&#x27;</span>&#125;</span><br><span class="line">df_new = df_new.groupby(df_new[<span class="string">&#x27;Label&#x27;</span>]).aggregate(aggregation_functions)</span><br><span class="line"></span><br><span class="line">df_new.sort_values(by=[<span class="string">&#x27;count&#x27;</span>, <span class="string">&#x27;Label&#x27;</span>], ascending=[<span class="literal">False</span>, <span class="literal">True</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">df_new.reset_index()</span><br><span class="line"></span><br><span class="line">print(df_new.head())</span><br><span class="line">df_new.to_csv(term+<span class="string">&#x27;.csv&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;, &#x27;</span>.join(df_new[df_new[<span class="string">&#x27;count&#x27;</span>]&lt;=<span class="number">3</span>].index.tolist()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="按照年份">按照年份</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_most_common</span>(<span class="params">all_candidates</span>):</span></span><br><span class="line">    sorted_gram_count = Counter(all_candidates.split(<span class="string">&#x27;,&#x27;</span>)).most_common()</span><br><span class="line">    sorted_gram_count = [i <span class="keyword">for</span> i <span class="keyword">in</span> sorted_gram_count <span class="keyword">if</span> <span class="string">&#x27; &#x27;</span> <span class="keyword">in</span> i[<span class="number">0</span>] <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">list</span>(<span class="built_in">set</span>(<span class="built_in">list</span>(i[<span class="number">0</span>])) &amp; <span class="built_in">set</span>(punctuation)) <span class="keyword">or</span> <span class="keyword">not</span> i[<span class="number">0</span>].islower()]</span><br><span class="line">    <span class="keyword">return</span> sorted_gram_count[:<span class="number">25</span>]</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df[<span class="string">&#x27;terms&#x27;</span>] = df[<span class="string">&#x27;candidate&#x27;</span>].apply(find_most_common)</span><br><span class="line"><span class="comment"># print(df)</span></span><br><span class="line">df = df[[<span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;terms&#x27;</span>, <span class="string">&#x27;paper_num&#x27;</span>]][:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_terms = pd.DataFrame()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(df.shape[<span class="number">0</span>]):</span><br><span class="line">    terms = pd.DataFrame(df[<span class="string">&#x27;terms&#x27;</span>][i])</span><br><span class="line">    terms.columns=[df[<span class="string">&#x27;year&#x27;</span>][i], <span class="string">&#x27;count&#x27;</span>]</span><br><span class="line">    df_terms = pd.concat([df_terms, terms], axis=<span class="number">1</span>, sort=<span class="literal">False</span>)</span><br><span class="line">df_terms.to_csv(<span class="string">&#x27;candidates_by_year4.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">print(df_terms)</span><br></pre></td></tr></table></figure>
<p>想使用外部知识但是失败了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;D:\pythonProject\KEB\All.md.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    s = f.read()</span><br><span class="line">seeds = re.findall(<span class="string">r&#x27;AITD-[0-9]&#123;5&#125;\|[^|]+\|&#x27;</span>, s, re.M)</span><br><span class="line">seeds = <span class="built_in">list</span>(<span class="built_in">set</span>([seed[<span class="number">11</span>:-<span class="number">1</span>] <span class="keyword">for</span> seed <span class="keyword">in</span> seeds]))</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">seed_list = random.sample(seeds, <span class="number">20</span>)</span><br><span class="line">print(seed_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;array&#125;&#123;|c|c|&#125;</span><br><span class="line">\hline \text &#123; FOCUS &#125; &amp; \begin&#123;array&#125;&#123;c&#125;</span><br><span class="line">\text &#123; present &#125; \rightarrow \text &#123; (direct object) &#125; \\</span><br><span class="line">\text &#123; work &#125; \rightarrow \text &#123; (preposition_on) &#125; \\</span><br><span class="line">\text &#123; propose &#125; \rightarrow \text &#123; (direct object) &#125;</span><br><span class="line">\end&#123;array&#125; \\</span><br><span class="line">\hline \text &#123; TECHNIQUE &#125; &amp; \text &#123; using &#125; \rightarrow \text &#123; (direct object) &#125; \\</span><br><span class="line">&amp; \text &#123; apply &#125; \rightarrow \text &#123; (direct object) &#125; \\</span><br><span class="line">&amp; \text &#123; extend &#125; \rightarrow \text &#123; (direct object) &#125; \\</span><br><span class="line">\hline \text &#123; DOMAIN &#125; &amp; \begin&#123;array&#125;&#123;c&#125;</span><br><span class="line">\text &#123; system &#125; \rightarrow \text &#123; (preposition_for) &#125; \\</span><br><span class="line">\text &#123; task &#125; \rightarrow \text &#123; (preposition_of) &#125; \\</span><br><span class="line">\text &#123; framework &#125; \rightarrow \text &#123; (preposition_for) &#125;</span><br><span class="line">\end&#123;array&#125; \\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;array&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">METHOD | MODELS | USEFUL | EFFORTS | PARADIGM | IMPROVING FOR (POBJ)</span><br><span class="line">GOAL | PROBLEM | TASK | DEVELOPMENT OF (POBJ)</span><br><span class="line">FOCUS | WORK ON (POBJ)</span><br><span class="line">IMPROVEMTN | PROGRESS | | EFFECTIVENESS | USED IN (POBJ)</span><br><span class="line">(NSUBJ) AIMS</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">USE | EMPLOY | PROPOSE | PRESENT | APPLY (DOBJ)</span><br><span class="line">TRAINED WITH (POBJ)</span><br><span class="line">BASED ON (POBJ)</span><br><span class="line">PERFORMANCE OF (POBJ)</span><br><span class="line">FASTER | BETTER | MORE THAN</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ON (POBJ) NSUBJ ACHIEVE</span><br><span class="line">SCORE | EXPERIMENTS | RESULTS | PERFORMANCE ON (POBJ)</span><br><span class="line">DATASET LIKE | AS | (POBJ)</span><br><span class="line">DATASETS APPOS</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GAIN ON (DOBJ)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>英文词性标记汇总</title>
    <url>/project2/</url>
    <content><![CDATA[<h1 id="全部词性获取代码">全部词性获取代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> nlp.get_pipe(<span class="string">&quot;parser&quot;</span>).labels:</span><br><span class="line">    print(<span class="string">&#x27;`&#x27;</span>+label+<span class="string">&#x27;`: &#x27;</span>+<span class="built_in">str</span>(spacy.explain(label)))</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="所有universal-pos标签">所有universal pos标签</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># POS tags</span></span><br><span class="line">    <span class="comment"># Universal POS Tags</span></span><br><span class="line">    <span class="comment"># http://universaldependencies.org/u/pos/</span></span><br><span class="line">    <span class="string">&quot;ADJ&quot;</span>: <span class="string">&quot;adjective&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ADP&quot;</span>: <span class="string">&quot;adposition&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ADV&quot;</span>: <span class="string">&quot;adverb&quot;</span>,</span><br><span class="line">    <span class="string">&quot;AUX&quot;</span>: <span class="string">&quot;auxiliary&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CONJ&quot;</span>: <span class="string">&quot;conjunction&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CCONJ&quot;</span>: <span class="string">&quot;coordinating conjunction&quot;</span>,</span><br><span class="line">    <span class="string">&quot;DET&quot;</span>: <span class="string">&quot;determiner&quot;</span>,</span><br><span class="line">    <span class="string">&quot;INTJ&quot;</span>: <span class="string">&quot;interjection&quot;</span>,</span><br><span class="line">    <span class="string">&quot;NOUN&quot;</span>: <span class="string">&quot;noun&quot;</span>,</span><br><span class="line">    <span class="string">&quot;NUM&quot;</span>: <span class="string">&quot;numeral&quot;</span>,</span><br><span class="line">    <span class="string">&quot;PART&quot;</span>: <span class="string">&quot;particle&quot;</span>,</span><br><span class="line">    <span class="string">&quot;PRON&quot;</span>: <span class="string">&quot;pronoun&quot;</span>,</span><br><span class="line">    <span class="string">&quot;PROPN&quot;</span>: <span class="string">&quot;proper noun&quot;</span>,</span><br><span class="line">    <span class="string">&quot;PUNCT&quot;</span>: <span class="string">&quot;punctuation&quot;</span>,</span><br><span class="line">    <span class="string">&quot;SCONJ&quot;</span>: <span class="string">&quot;subordinating conjunction&quot;</span>,</span><br><span class="line">    <span class="string">&quot;SYM&quot;</span>: <span class="string">&quot;symbol&quot;</span>,</span><br><span class="line">    <span class="string">&quot;VERB&quot;</span>: <span class="string">&quot;verb&quot;</span>,</span><br><span class="line">    <span class="string">&quot;X&quot;</span>: <span class="string">&quot;other&quot;</span>,</span><br><span class="line">    <span class="string">&quot;EOL&quot;</span>: <span class="string">&quot;end of line&quot;</span>,</span><br><span class="line">    <span class="string">&quot;SPACE&quot;</span>: <span class="string">&quot;space&quot;</span>,</span><br></pre></td></tr></table></figure>
<h1 id="所有细粒度pos标签">所有细粒度pos标签</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;.&quot;</span>: <span class="string">&quot;punctuation mark, sentence closer&quot;</span>,</span><br><span class="line">   <span class="string">&quot;,&quot;</span>: <span class="string">&quot;punctuation mark, comma&quot;</span>,</span><br><span class="line">   <span class="string">&quot;-LRB-&quot;</span>: <span class="string">&quot;left round bracket&quot;</span>,</span><br><span class="line">   <span class="string">&quot;-RRB-&quot;</span>: <span class="string">&quot;right round bracket&quot;</span>,</span><br><span class="line">   <span class="string">&quot;``&quot;</span>: <span class="string">&quot;opening quotation mark&quot;</span>,</span><br><span class="line">   <span class="string">&#x27;&quot;&quot;&#x27;</span>: <span class="string">&quot;closing quotation mark&quot;</span>,</span><br><span class="line">   <span class="string">&quot;&#x27;&#x27;&quot;</span>: <span class="string">&quot;closing quotation mark&quot;</span>,</span><br><span class="line">   <span class="string">&quot;:&quot;</span>: <span class="string">&quot;punctuation mark, colon or ellipsis&quot;</span>,</span><br><span class="line">   <span class="string">&quot;$&quot;</span>: <span class="string">&quot;symbol, currency&quot;</span>,</span><br><span class="line">   <span class="string">&quot;#&quot;</span>: <span class="string">&quot;symbol, number sign&quot;</span>,</span><br><span class="line">   <span class="string">&quot;AFX&quot;</span>: <span class="string">&quot;affix&quot;</span>,</span><br><span class="line">   <span class="string">&quot;CC&quot;</span>: <span class="string">&quot;conjunction, coordinating&quot;</span>,</span><br><span class="line">   <span class="string">&quot;CD&quot;</span>: <span class="string">&quot;cardinal number&quot;</span>,</span><br><span class="line">   <span class="string">&quot;DT&quot;</span>: <span class="string">&quot;determiner&quot;</span>,</span><br><span class="line">   <span class="string">&quot;EX&quot;</span>: <span class="string">&quot;existential there&quot;</span>,</span><br><span class="line">   <span class="string">&quot;FW&quot;</span>: <span class="string">&quot;foreign word&quot;</span>,</span><br><span class="line">   <span class="string">&quot;HYPH&quot;</span>: <span class="string">&quot;punctuation mark, hyphen&quot;</span>,</span><br><span class="line">   <span class="string">&quot;IN&quot;</span>: <span class="string">&quot;conjunction, subordinating or preposition&quot;</span>,</span><br><span class="line">   <span class="string">&quot;JJ&quot;</span>: <span class="string">&quot;adjective (English), other noun-modifier (Chinese)&quot;</span>,</span><br><span class="line">   <span class="string">&quot;JJR&quot;</span>: <span class="string">&quot;adjective, comparative&quot;</span>,</span><br><span class="line">   <span class="string">&quot;JJS&quot;</span>: <span class="string">&quot;adjective, superlative&quot;</span>,</span><br><span class="line">   <span class="string">&quot;LS&quot;</span>: <span class="string">&quot;list item marker&quot;</span>,</span><br><span class="line">   <span class="string">&quot;MD&quot;</span>: <span class="string">&quot;verb, modal auxiliary&quot;</span>,</span><br><span class="line">   <span class="string">&quot;NIL&quot;</span>: <span class="string">&quot;missing tag&quot;</span>,</span><br><span class="line">   <span class="string">&quot;NN&quot;</span>: <span class="string">&quot;noun, singular or mass&quot;</span>,</span><br><span class="line">   <span class="string">&quot;NNP&quot;</span>: <span class="string">&quot;noun, proper singular&quot;</span>,</span><br><span class="line">   <span class="string">&quot;NNPS&quot;</span>: <span class="string">&quot;noun, proper plural&quot;</span>,</span><br><span class="line">   <span class="string">&quot;NNS&quot;</span>: <span class="string">&quot;noun, plural&quot;</span>,</span><br><span class="line">   <span class="string">&quot;PDT&quot;</span>: <span class="string">&quot;predeterminer&quot;</span>,</span><br><span class="line">   <span class="string">&quot;POS&quot;</span>: <span class="string">&quot;possessive ending&quot;</span>,</span><br><span class="line">   <span class="string">&quot;PRP&quot;</span>: <span class="string">&quot;pronoun, personal&quot;</span>,</span><br><span class="line">   <span class="string">&quot;PRP$&quot;</span>: <span class="string">&quot;pronoun, possessive&quot;</span>,</span><br><span class="line">   <span class="string">&quot;RB&quot;</span>: <span class="string">&quot;adverb&quot;</span>,</span><br><span class="line">   <span class="string">&quot;RBR&quot;</span>: <span class="string">&quot;adverb, comparative&quot;</span>,</span><br><span class="line">   <span class="string">&quot;RBS&quot;</span>: <span class="string">&quot;adverb, superlative&quot;</span>,</span><br><span class="line">   <span class="string">&quot;RP&quot;</span>: <span class="string">&quot;adverb, particle&quot;</span>,</span><br><span class="line">   <span class="string">&quot;TO&quot;</span>: <span class="string">&#x27;infinitival &quot;to&quot;&#x27;</span>,</span><br><span class="line">   <span class="string">&quot;UH&quot;</span>: <span class="string">&quot;interjection&quot;</span>,</span><br><span class="line">   <span class="string">&quot;VB&quot;</span>: <span class="string">&quot;verb, base form&quot;</span>,</span><br><span class="line">   <span class="string">&quot;VBD&quot;</span>: <span class="string">&quot;verb, past tense&quot;</span>,</span><br><span class="line">   <span class="string">&quot;VBG&quot;</span>: <span class="string">&quot;verb, gerund or present participle&quot;</span>,</span><br><span class="line">   <span class="string">&quot;VBN&quot;</span>: <span class="string">&quot;verb, past participle&quot;</span>,</span><br><span class="line">   <span class="string">&quot;VBP&quot;</span>: <span class="string">&quot;verb, non-3rd person singular present&quot;</span>,</span><br><span class="line">   <span class="string">&quot;VBZ&quot;</span>: <span class="string">&quot;verb, 3rd person singular present&quot;</span>,</span><br><span class="line">   <span class="string">&quot;WDT&quot;</span>: <span class="string">&quot;wh-determiner&quot;</span>,</span><br><span class="line">   <span class="string">&quot;WP&quot;</span>: <span class="string">&quot;wh-pronoun, personal&quot;</span>,</span><br><span class="line">   <span class="string">&quot;WP$&quot;</span>: <span class="string">&quot;wh-pronoun, possessive&quot;</span>,</span><br><span class="line">   <span class="string">&quot;WRB&quot;</span>: <span class="string">&quot;wh-adverb&quot;</span>,</span><br><span class="line">   <span class="string">&quot;SP&quot;</span>: <span class="string">&quot;space (English), sentence-final particle (Chinese)&quot;</span>,</span><br><span class="line">   <span class="string">&quot;ADD&quot;</span>: <span class="string">&quot;email&quot;</span>,</span><br><span class="line">   <span class="string">&quot;NFP&quot;</span>: <span class="string">&quot;superfluous punctuation&quot;</span>,</span><br><span class="line">   <span class="string">&quot;GW&quot;</span>: <span class="string">&quot;additional word in multi-word expression&quot;</span>,</span><br><span class="line">   <span class="string">&quot;XX&quot;</span>: <span class="string">&quot;unknown&quot;</span>,</span><br><span class="line">   <span class="string">&quot;BES&quot;</span>: <span class="string">&#x27;auxiliary &quot;be&quot;&#x27;</span>,</span><br><span class="line">   <span class="string">&quot;HVS&quot;</span>: <span class="string">&#x27;forms of &quot;have&quot;&#x27;</span>,</span><br></pre></td></tr></table></figure>
<h1 id="所有dependency标签">所有dependency标签</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Dependency Labels (English)</span></span><br><span class="line">    <span class="comment"># ClearNLP / Universal Dependencies</span></span><br><span class="line">    <span class="comment"># https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md</span></span><br><span class="line">    <span class="string">&quot;acl&quot;</span>: <span class="string">&quot;clausal modifier of noun (adjectival clause)&quot;</span>,</span><br><span class="line">    <span class="string">&quot;acomp&quot;</span>: <span class="string">&quot;adjectival complement&quot;</span>,</span><br><span class="line">    <span class="string">&quot;advcl&quot;</span>: <span class="string">&quot;adverbial clause modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;advmod&quot;</span>: <span class="string">&quot;adverbial modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;agent&quot;</span>: <span class="string">&quot;agent&quot;</span>,</span><br><span class="line">    <span class="string">&quot;amod&quot;</span>: <span class="string">&quot;adjectival modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;appos&quot;</span>: <span class="string">&quot;appositional modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;attr&quot;</span>: <span class="string">&quot;attribute&quot;</span>,</span><br><span class="line">    <span class="string">&quot;aux&quot;</span>: <span class="string">&quot;auxiliary&quot;</span>,</span><br><span class="line">    <span class="string">&quot;auxpass&quot;</span>: <span class="string">&quot;auxiliary (passive)&quot;</span>,</span><br><span class="line">    <span class="string">&quot;case&quot;</span>: <span class="string">&quot;case marking&quot;</span>,</span><br><span class="line">    <span class="string">&quot;cc&quot;</span>: <span class="string">&quot;coordinating conjunction&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ccomp&quot;</span>: <span class="string">&quot;clausal complement&quot;</span>,</span><br><span class="line">    <span class="string">&quot;clf&quot;</span>: <span class="string">&quot;classifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;complm&quot;</span>: <span class="string">&quot;complementizer&quot;</span>,</span><br><span class="line">    <span class="string">&quot;compound&quot;</span>: <span class="string">&quot;compound&quot;</span>,</span><br><span class="line">    <span class="string">&quot;conj&quot;</span>: <span class="string">&quot;conjunct&quot;</span>,</span><br><span class="line">    <span class="string">&quot;cop&quot;</span>: <span class="string">&quot;copula&quot;</span>,</span><br><span class="line">    <span class="string">&quot;csubj&quot;</span>: <span class="string">&quot;clausal subject&quot;</span>,</span><br><span class="line">    <span class="string">&quot;csubjpass&quot;</span>: <span class="string">&quot;clausal subject (passive)&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dative&quot;</span>: <span class="string">&quot;dative&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dep&quot;</span>: <span class="string">&quot;unclassified dependent&quot;</span>,</span><br><span class="line">    <span class="string">&quot;det&quot;</span>: <span class="string">&quot;determiner&quot;</span>,</span><br><span class="line">    <span class="string">&quot;discourse&quot;</span>: <span class="string">&quot;discourse element&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dislocated&quot;</span>: <span class="string">&quot;dislocated elements&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dobj&quot;</span>: <span class="string">&quot;direct object&quot;</span>,</span><br><span class="line">    <span class="string">&quot;expl&quot;</span>: <span class="string">&quot;expletive&quot;</span>,</span><br><span class="line">    <span class="string">&quot;fixed&quot;</span>: <span class="string">&quot;fixed multiword expression&quot;</span>,</span><br><span class="line">    <span class="string">&quot;flat&quot;</span>: <span class="string">&quot;flat multiword expression&quot;</span>,</span><br><span class="line">    <span class="string">&quot;goeswith&quot;</span>: <span class="string">&quot;goes with&quot;</span>,</span><br><span class="line">    <span class="string">&quot;hmod&quot;</span>: <span class="string">&quot;modifier in hyphenation&quot;</span>,</span><br><span class="line">    <span class="string">&quot;hyph&quot;</span>: <span class="string">&quot;hyphen&quot;</span>,</span><br><span class="line">    <span class="string">&quot;infmod&quot;</span>: <span class="string">&quot;infinitival modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;intj&quot;</span>: <span class="string">&quot;interjection&quot;</span>,</span><br><span class="line">    <span class="string">&quot;iobj&quot;</span>: <span class="string">&quot;indirect object&quot;</span>,</span><br><span class="line">    <span class="string">&quot;list&quot;</span>: <span class="string">&quot;list&quot;</span>,</span><br><span class="line">    <span class="string">&quot;mark&quot;</span>: <span class="string">&quot;marker&quot;</span>,</span><br><span class="line">    <span class="string">&quot;meta&quot;</span>: <span class="string">&quot;meta modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;neg&quot;</span>: <span class="string">&quot;negation modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;nmod&quot;</span>: <span class="string">&quot;modifier of nominal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;nn&quot;</span>: <span class="string">&quot;noun compound modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;npadvmod&quot;</span>: <span class="string">&quot;noun phrase as adverbial modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;nsubj&quot;</span>: <span class="string">&quot;nominal subject&quot;</span>,</span><br><span class="line">    <span class="string">&quot;nsubjpass&quot;</span>: <span class="string">&quot;nominal subject (passive)&quot;</span>,</span><br><span class="line">    <span class="string">&quot;nounmod&quot;</span>: <span class="string">&quot;modifier of nominal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;npmod&quot;</span>: <span class="string">&quot;noun phrase as adverbial modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;num&quot;</span>: <span class="string">&quot;number modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;number&quot;</span>: <span class="string">&quot;number compound modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;nummod&quot;</span>: <span class="string">&quot;numeric modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;oprd&quot;</span>: <span class="string">&quot;object predicate&quot;</span>,</span><br><span class="line">    <span class="string">&quot;obj&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">    <span class="string">&quot;obl&quot;</span>: <span class="string">&quot;oblique nominal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;orphan&quot;</span>: <span class="string">&quot;orphan&quot;</span>,</span><br><span class="line">    <span class="string">&quot;parataxis&quot;</span>: <span class="string">&quot;parataxis&quot;</span>,</span><br><span class="line">    <span class="string">&quot;partmod&quot;</span>: <span class="string">&quot;participal modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;pcomp&quot;</span>: <span class="string">&quot;complement of preposition&quot;</span>,</span><br><span class="line">    <span class="string">&quot;pobj&quot;</span>: <span class="string">&quot;object of preposition&quot;</span>,</span><br><span class="line">    <span class="string">&quot;poss&quot;</span>: <span class="string">&quot;possession modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;possessive&quot;</span>: <span class="string">&quot;possessive modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;preconj&quot;</span>: <span class="string">&quot;pre-correlative conjunction&quot;</span>,</span><br><span class="line">    <span class="string">&quot;prep&quot;</span>: <span class="string">&quot;prepositional modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;prt&quot;</span>: <span class="string">&quot;particle&quot;</span>,</span><br><span class="line">    <span class="string">&quot;punct&quot;</span>: <span class="string">&quot;punctuation&quot;</span>,</span><br><span class="line">    <span class="string">&quot;quantmod&quot;</span>: <span class="string">&quot;modifier of quantifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;rcmod&quot;</span>: <span class="string">&quot;relative clause modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;relcl&quot;</span>: <span class="string">&quot;relative clause modifier&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reparandum&quot;</span>: <span class="string">&quot;overridden disfluency&quot;</span>,</span><br><span class="line">    <span class="string">&quot;root&quot;</span>: <span class="string">&quot;root&quot;</span>,</span><br><span class="line">    <span class="string">&quot;vocative&quot;</span>: <span class="string">&quot;vocative&quot;</span>,</span><br><span class="line">    <span class="string">&quot;xcomp&quot;</span>: <span class="string">&quot;open clausal complement&quot;</span>,</span><br></pre></td></tr></table></figure>
<h1 id="链接">链接</h1>
<p>https://github.com/explosion/spaCy/blob/master/spacy/glossary.py</p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>正则表达式进阶</title>
    <url>/regex-1/</url>
    <content><![CDATA[<p>至此，我们已经建立了正则表达式的思维框架（参见「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">正则表达式入门</a>」），以及如何用python中的re模块编译正则表达式来匹配文本（参见「<a href="https://mp.weixin.qq.com/s/ibNb0rOSnBr4YC0PzCyIBA">Python实操篇</a>」）。尽管此时我们已经可以流畅地编写和使用它，但我们需要一些进阶知识，让我们的正则表达式更准确和精练。本文使用的语言依然是python。 <span id="more"></span></p>
<p><strong>全文概览：</strong></p>
<p>1.分组与捕获：<code>MatchObject.group()</code>的奥秘</p>
<p>2.四种类型的环视：匹配位置，而非匹配文本</p>
<p>3.贪婪与非贪婪：匹配优先 VS 忽略优先</p>
<h1 id="分组与捕获">分组与捕获</h1>
<p>在「<a href="https://mp.weixin.qq.com/s/ibNb0rOSnBr4YC0PzCyIBA">Python实操篇</a>」我们讲到<code>MatchObject</code>的方法<code>group()</code>可以返回匹配的整个字符串，但这并不全面。本节，我们将谈谈括号的一个关键功能：捕获（capturing）。</p>
<p><strong>捕获</strong>就是用括号提取文本以便后续访问，我们可以把这个过程想象成“闰土捕鸟”，把每个括号里的鸟儿都抓起来放进“笼子”。</p>
<p>而捕获进所有“笼子”里的内容，正是通过<code>MatchObject.groups()</code>访问。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.search(<span class="string">r&quot;(\w+) (\w+)&quot;</span>, <span class="string">&quot;John Smith&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：(&#x27;John&#x27;, &#x27;Smith&#x27;)</span></span><br></pre></td></tr></table></figure>
<p>我们可以通过索引来访问每个“笼子”，<code>group(0)</code>返回整个正则表达式匹配的文本，相当于编号为0的隐式捕获，而<code>group(1)</code>、<code>group(2)</code>则按顺序返回显式捕获分组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = re.search(<span class="string">r&quot;(\w+) (\w+)&quot;</span>, <span class="string">&quot;John Smith&quot;</span>)</span><br><span class="line">m.group(<span class="number">0</span>) <span class="comment"># 输出：&#x27;John Smith&#x27;</span></span><br><span class="line">m.group(<span class="number">1</span>)  <span class="comment"># 输出：&#x27;John&#x27;</span></span><br><span class="line">m.group(<span class="number">2</span>)  <span class="comment"># 输出：&#x27;Smith&#x27;</span></span><br></pre></td></tr></table></figure>
<p>你可能会想到「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">入门篇</a>」讲过的<strong>反向引用</strong>，在python中我们依然可以使用序号<code>\1</code>、<code>\2</code>来引用捕获的组别。下面这个例子将“数字-字母”组成的产品ID进行替换，变成了“字母-数字”的形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\d+)-(\w+)&quot;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&quot;\2-\1&quot;</span>, <span class="string">&quot;1-a\n20-baer\n34-afcr&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;a-1\nbaer-20\nafcr-34&#x27;</span></span><br></pre></td></tr></table></figure>
<p>我们也可以给每个“笼子”取上名字，即<strong>命名捕获</strong>，在python正则表达式中表示为<code>(?P&lt;name&gt;pattern)</code>，依然是通过<code>group()</code>访问单个“笼子”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(?P&lt;first&gt;\w+) (?P&lt;last&gt;\w+)&quot;</span>)</span><br><span class="line">match = pattern.search(<span class="string">&quot;John Smith&quot;</span>)</span><br><span class="line">match.group(<span class="string">&quot;first&quot;</span>) <span class="comment"># 输出：&#x27;John&#x27;</span></span><br><span class="line">match.group(<span class="string">&quot;last&quot;</span>) <span class="comment"># 输出：&#x27;Smith&#x27;</span></span><br></pre></td></tr></table></figure>
<p>需要注意，在<code>sub()</code>替换操作中，如果用名称来引用组别，我们需要写成<code>\g&lt;name&gt;</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(?P&lt;country&gt;\d+)-(?P&lt;id&gt;\w+)&quot;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&quot;\g&lt;id&gt;-\g&lt;country&gt;&quot;</span>, <span class="string">&quot;1-a\n20-baer\n34-afcr&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;a-1\nbaer-20\nafcr-34&#x27;</span></span><br></pre></td></tr></table></figure>
<p>当然，很多时候我们使用括号不是为了捕获，而仅仅是为了<strong>分组</strong>，即用于构建子表达式、多选结构或者量词作用的对象。此时，我们可以使用<strong>非捕获型括号</strong><code>(?:)</code>，告诉正则引擎，不需要提取括号内的任何信息。非捕获型括号能够提高效率、节约内存，不浪费咱们的“笼子”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.search(<span class="string">r&quot;gr(a|e)y&quot;</span>, <span class="string">&quot;gray&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：(&#x27;a&#x27;,)</span></span><br><span class="line">re.search(<span class="string">r&quot;gr(?:a|e)y&quot;</span>, <span class="string">&quot;gray&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：()</span></span><br></pre></td></tr></table></figure>
<h1 id="四种类型的环视">四种类型的环视</h1>
<p><strong>环视</strong>（Look Around）是正则表达式最强大的技术之一。我们可以把环视想象成前视镜和后视镜，<strong>顺序环视</strong>就是向前看（从左到右），<strong>逆序环视</strong>就是向后看（从右到左）。通过左右环视，我们进行“闰土捕鸟”的位置会更精确，确保匹配内容的上下文满足特定要求。</p>
<p>以下是四种类型的环视：</p>
<table>
<thead>
<tr class="header">
<th>类型</th>
<th>正则表达式</th>
<th>匹配成功的条件</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>肯定顺序环视</td>
<td>(?=……)</td>
<td>子表达式能够匹配右侧文本</td>
</tr>
<tr class="even">
<td>否定顺序环视</td>
<td>(?!……)</td>
<td>子表达式不能匹配右侧文本</td>
</tr>
<tr class="odd">
<td>肯定逆序环视</td>
<td>(?&lt;=……)</td>
<td>子表达式能够匹配左侧文本</td>
</tr>
<tr class="even">
<td>否定逆序环视</td>
<td>(?&lt;!……)</td>
<td>子表达式不能匹配左侧文本</td>
</tr>
</tbody>
</table>
<p>我们要注意，环视相当于作用于匹配位置的附加条件、不占用任何字符。因此它和分界符类似，是一种<strong>零宽度断言</strong>（zero-width assertions）。我们拿肯定顺序环视举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?=fox)&#x27;</span>)</span><br><span class="line">result = pattern.search(<span class="string">&quot;The quick brown fox jumps over the lazy dog&quot;</span>)</span><br><span class="line">result.span()</span><br><span class="line"><span class="comment">#输出 (16, 16)</span></span><br></pre></td></tr></table></figure>
<p>我们发现，表达式<code>(?=fox)</code>只匹配fox之前的位置，也就是索引16。</p>
<p><img src="C:\Users\13607\AppData\Roaming\Typora\typora-user-images\image-20210218222948086.png" alt="image-20210218222948086" style="zoom: 33%;" /></p>
<p>环视的作用不可小觑，我们可以精准定位，并剔除多余字符，保留更加干净、准确的匹配文本。下面的例子定位<code>&lt;p&gt;</code>标签的同时，只匹配标签里的内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;=&lt;p&gt;)[^&lt;]+(?=&lt;p&gt;)&#x27;</span>)</span><br><span class="line">pattern.search(<span class="string">&quot;&lt;p&gt;test&lt;p&gt;&quot;</span>).group(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;test&#x27;</span></span><br></pre></td></tr></table></figure>
<p>环视的另一典型应用就是将文本变成逗号分隔的货币形式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;\d&#123;1,3&#125;(?=(\d&#123;3&#125;)+(?!\d))&#x27;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&#x27;\g&lt;0&gt;,&#x27;</span>, <span class="string">&quot;123456789&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;123,456,789&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="贪婪与非贪婪">贪婪与非贪婪</h1>
<p>在「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">入门篇</a>」中，我们接触了量词，但并未讲到贪婪与非贪婪的区别。</p>
<p>在python的re模块中，量词默认为<strong>贪婪模式</strong>：尽可能多地匹配更长的字符串。这就是为什么，<code>.*</code>通常会匹配到一行文本的末尾。如果要采用<strong>非贪婪模式</strong>，我们可以在量词后添加一个额外的问号，例如<code>??</code>、<code>*?</code>或<code>+?</code>，使得匹配的长度最小。这两种模式又称为<strong>匹配优先</strong>和<strong>忽略优先</strong>。</p>
<ul>
<li><p>贪婪量词（Greedy quantifiers）：<code>?</code>, <code>*</code>, <code>+</code>, <code>&#123;num,num&#125;</code></p></li>
<li><p>非贪婪/懒惰量词（Lazy quantifiers）：<code>??</code>, <code>*?</code>, <code>+?</code> , <code>&#123;num,num&#125;?</code></p></li>
</ul>
<p>比如，如果要匹配引号内的内容，会得到下面的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = <span class="string">&#x27;The name &quot;McDonald\&#x27;s&quot; ! is said &quot;makudonarudo&quot;in Japanese.&#x27;</span></span><br><span class="line">re.search(<span class="string">r&#x27;&quot;.*&quot;&#x27;</span>, s).group()</span><br><span class="line"><span class="comment"># 输出：&#x27;&quot;McDonald\&#x27;s&quot; is said &quot;makudonarudo&quot;&#x27;</span></span><br><span class="line">re.search(<span class="string">r&#x27;&quot;.*?&quot;&#x27;</span>, s).group()</span><br><span class="line"><span class="comment"># 输出：&#x27;&quot;McDonald\&#x27;s&quot;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>事实上，还有第三种量词，但目前python并不支持：</p>
<ul>
<li>占有量词（Possessive quantifiers）：<code>?+</code>, <code>*+</code>, <code>++</code>, <code>&#123;num,num&#125;+</code></li>
</ul>
<p>占有量词与贪婪量词类似，只是它们从不会交还已经匹配的字符。当然，我们也可以用固化分组<code>(?&gt;...)</code>来代替，比如<code>!.++</code>其实等同于<code>(?&gt;!.+)</code>。然而，固化分组在python中同样不被支持，所以此处不过多讨论。</p>
<p>以上です！</p>
<p>整理完python正则表达式的进阶知识，相信你已经能得心应手解决很多问题。但是，要真正打造高效、规范、美妙的正则表达式，我们仍需要了解正则引擎的原理，以及一些平衡法则、以及测试和优化的技巧，我们下篇再谈！</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title>利用python监听剪贴板内容并修改-工具开发</title>
    <url>/pyperclip/</url>
    <content><![CDATA[<p>使用得工具是pyperclip，目的是复制论文片段，去掉pdf中多余的换行符。要实现的操作是：获取当前剪贴板的数据后修改数据，把新的数据放入剪贴板，用于粘贴。</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyperclip</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">iter_1 = pyperclip.paste()</span><br><span class="line">iter_3 = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">while</span> iter_1 != <span class="string">&quot;stop &quot;</span>:</span><br><span class="line">	time.sleep(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">	iter_2 = pyperclip.paste()</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 如果剪切板发生变化</span></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(iter_2) &gt; <span class="number">1</span> <span class="keyword">and</span> iter_1 != iter_2 <span class="keyword">and</span> iter_1 != iter_3:</span><br><span class="line">		<span class="comment"># print(&#x27;监听到变化！&#x27;)</span></span><br><span class="line">		<span class="comment"># 改变现在的剪切板</span></span><br><span class="line">		iter_3 = re.sub(<span class="string">r&quot;\s&#123;2,&#125;&quot;</span>, <span class="string">&quot; &quot;</span>, iter_2) + <span class="string">&#x27; &#x27;</span></span><br><span class="line">		<span class="comment"># 把改变后的剪切板保存起来</span></span><br><span class="line">		pyperclip.copy(iter_3)</span><br><span class="line">		print(<span class="string">&#x27;改变后的剪切板: &#x27;</span>+ iter_3)</span><br><span class="line"></span><br><span class="line">		iter_1 = iter_3</span><br><span class="line">		iter_3 = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>https://stevepython.wordpress.com/2020/03/10/detect-clipboard-text-and-images/</p>
<p>以后使用输入如下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python D:\documents\tools\mini-tools\text-process\copy.py  </span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>期刊翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫代码-全</title>
    <url>/scraping/</url>
    <content><![CDATA[<h1 id="常用框架工具和代码">常用框架、工具和代码</h1>
<h2 id="框架">框架</h2>
<p>熟悉scrapy框架、pyspider框架等。</p>
<h1 id="期待完成的项目">期待完成的项目</h1>
<h2 id="多媒体">多媒体</h2>
<h3 id="爬取网站音视频文件并整合">爬取网站音视频文件并整合</h3>
<p>https://www.naturalreaders.com/online/</p>
<h2 id="文本">文本</h2>
<h3 id="爬取aminer文献的标题和pdf文件">爬取Aminer文献的标题和pdf文件</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;ACL.html&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    html = f.read()</span><br><span class="line"><span class="comment"># print(&#x27;读取到以下html内容：&#123;&#125;...&#x27;.format(html[:20]))</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lst = re.split(<span class="string">&#x27;&lt;/?script.*&gt;&#x27;</span>, html)</span><br><span class="line"><span class="keyword">for</span> string <span class="keyword">in</span> lst:</span><br><span class="line">	<span class="keyword">if</span> <span class="string">&quot;g_initialProps&quot;</span> <span class="keyword">in</span> string:</span><br><span class="line">		json_file = re.sub(<span class="string">&#x27;undefined&#x27;</span>, <span class="string">&#x27;&quot;undefined&quot;&#x27;</span>, string.split(<span class="string">&#x27;;&#x27;</span>, <span class="number">1</span>)[<span class="number">1</span>].split(<span class="string">&#x27;= &#x27;</span>)[-<span class="number">1</span>].rsplit(<span class="string">&#x27;;&#x27;</span>, <span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">dict_json = json.loads(json_file)</span><br><span class="line">top_cited_papers = dict_json[<span class="string">&#x27;rank&#x27;</span>][<span class="string">&#x27;confInfo&#x27;</span>][<span class="string">&#x27;top_cited_papers&#x27;</span>]</span><br><span class="line"></span><br><span class="line">lst_of_ids = [paper[<span class="string">&#x27;id&#x27;</span>] <span class="keyword">for</span> paper <span class="keyword">in</span> top_cited_papers]</span><br><span class="line">lst_of_titles = [paper[<span class="string">&#x27;title&#x27;</span>] <span class="keyword">for</span> paper <span class="keyword">in</span> top_cited_papers]</span><br><span class="line">print(<span class="built_in">len</span>(lst_of_ids))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(lst_of_ids[<span class="number">1</span>:])):</span><br><span class="line">	page_link = <span class="string">&#x27;https://www.aminer.cn/pub/&#123;&#125;/&#x27;</span>.<span class="built_in">format</span>(lst_of_ids[i])</span><br><span class="line">	headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: ua.random&#125; <span class="comment"># 更换headers来应对反爬虫</span></span><br><span class="line">	bs = BeautifulSoup(requests.get(page_link).content, <span class="string">&quot;lxml&quot;</span>)</span><br><span class="line">	pdf_link = <span class="string">&quot;http://&quot;</span> + bs.find(<span class="string">&#x27;iframe&#x27;</span>)[<span class="string">&#x27;src&#x27;</span>].split(<span class="string">&#x27;file=//&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">	print(pdf_link)</span><br><span class="line">	<span class="keyword">with</span> <span class="built_in">open</span>(lst_of_titles[i] +<span class="string">&#x27;.pdf&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">		print(<span class="string">&#x27;Downloading...&#x27;</span>)</span><br><span class="line">		headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: ua.random&#125;</span><br><span class="line">		response = requests.get(pdf_link, headers=headers)</span><br><span class="line">		f.write(response.content)</span><br><span class="line">	print(<span class="string">&#x27;Downloaded: &#x27;</span>+ lst_of_titles[i])</span><br><span class="line">	time.sleep(random.random()*<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>web scraping</tag>
      </tags>
  </entry>
  <entry>
    <title>语义学框架</title>
    <url>/semantics-concepts/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2021/04/29/cwbXFoxvJr14Wf5.png"/></p>
<p>See <a href="https://share.mubu.com/doc/7e-TFn3zFCV">mubu</a> for more details.</p>
<h1 id="chapter-1">Chapter 1</h1>
<p>In this chapter we will introduce some important concepts for the study of semantics. In 1.1 we place the notion of linguistic meaning in the wider context of human communication and behaviour. Section 1.2 then examines some of the vocabulary that English and other languages use for ordinary talk about meaning in language and related phenomena. A consideration of how this everyday nontechnical vocabulary varies cross-linguistically can show some of the important different aspects of linguistic meaning. In section 1.3 the semiotic triangle of mind, world and language is discussed, followed in 1.4 by an introduction to five fundamental concepts:</p>
<ul>
<li><p>lexemes;</p></li>
<li><p>sense and reference;</p></li>
<li><p>denotation and connotation;</p></li>
<li><p>compositionality; and</p></li>
<li><p>levels of meaning.</p></li>
</ul>
<p>Next (1.5), we introduce the concepts of object language and metalanguage, and distinguish a number of different possible relations between the language in which meanings are described (the ‘metalanguage’) and the language whose meanings are described (the ‘object language’). We will then consider three different identifications of meaning: meanings as objects in the world (referents: 1.6.1), as objects in the mind (concepts: 1.6.2), and as brain states (1.6.3). An alternative identification is the notion of meanings as uses, discussed in 1.6.4. To end the chapter, we consider a view of meaning on which meanings are unobservable, hypothetical constructs posited to explain facts about language use (1.7).</p>
<h1 id="chapter-2">Chapter 2</h1>
<p>This chapter considers the role of definition in the description of meaning, through four main questions:</p>
<ul>
<li><p>What units need to receive definition?</p></li>
<li><p>What forms should the definitions take?</p></li>
<li><p>Can definitions be grounded in a set of semantic primitives?</p></li>
<li><p>What is the place of definition in semantics generally?</p></li>
</ul>
<p>We begin by contrasting the types of definition that might appear in dictionaries from the types that interest a theoretical semantic analysis (2.1). Before any definition can begin, we have to confront an initial question: what are the meaning-bearing units of the language for which definitions are required? We explore this question by looking at meaning on, above and below the word level in 2.2, paying particular attention to certain problematic cases. The next section distinguishes definition of things (real definition) from definition of meanings (nominal definition), and cognitive from extensional definitions, and discusses some differences of opinion in linguistics as to what the proper objects of linguistic definition are (2.3.1). We then distinguish different possible definitional strategies, including</p>
<ul>
<li><p>definition by ostension (2.3.2)</p></li>
<li><p>definition by synonymy (2.3.3)</p></li>
<li><p>definition by context and typical exemplar (2.3.4)</p></li>
<li><p>definition by genus and differentia (2.3.5).</p></li>
</ul>
<p>The test of truth preserving substitutability is introduced as a standard criterion of definitional adequacy (2.4), and we discuss the problem of definitional circularity and the question of semantic primitives (2.5).</p>
<p>　We then exemplify the extreme difficulty involved in couching successful definitions of words (2.6), before finally devoting some discussion to the relationship between definition and understanding (2.7). 　</p>
<h1 id="chapter-3">Chapter 3</h1>
<p>Linguistic expressions can only occur in particular contexts; as a result, working out what role context plays in the determination of meaning is an important part of semantic analysis. This chapter considers one essential type of context: the external or real-world context to which linguistic expressions refer. 　We begin by discussing an important distinction: the distinction between what a word inherently means, and what it can be used to mean in a particular context, showing that this distinction is often not self-evident. We then distinguish the different types of task a hearer must perform to correctly understand a linguistic expression in its context (3.1).</p>
<p>　In 3.2 we begin the treatment of external context by considering the relation between sense and reference, discussing - the origins of this distinction in Frege; - its applications in linguistics; and</p>
<ul>
<li>the nature of deictic expressions, which can be seen as a bridge between language and its surrounding external context.</li>
</ul>
<p>In 3.3. we discuss, and reject, a possible distinction between knowledge of a word’s inherent, linguistic meaning (dictionary knowledge) and knowledge of facts about the word’s external context (encyclopaedic knowledge).</p>
<h1 id="chapter-4">Chapter 4</h1>
<p>Following the treatment of external context in the previous chapter, this chapter considers the interpersonal context of linguistic action in which any utterance is placed.</p>
<p>　Section 4.1 introduces the notion of illocutionary force, which refers to the different interpersonal functions or speech acts which a linguistic expression may be made to perform (stating, questioning, ordering, requesting, advising, warning, promising, etc.).</p>
<p>　Section 4.2 considers the role of speaker’s intention and hearer’s inference in meaning: in general, the meaning of an expression can often be described as whatever it was that the speaker intended the hearer to understand in using the expression; the hearer’s task, on this picture, is to make inferences about what this intention was.</p>
<p>　In 4.3 we discuss the Gricean theory of implicature, which is the theory of how meanings may be implied rather than explicitly stated. In 4.4 and 4.5 we turn to an exploration of the principles which have been proposed as governing the operation of implicature in conversation. Section 4.6 considers an important alternative tradition in the analysis of interpersonal context, Relevance Theory, and 4.7 discusses, in general terms, the interrelation between semantics and pragmatics, the branch of linguistics in which the relations between language and context are specifically studied. 　</p>
<h1 id="chapter-5">Chapter 5</h1>
<p>The different sections of this chapter follow three logical steps in meaning analysis. In 5.1, some of the different possible semantic relations among words are exemplified and discussed. We concentrate on those relations which are of most use for semantic description: - antonymy (oppositeness; 5.1.1), - meronymy ( part of-ness; 5.1.2),</p>
<ul>
<li><p>the class-inclusion relations of hyponymy and taxonomy (kind of-ness; 5.1.3–4) and</p></li>
<li><p>synonymy (5.1.5).</p></li>
</ul>
<p>These meaning relations can be seen as reflecting the presence of various isolable components in the meanings of the related words; accordingly, Section 5.2 introduces the possibility of analysing senses as composed of bundles of semantic components, and considers the wider applicability of componential analysis as well as the problems it faces. The third section (5.3) discusses the necessity for a theory of meaning to specify the number of senses associated with a lexeme in a rigorous way. In 5.3.1 we distinguish the case where a single lexeme possesses several related meanings (polysemy) from two other cases: the case where it possesses only a single meaning (monosemy) and the case where it possesses two unrelated meanings (homonymy). Section 5.3.2 then shows that any attempt to make these definitions rigorous confronts serious problems, the implications of which are discussed in 5.3.3.</p>
<h1 id="chapter-6">Chapter 6</h1>
<p>Logic is the study of the nature of valid inferences and reasoning. The logical tradition constitutes one of the major strands in the study of meaning, and some knowledge of its background is indispensable in linguistic semantics. In this chapter we will study some basic logical tools and concepts. Our aim is twofold:</p>
<ul>
<li>first, to understand the ways in which some types of meaning can be represented in logical symbolism</li>
<li>second, to appreciate the advantages and disadvantages of this type of representation.</li>
</ul>
<p>We begin by introducing the ideas of validity, soundness and logical form (6.1): these define the context and aims of a logical approach to language. In 6.2 we present an exposition of the basic principles of propositional logic, the logic of basic sentences, including a treatment of the principal logical operators: and, not, or and if . . . then. In 6.3 we discuss the extent to which these logical concepts overlap with the meanings of their ordinary language equivalents. Section 6.4 introduces predicate logic, the logic of expressions like some and all. In 6.5 we discuss the ways in which the concept of a model allows us to describe reference using logical techniques. Section 6.6 contains a discussion of the sentence relations of entailment, presupposition and contradiction. This leads to a discussion of meaning postulates in Section 6.7, which use the sentence relations introduced in 6.6 as part of a non-decompositional approach to meaning. In 6.8 Russell’s theory of descriptions is discussed. This is a proposal for the analysis of noun phrases containing the definite article, and provides an instructive example of the advantages and problems of applying logical tools to the analysis of natural language.</p>
<p>　We end the chapter in 6.9 with a short discussion of the controversies surrounding the use of logic as an aid in the analysis of natural language. 　</p>
<h1 id="chapter-7">Chapter 7</h1>
<p>This chapter considers meaning from the perspective of the cognitive operations which the mind can be hypothesized to perform in using language. We begin by introducing the idea that words in natural language can be seen as categories, and discuss two different models of the way categories work, the classical view of categorization and the prototype view (7.1), exploring the advantages and problems of each. We then discuss cognitive approaches to meaning, which developed out of the prototypical model of categorization. These approaches have introduced a rich model of the cognitive architecture underlying language (7.2).</p>
<h1 id="chapter-8">Chapter 8</h1>
<p>In the previous chapter we looked at some proposals about the types of cognitive operation that underlie semantic ability. In this chapter, we examine some attempts to formalize and model the conceptual representations involved in language. In 8.1 we examine Jackendoff’s conceptual semantics, a theory about the cognitive structures behind language and the modes of their interaction. This is followed by a discussion of the treatment of meaning in computational linguistics, which uses computer models of language as an aid to understanding the mental processes involved in language production and understanding (8.2). We will concentrate on the aspects of computational linguistics which give insight into the nature of the task of meaning-processing. We specifically look at WordNet, an online lexical database, at the problems of word-sense disambiguation, and at Pustejovsky’s solution to this in his model of qualia structure.</p>
<h1 id="chapter-9">Chapter 9</h1>
<p>This chapter and the next investigate a range of semantic phenomena which are relevant to morphosyntax. This chapter focuses on morphosyntactic categories such as noun and verb and tense and aspect. The major questions are these: - Does a word’s meaning determine its grammatical category?</p>
<ul>
<li>How can we describe the meanings of major verbal categories like tense and aspect?</li>
</ul>
<p>We begin with a discussion of the meaning of lexical categories (parts of speech), exploring the possible semantic contribution made by a word’s categorization as noun, verb, adjective, and so on (9.1). Section 9.2 focuses on the verb, investigating the semantics of tense and aspect: two central dimensions of verb meaning with major consequences on the verbal and clausal levels.</p>
<h1 id="chapter-10">Chapter 10</h1>
<p>This chapter discusses the semantics of the clause, particularly the relationship between a verb and its noun participants. This relationship is called the verb’s argument structure. There are three basic questions:</p>
<ul>
<li><p>What principles determine which of the noun phrases associated with a transitive verb will be expressed as subject and which as object?</p></li>
<li><p>Can verbs be grouped into classes about which argument structure generalizations can be made?</p></li>
<li><p>Can constructions have meanings on their own?</p></li>
</ul>
<p>We begin by looking at the semantics of argument structure, a central topic in investigation of the way semantics and syntax are connected. We introduce and motivate the notion of thematic role, and go on to consider the modifications this notion has undergone in research into argument structure (10.1). We then consider argument structure alternations (10.2), the name for situations where a single verb can take several different argument structures. Lastly, we consider construction grammar, which attributes many apparently lexical meanings to the grammatical constructions in which they occur (10.3).</p>
<h1 id="chapter-11">Chapter 11</h1>
<p>Variation is one of the most immediately obvious facts about meaning. Everyone is aware of how the meaning of identical expressions can differ from one person to another, sometimes significantly. There are two aspects of meaning variation: a synchronic and a diachronic (historical) one; we examine each in turn in this chapter. After a quick tour of some important preliminary questions (11.1), we begin diachronically by illustrating the traditional categories with which meaning change has been described, and we consider some of the shortcomings of this approach (11.2.1). We then move on to more recent studies of the pathways and mechanisms of semantic change (11.2.2) and a brief discussion of grammaticalization, the process by which full lexical words are converted into grammatical morphemes (11.2.3). The second half of the chapter discusses synchronic meaning variation. We start by examining the subtle types of semantic variation which exist within a single language community at any one time. Powerful new tools developed within corpus linguistics allow this kind of variation to be studied in a way that was not previously available: these are illustrated in 11.3. We then look at the field of semantic typology, which studies possible constraints on meaning variation and seeks out possible semantic universals in various semantic fields such as the body, colour, space and motion (11.4). Lastly, we consider the implications of these studies for the question of the influences between language and cognition, discussing the famous Sapir–Whorf or linguistic relativity hypothesis (11.5).</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>语义</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>semantics</tag>
      </tags>
  </entry>
  <entry>
    <title>句子嵌入技术笔记</title>
    <url>/sentence-embedding/</url>
    <content><![CDATA[<p>学习并实现Doc2Vec、SentenceBert、InferSent、Universal Sentence Encoder，并计算余弦相似度。</p>
<span id="more"></span>
<p>机器所能理解的语言是数字，因此需要把文本表示为数字或嵌入，包括单词、短语、句子、段落、文档的嵌入。流行的词嵌入技术包括 Word2Vec、GloVe、ELMo、FastText 等，由此衍生出句子嵌入技术，将整个句子及其语义信息表示为向量。</p>
<h1 id="doc2vec-2014">Doc2Vec (2014)</h1>
<p>这是<a href="http://see%20here%20https//arxiv.org/pdf/1405.4053v2.pdf"><strong>M</strong></a><strong>ikilov 和 Le</strong>于2014 年在<a href="https://arxiv.org/abs/1405.4053">本文中</a>提出的概念，有两种训练方法：</p>
<h2 id="pv-dmdistributed-memory-version-of-paragraph-vector">1.1) PV-DM(Distributed Memory version of Paragraph Vector)</h2>
<p>分配一个段落向量句子，同时在所有句子中共享单词向量。然后，我们要么average（平均）或concatenate（连接）（段落向量和单词向量）得到最终的句子表示。如果你注意到了，它是Word2Vec的单词类型的连续包的一个扩展，在这里我们预测给定一组单词的下一个单词。只是在PVDM中，我们通过一组句子来预测下一个句子。</p>
<p>Mikilov 和 Le 的方法很简单但很聪明：他们使用了<strong>word2vec</strong>模型，并添加了另一个特征向量（下面的Paragraph ID），如下所示：</p>
<p><img src="https://i.loli.net/2021/07/13/nI5lpW4GCzYeHFS.png"/></p>
<p>因此，在训练词向量 W 时，也训练了文档向量 D。</p>
<p>上面的模型称为<em>段落向量的分布式内存版本</em>（PV-DM）。它作为一种记忆，可以记住当前上下文中缺少的内容——或者作为段落的主题。 词向量表示词的概念，而文档向量则是表示文档的概念。</p>
<h2 id="distributed-bag-of-words-version-of-paragraph-vector-pv-dbow">1.2) Distributed Bag of Words version of Paragraph Vector (PV-DBOW)</h2>
<p>和 word2vec 一样，可以使用另一种类似于 skip-gram 的算法<strong>段落向量的</strong>分布式<strong>词</strong>袋<strong>版本</strong>（PV-DBOW）。</p>
<p><img src="https://i.loli.net/2021/07/13/lsuf736PxSErCaI.png"/></p>
<p>在这里，这个算法实际上更快（与word2vec相反），而且占用的内存更少，因为不需要保存单词向量。该论文的作者建议将两者结合使用，但表示通常 PV-DM 表现更好，对大多数任务来说已经绰绰有余。</p>
<p>Doc2Vec模型可以通过以下方式使用：训练需要一组文档，为每个单词生成一个词向量W，并且为每个文档生成文档向量D。该模型还训练softmax隐藏层的权重。在inference阶段，可以提供一个新的文档，通过固定所有权重来计算文档向量。</p>
<h2 id="gensim-实现"><strong>Gensim</strong> <a href="https://rare-technologies.com/doc2vec-tutorial/"><strong>实现</strong></a></h2>
<h4 id="步骤1">步骤1</h4>
<p>我们将使用<em>Gensim</em>来展示如何使用 Doc2Vec ，并且我们已经有了一个句子列表。我们将首先导入模型和其他库，然后构建一个带标签的句子语料库。每个句子现在都表示为一个 TaggedDocument，其中包含一个单词列表和一个与之关联的标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import</span></span><br><span class="line"><span class="keyword">from</span> gensim.models.doc2vec <span class="keyword">import</span> Doc2Vec, TaggedDocument</span><br><span class="line">tagged_data = [TaggedDocument(d, [i]) <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokenized_sent)]</span><br><span class="line">tagged_data</span><br></pre></td></tr></table></figure>
<h4 id="步骤2">步骤2</h4>
<p>然后我们用参数训练模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Train doc2vec model</span></span><br><span class="line">model = Doc2Vec(tagged_data, vector_size = <span class="number">20</span>, window = <span class="number">2</span>, min_count = <span class="number">1</span>, epochs = <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">vector_size = Dimensionality of the feature vectors.</span></span><br><span class="line"><span class="string">window = The maximum distance between the current and predicted word within a sentence.</span></span><br><span class="line"><span class="string">min_count = Ignores all words with total frequency lower than this.</span></span><br><span class="line"><span class="string">alpha = The initial learning rate.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Print model vocabulary</span></span><br><span class="line">model.wv.vocab</span><br></pre></td></tr></table></figure>
<h4 id="步骤3">步骤3</h4>
<p>采用一个新的测试句子，并从我们的数据中找到前 5 个最相似的句子。我们还将按相似度递减的顺序显示它们。infer_vector 方法返回测试句子的向量化形式（包括段落向量），most_similar 方法返回相似的句子</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_doc = word_tokenize(<span class="string">&quot;I had pizza and pasta&quot;</span>.lower())</span><br><span class="line">test_doc_vector = model.infer_vector(test_doc)</span><br><span class="line">model.docvecs.most_similar(positive = [test_doc_vector])</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">positive = List of sentences that contribute positively.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="sentencebert">SentenceBERT</h1>
<h2 id="实现">实现</h2>
<p>我们可以使用以下方法安装 Sentence BERT：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!pip install sentence-transformers</span><br></pre></td></tr></table></figure>
<h4 id="步骤1-1">步骤1</h4>
<p>然后我们将加载预训练的 BERT 模型。还有许多其他预训练模型可用。您可以在<a href="https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md">此处</a>找到完整的模型列表<a href="https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md">。</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">sbert_model = SentenceTransformer(<span class="string">&#x27;bert-base-nli-mean-tokens&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="步骤2-1">步骤2</h4>
<p>然后我们将对提供的句子进行编码。我们还可以显示句子向量（只需取消注释下面的代码）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentence_embeddings = model.encode(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment">#print(&#x27;Sample BERT embedding vector - length&#x27;, len(sentence_embeddings[0]))</span></span><br><span class="line"><span class="comment">#print(&#x27;Sample BERT embedding vector - note includes negative values&#x27;, sentence_embeddings[0])</span></span><br></pre></td></tr></table></figure>
<h4 id="步骤3-1">步骤3</h4>
<p>然后我们将定义一个测试查询并对其进行编码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query = <span class="string">&quot;I had pizza and pasta&quot;</span></span><br><span class="line">query_vec = model.encode([query])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h4 id="步骤4">步骤4</h4>
<p>然后我们将使用 scipy 或 sklearn 计算余弦相似度。我们将检索句子和我们的测试查询之间的相似度值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">cos_lib = cosine_similarity(vectors[<span class="number">1</span>,:],vectors[<span class="number">2</span>,:])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> sentences:</span><br><span class="line">  sim = cosine(query_vec, model.encode([sent])[<span class="number">0</span>])</span><br><span class="line">  print(<span class="string">&quot;Sentence = &quot;</span>, sent, <span class="string">&quot;; similarity = &quot;</span>, sim)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2021/07/13/Nf2A38mDtx6JKBb.png"/></p>
<p>好了，我们已经获得了文本中的句子和我们的测试句子之间的相似性。需要注意的一个关键点是，如果您想从头开始训练 SentenceBERT，它会非常慢。</p>
<h1 id="推荐系统的借鉴">推荐系统的借鉴</h1>
<h2 id="双塔模型召回">双塔模型召回</h2>
<p>两侧分别对user和item特征通过DNN输出向量，并在最后一层计算二个输出向量的内积。</p>
<p><img src="https://i.loli.net/2021/07/13/v1FNUWnmD3HJbRp.png"/></p>
<blockquote>
<p>例如YouTube今年刚发的一篇文章就应用了经典的双塔结构：</p>
</blockquote>
<p><img src="https://i.loli.net/2021/07/13/4rMjtYQ7iLSHOIF.png"/></p>
<h2 id="dssm">DSSM</h2>
<p>通过对用户的Query历史和Document进行embedding编码，使用余弦相似度计算用户query的embedding和document的相似度，继而达到语义相似度计算的目的。</p>
<p><img src="https://i.loli.net/2021/07/13/KkiU41uEOLWHz7I.png"/></p>
<p>从上图可以看出，输入DSSM的是一个高维的向量，经过若干层的神经网络，输出一个低维的向量，分别用来表示user的query意图和document，最后通过余弦相似度计算Q和D的相似度。</p>
<p>论文的另一个出色的点在于抛弃了传统的Bag-of-word模型，因为其会带来高维度的向量特征，这里使用word hashing技术来代替词袋模型，word hashing基于n-gram，比如一个单词<img src="https://www.zhihu.com/equation?tex=good" alt="[公式]" />，使用word hashing技术进行拆分，首先在其两端补充标记符 “#”，假设n=3，则“#good#” 可以表示为：#go、goo、ood、od#。</p>
<p>但采用word hashing技术会带来一个问题就是：词汇冲突，即两个表达含义不同的词拥有相同的n-gram向量表示。但是论文作者也提到了这种概率很小，冲突的概率在0.0044%，如下图所示：</p>
<p><img src="https://i.loli.net/2021/07/13/j76c5AT9Sh1Irnm.png"/></p>
<p><strong>参考：</strong></p>
<p>sentence embedding</p>
<p>https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/</p>
<p>https://towardsdatascience.com/paper-summary-evaluation-of-sentence-embeddings-in-downstream-and-linguistic-probing-tasks-5e6a8c63aab1</p>
<p>https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a</p>
<p>双塔模型</p>
<p>https://zhuanlan.zhihu.com/p/166469248</p>
<p>https://zhuanlan.zhihu.com/p/97821040</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>表示学习</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>英文句子切分的要点和工具</title>
    <url>/sentence-segmentation/</url>
    <content><![CDATA[<h1 id="英文句子切分的规则">英文句子切分的规则</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line">text = <span class="string">&quot;This \n This is Ms. Right&#x27;s first sentence.\n\nNext: is numbered list？\n1.Hello World!\n2.Hello World2!\n3.Hello World!&quot;</span></span><br><span class="line">text_sentences = nlp(text)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> text_sentences.sents:</span><br><span class="line">    print(<span class="string">&#x27;句子：&#x27;</span>+sentence.text)</span><br></pre></td></tr></table></figure>
<p>一般句子以<code>.?!</code>结尾，但<code>.</code>有时也出现在句中。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>句子切分</category>
      </categories>
      <tags>
        <tag>sentence segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>二语习得课本</title>
    <url>/sla-3/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2021/04/30/FMh8DeCfcWELUd5.png"/></p>
<p>See <a href="https://share.mubu.com/doc/71bKL8XSabV">mubu</a> for more details.</p>
<ul>
<li><p>1 Introduction</p>
<ul>
<li><p>1.1 The study of second language acquisition</p></li>
<li><p>1.2 Definitions</p></li>
<li><p>1.3 The nature of language</p>
<ul>
<li><p>1.3.1 Sound systems</p></li>
<li><p>1.3.2 Syntax</p></li>
<li><p>1.3.3 Morphology and the lexicon</p></li>
<li><p>1.3.4 Semantics</p></li>
<li><p>1.3.5 Pragmatics</p></li>
</ul></li>
<li><p>1.4 The nature of nonnative speaker knowledge</p></li>
</ul></li>
<li><p>2 Related disciplines</p>
<ul>
<li><p>2.1 SLA and related disciplines</p></li>
<li><p>2.2 Third language acquisition/multilingualism</p></li>
<li><p>2.3 Heritage language acquisition</p></li>
<li><p>2.4 Bilingual acquisition</p></li>
<li><p>2.5 First language acquisition</p>
<ul>
<li><p>2.5.1 Babbling</p></li>
<li><p>2.5.2 Words</p></li>
<li><p>2.5.3 Sounds and pronunciation</p></li>
<li><p>2.5.4 Syntax</p></li>
<li><p>2.5.5 Morphology</p></li>
</ul></li>
</ul></li>
<li><p>3 Second and foreign language data</p>
<ul>
<li><p>3.1 Data analysis</p>
<ul>
<li><p>3.1.1 Data set I: plurals</p></li>
<li><p>3.1.2 Data set II: verb + -ing markers</p></li>
<li><p>3.1.3 Data set III: prepositions</p></li>
</ul></li>
<li><p>3.2 What data analysis does not reveal</p></li>
<li><p>3.3 Data collection</p>
<ul>
<li><p>3.3.1 Eliciting speech samples</p></li>
<li><p>3.3.2 Eliciting reactions to data</p></li>
<li><p>3.3.3 Verbal report data</p></li>
<li><p>3.3.4 Measuring non-linguistic information</p>
<ul>
<li>3.3.5 Measuring general proficiency:</li>
</ul></li>
<li><p>standardized language tests</p></li>
</ul></li>
<li><p>3.4 Replication</p></li>
<li><p>3.5 Issues in data analysis</p></li>
<li><p>3.6 What is acquisition?</p></li>
</ul></li>
<li><p>4 The role of the native language: an historical overview</p>
<ul>
<li><p>4.1 Introduction</p></li>
<li><p>4.2 Behaviorism</p>
<ul>
<li><p>4.2.1 Linguistic background</p></li>
<li><p>4.2.2 Psychological background</p></li>
</ul></li>
<li><p>4.3 Contrastive Analysis Hypothesis</p></li>
<li><p>4.4 Error analysis</p></li>
</ul></li>
<li><p>5 Recent perspectives on the role of previously known languages</p>
<ul>
<li><p>5.1 Theories of learning</p></li>
<li><p>5.2 Child second language acquisition</p></li>
<li><p>5.3 Child second language morpheme order studies</p></li>
<li><p>5.4 Adult second language morpheme order studies</p></li>
<li><p>5.5 Revised perspectives on the role of the native language</p>
<ul>
<li><p>5.5.1 Avoidance</p></li>
<li><p>5.5.2 Differential learning rates</p></li>
<li><p>5.5.3 Different paths</p></li>
<li><p>5.5.4 Overproduction</p></li>
<li><p>5.5.5 Predictability/selectivity</p></li>
<li><p>5.5.6 Second language processing</p></li>
</ul></li>
<li><p>5.6 Interlanguage transfer</p></li>
</ul></li>
<li><p>6 Formal approaches to SLA</p>
<ul>
<li><p>6.1 Introduction</p></li>
<li><p>6.2 Universal Grammar</p>
<ul>
<li><p>6.2.1 Initial state</p></li>
<li><p>6.2.2 UG principles</p></li>
<li><p>6.2.3 UG parameters</p></li>
<li><p>6.2.4 Falsification</p></li>
</ul></li>
<li><p>6.3 Transfer: the UG perspective</p>
<ul>
<li><p>6.3.1 Levels of representation</p></li>
<li><p>6.3.2 Clustering</p></li>
<li><p>6.3.3 Learnability</p></li>
</ul></li>
<li><p>6.4 Phonology</p>
<ul>
<li><p>6.4.1 Markedness Differential Hypothesis</p></li>
<li><p>6.4.2 Similarity/dissimilarity: Speech Learning Model</p></li>
<li><p>6.4.3 Optimality Theory</p></li>
<li><p>6.4.4 Ontogeny Phylogeny Model</p></li>
</ul></li>
</ul></li>
<li><p>7 Typological and functional approaches</p>
<ul>
<li><p>7.1 Introduction</p></li>
<li><p>7.2 Typological universals</p>
<ul>
<li><p>7.2.1 Test case I: the Accessibility Hierarchy</p></li>
<li><p>7.2.2 Test case II: the acquisition of questions</p></li>
<li><p>7.2.3 Test case III: voiced/voiceless consonants</p></li>
<li><p>7.2.4 Falsifiability</p></li>
<li><p>7.2.5 Typological universals: conclusions</p></li>
</ul></li>
<li><p>7.3 Functional approaches</p>
<ul>
<li><p>7.3.1 Tense and aspect: the Aspect Hypothesis</p></li>
<li><p>7.3.2 The Discourse Hypothesis</p></li>
<li><p>7.3.3 Concept-oriented approach</p></li>
</ul></li>
</ul></li>
<li><p>8 Looking at interlanguage processing</p>
<ul>
<li><p>8.1 Introduction</p></li>
<li><p>8.2 Connectionist/emergentist models</p></li>
<li><p>8.3 Processing approaches</p>
<ul>
<li><p>8.3.1 Processability Theory</p>
<ul>
<li>8.3.2 Information processing: automaticity,</li>
</ul></li>
<li><p>restructuring, and U-shaped learning</p></li>
<li><p>8.3.3 Input Processing</p></li>
</ul></li>
<li><p>8.4 Knowledge types</p>
<ul>
<li><p>8.4.1 Acquisition–Learning</p></li>
<li><p>8.4.2 Declarative/procedural</p></li>
<li><p>8.4.3 Implicit/explicit</p></li>
<li><p>8.4.4 Representation and control</p></li>
</ul></li>
<li><p>8.5 Interface of knowledge types</p>
<ul>
<li><p>8.5.1 No interface</p></li>
<li><p>8.5.2 Weak interface</p></li>
<li><p>8.5.3 Strong interface</p></li>
</ul></li>
<li><p>8.6 Psycholinguistic constructs</p>
<ul>
<li><p>8.6.1 Attention</p></li>
<li><p>8.6.2 Working memory</p></li>
<li><p>8.6.3 Monitoring</p></li>
</ul></li>
</ul></li>
<li><p>9 Interlanguage in context</p>
<ul>
<li><p>9.1 Introduction</p></li>
<li><p>9.2 Variation</p></li>
<li><p>9.3 Systematic variation</p>
<ul>
<li><p>9.3.1 Linguistic context</p></li>
<li><p>9.3.2 Social context relating to the native language</p>
<ul>
<li>9.3.3 Social context relating to interlocutor, task</li>
</ul></li>
<li><p>type, and conversational topic</p></li>
</ul></li>
<li><p>9.4 Social interactional approaches</p>
<ul>
<li><p>9.4.1 Conversation Analysis</p></li>
<li><p>9.4.2 Sociocultural theory</p></li>
</ul></li>
<li><p>9.5 Communication strategies</p></li>
<li><p>9.6 Interlanguage pragmatics</p></li>
</ul></li>
<li><p>10 Input, interaction, and output</p>
<ul>
<li><p>10.1 Introduction</p></li>
<li><p>10.2 Input</p></li>
<li><p>10.3 Comprehension</p></li>
<li><p>10.4 Interaction</p></li>
<li><p>10.5 Output</p>
<ul>
<li><p>10.5.1 Feedback</p></li>
<li><p>10.5.2 Hypothesis testing</p></li>
<li><p>10.5.3 Automaticity</p></li>
<li><p>10.5.4 Meaning-based to grammar-based processing</p></li>
</ul></li>
<li><p>10.6 The role of input and interaction in language learning</p>
<ul>
<li><p>10.6.1 Attention</p></li>
<li><p>10.6.2 Contrast theory</p></li>
<li><p>10.6.3 Metalinguistic awareness</p></li>
</ul></li>
<li><p>10.7 Limitations of input</p></li>
</ul></li>
<li><p>11 Instructed second language learning</p>
<ul>
<li><p>11.1 Introduction</p></li>
<li><p>11.2 Classroom language</p></li>
<li><p>11.3 Processing instruction</p></li>
<li><p>11.4 Teachability/learnability</p></li>
<li><p>11.5 Focus on form</p>
<ul>
<li><p>11.5.1 Timing</p></li>
<li><p>11.5.2 Forms to focus on</p></li>
<li><p>11.5.3 Input manipulation and input enhancement</p></li>
</ul></li>
<li><p>11.6 Uniqueness of instruction</p></li>
<li><p>11.7 Effectiveness of instruction</p></li>
</ul></li>
<li><p>12 Beyond the domain of language</p>
<ul>
<li><p>12.1 Introduction</p></li>
<li><p>12.2 Research traditions</p>
<ul>
<li><p>12.2.1 Linguistics</p></li>
<li><p>12.2.2 Psychology</p></li>
<li><p>12.2.3 Psycholinguistics</p></li>
</ul></li>
<li><p>12.3 Affect</p>
<ul>
<li><p>12.3.1 Language shock and culture shock</p></li>
<li><p>12.3.2 Anxiety</p></li>
<li><p>12.3.3 Affective Filter</p></li>
</ul></li>
<li><p>12.4 Social distance</p></li>
<li><p>12.5 Age differences</p></li>
<li><p>12.6 Aptitude</p></li>
<li><p>12.7 Motivation</p>
<ul>
<li><p>12.7.1 Motivations as a function of time and success</p></li>
<li><p>12.7.2 Changes over time</p>
<ul>
<li>12.7.3 Influence of success on motivation and</li>
</ul></li>
<li><p>demotivation</p></li>
</ul></li>
<li><p>12.8 Personality and learning style</p>
<ul>
<li><p>12.8.1 Extroversion and introversion</p></li>
<li><p>12.8.2 Risk taking</p></li>
<li><p>12.8.3 Field independence/dependence</p></li>
<li><p>12.8.4 Visual/auditory/kinesthetic</p></li>
<li><p>12.8.5 Obtaining learning style information</p></li>
</ul></li>
<li><p>12.9 Learning strategies</p></li>
</ul></li>
<li><p>13 The lexicon</p>
<ul>
<li><p>13.1 The significance of the lexicon</p></li>
<li><p>13.2 Categories of lexical knowledge: some dichotomies</p>
<ul>
<li><p>13.2.1 Production and reception</p></li>
<li><p>13.2.2 Knowledge and control</p></li>
<li><p>13.2.3 Breadth and depth</p></li>
</ul></li>
<li><p>13.3 Lexical knowledge, development, and influences</p>
<ul>
<li><p>13.3.1 Subcategorization</p></li>
<li><p>13.3.2 Word associations and networks</p></li>
<li><p>13.3.3 Word formation</p></li>
<li><p>13.3.4 Word combinations, collocations, and phraseology</p></li>
</ul></li>
<li><p>13.4 L1 influence</p>
<ul>
<li><p>13.4.1 Incidental vocabulary learning</p></li>
<li><p>13.4.2 Incremental vocabulary learning</p></li>
</ul></li>
<li><p>13.5. Using lexical skills</p>
<ul>
<li><p>13.5.1 Production</p></li>
<li><p>13.5.2 Perception</p></li>
</ul></li>
</ul></li>
<li><p>14 An integrated view of second language acquisition</p>
<ul>
<li><p>14.1 An integration of subareas</p>
<ul>
<li><p>14.1.1 Apperceived input</p></li>
<li><p>14.1.2 Comprehended input</p></li>
<li><p>14.1.3 Intake</p></li>
<li><p>14.1.4 Integration</p></li>
<li><p>14.1.5 Output</p></li>
</ul></li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>语言学</category>
        <category>二语习得</category>
      </categories>
  </entry>
  <entry>
    <title>二语习得</title>
    <url>/sla/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2021/04/30/Fb5mVBlC9XPAGpg.png"/></p>
<p>See <a href="https://share.mubu.com/doc/3f9VeztlAXV">mubu</a> for more details.</p>
<h2 id="what-is-second-language-acquisition">What is second language acquisition?</h2>
<h2 id="a-brief-history-of-sla">A brief history of SLA</h2>
<p>input</p>
<p>intake</p>
<p>interlanguage</p>
<p>transfer</p>
<p>fossilization</p>
<h3 id="the-1970s">The 1970s</h3>
<p>acquisition orders</p>
<p>morpheme studies</p>
<p>transitional stages</p>
<p>error analysis</p>
<h3 id="the-1980s">The 1980s</h3>
<p>Monitor</p>
<p>Theory</p>
<p>acquisition versus learning</p>
<p>Input Hypothesis</p>
<h3 id="the-1990s">The 1990s</h3>
<p>noticing</p>
<p>Output Hypothesis</p>
<p>Interaction Hypothesis</p>
<p>connectionism</p>
<p>Universal Grammar</p>
<p>processability</p>
<p>input processing</p>
<p>Sociocultural Theory</p>
<h3 id="the-2000s-and-beyond">The 2000s and beyond</h3>
<h2 id="second-language-acquisition-and">Second language acquisition and</h2>
<h2 id="second-language-teaching">second language teaching</h2>
<h2 id="about-this-book">About this book</h2>
<p>noticing</p>
<p>Output Hypothesis</p>
<p>Interaction Hypothesis</p>
<p>Can L2</p>
<p>learners become native-like?</p>
<h1 id="key-questions-in-second-language-acquisition">Key Questions in Second Language Acquisition</h1>
<h2 id="question-1-what-is-the-initial-state">Question 1: What is the initial state?</h2>
<h3 id="l1-initial-state">L1 = Initial state</h3>
<p>(Universal Grammar)</p>
<p>parameter</p>
<p>form-function</p>
<p>functional approaches</p>
<p>connectionism</p>
<p>processing</p>
<p>parsing</p>
<p>grammaticality judgment</p>
<h3 id="universals-initial-state">Universals = Initial state</h3>
<h3 id="limited-or-partial-l1-transfer">Limited or partial L1 transfer</h3>
<p>lexicon</p>
<p>processability</p>
<h3 id="assessment">Assessment</h3>
<h2 id="question-2-can-l2-learners">Question 2: Can L2 learners</h2>
<h2 id="become-native-like">become native-like?</h2>
<p>ultimate attainment</p>
<p>critical period</p>
<h3 id="l2-learners-cannot-become-native-like">L2 learners cannot become native-like</h3>
<p>subjacency</p>
<h3 id="l2-learners-can-become-native-like">L2 learners can become native-like</h3>
<p>syntax</p>
<p>morphology</p>
<h3 id="l2-learners-can-achieve-native-likeness">L2 learners can achieve native-likeness</h3>
<h3 id="in-some-domains">in some domains</h3>
<p>competence</p>
<p>mental representation of language</p>
<p>performance</p>
<p>(9a)</p>
<p>(9b)</p>
<p>(10b)</p>
<h3 id="assessment-1">Assessment</h3>
<p>fossilization</p>
<p>Critical Period Hypothesis</p>
<h2 id="question-3-is-there-a-critical-period">Question 3: Is there a critical period?</h2>
<p>Critical Period Hypothesis</p>
<h3 id="there-is-a-critical-period">There is a critical period</h3>
<p>What are the roles of</p>
<p>explicit and implicit learning in SLA?</p>
<p>Fundamental Difference</p>
<p>Hypothesis.</p>
<p>Universal Grammar</p>
<h3 id="there-is-no-critical-period-or-at-least">There is no critical period (or at least,</h3>
<h3 id="its-questionable">it’s questionable)</h3>
<p>Can L2 learners become</p>
<p>native-like?</p>
<p>Universal Grammar</p>
<h3 id="there-are-critical-periods-for-some-things">There are critical periods for some things</h3>
<p>syntax</p>
<p>morphology</p>
<p>phonology</p>
<p>principles</p>
<p>parameters</p>
<h3 id="assessment-2">Assessment</h3>
<h3 id="sla-includes-stage-like-development">SLA includes stage-like development</h3>
<p>developmental sequences</p>
<p>Does instruction</p>
<p>make a difference?</p>
<p>U-shaped acquisition</p>
<h3 id="ordered-development">Ordered development</h3>
<p>acquisition orders</p>
<p>morphemes</p>
<p>salience</p>
<p>form-function</p>
<h3 id="variation-and-variability">Variation and variability</h3>
<p>variation</p>
<h3 id="a-comment-about-first-language-influence">A comment about first language influence</h3>
<p>Universal Grammar</p>
<h3 id="assessment-3">Assessment</h3>
<p>processability</p>
<h2 id="question-5-what-are-the-roles-of-explicit-and-implicit-learning-in-sla">Question 5: What are the roles of explicit and implicit learning in SLA?</h2>
<h3 id="sla-is-largely-implicit">SLA is largely implicit</h3>
<p>acquisition</p>
<p>learning</p>
<p>competence</p>
<p>monitoring</p>
<p>Monitor</p>
<p>Theory</p>
<p>Universal</p>
<p>Grammar</p>
<p>connectionism</p>
<h3 id="sla-is-largely-explicit">SLA is largely explicit</h3>
<p>Adaptive Control of Thought model</p>
<p>skills</p>
<p>noticing</p>
<h3 id="assessment-4">Assessment</h3>
<p>poverty of the stimulus</p>
<p>Does instruction</p>
<p>make a difference?</p>
<h2 id="question-6-what-are-the-roles-of-input-and-output-in-sla">Question 6: What are the roles of input and output in SLA?</h2>
<p>input</p>
<p>output</p>
<h3 id="input">Input</h3>
<p>competence, mental representation of language</p>
<p>Universal Grammar</p>
<p>principles</p>
<p>connectionism</p>
<p>input enhancement</p>
<h3 id="output">Output</h3>
<p>Interaction</p>
<p>Hypothesis</p>
<p>feedback</p>
<p>negative evidence</p>
<h3 id="other-perspectives">Other perspectives</h3>
<p>Adaptive Control of Thought</p>
<p>model</p>
<p>skills</p>
<h3 id="assessment-5">Assessment</h3>
<p>syntax</p>
<h3 id="question-7-what-are-individual-differences-and-how-do-they-affect-acquisition">Question 7: What are individual differences and how do they affect acquisition?</h3>
<p>developmental sequences, acquisition orders, Universal</p>
<p>Grammar</p>
<h3 id="aptitude">Aptitude</h3>
<p>working memory</p>
<h3 id="motivation">Motivation</h3>
<h3 id="learning-styles">Learning styles</h3>
<h3 id="learning-strategies">Learning strategies</h3>
<h3 id="assessment-6">Assessment</h3>
<p>Can L2</p>
<p>learners become native-like?</p>
<h2 id="question-8-does-instruction">Question 8: Does instruction</h2>
<h2 id="make-a-difference">make a difference?</h2>
<p>formal instruction</p>
<h3 id="instruction-makes-no-difference">Instruction makes no difference</h3>
<p>acquisition/acquisition versus</p>
<p>learning, Monitory Theory</p>
<p>acquisition orders</p>
<p>developmental sequences</p>
<p>morphemes</p>
<p>Universal</p>
<p>Grammar</p>
<p>principles</p>
<p>parameters</p>
<h3 id="instruction-is-constrained">Instruction is constrained</h3>
<p>processability</p>
<h3 id="instruction-is-beneficial">Instruction is beneficial</h3>
<p>Noticing Hypothesis</p>
<h3 id="instruction-is-necessary">Instruction is necessary</h3>
<p>fossilization</p>
<p>Can L2 learners become native-like?</p>
<h3 id="assessment-7">Assessment</h3>
<p>input enhancement</p>
<p>focus on form</p>
<h2 id="question-9-what-constraints-are">Question 9: What constraints are</h2>
<h2 id="there-on-acquisition">there on acquisition?</h2>
<h3 id="linguistic-constraints">Linguistic constraints</h3>
<p>Universal Grammar</p>
<p>typological universals</p>
<p>markedness</p>
<h3 id="processing-constraints">Processing constraints</h3>
<p>processability</p>
<h3 id="other-constraints">Other constraints</h3>
<p>developmental sequences</p>
<p>ultimate attainment</p>
<h1 id="key-theories-and-frameworks-in-second-language-acquisition">Key Theories and Frameworks in Second Language Acquisition</h1>
<h2 id="universal-grammar-and-linguistic-theory">Universal Grammar and linguistic theory</h2>
<h3 id="the-basics">The basics</h3>
<p>mental representation</p>
<p>mental representation</p>
<p>poverty of the stimulus</p>
<h3 id="the-claims">The claims</h3>
<h3 id="conclusion">Conclusion</h3>
<h2 id="emergentism-and-usage-based-theories">Emergentism and usage-based theories</h2>
<h3 id="the-basics-1">The basics</h3>
<p>Universal Grammar</p>
<p>input</p>
<h3 id="the-claims-1">The claims</h3>
<h3 id="conclusion-1">Conclusion</h3>
<p>behaviorism</p>
<p>poverty of the stimulus</p>
<h2 id="the-declarativeprocedural-model">The Declarative/Procedural Model</h2>
<h3 id="the-basics-2">The basics</h3>
<p>grammaticality judgments</p>
<p>truth-value tasks</p>
<p>event-related potentials/ERPs, fMRIs, PET or positive</p>
<p>emission topography</p>
<h3 id="the-claims-2">The claims</h3>
<p>morphology</p>
<h3 id="conclusion-2">Conclusion</h3>
<h2 id="complexity-theorydynamic-systems">Complexity Theory/Dynamic Systems</h2>
<h3 id="the-basics-3">The basics</h3>
<h3 id="the-claims-3">The claims</h3>
<h3 id="conclusion-3">Conclusion</h3>
<h2 id="input-processing">Input processing</h2>
<h3 id="the-basics-4">The basics</h3>
<p>form-meaning connections</p>
<p>mental representation</p>
<p>input processing</p>
<p>noticing</p>
<h3 id="the-claims-4">The claims</h3>
<p>developing system</p>
<h3 id="conclusion-4">Conclusion</h3>
<p>processing instruction</p>
<h2 id="the-interaction-hypothesis">The Interaction Hypothesis</h2>
<h3 id="the-basics-5">The basics</h3>
<p>Interaction</p>
<p>feedback</p>
<p>feedback</p>
<h3 id="the-claims-5">The claims</h3>
<h3 id="conclusion-5">Conclusion</h3>
<h2 id="processability-theory">Processability Theory</h2>
<h3 id="the-basics-6">The basics</h3>
<h3 id="the-claims-6">The claims</h3>
<h3 id="conclusion-6">Conclusion</h3>
<p>Key Question “Does Instruction Make a</p>
<p>Difference?”</p>
<h2 id="sociocultural-theory">Sociocultural Theory</h2>
<h3 id="the-basics-7">The basics</h3>
<h3 id="the-claims-7">The claims</h3>
<h3 id="conclusion-7">Conclusion</h3>
<h2 id="skill-acquisition-theory">Skill Acquisition Theory</h2>
<h3 id="the-basics-8">The basics</h3>
<p>declarative knowledge</p>
<p>procedural knowledge</p>
<p>automatization</p>
<h3 id="the-claims-8">The claims</h3>
<h3 id="conclusion-8">Conclusion</h3>
]]></content>
      <categories>
        <category>语言学</category>
        <category>二语习得</category>
      </categories>
  </entry>
  <entry>
    <title>spaCy超强指南之文本预处理和语言特征表</title>
    <url>/spacy-1/</url>
    <content><![CDATA[<h1 id="语言特征表token属性">语言特征表：token属性</h1>
<p>https://spacy.io/api/token#attributes</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&quot;I took a walk happily yesterday.&quot;</span></span><br><span class="line">doc = nlp(sentence)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> tok <span class="keyword">in</span> doc:</span><br><span class="line">    row = <span class="string">&quot;\t&quot;</span>.join([<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> [tok.text, tok.pos_, tok.dep_, tok.head, tok.left_edge, tok.right_edge, tok.lemma_, tok.morph, tok.suffix_, tok.is_stop, tok.tag_]])</span><br><span class="line">    print(row)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<table>
<colgroup>
<col style="width: 8%" />
<col style="width: 7%" />
<col style="width: 3%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 39%" />
<col style="width: 2%" />
<col style="width: 4%" />
<col style="width: 2%" />
<col style="width: 4%" />
</colgroup>
<thead>
<tr class="header">
<th>text</th>
<th>pos_</th>
<th>dep_</th>
<th>head</th>
<th>left_edge</th>
<th>right_edge</th>
<th>lemma_</th>
<th>morph</th>
<th>suffix</th>
<th>is_stop</th>
<th>tag_</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>I</td>
<td>PRON</td>
<td>nsubj</td>
<td>took</td>
<td>I</td>
<td>I</td>
<td>I</td>
<td>Case=Nom|Number=Sing|Person=1|PronType=Prs</td>
<td>I</td>
<td>True</td>
<td>PRP</td>
</tr>
<tr class="even">
<td>took</td>
<td>VERB</td>
<td>ROOT</td>
<td>took</td>
<td>I</td>
<td>.</td>
<td>take</td>
<td>Tense=Past|VerbForm=Fin</td>
<td>ook</td>
<td>False</td>
<td>VBD</td>
</tr>
<tr class="odd">
<td>a</td>
<td>DET</td>
<td>det</td>
<td>walk</td>
<td>a</td>
<td>a</td>
<td>a</td>
<td>Definite=Ind|PronType=Art</td>
<td>a</td>
<td>True</td>
<td>DT</td>
</tr>
<tr class="even">
<td>walk</td>
<td>NOUN</td>
<td>dobj</td>
<td>took</td>
<td>a</td>
<td>walk</td>
<td>walk</td>
<td>Number=Sing</td>
<td>alk</td>
<td>False</td>
<td>NN</td>
</tr>
<tr class="odd">
<td>happily</td>
<td>ADV</td>
<td>advmod</td>
<td>took</td>
<td>happily</td>
<td>happily</td>
<td>happily</td>
<td></td>
<td>ily</td>
<td>False</td>
<td>RB</td>
</tr>
<tr class="even">
<td>yesterday</td>
<td>NOUN</td>
<td>npadvmod</td>
<td>took</td>
<td>yesterday</td>
<td>yesterday</td>
<td>yesterday</td>
<td>Number=Sing</td>
<td>day</td>
<td>False</td>
<td>NN</td>
</tr>
<tr class="odd">
<td>.</td>
<td>PUNCT</td>
<td>punct</td>
<td>took</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>PunctType=Peri</td>
<td>.</td>
<td>False</td>
<td>.</td>
</tr>
</tbody>
</table>
<h1 id="文本预处理">文本预处理</h1>
<p>spaCy的功能：把原始文本处理和标记并返回一个<a href="https://spacy.io/api/doc"><code>Doc</code></a>对象。以下是处理的常见操作，其中lemmatization和形态学是spacy使用的：</p>
<table>
<colgroup>
<col style="width: 38%" />
<col style="width: 61%" />
</colgroup>
<thead>
<tr class="header">
<th>特征</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>规范化（normalization）</td>
<td>transforming a text into a canonical (standard) form, e.g. :) to smile, and goooood to good.</td>
</tr>
<tr class="even">
<td>词形还原（lemmatization）</td>
<td>Base form of the token, with no inflectional suffixes. Lemmatization is typically seen as much more informative than simple stemming, which is why Spacy has opted to only have Lemmatization available instead of Stemming.</td>
</tr>
<tr class="odd">
<td>词干提取（stemming）</td>
<td>reducing inflection in words (e.g. thanks, troubling) to their root form (e.g. thank, troubl).</td>
</tr>
<tr class="even">
<td>形态学（morphology）</td>
<td>"you": Case=Nom|Person=2|PronType=Prs</td>
</tr>
</tbody>
</table>
<h1 id="其他文本预处理的资料">其他文本预处理的资料</h1>
<p>https://medium.com/<span class="citation" data-cites="datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908">@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908</span></p>
<p>https://towardsdatascience.com/stemming-vs-lemmatization-2daddabcb221</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>spaCy</tag>
      </tags>
  </entry>
  <entry>
    <title>计算stopwords</title>
    <url>/stopwords/</url>
    <content><![CDATA[<h1 id="计算最频繁的词组">计算最频繁的词组</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> ngrams</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line">total_num_docs = <span class="built_in">len</span>(papers_dict)</span><br><span class="line">abstracts_list = [paper[<span class="string">&quot;abstract&quot;</span>] <span class="keyword">for</span> paper <span class="keyword">in</span> papers_dict <span class="keyword">if</span> <span class="string">&quot;abstract&quot;</span> <span class="keyword">in</span> paper.keys()]</span><br><span class="line"></span><br><span class="line">total_ngrams_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> abstract <span class="keyword">in</span> abstracts_list:</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">        total_ngrams_list.extend(<span class="built_in">list</span>(<span class="built_in">set</span>([<span class="string">&#x27; &#x27;</span>.join(grams) <span class="keyword">for</span> grams <span class="keyword">in</span> ngrams(abstract.split(), n)])))</span><br><span class="line"></span><br><span class="line">c = Counter(total_ngrams_list).most_common()</span><br><span class="line">ngrams_df = pd.DataFrame(c, columns=[<span class="string">&quot;ngram&quot;</span>, <span class="string">&quot;count&quot;</span>])</span><br><span class="line">print(ngrams_df.head(<span class="number">50</span>))</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="计算tf-idf">计算tf-idf</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先是找到最常见的1-gram, 2-grams, 3-grams和4-grams</span></span><br><span class="line">total_list_grams = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> paper <span class="keyword">in</span> papers_dict:</span><br><span class="line">    abstract = paper[<span class="string">&quot;abstract&quot;</span>]</span><br><span class="line">    total_list_grams.append([<span class="string">&#x27; &#x27;</span>.join(grams) <span class="keyword">for</span> grams <span class="keyword">in</span> ngrams(abstract.split(), <span class="number">3</span>)])</span><br><span class="line">print(total_list_grams)</span><br><span class="line"></span><br><span class="line">计算 tf-idf 权重</span><br><span class="line">TF = ngram在某文档出现的次数/该文档中ngram的个数</span><br><span class="line">IDF = log(语料库的文档总数/包含ngram的文档数+<span class="number">1</span>)</span><br><span class="line">TF-IDF = TF * IDF</span><br><span class="line">实际上，仅凭 IDF 高分就可以解决问题! 计算每个ngram的idf值，排序得到stopwords</span><br><span class="line">list_grams = [[ngram, ngram], [ngram], [ngram]]</span><br><span class="line"><span class="keyword">for</span> list_gram <span class="keyword">in</span> total_list_grams:</span><br><span class="line">    <span class="keyword">if</span> ngram <span class="keyword">in</span> list_gram:</span><br><span class="line">idf = np.log(total_num_docs/)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>句法学最终</title>
    <url>/syntax-final/</url>
    <content><![CDATA[<h3 id="the-logical-problem-of-language-acquisition">the logical problem of language acquisition</h3>
<p>The linguistic data to which children are exposed appear to be insufficient to determine, by themselves, the linguistic knowledge that children eventually attain. The gap between available experience and attained competence forms what has been called the logical problem of language acquisition.</p>
<h3 id="binding-principles">Binding Principles</h3>
<p>A: An anaphor must be bound in its binding domain. Principle B: A pronoun must be free in its binding domain. Principle C: An R-expression must be free.</p>
<h3 id="the-theta-criterion">The theta criterion</h3>
<ol type="a">
<li>Each argument is assigned one and only one theta role. b. Each theta role is assigned to one and only one argument.</li>
</ol>
<p>The theta criterion is a constraint or filter that rules out otherwise well-formed sentences.</p>
<p>The theta criterion requires that there is a strict one-to-one matching between the number and kind of theta roles and the number and kind of arguments (Slide 2, 5).</p>
<h3 id="i-language-vs.-e-language">I-Language vs. E-language</h3>
<p>An I-language is a computational system that is encoded in, or internalized within, an individual brain. It is acquired early and employed by users through life.</p>
<p>The linguistic forms that we see and hear are expressions of externalized language that are used (in conversations, discourse, etc.) in real contexts.</p>
<h3 id="head-specifier-and-complement">head, specifier and complement</h3>
<p>Specifier: Sister to X', daughter of XP.</p>
<p>Adjunct: Sister to X', daughter of X'.</p>
<p>Complement: Sister to X, daughter of X'.</p>
<p>Head: The word that gives its category to the phrase.</p>
<h3 id="dp-movement">DP-movement</h3>
<p>Non-finite T can’t assign NOM case. NOM case is assigned by finite T.</p>
<p>Subject-to-object Raising (also called Exceptional Case Marking or ECM): A kind of DP movement where the subject of an embedded non-finite clause moves to the specifier of AgrO in the main clause to get accusative Case. Expect is an ECM verb. It exceptionally marks ACC case to the Spec of TP (a defective TP which does not have a CP layer).</p>
<p>Extended projection principle (EPP): All clauses must have subjects.</p>
<h2 id="types-of-verbs">types of verbs</h2>
<p><strong>Unaccusative</strong></p>
<p>In modern linguistics, an unaccusative verb is an intransitive verb whose grammatical subject is not a semantic agent. In other words, it does not actively initiate, or is not actively responsible for, the action of the verb.</p>
<p>Like middles, unaccusative verbs project only the internal argument into syntax. Unlike middles, no agentivity is involved in an unaccusative sentence. Unaccusatives are eventive. They depict internally caused events or results. An unaccusative verb is unable to assign a structural case to its internal argument.</p>
<p>Most unaccusative verbs are derivationally (morphologically) related to corresponding transitive verbs.</p>
<p><strong>Middle</strong></p>
<p>Middle constructions depict a state, which does not involve a particular agent and the state does not change with time. A middle sentence is generic; the generic agent is not syntactically present in a middle construction; a middle verb does not project an external argument into syntax.</p>
<p>Middle verbs are derived morphologically from corresponding transitive verbs, e.g. read, sell, bribe, translate, etc..</p>
<p><strong>Passive</strong></p>
<p>A passive sentence depicts an event, involving a particular agent. The agent may or may not appear as an adjunct, and the adjunct can be modified by time, location, manner, intention, etc.</p>
<h3 id="lexemes">Lexemes</h3>
<p>In providing a semantic description of a language, we do not need to treat all the variant morphological forms of a single word separately. Instead, we describe the meanings of a language’s lexemes, or the abstract units which unite all the morphological variants of a single word.</p>
<h3 id="deictic">Deictic</h3>
<p>Deictic expressions (otherwise known as deictics or indexicals) are defined as those which make reference to some aspect of the context of utterance as an essential part of their meaning. Examples of deictics in English include the words I, you and here.</p>
<h3 id="x-bar-theory-recapitulated">X-bar theory recapitulated</h3>
<p>• Motivation of X-bar theory • Head, projection, complement, specifier, adjunct • Lexical head projection, functional head projection • Root/matrix/main clause vs. embedded/subordinate clause • Tensed/finite clause vs. tenseless/non-finite clause • Subcategory, selectional restrictions • Theta roles, theta criterion • The projection principle, EPP, expletive insertion • Two parts of the human language faculty: a. computational component b. lexicon</p>
<p>Aspect is defined by making reference to some other point, typically other than the speech time, then looking at when the event happens relative to that reference point. • perfect aspect: The perfect aspect indicates that the time of an event occurs before the reference time. HAVE + PARTICIPLE (-en)</p>
<p>• progressive aspect: The progressive aspect indicates an event that is ongoing in relation to the reference time. BE + GERUND (-ing)</p>
<p>Combination of tense and aspect: a. I had taken an umbrella. (past perfect) b. I was taking an umbrella. (past progressive) c. I have taken an umbrella. (present perfect) d. I am taking an umbrella. (present progressive) e. I will have taken an umbrella. (future perfect) f. I will be taking an umbrella. (future progressive)</p>
<p>Voice refers to a phenomenon that changes the number of arguments and position of arguments that a verb uses. • Active voice: The agent of the action occupies the subject position, whereas the theme occupies the object position. e.g. John did the work. • Passive voice: The theme appears in subject position, whereas the agent is either introduced by a “by-phrase” or omitted. BE + PARTICIPLE (-en) e.g. The work was done.</p>
<p>Mood refers to the speaker’s perspective on the event— whether the event described is a possibility, a probability, a necessity, or an obligation. •Modals of English: can, could, may, might, would, shall, should, must, ought a. John must have done his work. a’. * John has must done his work. b. John must not have done his work. b’. * John not must have done his work. c. * John musts have done his work.</p>
<p>Predicates in syntax defines the relation between the individuals being talked about and the real world—as well as with each other. The participants participating in the relation are called arguments.</p>
<p>Selectional restrictions A verb subcategorizes (takes/selects) for zero, one or more complements There are semantic restrictions on what can appear in a particular syntactic position or what can be selected by predicates</p>
<p>Information relating to arguments of predicates is called the argument structure (of predicates).</p>
<p>Argument structure indicates what kinds of XP arguments the verbs require in order to complete their meaning. They assign theta-roles to these required XPs, which indicate the semantic role that the argument plays in syntax. • Theta-roles encode the semantic roles possessed by the various elements which are required by the predicate.</p>
<p>Theta roles (Ɵ) : bundles of thematic relations that cluster on one argument.</p>
<p>Thematic relations Agent: the initiator or doer of an action a. John hit him on the nose. b. A cat is chasing a dog. Theme: an entity that is moved or affected by an action; an entity that is experienced or perceived a. John hit him on the nose. b. A cat is chasing a dog. c. John watched the football match at home. Patient: an entity undergoing an action a. The enemies destroyed the city. 8 Thematic relations Experiencer: an entity that feels or perceives events a. Mary really enjoyed the movie. b. The tigers frightened the children. c. Mary saw Bill in the library. Goal: an entity towards which a motion takes place a. John will go to Shanghai tomorrow. b. An evil thought struck Dave. Recipient: an entity that receives. It only occurs with verbs that denote the change of possession. a. John gave a book to Mary. b. John gave Mary a book. c. Mary received John’s book. 9 Thematic relations Source: an entity from which a motion takes place a. John gave Mary a book. b. John came from Beijing. Location: a place where an action occurs a. We have our syntax class in 304. b. The accident occurred at the crossroads. Instrument: an object with which an action is performed a. This key opens Rm. 307. b. He opened the door with a key. Beneficiary: an entity for whose benefit an event takes place a. John baked Mary a cake. b. John bought Mary a bunch of flowers.</p>
<p>The fact that lexical information affects the form of the sentence is formalized in Projection Principle. • The Projection Principle: Lexical information is syntactically represented at all levels.</p>
<p>Expletive (or Pleonastic) Pronoun: A pronoun (usually it or there) without a theta role. Usually found in subject position.</p>
<p>Extended projection principle (EPP): All clauses must have subjects. • The EPP is an extension of an older principle the Projection Principle, which said that all information in the argument structure of a lexical item had to be present in syntax. • Expletive insertion rule: Insert an expletive pronoun into the specifier of TP.</p>
<p>Unpronounced DP: PRO a. She i was wondering whether PRO i to trust Bill. b. [ PRO To buy a bicycle now] is very important.</p>
<p>Theta criterion is observed. EPP is satisfied.</p>
<p>The VP-internal Subject Hypothesis: Subjects are generated in the specifier of the voice-headed VP.</p>
<p>VP-internal subject hypothesis The arguments of a predicate are within the immediate dominance domain of the verb: DP1 _ DP2. The external argument originates from the specifier position of the VP. The surface presence of the grammatical subject is a result of moving (A-movement/DP-movement, to be discussed), i.e., it moves/raises to the specifier position of TP (Spec, TP). ◼ The Locality Constraint on Theta Role Assignment Theta roles are assigned within the projection of the head that assigns them (i.e., the VP or other predicate).</p>
<p>DP movement/A-movement moves an argument to a non-argument position. It is a movement motivated/triggered by the requirement of Structural Case. Some of the universal principles regarding such a movement include Theta Criterion, Case Theory and the Extended Projection Principle (EPP)</p>
<p>Related theories • Theta Criterion: Each argument is assigned one and only one theta role; each theta role is assigned to one and only one argument. • The Locality Constraint on Theta Role Assignment: Theta roles are assigned within the projection of the head that assigns them. • VP Internal-subject Hypothesis: Subjects are generated in the specifier position of the VP. • EPP: All clauses must have subjects. • It-insertion: Insert an expletive pronoun it or there into the specifier of TP. • Case Theory: Case theory deals with the assignment of particular morphological case (overt or covert) to DPs in the sentences. Case assignment has been theorized as the Case Filter. • Case Filter: *DP if DP has phonetic content and has no Case. In other words: Every phonetically realized DP must be marked with Case. If a DP doesn’t get Case, the derivation will crash.</p>
<p>DP movement/A-movement: Move a DP to a specifier position. Trigger of movement: EPP and Case Filter Non-finite T can’t assign NOM case. It-insertion for EPP and NOM case is assigned by finite T. ø</p>
<p>Wh-movement: Move a wh-phrase to the specifier of CP to check the [+WH] feature in C.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>句法</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow基础知识</title>
    <url>/tensorflow-0/</url>
    <content><![CDATA[<h2 id="全连接层">全连接层</h2>
<p>全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。 由于其全相连的特性，一般全连接层的参数也是最多的。 例如在VGG16中，第一个全连接层FC1有4096个节点，上一层POOL2是7<em>7</em>512 = 25088个节点，则该传输需要4096*25088个权值，需要耗很大的内存。</p>
<p>其实就是f = ax+by+cz+d</p>
<p>Dense layer: Dense layer, also called fully-connected layer, refers to the layer whose inside neurons connect to every neuron in the preceding layer.</p>
<p><img src="https://i.loli.net/2021/07/15/NZTFxOtYi9jqucv.png"/></p>
<p>参考：<a href="https://www.zhihu.com/question/41037974">全连接层的作用是什么？</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/33841176">CNN中的全连接层</a></p>
<h2 id="梯度下降法">梯度下降法</h2>
<h2 id="交叉熵">交叉熵</h2>
<p><a href="https://blog.csdn.net/u014453898/article/details/81559462">损失函数之交叉熵(一般用于分类问题)</a></p>
]]></content>
  </entry>
  <entry>
    <title>关于Transformer的总结与感悟</title>
    <url>/transformer-summary/</url>
    <content><![CDATA[<p>Transformer模型于2017年在论文<a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>中提出，但我是近期才真正开始研究其内部架构，在本文中我将记录阅读相关资料及论文的心得。参考文献包括<a href="https://medium.com/analytics-vidhya/transformers-lets-dive-deeeep-7784bdb20807">Transformers — Let’s Dive Deeeep!</a>以及<a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>。</p>
<ul>
<li>研究一种模型</li>
</ul>
<h2 id="transformer的特点及优点">1. Transformer的特点及优点</h2>
<ul>
<li>使用<strong>注意力</strong>，通过<strong>并行化</strong>来提升模型训练速度</li>
<li>无需<strong>按顺序</strong>处理sequential data（优于RNN）</li>
</ul>
<h2 id="输入与输出">2. 输入与输出</h2>
<p>对输入的数据进行embedding操作</p>
<h2 id="组成单元编码器解码器">3. 组成单元：编码器/解码器</h2>
<ul>
<li><strong>编码器</strong>将输入序列映射到一个抽象的连续表示</li>
<li><strong>解码器</strong>采用该连续表示并<u>逐步</u>生成单个输出（同时根据前几个输出）</li>
</ul>
<p>论文采用6个编码器+6个解码器</p>
<h2 id="编码器架构自注意层前馈神经网络层">4. <strong>编码器架构</strong>：<strong>自注意层</strong>+<strong>前馈神经网络层</strong></h2>
<ul>
<li>自注意层：学习<strong>当前单词与句子其他位置所有单词之间的相关性。</strong></li>
</ul>
<p><span class="math inline">\(\boldsymbol{H}=\boldsymbol{V} \operatorname{softmax}\left(\frac{\boldsymbol{K}^{\top} \boldsymbol{Q}}{\sqrt{D_{k}}}\right)\)</span></p>
<ul>
<li>前馈神经网络层（FFN）</li>
</ul>
<p><span class="math inline">\(\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}\)</span></p>
<h2 id="解码器架构">5. 解码器架构</h2>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语言模型</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>纯文本文件的处理</title>
    <url>/txt/</url>
    <content><![CDATA[<h1 id="正则表达式">正则表达式</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">file_path = <span class="string">r&#x27;D:\books\Psychology_of_Language.txt&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    txt_string = f.read()</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;第一 初见.*第二 再见&#x27;</span>, re.DOTALL)</span><br><span class="line">cha1 = re.search(pattern, txt_string).group()</span><br><span class="line">print(re.findall(<span class="string">r&#x27;蓝色&#x27;</span>, cha1))</span><br><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;第一 初见[^蓝色]*(蓝色)*.*第二 再见&#x27;</span>, re.DOTALL)</span><br><span class="line">match = re.search(pattern, txt_string)</span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">    print(match.groups())</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="获取文章目录">获取文章目录</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">file_path = <span class="string">r&#x27;D:\project\hexo-final\source\_posts&#x27;</span></span><br><span class="line"><span class="comment"># 这里要修改</span></span><br><span class="line"><span class="comment"># weblink = &#x27;https://github.com/MissFreak/writings/blob/main/&#x27;</span></span><br><span class="line">weblink = <span class="string">&#x27;http://nlpcourse.cn/&#x27;</span></span><br><span class="line">lst = os.listdir(file_path)</span><br><span class="line">post_list = []</span><br><span class="line"><span class="comment"># 这里要修改</span></span><br><span class="line">md_name = <span class="string">&#x27;all-posts.md&#x27;</span></span><br><span class="line"><span class="comment"># md_name = &#x27;README.md&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain the titles and categories of all posts</span></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> lst:</span><br><span class="line">	<span class="comment"># make sure it is a markdown file</span></span><br><span class="line">	<span class="keyword">if</span> filename[-<span class="number">3</span>:] == <span class="string">&#x27;.md&#x27;</span>:</span><br><span class="line">		<span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(file_path, filename), encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">			s = f.read()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># create a dict that stores attributes: title and category_1 and category_2</span></span><br><span class="line">		post_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment"># get the title</span></span><br><span class="line">		match_title = re.search(<span class="string">r&#x27;(?&lt;=title: ).*(?=\n)&#x27;</span>, s)</span><br><span class="line">		<span class="keyword">if</span> match_title:</span><br><span class="line">			title = match_title.group().strip(<span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			title = filename</span><br><span class="line"></span><br><span class="line">		<span class="comment"># convert into linked title</span></span><br><span class="line">        <span class="comment"># 这里要修改</span></span><br><span class="line">		<span class="comment"># linked_title = &#x27;[&#123;&#125;](&#123;&#125;&#123;&#125;)&#x27;.format(title, weblink, filename)</span></span><br><span class="line">		linked_title = <span class="string">&#x27;[&#123;&#125;](&#123;&#125;&#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(title, weblink, filename[:-<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">		<span class="comment"># get the categories</span></span><br><span class="line">		match_categories = re.search(<span class="string">r&#x27;categories:\n- (.*)\n- (.*)\n&#x27;</span>, s)</span><br><span class="line">		<span class="keyword">try</span>:</span><br><span class="line">			category_1 = match_categories.group(<span class="number">1</span>)</span><br><span class="line">			category_2 = match_categories.group(<span class="number">2</span>)</span><br><span class="line">		<span class="keyword">except</span>:</span><br><span class="line">			category_1 = <span class="string">&#x27;未分类&#x27;</span></span><br><span class="line">			category_2 = <span class="string">&#x27;未分类&#x27;</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># save into a list</span></span><br><span class="line">		post_dict[<span class="string">&#x27;title&#x27;</span>] = linked_title</span><br><span class="line">		post_dict[<span class="string">&#x27;category_1&#x27;</span>] = category_1</span><br><span class="line">		post_dict[<span class="string">&#x27;category_2&#x27;</span>] = category_2</span><br><span class="line">		post_list.append(post_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sort and group by the first category</span></span><br><span class="line">get_first_category = <span class="keyword">lambda</span> dct: dct.get(<span class="string">&#x27;category_1&#x27;</span>)</span><br><span class="line">group_1 = itertools.groupby(<span class="built_in">sorted</span>(post_list, key=get_first_category), get_first_category)</span><br><span class="line">num_post = <span class="built_in">len</span>(post_list)</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;---\ntitle: 所有文章目录\n---\n&lt;center&gt;目前共有&#123;&#125;篇文章：&lt;/center&gt;\n&lt;!-- more --&gt;\n\n&#x27;</span>.<span class="built_in">format</span>(num_post)</span><br><span class="line"><span class="keyword">for</span> k1,v1 <span class="keyword">in</span> group_1:</span><br><span class="line">	<span class="comment"># add the first category into content</span></span><br><span class="line">	content += (<span class="string">&#x27;# &#x27;</span>+k1+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">	<span class="comment"># sort and group by the first category</span></span><br><span class="line">	get_second_category = <span class="keyword">lambda</span> dct: dct.get(<span class="string">&#x27;category_2&#x27;</span>)</span><br><span class="line">	group_2 = itertools.groupby(<span class="built_in">sorted</span>(v1, key=get_second_category), get_second_category)</span><br><span class="line">	<span class="keyword">for</span> k2,v2 <span class="keyword">in</span> group_2:</span><br><span class="line">		<span class="comment"># add the second category into content</span></span><br><span class="line">		<span class="keyword">if</span> k2 != <span class="string">&#x27;未分类&#x27;</span>:</span><br><span class="line">			content += (<span class="string">&#x27;- _&#x27;</span>+k2+<span class="string">&#x27;_\n&#x27;</span>)</span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> v2:</span><br><span class="line">				<span class="comment"># add the title into content</span></span><br><span class="line">				content += (<span class="string">&#x27;\t- &#x27;</span>+i[<span class="string">&#x27;title&#x27;</span>]+<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> v2:</span><br><span class="line">				<span class="comment"># add the title into content</span></span><br><span class="line">				content += (<span class="string">&#x27;- &#x27;</span>+i[<span class="string">&#x27;title&#x27;</span>]+<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">print(content)</span><br><span class="line"><span class="comment"># write the content into md file</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(file_path, md_name), <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	f.write(content)</span><br></pre></td></tr></table></figure>
<h1 id="分离章节">分离章节</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">file_path = <span class="string">r&#x27;D:\books\Psychology_of_Language.txt&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    txt_string = f.read()</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;\ng\n&#x27;</span>)</span><br><span class="line">chapters = pattern.split(txt_string)[<span class="number">6</span>:]</span><br><span class="line"></span><br><span class="line">cha8 = chapters[<span class="number">7</span>]</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;\nn\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>, cha8)</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;\n&#123;2,&#125;&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, cha8)</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;-&#x27;</span>, <span class="string">&#x27;&#x27;</span>, cha8)</span><br><span class="line"><span class="comment"># print(repr(cha8))</span></span><br><span class="line">print(cha8)</span><br></pre></td></tr></table></figure>
<h1 id="去除字符">去除字符</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，\n 请加入，谢谢\n\n\n&#x27;</span></span><br><span class="line">print(ss.strip(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，\n 请加入，谢谢\n\n\n&#x27;</span></span><br><span class="line">print(ss.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>))</span><br><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，请加入，谢谢啦啦嗯&#x27;</span></span><br><span class="line">print(ss.strip(<span class="string">&#x27;嗯啦&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h1 id="表格pytablewriter">表格：<a href="https://github.com/thombashi/pytablewriter/">pytablewriter</a></h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MarkdownTableWriter, ExcelXlsxTableWriter, UnicodeTableWriter, JavaScriptTableWriter, JsonTableWriter, HtmlTableWriter</span><br></pre></td></tr></table></figure>
<p>先获得matrix：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tempo.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	s = f.read()</span><br><span class="line">matrix = [i.split(<span class="string">&#x27;: &#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> s.split(<span class="string">&#x27;\n&#x27;</span>)] <span class="comment"># 行分隔符和列分隔符</span></span><br><span class="line">print(matrix)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tempo2.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	s = f.read()</span><br><span class="line">matrix = [i.split(<span class="string">&#x27; &#x27;</span>, <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> s.split(<span class="string">&#x27;\n&#x27;</span>)] <span class="comment"># split on first occurrence</span></span><br><span class="line">print(matrix)</span><br></pre></td></tr></table></figure>
<p>然后生成相应的表格：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line">writer = pytablewriter.MarkdownTableWriter()</span><br><span class="line">writer.value_matrix = matrix</span><br><span class="line">writer.write_table()</span><br></pre></td></tr></table></figure>
<h2 id="markdown表格">markdown表格</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># s = spacy.explain(&#x27;``&#x27;)</span></span><br><span class="line"><span class="comment"># print(repr(s))</span></span><br><span class="line">lst1 = <span class="string">&#x27;`$`, `&#x27;</span><span class="string">&#x27;`, `,`, `-LRB-`, `-RRB-`, `.`, `:`, `ADD`, `AFX`, `CC`, `CD`, `DT`, `EX`, `FW`, `HYPH`, `IN`, `JJ`, `JJR`, `JJS`, `LS`, `MD`, `NFP`, `NN`, `NNP`, `NNPS`, `NNS`, `PDT`, `POS`, `PRP`, `PRP$`, `RB`, `RBR`, `RBS`, `RP`, `SYM`, `TO`, `UH`, `VB`, `VBD`, `VBG`, `VBN`, `VBP`, `VBZ`, `WDT`, `WP`, `WP$`, `WRB`, `XX`&#x27;</span>.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">lst2 = [spacy.explain(i.strip(<span class="string">&#x27; `&#x27;</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> lst1]</span><br><span class="line"><span class="comment"># print(lst2)</span></span><br><span class="line"></span><br><span class="line">matrix = np.column_stack((lst1, lst2)).tolist()</span><br><span class="line"><span class="comment"># print(type(matrix))</span></span><br><span class="line"><span class="comment"># print(matrix)</span></span><br><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line">writer = pytablewriter.MarkdownTableWriter()</span><br><span class="line">writer.value_matrix = matrix</span><br><span class="line">writer.write_table()</span><br></pre></td></tr></table></figure>
<h2 id="csv">csv</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    writer = pytablewriter.CsvTableWriter()</span><br><span class="line">    writer.headers = [<span class="string">&quot;int&quot;</span>, <span class="string">&quot;float&quot;</span>, <span class="string">&quot;str&quot;</span>, <span class="string">&quot;bool&quot;</span>, <span class="string">&quot;mix&quot;</span>, <span class="string">&quot;time&quot;</span>]</span><br><span class="line">    writer.value_matrix = [</span><br><span class="line">        [<span class="number">0</span>,   <span class="number">0.1</span>,      <span class="string">&quot;hoge&quot;</span>, <span class="literal">True</span>,   <span class="number">0</span>,      <span class="string">&quot;2017-01-01 03:04:05+0900&quot;</span>],</span><br><span class="line">        [<span class="number">2</span>,   <span class="string">&quot;-2.23&quot;</span>,  <span class="string">&quot;foo&quot;</span>,  <span class="literal">False</span>,  <span class="literal">None</span>,   <span class="string">&quot;2017-12-23 45:01:23+0900&quot;</span>],</span><br><span class="line">        [<span class="number">3</span>,   <span class="number">0</span>,        <span class="string">&quot;bar&quot;</span>,  <span class="string">&quot;true&quot;</span>,  <span class="string">&quot;inf&quot;</span>, <span class="string">&quot;2017-03-03 33:44:55+0900&quot;</span>],</span><br><span class="line">        [-<span class="number">10</span>, -<span class="number">9.9</span>,     <span class="string">&quot;&quot;</span>,     <span class="string">&quot;FALSE&quot;</span>, <span class="string">&quot;nan&quot;</span>, <span class="string">&quot;2017-01-01 00:00:00+0900&quot;</span>],</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    writer.write_table()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="其他格式">其他格式</h2>
<p><a href="https://pytablewriter.readthedocs.io/en/latest/pages/examples/table_format/text/json.html">JSON</a></p>
<p><a href="https://pytablewriter.readthedocs.io/en/latest/pages/examples/table_format/text/html.html">HTML</a></p>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>txt</tag>
      </tags>
  </entry>
  <entry>
    <title>计算语言学文献语料库的可视化</title>
    <url>/visualization/</url>
    <content><![CDATA[<p>展示标注。</p>
<span id="more"></span>
<h1 id="最终版">最终版</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> render_template</span><br><span class="line"><span class="keyword">from</span> flask_paginate <span class="keyword">import</span> Pagination, get_page_parameter</span><br><span class="line"></span><br><span class="line">app = Flask(__name__, static_folder=<span class="string">&quot;templates/static&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;D:\pythonProject\result\papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span>():</span></span><br><span class="line">    <span class="comment"># Set the pagination configuration</span></span><br><span class="line">    page = request.args.get(get_page_parameter(), <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>)</span><br><span class="line">    start = (page-<span class="number">1</span>) * <span class="number">5</span></span><br><span class="line">    end = start + <span class="number">5</span></span><br><span class="line">    papers = papers_dict[start:end]</span><br><span class="line">    pagination = Pagination(page=page, per_page=<span class="number">5</span>, total=<span class="built_in">len</span>(papers_dict), bs_version=<span class="number">4</span>, search=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&#x27;index.html&#x27;</span>, papers_dict=papers, pagination=pagination)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    app.run(debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="第一步在网页分页展示1000篇文章">第一步：在网页分页展示1000篇文章</h1>
<p><span class="citation" data-cites="app.py">@app.py</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> render_template</span><br><span class="line"><span class="keyword">from</span> flask_paginate <span class="keyword">import</span> Pagination, get_page_parameter</span><br><span class="line"></span><br><span class="line">app = Flask(__name__, static_folder=<span class="string">&quot;templates/static&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;data/papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span>():</span></span><br><span class="line">    <span class="comment"># Set the pagination configuration</span></span><br><span class="line">    page = request.args.get(get_page_parameter(), <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>)</span><br><span class="line">    start = (page-<span class="number">1</span>) * <span class="number">5</span></span><br><span class="line">    end = start + <span class="number">5</span></span><br><span class="line">    papers = papers_dict[start:end]</span><br><span class="line">    pagination = Pagination(page=page, per_page=<span class="number">5</span>, total=<span class="built_in">len</span>(papers_dict), bs_version=<span class="number">4</span>, search=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&#x27;index.html&#x27;</span>, papers_dict=papers, pagination=pagination)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><span class="citation" data-cites="index.html">@index.html</span></p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">&#123;% extends &#x27;base.html&#x27; %&#125;</span><br><span class="line">&#123;% block content %&#125;</span><br><span class="line">&#123;&#123; pagination.info &#125;&#125;</span><br><span class="line">&#123;% for paper in papers_dict %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">h2</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;index&quot;</span>&gt;</span>&#123;&#123; loop.index + pagination.skip &#125;&#125;<span class="tag">&lt;/<span class="name">span</span>&gt;</span> &#123;&#123; paper.title &#125;&#125;<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;text-muted&quot;</span>&gt;</span>&#123;&#123; paper.author &#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>&#123;&#123; paper.abstract &#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;&#123;&#123; paper.url &#125;&#125;&quot;</span> <span class="attr">class</span>=<span class="string">&quot;url&quot;</span>&gt;</span>Link<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">&#123;% endfor %&#125;</span><br><span class="line">&#123;&#123; pagination.links &#125;&#125;</span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure>
<h1 id="第二步使用ftd数据集修改网页展示">第二步：使用FTD数据集修改网页展示</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">app = Flask(__name__, static_folder=<span class="string">&quot;templates/static&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;data/papers.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    papers_dict = json.load(read_file)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span>():</span></span><br><span class="line">    <span class="comment"># Set the pagination configuration</span></span><br><span class="line">    page = request.args.get(get_page_parameter(), <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>)</span><br><span class="line">    start = (page-<span class="number">1</span>) * <span class="number">5</span></span><br><span class="line">    end = start + <span class="number">5</span></span><br><span class="line">    papers = papers_dict[start:end]</span><br><span class="line">    pagination = Pagination(page=page, per_page=<span class="number">5</span>, total=<span class="built_in">len</span>(papers_dict), bs_version=<span class="number">4</span>, search=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&#x27;index.html&#x27;</span>, papers_dict=papers, pagination=pagination)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>wikipedia API</title>
    <url>/wikipedia/</url>
    <content><![CDATA[<p>获取维基百科信息可以通过其提供的数据库（<a href="http://dumps.wikimedia.org/">http://dumps.wikimedia.org</a>）来实现，但数据量巨大，Python的第三方库wikipedia可以访问但是太慢了，因此使用<a href="https://blog.csdn.net/qq_43549752/article/details/88894616">博客</a>的方法，但还是不方便，最后使用了<a href="https://stackoverflow.com/questions/24474288/how-to-obtain-a-list-of-titles-of-all-wikipedia-articles">Stack Overflow</a>的方法，在https://dumps.wikimedia.org/enwiki/latest/页面成功下载了<a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-all-titles-in-ns0.gz">enwiki-latest-all-titles-in-ns0.gz</a></p>
<span id="more"></span>
<p>另一种方法是用<a href="https://sobigdata.d4science.org/web/tagme/tagme-help">tagme</a>来进行实体链接：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">API_URL = <span class="string">&quot;https://tagme.d4science.org/tagme/tag&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query_params = &#123;</span><br><span class="line">        <span class="string">&#x27;gcube-token&#x27;</span>: <span class="string">&#x27;490ad154-2e24-490f-bee9-02affb1a0a46-843339462&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;lang&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Sarcasm is a linguistic expression often used to communicate the opposite of what is said, usually something that is very unpleasant with an intention to insult or ridicule. Inherent ambiguity in sarcastic expressions makes sarcasm detection very difficult. In this work, we focus on detecting sarcasm in textual conversations, written in English, from various social networking platforms and online media. To this end, we develop an interpretable deep learning model using multi-head self-attention and gated recurrent units. We show the effectiveness and interpretability of our approach by achieving state-of-the-art results on datasets from social networking platforms, online discussion forums, and political dialogues.&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">r = requests.get(API_URL, params=query_params)</span><br><span class="line"></span><br><span class="line">entity_list = json.loads(r.text)[<span class="string">&#x27;annotations&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> entity <span class="keyword">in</span> entity_list:</span><br><span class="line">        print(entity[<span class="string">&#x27;spot&#x27;</span>] + <span class="string">&#x27;\t&#x27;</span> + entity[<span class="string">&#x27;title&#x27;</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>wikipedia</tag>
      </tags>
  </entry>
  <entry>
    <title>好用的API集锦</title>
    <url>/wiki-api/</url>
    <content><![CDATA[<h1 id="wikipedia-python库">Wikipedia Python库</h1>
<p><strong>Wikipedia</strong>是一个Python库，可轻松访问和解析Wikipedia中的数据，搜索Wikipedia、获取文章摘要、从页面获取链接和图像等数据等等。Wikipedia封装了<a href="https://www.mediawiki.org/wiki/API">MediaWiki API，</a>因此您可以专注于使用Wikipedia数据，而不是获取数据。 <span id="more"></span></p>
]]></content>
      <tags>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>认知语言学论文代码</title>
    <url>/word_embeddings2/</url>
    <content><![CDATA[<p>用到BERT词向量、PCA、聚类和分类算法（及相关衡量指标）、画热力图。原文题目：Classifying Semantic Categories and Identifying Prototypes using BERT Word Representations.</p>
<span id="more"></span>
<h1 id="实验一代码">实验一代码</h1>
<h2 id="区分类别">区分类别</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">category1 = <span class="string">&#x27;furniture&#x27;</span></span><br><span class="line">category2 = <span class="string">&#x27;fruit&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line">tsne = TSNE(n_components=<span class="number">2</span>, init=<span class="string">&#x27;pca&#x27;</span>, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">df = df[(df[<span class="string">&#x27;category&#x27;</span>]==category1) | (df[<span class="string">&#x27;category&#x27;</span>]==category2)]</span><br><span class="line">df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">&#x27;category&#x27;</span>] = pd.Categorical(df[<span class="string">&#x27;category&#x27;</span>])</span><br><span class="line">y = df.category.cat.codes</span><br><span class="line"></span><br><span class="line">words_embeddings = model.encode(df[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line">embedd = tsne.fit_transform(words_embeddings)</span><br><span class="line">print(embedd.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">plt.scatter(embedd[:, <span class="number">0</span>], embedd[:, <span class="number">1</span>], c=y)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df)):  <span class="comment"># for each point</span></span><br><span class="line">    plt.annotate(text=df.iloc[i, -<span class="number">2</span>], xy=(embedd[:, <span class="number">0</span>][i], embedd[:, <span class="number">1</span>][i]),</span><br><span class="line">                 xytext=(embedd[:, <span class="number">0</span>][i] + <span class="number">0.1</span>, embedd[:, <span class="number">1</span>][i] + <span class="number">0.1</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;2-D projection of BERT embeddings of instances of the category &quot;&#123;&#125;&quot; and the category &quot;&#123;&#125;&quot;&#x27;</span>.<span class="built_in">format</span>(category1, category2), fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="两两比较自动作图">两两比较自动作图</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># category1 = &#x27;sport&#x27;</span></span><br><span class="line">category2 = <span class="string">&#x27;fruit&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line">tsne = TSNE(n_components=<span class="number">2</span>, init=<span class="string">&#x27;pca&#x27;</span>, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_two_categories</span>(<span class="params">df, category1, category2</span>):</span></span><br><span class="line">    df = df[(df[<span class="string">&#x27;category&#x27;</span>]==category1) | (df[<span class="string">&#x27;category&#x27;</span>]==category2)]</span><br><span class="line">    df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line">    df[<span class="string">&#x27;category&#x27;</span>] = pd.Categorical(df[<span class="string">&#x27;category&#x27;</span>])</span><br><span class="line">    y = df.category.cat.codes</span><br><span class="line"></span><br><span class="line">    words_embeddings = model.encode(df[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line">    embedd = tsne.fit_transform(words_embeddings)</span><br><span class="line">    print(embedd.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">9</span>))</span><br><span class="line">    plt.scatter(embedd[:, <span class="number">0</span>], embedd[:, <span class="number">1</span>], c=y)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df)):  <span class="comment"># for each point</span></span><br><span class="line">        plt.annotate(text=df.iloc[i, -<span class="number">2</span>], xy=(embedd[:, <span class="number">0</span>][i], embedd[:, <span class="number">1</span>][i]),</span><br><span class="line">                     xytext=(embedd[:, <span class="number">0</span>][i] + <span class="number">0.1</span>, embedd[:, <span class="number">1</span>][i] + <span class="number">0.1</span>))</span><br><span class="line">    plt.title(<span class="string">&#x27;2-D projection of BERT embeddings of instances of the category &quot;&#123;&#125;&quot; and the category &quot;&#123;&#125;&quot;&#x27;</span>.<span class="built_in">format</span>(category1, category2), fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">    plt.savefig(<span class="string">r&#x27;D:\python\&#123;&#125;&amp;&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(category1, category2), dpi=<span class="number">300</span>)</span><br><span class="line">    print(<span class="string">&quot;saved: &#123;&#125;&amp;&#123;&#125;.png&quot;</span>.<span class="built_in">format</span>(category1, category2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> category <span class="keyword">in</span> df[<span class="string">&#x27;category&#x27;</span>].unique():</span><br><span class="line">    <span class="keyword">if</span> category != <span class="string">&quot;fruit&quot;</span>:</span><br><span class="line">        plot_two_categories(df, category, category2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="实验二代码k-means">实验二代码K-means</h1>
<h2 id="聚类并计算completeness-homogenity-v-measure">聚类并计算completeness, homogenity, v-measure</h2>
<p>https://stackoverflow.com/questions/51320227/determining-accuracy-for-k-means-clustering</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line">df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">&#x27;category&#x27;</span>] = pd.Categorical(df[<span class="string">&#x27;category&#x27;</span>])</span><br><span class="line">y = df.category.cat.codes</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line">words_embeddings = model.encode(df[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> v_measure_score, homogeneity_score, completeness_score, contingency_matrix</span><br><span class="line"></span><br><span class="line">num_clusters = <span class="built_in">len</span>(df[<span class="string">&#x27;category&#x27;</span>].unique())</span><br><span class="line">cluster = KMeans(n_clusters=num_clusters)</span><br><span class="line">cluster.fit(words_embeddings)</span><br><span class="line">pred = cluster.labels_</span><br><span class="line"></span><br><span class="line">score = v_measure_score(pred, y)</span><br><span class="line">print(<span class="string">&#x27;V_measure scored using k-means clustering: &#x27;</span>, score)</span><br><span class="line"><span class="comment"># V_measure scored using k-means clustering:  0.3291335241572588</span></span><br><span class="line">print(contingency_matrix(pred, y))</span><br><span class="line"></span><br><span class="line">[[ <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span> <span class="number">14</span>  <span class="number">0</span>  <span class="number">3</span>  <span class="number">0</span>  <span class="number">3</span>  <span class="number">1</span>]</span><br><span class="line"> [<span class="number">11</span> <span class="number">39</span> <span class="number">11</span> <span class="number">12</span>  <span class="number">6</span> <span class="number">29</span> <span class="number">11</span> <span class="number">15</span>  <span class="number">2</span> <span class="number">15</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">2</span>  <span class="number">0</span>  <span class="number">0</span> <span class="number">12</span>  <span class="number">0</span>  <span class="number">2</span>  <span class="number">2</span>  <span class="number">9</span>  <span class="number">1</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">0</span> <span class="number">37</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span> <span class="number">27</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">35</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">6</span>  <span class="number">0</span>  <span class="number">3</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">5</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">3</span> <span class="number">10</span>  <span class="number">3</span> <span class="number">16</span>  <span class="number">9</span>  <span class="number">5</span> <span class="number">11</span>]</span><br><span class="line"> [ <span class="number">0</span>  <span class="number">2</span>  <span class="number">0</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">8</span>  <span class="number">6</span>  <span class="number">1</span> <span class="number">22</span>  <span class="number">6</span>]</span><br><span class="line"> [ <span class="number">0</span>  <span class="number">8</span>  <span class="number">0</span> <span class="number">17</span>  <span class="number">0</span>  <span class="number">8</span>  <span class="number">7</span>  <span class="number">0</span>  <span class="number">3</span>  <span class="number">1</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">0</span> <span class="number">27</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">3</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span> <span class="number">14</span>  <span class="number">6</span>  <span class="number">6</span>  <span class="number">1</span>  <span class="number">2</span> <span class="number">19</span>]]</span><br><span class="line"></span><br><span class="line">print(homogeneity_score(pred, y))</span><br><span class="line">print(completeness_score(pred, y))</span><br><span class="line">print(v_measure_score(pred, y))</span><br><span class="line"></span><br><span class="line"><span class="number">0.34573277828633814</span></span><br><span class="line"><span class="number">0.3272232730001905</span></span><br><span class="line"><span class="number">0.3362234757471136</span></span><br></pre></td></tr></table></figure>
<h2 id="画热力图">画热力图</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># import re</span></span><br><span class="line"><span class="comment"># text = &#x27;&#x27;&#x27;[[ 0  0  0  0 14  0  3  0  3  1]</span></span><br><span class="line"><span class="comment">#  [11 39 11 12  6 29 11 15  2 15]</span></span><br><span class="line"><span class="comment">#  [ 1  2  0  0 12  0  2  2  9  1]</span></span><br><span class="line"><span class="comment">#  [ 1  0 37  0  0  0  0 27  0  0]</span></span><br><span class="line"><span class="comment">#  [35  0  0  0  1  3  6  0  3  3]</span></span><br><span class="line"><span class="comment">#  [ 5  2  3  3 10  3 16  9  5 11]</span></span><br><span class="line"><span class="comment">#  [ 0  2  0  1  1  8  6  1 22  6]</span></span><br><span class="line"><span class="comment">#  [ 0  8  0 17  0  8  7  0  3  1]</span></span><br><span class="line"><span class="comment">#  [ 1  1  0 27  1  3  3  1  1  3]</span></span><br><span class="line"><span class="comment">#  [ 0  1  0  0 14  6  6  1  2 19]]&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># text = re.sub(r&#x27;(?&lt;=[0-9]) &#x27;, &#x27;,&#x27;, text)</span></span><br><span class="line"><span class="comment"># text = re.sub(r&#x27;\n&#x27;, &#x27;,\n&#x27;, text)</span></span><br><span class="line"><span class="comment"># print(text)</span></span><br><span class="line"></span><br><span class="line">a = np.array([[ <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,<span class="number">14</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line"> [<span class="number">11</span>,<span class="number">39</span>,<span class="number">11</span>,<span class="number">12</span>, <span class="number">6</span>,<span class="number">29</span>,<span class="number">11</span>,<span class="number">15</span>, <span class="number">2</span>,<span class="number">15</span>],</span><br><span class="line"> [ <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>,<span class="number">12</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">1</span>],</span><br><span class="line"> [ <span class="number">1</span>, <span class="number">0</span>,<span class="number">37</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,<span class="number">27</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">35</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line"> [ <span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>,<span class="number">10</span>, <span class="number">3</span>,<span class="number">16</span>, <span class="number">9</span>, <span class="number">5</span>,<span class="number">11</span>],</span><br><span class="line"> [ <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">1</span>,<span class="number">22</span>, <span class="number">6</span>],</span><br><span class="line"> [ <span class="number">0</span>, <span class="number">8</span>, <span class="number">0</span>,<span class="number">17</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line"> [ <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>,<span class="number">27</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line"> [ <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,<span class="number">14</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">2</span>,<span class="number">19</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line">df[<span class="string">&#x27;category&#x27;</span>] = pd.Categorical(df[<span class="string">&#x27;category&#x27;</span>])</span><br><span class="line">y = df.category.cat.codes</span><br><span class="line">df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line">l1 = df[<span class="string">&#x27;category&#x27;</span>].unique()</span><br><span class="line">l2 = y.unique()</span><br><span class="line">d_ = &#123;l2[i]: l1[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(l1))&#125;</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line">od = collections.OrderedDict(<span class="built_in">sorted</span>(d_.items()))</span><br><span class="line">labels = <span class="built_in">list</span>(od.values())</span><br><span class="line"></span><br><span class="line">df_new = pd.DataFrame(a, columns = labels)</span><br><span class="line">sns.heatmap(df_new, cmap=<span class="string">&#x27;YlGnBu&#x27;</span>, square=<span class="literal">True</span>, cbar_kws=&#123;<span class="string">&#x27;label&#x27;</span>: <span class="string">&#x27;Clustering colorbar&#x27;</span>&#125;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="两两比较">两两比较</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">category1 = <span class="string">&#x27;furniture&#x27;</span></span><br><span class="line">category2 = <span class="string">&#x27;fruit&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line">df = df[(df[<span class="string">&#x27;category&#x27;</span>]==category1) | (df[<span class="string">&#x27;category&#x27;</span>]==category2)]</span><br><span class="line">df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">&#x27;category&#x27;</span>] = pd.Categorical(df[<span class="string">&#x27;category&#x27;</span>])</span><br><span class="line">y = df.category.cat.codes</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line">words_embeddings = model.encode(df[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> v_measure_score, homogeneity_score, completeness_score, contingency_matrix</span><br><span class="line"></span><br><span class="line">num_clusters = <span class="built_in">len</span>(df[<span class="string">&#x27;category&#x27;</span>].unique())</span><br><span class="line">cluster = KMeans(n_clusters=num_clusters)</span><br><span class="line">cluster.fit(words_embeddings)</span><br><span class="line">pred = cluster.labels_</span><br><span class="line"></span><br><span class="line">score = v_measure_score(pred, y)</span><br><span class="line">print(<span class="string">&#x27;V_measure scored using k-means clustering: &#x27;</span>, score)</span><br><span class="line"><span class="comment"># V_measure scored using k-means clustering:  0.5942604915790978</span></span><br><span class="line">print(contingency_matrix(pred, y))</span><br><span class="line"><span class="comment">#[[39  0]</span></span><br><span class="line"><span class="comment"># [12 60]]</span></span><br><span class="line"></span><br><span class="line">print(homogeneity_score(pred, y))</span><br><span class="line">print(completeness_score(pred, y))</span><br><span class="line">print(v_measure_score(pred, y))</span><br><span class="line"><span class="number">0.6133169155211203</span></span><br><span class="line"><span class="number">0.5763525894699734</span></span><br><span class="line"><span class="number">0.5942604915790978</span></span><br></pre></td></tr></table></figure>
<h1 id="用knn聚类">用KNN聚类</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line">df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line">y = pd.Categorical(df[<span class="string">&#x27;category&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line">words_embeddings = model.encode(df[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">k = <span class="built_in">int</span>(<span class="built_in">round</span>(np.sqrt(df.shape[<span class="number">0</span>])))</span><br><span class="line">neigh = KNeighborsClassifier(n_neighbors = k)</span><br><span class="line">print(k) <span class="comment"># k = 24 这时结果还是没有k=10的时候好</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">neigh.fit(words_embeddings, y)</span><br><span class="line">pred = neigh.predict(words_embeddings)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(pred, y))</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr class="header">
<th>category</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.81</td>
<td>0.63</td>
<td>0.71</td>
<td>70</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.85</td>
<td>0.66</td>
<td>0.75</td>
<td>71</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.78</td>
<td>0.67</td>
<td>0.72</td>
<td>60</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.82</td>
<td>0.78</td>
<td>0.80</td>
<td>63</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.56</td>
<td>0.63</td>
<td>0.59</td>
<td>52</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.83</td>
<td>0.51</td>
<td>0.63</td>
<td>99</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.22</td>
<td>0.54</td>
<td>0.31</td>
<td>24</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.52</td>
<td>0.72</td>
<td>0.60</td>
<td>40</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.60</td>
<td>0.67</td>
<td>0.63</td>
<td>45</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.47</td>
<td>0.68</td>
<td>0.55</td>
<td>41</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>accuracy</td>
<td></td>
<td></td>
<td>0.64</td>
<td>565</td>
</tr>
<tr class="odd">
<td>macro</td>
<td>avg</td>
<td>0.65</td>
<td>0.65</td>
<td>0.63</td>
</tr>
<tr class="even">
<td>avg</td>
<td>0.71</td>
<td>0.64</td>
<td>0.66</td>
<td>565</td>
</tr>
</tbody>
</table>
<h2 id="比较两个">比较两个</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">category1 = <span class="string">&#x27;furniture&#x27;</span></span><br><span class="line">category2 = <span class="string">&#x27;fruit&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line">df = df[(df[<span class="string">&#x27;category&#x27;</span>]==category1) | (df[<span class="string">&#x27;category&#x27;</span>]==category2)]</span><br><span class="line">df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line">y = pd.Categorical(df[<span class="string">&#x27;category&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line">words_embeddings = model.encode(df[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">k = <span class="built_in">int</span>(<span class="built_in">round</span>(np.sqrt(df.shape[<span class="number">0</span>])))</span><br><span class="line">neigh = KNeighborsClassifier(n_neighbors = k)</span><br><span class="line">print(k) <span class="comment"># k=11</span></span><br><span class="line"></span><br><span class="line">neigh.fit(words_embeddings, y)</span><br><span class="line">pred = neigh.predict(words_embeddings)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(pred, y))</span><br></pre></td></tr></table></figure>
<h1 id="实验三代码">实验三代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>]==<span class="string">&#x27;fruit&#x27;</span>]</span><br><span class="line">df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line">tsne = TSNE(n_components=<span class="number">2</span>, init=<span class="string">&#x27;pca&#x27;</span>, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">words_embeddings = model.encode(df[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line">embedd = tsne.fit_transform(words_embeddings)</span><br><span class="line">print(embedd.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">10</span>))</span><br><span class="line">plt.scatter(embedd[:,<span class="number">0</span>], embedd[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df)):</span><br><span class="line">    x = embedd[i][<span class="number">0</span>]</span><br><span class="line">    y = embedd[i][<span class="number">1</span>]</span><br><span class="line">    plt.text(x, y, df.iloc[i,-<span class="number">2</span>])</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x_point = np.mean(embedd, axis=<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">y_point = np.mean(embedd, axis=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">plt.scatter(x_point, y_point, color=<span class="string">&#x27;#ff0000&#x27;</span>, zorder=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="自动存图">自动存图</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">r&#x27;D:\python\categories.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;category&#x27;</span>] != <span class="string">&#x27;verbs&#x27;</span>][[<span class="string">&#x27;word&#x27;</span>, <span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_category_centroid</span>(<span class="params">df, category</span>):</span></span><br><span class="line"> <span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"> tsne = TSNE(n_components=<span class="number">2</span>, init=<span class="string">&#x27;pca&#x27;</span>, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"> df = df[df[<span class="string">&#x27;category&#x27;</span>]==category]</span><br><span class="line"> df.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"> words_embeddings = model.encode(df[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line"> embedd = tsne.fit_transform(words_embeddings)</span><br><span class="line"> print(embedd.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment">#可视化</span></span><br><span class="line"> plt.figure(figsize=(<span class="number">14</span>,<span class="number">10</span>))</span><br><span class="line"> plt.scatter(embedd[:,<span class="number">0</span>], embedd[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df)):</span><br><span class="line">     x = embedd[i][<span class="number">0</span>]</span><br><span class="line">     y = embedd[i][<span class="number">1</span>]</span><br><span class="line">     plt.text(x, y, df.iloc[i,-<span class="number">2</span>])</span><br><span class="line"> <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> x_point = np.mean(embedd, axis=<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"> y_point = np.mean(embedd, axis=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"> plt.scatter(x_point, y_point, color=<span class="string">&#x27;#ff0000&#x27;</span>, zorder=<span class="number">1</span>)</span><br><span class="line"> plt.title(<span class="string">&#x27;2-D projection of BERT embeddings to identify the centroid of the category &quot;&#123;&#125;&quot;&#x27;</span>.<span class="built_in">format</span>(category), fontsize=<span class="number">14</span>)</span><br><span class="line"> plt.savefig(<span class="string">r&#x27;D:\python\&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(category), dpi=<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> category <span class="keyword">in</span> df[<span class="string">&#x27;category&#x27;</span>].unique():</span><br><span class="line"> plot_category_centroid(df, category)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>词向量</category>
      </categories>
  </entry>
  <entry>
    <title>提取句子中的每个子树</title>
    <url>/constituency/</url>
    <content><![CDATA[<h2 id="画句法树并保存">画句法树并保存</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line">svg = displacy.render(doc, style=<span class="string">&quot;dep&quot;</span>)</span><br><span class="line"></span><br><span class="line">output_path = Path(<span class="string">&quot;dependency_plot.svg&quot;</span>) <span class="comment"># you can keep there only &quot;dependency_plot.svg&quot; if you want to save it in the same folder where you run the script</span></span><br><span class="line">output_path.<span class="built_in">open</span>(<span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>).write(svg)</span><br></pre></td></tr></table></figure>
<h1 id="提取名词短语constituency">提取名词短语constituency</h1>
<p>我们找到句中作为主语、宾语、介宾、be动词补语、连词的名词，然后提取该名词的subtree也就是名词短语。条件是，名词短语内不出现标点，名词短语重合出现时取较短的短语。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="comment"># from io import StringIO</span></span><br><span class="line"><span class="comment"># import re</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./corpus/abstracts.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    corpus_list = f.read().splitlines()</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> corpus <span class="keyword">in</span> corpus_list[:<span class="number">5</span>]:</span><br><span class="line">    print(corpus)</span><br><span class="line">    doc = nlp(corpus)</span><br><span class="line">    keywords = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">        <span class="keyword">if</span> token.pos_ == <span class="string">&#x27;NOUN&#x27;</span>:</span><br><span class="line">            <span class="keyword">if</span> token.dep_ <span class="keyword">in</span> [<span class="string">&#x27;dobj&#x27;</span>, <span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;pobj&#x27;</span>, <span class="string">&#x27;conj&#x27;</span>, <span class="string">&#x27;attr&#x27;</span>, <span class="string">&#x27;pcomp&#x27;</span>]:</span><br><span class="line">                subtree_list_new = []</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">list</span>(token.subtree)) &lt; <span class="number">10</span>:</span><br><span class="line">                    <span class="keyword">for</span> t <span class="keyword">in</span> token.subtree:</span><br><span class="line">                        <span class="keyword">if</span> t.is_punct <span class="keyword">and</span> t.text != <span class="string">&quot;-&quot;</span>:</span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line">                        subtree_list_new.append(t)</span><br><span class="line">                    subtree_list_new = <span class="string">&#x27; &#x27;</span>.join(t.text <span class="keyword">for</span> t <span class="keyword">in</span> subtree_list_new).replace(<span class="string">r&#x27; - &#x27;</span>, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">                    <span class="comment"># print(token.head.head, &#x27; + &#x27;, &#x27; &#x27;.join(t.text for t in token.head.subtree), &#x27; + &#x27;, token.head, &#x27; | &#x27;, token.head.tag_, &#x27; | &#x27;, token.head.dep_, &#x27; + &#x27;,token.dep_)</span></span><br><span class="line">                    <span class="keyword">if</span> keywords:</span><br><span class="line">                        <span class="keyword">if</span> subtree_list_new <span class="keyword">in</span> keywords[-<span class="number">1</span>]:</span><br><span class="line">                            keywords.pop(-<span class="number">1</span>)</span><br><span class="line">                            keywords.append(subtree_list_new)</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            keywords.append(subtree_list_new)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        keywords.append(subtree_list_new)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;, &#x27;</span>.join(keywords)+<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>输入结果：</p>
<blockquote>
<p>Sarcasm is a linguistic expression often used to communicate the opposite of what is said, usually something that is very unpleasant with an intention to insult or ridicule. Inherent ambiguity in sarcastic expressions makes sarcasm detection very difficult. In this work, we focus on detecting sarcasm in textual conversations, written in English, from various social networking platforms and online media. To this end, we develop an interpretable deep learning model using multi-head self-attention and gated recurrent units. We show the effectiveness and interpretability of our approach by achieving state-of-the-art results on datasets from social networking platforms, online discussion forums, and political dialogues.</p>
<p><strong>ridicule, sarcastic expressions, sarcasm detection, this work, sarcasm, online media, this end, interpretability, our approach, the-art, social networking platforms, online discussion forums, political dialogues</strong></p>
<p>While emotions are universal aspects of human psychology, they are expressed differently across different languages and cultures. We introduce a new data set of over 530k anonymized public Facebook posts across 18 languages, labeled with five different emotions. Using multilingual BERT embeddings, we show that emotions can be reliably inferred both within and across languages. Zero-shot learning produces promising results for low-resource languages. Following established theories of basic emotions, we provide a detailed analysis of the possibilities and limits of cross-lingual emotion classification. We find that structural and typological similarity between languages facilitates cross-lingual learning, as well as linguistic diversity of training data. Our results suggest that there are commonalities underlying the expression of emotion in different languages. We publicly release the anonymized data for future research.</p>
<p><strong>emotions, human psychology, cultures, a new data set of over 530k, public Facebook posts, 18 languages, five different emotions, multilingual BERT embeddings, languages, Zero-shot learning, low-resource languages, basic emotions, limits, cross-lingual emotion classification, languages, training data, Our results, emotion, different languages, future research</strong></p>
<p>Customers of machine learning systems demand accountability from the companies employing these algorithms for various prediction tasks. Accountability requires understanding of system limit and condition of erroneous predictions, as customers are often interested in understanding the incorrect predictions, and model developers are absorbed in finding methods that can be used to get incremental improvements to an existing system. Therefore, we propose an accountable error characterization method, AEC, to understand when and where errors occur within the existing black-box models. AEC, as constructed with human-understandable linguistic features, allows the model developers to automatically identify the main sources of errors for a given classification system. It can also be used to sample for the set of most informative input points for a next round of training. We perform error detection for a sentiment analysis task using AEC as a case study. Our results on the sample sentiment task show that AEC is able to characterize erroneous predictions into human understandable categories and also achieves promising results on selecting erroneous samples when compared with the uncertainty-based sampling. <strong>machine learning systems, these algorithms, various prediction tasks, Accountability, erroneous predictions, customers, the incorrect predictions, an existing system, an accountable error characterization method, errors, the existing black-box models, human-understandable linguistic features, the model developers, errors, a given classification system, most informative input points, training, error detection, a case study, the sample sentiment task, erroneous predictions, human understandable categories, erroneous samples, the uncertainty-based sampling</strong></p>
<p>Understanding and executing natural language instructions in a grounded domain is one of the hallmarks of artificial intelligence. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two top-performing systems for the task. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the natural language instructions relative to it, or whether they merely over-fit spurious signals in the dataset. We formulate a set of expectations one might have from an instruction following model and concretely characterize the different dimensions of robustness such a model should possess. Despite decent test performance, we find that state-of-the-art models fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves data augmentation and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better. <strong>natural language instructions, a grounded domain, artificial intelligence, this paper, instruction understanding, the blocks world domain, two top-performing systems, the task, these models, the spatial domain, the natural language instructions relative to it, the dataset, expectations, model, robustness, such a model, decent test performance, the-art, state-of-the-art models, these expectations, data augmentation, extensive experiments, our expectations</strong></p>
<p>State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose <u>FullyQT</u>: an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to full-precision, our 8-bit models score greater or equal BLEU on most tasks. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results. <strong>the-art, parameters, such methods, performance, this point, this end, an all-inclusive quantization strategy for the Transformer, our knowledge, translation quality, full-precision, our 8-bit models, most tasks, all previously proposed methods, the-art, state-of-the-art quantization results</strong></p>
</blockquote>
<p>现在的问题是怎么自动识别patterns，这个pattern应该怎么表示？</p>
<h1 id="方法一">方法一</h1>
<p>每个<strong>语义模式</strong>都是<span class="math inline">\(T→d\)</span>的形式，其中<span class="math inline">\(T\)</span>是一个触发词（如'使用'、'展示'），<span class="math inline">\(d\)</span>是一个依存关系（如'直接-宾语'）</p>
<p><img src="https://i.loli.net/2021/07/17/BrqQVJfDI8yaU5K.png" width="400"/></p>
<p>因此每个模式由两部分组成：</p>
<p>触发词（要是名词或动词）+子树（宾语、主语、补语）</p>
<p>然后我们把这些模式放进svm分类器中学习，但是这样就是有监督，为了半监督我们还是bootstraping。</p>
<p>遇到and conjunction我们直接拆分，例如：</p>
<p>multi-head self-attention and gated recurrent units拆成multi-head self-attention 和gated recurrent units，因为前面是using所以是方法。</p>
<p>我们第一步先只抽方法：</p>
<ul>
<li><p>有一堆种子之后，我们把这些种子的名词中心词的head和它们所处的dep_找到，作为新的pattern再继续抽。</p></li>
<li><p>我们把每个&lt;模式, 关键短语, 关键短语的pos&gt;保存为三元组，作为候选词。</p></li>
<li><p>我们按照这个模式抽取新的种子，抽取的方式是找到head的subtree并且是名词，把得到的名词短语再次加入；</p></li>
<li><p>循环迭代；</p></li>
</ul>
<h1 id="方法二">方法二</h1>
<p>先把所有候选的名词短语都抽取出来，然后用bootstrap方法得到关键短语。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 摘要列表</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;../corpus/abstracts.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    corpus_list = f.read().splitlines()</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_candidates</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="comment">#对于每个摘要，把所有keywords的pattern保存在列表</span></span><br><span class="line">    doc = nlp(corpus_list[i])</span><br><span class="line">    keywords = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">        <span class="comment"># 找到名词短语的head</span></span><br><span class="line">        <span class="keyword">if</span> token.pos_ == <span class="string">&#x27;NOUN&#x27;</span> <span class="keyword">and</span> token.dep_ <span class="keyword">in</span> [<span class="string">&#x27;dobj&#x27;</span>, <span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;pobj&#x27;</span>, <span class="string">&#x27;conj&#x27;</span>, <span class="string">&#x27;attr&#x27;</span>, <span class="string">&#x27;pcomp&#x27;</span>]:</span><br><span class="line">            subtree_list = []</span><br><span class="line">            <span class="comment"># 找到名词短语head的子树</span></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> token.subtree:</span><br><span class="line">                <span class="comment"># 名词短语head的子树中不能存在&quot;,&quot;</span></span><br><span class="line">                <span class="keyword">if</span> t.is_punct <span class="keyword">and</span> t.text != <span class="string">&quot;-&quot;</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                subtree_list.append(t)</span><br><span class="line"></span><br><span class="line">            pattern = [i, <span class="built_in">str</span>(token.head.head), token.head.head.lemma_, token.head.head.tag_, <span class="built_in">str</span>(token.head), token.head.tag_, token.head.dep_, token.dep_]</span><br><span class="line">            subtree_list_new = <span class="string">&#x27; &#x27;</span>.join(t.text <span class="keyword">for</span> t <span class="keyword">in</span> subtree_list).replace(<span class="string">r&#x27; - &#x27;</span>, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;and&quot;</span> <span class="keyword">in</span> subtree_list_new:</span><br><span class="line">                keyword_1 = (subtree_list_new.split(<span class="string">&quot;and&quot;</span>)[<span class="number">0</span>]).strip()</span><br><span class="line">                keyword_2 = (subtree_list_new.split(<span class="string">&quot;and&quot;</span>)[<span class="number">1</span>]).strip()</span><br><span class="line"></span><br><span class="line">                pattern_new = pattern.copy()</span><br><span class="line"></span><br><span class="line">                pattern.append(keyword_1)</span><br><span class="line">                keywords.append(pattern)</span><br><span class="line">                pattern_new.append(keyword_2)</span><br><span class="line">                keywords.append(pattern_new)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> <span class="string">&quot;PRON&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> [t.pos_ <span class="keyword">for</span> t <span class="keyword">in</span> subtree_list] <span class="keyword">and</span> <span class="string">&quot;this&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> subtree_list_new:</span><br><span class="line">                pattern.append(subtree_list_new)</span><br><span class="line">                keywords.append(pattern)</span><br><span class="line">    <span class="keyword">return</span> keywords</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    keywords_list = []</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">        <span class="keyword">for</span> keywords <span class="keyword">in</span> executor.<span class="built_in">map</span>(extract_candidates, <span class="built_in">range</span>(<span class="built_in">len</span>(corpus_list))):</span><br><span class="line">            keywords_list.extend(keywords)</span><br><span class="line">    print(keywords_list)</span><br><span class="line">    df = pd.DataFrame(keywords_list)</span><br><span class="line">    df.columns = [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;head_head&#x27;</span>, <span class="string">&#x27;head_head_lemma&#x27;</span>, <span class="string">&#x27;head_head_tag&#x27;</span>, <span class="string">&#x27;head&#x27;</span>, <span class="string">&#x27;head_tag&#x27;</span>, <span class="string">&#x27;head_dep&#x27;</span>, <span class="string">&#x27;dep&#x27;</span>, <span class="string">&#x27;candidates&#x27;</span>]</span><br><span class="line">    df.drop_duplicates(inplace=<span class="literal">True</span>, subset=[<span class="string">&quot;candidates&quot;</span>])</span><br><span class="line">    df.to_csv(<span class="string">&#x27;../result/candidates.csv&#x27;</span>)</span><br><span class="line">    t2 = time.perf_counter()</span><br><span class="line">    print(<span class="string">f&#x27;Finished in <span class="subst">&#123;t2-t1&#125;</span> seconds&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>然后根据seed找到pattern:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;../result/candidates.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处仅仅是方法的种子</span></span><br><span class="line">seed_list = [<span class="string">&quot;multi-head self-attention&quot;</span>, <span class="string">&quot;gated recurrent units&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果种子出现在文章中，就把种子所在的pattern提取出来</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_pattern</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        item_json = &#123;&#125;</span><br><span class="line">        f = <span class="built_in">open</span>(<span class="string">&#x27;seeds.txt&#x27;</span>,<span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">        seed_list = pickle.load(f)</span><br><span class="line">        f.close()</span><br><span class="line">        tokenized_candidates = re.split(<span class="string">r&quot;[^0-9a-z]&quot;</span>, <span class="built_in">str</span>(df[<span class="string">&#x27;candidates&#x27;</span>][i].lower()))</span><br><span class="line">        <span class="keyword">for</span> seed <span class="keyword">in</span> seed_list:</span><br><span class="line">            same = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> re.split(<span class="string">r&quot;[^0-9a-z]&quot;</span>, seed.lower()):</span><br><span class="line">                <span class="keyword">if</span> token <span class="keyword">in</span> tokenized_candidates:</span><br><span class="line">                    same += <span class="number">1</span></span><br><span class="line">            different = <span class="built_in">len</span>(tokenized_candidates) - same</span><br><span class="line">            <span class="keyword">if</span> same &gt; <span class="number">1</span> <span class="keyword">and</span> different &lt; <span class="number">3</span>:</span><br><span class="line">                item_json[<span class="string">&quot;paper_id&quot;</span>] = i</span><br><span class="line">                item_json[<span class="string">&quot;keyword&quot;</span>] =  df[<span class="string">&#x27;candidates&#x27;</span>][i]</span><br><span class="line">                item_json[<span class="string">&quot;relationship&quot;</span>] = <span class="string">&quot;method&quot;</span></span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    index_list = []</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> executor.<span class="built_in">map</span>(find_pattern, <span class="built_in">range</span>(df.shape[<span class="number">0</span>])):</span><br><span class="line">            index_list.append(i)</span><br><span class="line">    index_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> index_list <span class="keyword">if</span> i]</span><br><span class="line">    df.loc[index_list].to_csv(<span class="string">&#x27;../result/patterns.csv&#x27;</span>)</span><br><span class="line">    t2 = time.perf_counter()</span><br><span class="line">    print(<span class="string">f&#x27;Finished in <span class="subst">&#123;t2-t1&#125;</span> seconds&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>为了构建知识图谱，我们把关键短语和所在的文档分别保存在json文件中：</p>
<p>首先是keyword.json：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;gated recurrent units&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;method&quot;</span></span><br><span class="line">	&#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;multi-head self-attention&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;method&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;sarcasm detection&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;task&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;social networking platforms&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;mateiral&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;multilingual BERT embeddings&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;method&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;public Facebook posts&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;material&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;Zero-shot learning&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;material&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;low-resource languages&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;material&quot;</span>       </span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;cross-lingual emotion classification&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;task&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;reinforcement learning&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;method&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">11</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;distantly supervised relation extraction&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;task&quot;</span>        </span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">13</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;grounded embeddings&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;method&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">13</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;ungrounded embeddings&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;method&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">15</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;argument mining&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;task&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">15</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;a human-generated dataset&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;material&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">16</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;CoNLL-14 benchmark datasets&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;material&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;	</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">16</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;adversarial learning&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;method&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">18</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;visual question answering&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;task&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">18</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;supervised domain adaptation&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;task&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">18</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;pre-trained source model&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;task&quot;</span>     </span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;paper_id&quot;</span>: <span class="number">18</span>,</span><br><span class="line">        <span class="attr">&quot;keyword&quot;</span>: <span class="string">&quot;the benchmark VQA 2.0&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;relationship&quot;</span>: <span class="string">&quot;material&quot;</span>       </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>然后是paper.json：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">	&#123;</span><br><span class="line">        <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;英格兰&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;abstract&quot;</span>: <span class="string">&quot;麻瓜&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;url&quot;</span>: <span class="string">&quot;已婚&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;authors&quot;</span>: <span class="string">&quot;人类&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;year&quot;</span>: <span class="string">&quot;男&quot;</span></span><br><span class="line"> 	&#125;,</span><br><span class="line"> 	&#123;</span><br><span class="line">     	<span class="attr">&quot;title&quot;</span>: <span class="string">&quot;英格兰&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;abstract&quot;</span>: <span class="string">&quot;麻瓜&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;url&quot;</span>: <span class="string">&quot;已婚&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;authors&quot;</span>: <span class="string">&quot;人类&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;year&quot;</span>: <span class="string">&quot;男&quot;</span></span><br><span class="line"> 	&#125;</span><br><span class="line">]</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p>https://shyambhu20.blogspot.com/2020/09/dependency-parsing-using-spacy-spacy.html</p>
]]></content>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（中）</title>
    <url>/information-retrieval-2/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<h1 id="关系抽取算法">17.2 关系抽取算法</h1>
<p>关系抽取的算法主要有五类：<strong>手写模式</strong>、<strong>监督机器学习</strong>、<strong>半监督</strong>（通过<strong>bootstrapping</strong>和通过<strong>远程监督</strong>）以及<strong>无监督</strong>。我们将在接下来的章节中分别介绍这些算法。</p>
<h2 id="使用模式抽取关系">17.2.1 使用模式抽取关系</h2>
<p>最早并且现在依然常用的关系抽取算法是词法-句法模式（ <strong>lexico-syntactic pattern</strong>），由Hearst（1992a）第一个开发，因此通常被称为<u>Hearst patterns</u>。例如以下句子：</p>
<blockquote>
<p>Agar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use.</p>
<p>琼脂是一种由红藻（如Gelidium）混合制备的物质，用于实验室或工业用途。</p>
</blockquote>
<p>赫斯特指出，大多数人类读者不会知道Gelidium是什么，但他们可以很容易推断出它是一种红藻（的<u>下义词</u><em>hyponym</em>）。她认为以下词法-句法模式：</p>
<p><span class="math display">\[
N P_{0} \text { such as } N P_{1}\left\{, N P_{2} \ldots,(\text { and } \mid \text { or }) N P_{i}\right\}, i \geq 1
\]</span></p>
<p>意味着以下语义 <span class="math display">\[
\forall N P_{i}, i \geq 1, \text { hyponym }\left(N P_{i}, N P_{0}\right)
\]</span> 让我们可以推断出 <span class="math display">\[
hyponym(Gelidium,red algae)
\]</span></p>
<table>
<caption>Figure 17.5 Hand-built lexico-syntactic patterns for finding hypernyms, using {} to mark optionality (Hearst 1992a, Hearst 1998).</caption>
<colgroup>
<col style="width: 40%" />
<col style="width: 59%" />
</colgroup>
<thead>
<tr class="header">
<th>NP {, NP}* {,} (and|or) other NP<sub>H</sub></th>
<th>temples, treasuries, and other important <strong>civic buildings</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NP<sub>H</sub> such as {NP,}* {(or|and)} NP</td>
<td><strong>red algae</strong> such as Gelidium</td>
</tr>
<tr class="even">
<td>such NPH as {NP,}* {(or|and)} NP</td>
<td>such <strong>authors</strong> as Herrick, Goldsmith, and Shakespeare</td>
</tr>
<tr class="odd">
<td>NP<sub>H</sub> {,} including {NP,}* {(or|and)} NP</td>
<td><strong>common-law countries</strong>, including Canada and England</td>
</tr>
<tr class="even">
<td>NP<sub>H</sub> {,} especially {NP}* {(or|and)} NP</td>
<td><strong>European countries</strong>, especially France, England, and Spain</td>
</tr>
</tbody>
</table>
<span id="more"></span>
<p>图17.5显示了Hearst(1992a, 1998)提出的五种模式，用于推断上下位关系；我们用NP<sub>H</sub>表示parent/hyponym。现代版本的基于模式的方法通过增加命名实体约束来扩展它。例如，如果我们的目标是回答关于“谁在哪个组织中担任什么职务”的问题，我们可以使用如下模式。</p>
<p><img src="https://i.loli.net/2021/03/18/iyZINETdVrXG3cn.png" width="500"/></p>
<p>手工构建的模式具有高精度的优势，并且可以针对特定领域进行定制。但另一方面，它们通常是低召回率的，而且如果要创建所有可能的模式，工作量很大。</p>
<h2 id="通过监督学习进行关系抽取">17.2.2 通过监督学习进行关系抽取</h2>
<p>监督机器学习的关系抽取方法遵循的是一种大家现在应该很熟悉的方案。选择一组固定的关系和实体，用这些关系和实体手动标注训练语料，然后用标注后的文本来训练分类器标注一个未出现过的测试集。</p>
<p>最直接的方法，如图17.6所示：(1) 找到命名的实体对(通常在同一个句子中)；(2) 为每对实体进行关系分类。分类器可以使用任何有监督的技术（逻辑回归、RNN、Transformer、随机森林等）。</p>
<p>一个可选的中间过滤分类器可以用来加快处理速度，通过二元决策判定给定的一对实体是否相关（通过任何关系）。它的训练对象是直接从标注语料中的所有关系中抽取的正例，以及从未标注关系的句内实体对中生成的负例。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">FINDRELATIONS</span><span class="params">(words)</span> returns relations</span></span><br><span class="line">	relations←nil</span><br><span class="line">	entities←FINDENTITIES(words)</span><br><span class="line">	forall entity pairs &lt;e1, e2&gt; in entities <span class="keyword">do</span></span><br><span class="line">	<span class="keyword">if</span> RELATED?(e1, e2)</span><br><span class="line">		relations←relations+CLASSIFYRELATION(e1, e2)</span><br></pre></td></tr></table></figure>
<p>基于特征的监督关系分类器。让我们考虑基于特征的分类器（如逻辑回归或随机森林）的样本特征，对这个句子中的<em>美国航空公司</em>（Mention 1，或M1）和<em>Tim Wagner</em>（Mention 2，M2）之间的关系进行分类。</p>
<blockquote>
<p>(17.5) <strong>American Airlines</strong>, a unit of AMR, immediately matched the move, spokesman <strong>Tim Wagner</strong> said</p>
</blockquote>
<p>这些包括词的特征（如词嵌入、one-hot、无论是否抽取词干<em>stemming</em>）：</p>
<blockquote>
<ul>
<li><p>M1和M2的核心词（headwords）及其连接 （concatenation）</p>
<p><strong>Airlines Wagner Airlines-Wagner</strong></p></li>
<li><p>M1和M2的词袋和ngram</p>
<p><strong>American, Airlines, Tim, Wagner, American Airlines, Tim Wagner</strong></p></li>
<li><p>特定位置的词和ngram</p>
<p>M2: <strong>-1 spokesman</strong></p>
<p>M2: <strong>+1 said</strong></p></li>
<li><p>M1和M2之间的词袋或ngram</p>
<p><strong>a, AMR, of, immediately, matched, move, spokesman, the, unit</strong></p></li>
</ul>
</blockquote>
<p>以及<strong>命名实体</strong>特征：</p>
<blockquote>
<ul>
<li><p>命名实体类型及其连接（concatenation）</p>
<p>M1: <strong>ORG</strong>, M2: <strong>PER</strong>, M1M2: <strong>ORG-PER</strong></p></li>
<li><p>M1和M2的实体级别（从集合NAME、NOMINAL、PRONOUN中选择）。</p>
<p>M1: <strong>NAME</strong> [it or he would be <strong>PRONOUN</strong>]</p>
<p>M2: <strong>NAME</strong> [the company would be <strong>NOMINAL</strong>]</p></li>
<li><p>参数之间的实体数量（在本例中，对于AMR）。</p></li>
</ul>
</blockquote>
<p><strong>句法结构</strong>是一个有用的信号，通常表示为依存关系或穿越实体之间的树的成分<strong>句法路径</strong>。</p>
<blockquote>
<ul>
<li><p>M1和M2之间的成分路径</p>
<p><strong>NP ↑ NP ↑ S ↑ S ↓ NP</strong></p></li>
<li><p>依存树路径</p>
<p><strong>Airlines ←<sub>subj</sub> matched ←<sub>comp</sub>said →<sub>subj</sub> Wagner</strong></p></li>
</ul>
</blockquote>
<p><strong>神经监督关系分类器</strong> 关系抽取的神经模型同样将这个任务视为监督分类。让我们考虑一个应用于TACRED关系抽取数据集和任务的典型系统 (Zhang et al., 2017) 。在TACRED中，提供一个句子和其中的两个spans：主语，即人或组织；宾语，即任何其他实体。任务是从42个TAC关系中分配一个关系（或者没有关系）。</p>
<p>一个典型的Transformer-encoder算法，如图17.7所示，简单的说就是把一个像BERT这样的预训练编码器，在句子表示的基础上增加一个线性层（例如BERT [CLS] token），这个线性层被微调为1-of-N分类器，以分配43个标注中的一个。BERT编码器的输入是部分去词汇化的；主语和宾语实体在输入中被其NER标注所取代。这有助于保持系统不会对单个词项过度拟合 (Zhang et al., 2017) 。当使用BERT类型的Transformers进行关系抽取时，使用类似RoBERTa (Liu et al., 2019) 或SPANbert (Joshi et al., 2020) 这样的BERT版本会很有帮助，这些版本没有用[SEP]标记分隔的两个序列，而是将单个长序列的句子形成输入。</p>
<p><img src="https://i.loli.net/2021/03/18/mpbfZs9QJoPnv7a.png" width="700"/></p>
<p>一般来说，如果测试集与训练集足够相似，并且有足够多的手工标注数据，监督关系抽取系统可以获得很高的准确率。但对一个庞大的训练集进行标注是非常昂贵的，而且监督模型也很脆弱：它们不能很好地泛化到不同的文本类型。由于这个原因，关系抽取的很多研究都集中在我们接下来要讲的半监督和无监督方法上。</p>
<h2 id="通过-bootstrapping-进行半监督关系抽取">17.2.3 通过 Bootstrapping 进行半监督关系抽取</h2>
<p>监督机器学习假设我们有大量的标注数据。不幸的是，这很昂贵。但假设我们只有一些高精度的<u>种子模式</u>（seed patterns），就像第17.2.1节中的那些，或者可能有一些<u>种子元组</u>（seed tuples）。这已经足够boostrap一个分类器了！<u>Bootstrapping</u>的过程是获取种子对中的实体，然后找到包含这两个实体的句子（通过网络或者我们使用的任何数据集）。从所有这些句子中，我们抽取并归纳实体周围的上下文，以学习新的模式。图17.8给出了一个基本算法。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">BOOTSTRAP</span><span class="params">(Relation R)</span> returns <span class="keyword">new</span> relation tuples</span></span><br><span class="line">	tuples←Gather a set of seed tuples that have relation R</span><br><span class="line">	iterate</span><br><span class="line">		sentences←find sentences that contain entities in tuples</span><br><span class="line">		patterns←generalize the context between <span class="keyword">and</span> around entities in sentences</span><br><span class="line">		newpairs←use patterns to grep <span class="keyword">for</span> more tuples</span><br><span class="line">		newpairs←newpairs with high confidence</span><br><span class="line">		tuples←tuples + newpairs</span><br><span class="line">	<span class="keyword">return</span> tuples</span><br></pre></td></tr></table></figure>
<p>假设我们需要创建一个航空公司/枢纽对的列表，我们只知道Ryanair在Charleroi有一个枢纽。我们可以使用这个种子事实来发现新的模式，在我们的语料库中找到这个关系的其他提及。我们搜索Ryanair、Charleroi和hub在某种程度上接近的术语。也许我们会发现以下一组句子。</p>
<blockquote>
<p>(17.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all weekend flights out of the airport.</p>
<p>以沙勒罗瓦为枢纽的低价航空公司Ryanair取消了所有周末从机场起飞的航班。</p>
<p>(17.7) All flights in and out of Ryanair’s hub at Charleroi airport were grounded on Friday...</p>
<p>所有进出沙勒罗伊机场枢纽的航班都在周五停飞。</p>
<p>(17.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000 passengers had already been affected.</p>
<p>瑞安航空的核心枢纽查勒罗伊的一位发言人估计，已经有8000名乘客受到影响。</p>
</blockquote>
<p>从这些结果中，我们可以利用实体提及之间的上下文单词、提及一之前的词、提及二之后的词，以及两个提及的命名实体类型，还有其他可能的特征，来抽取如下的通用模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F; [ORG], which uses [LOC] as a hub &#x2F; </span><br><span class="line">&#x2F; [ORG]’s hub at [LOC] &#x2F; </span><br><span class="line">&#x2F; [LOC], a main hub for [ORG] &#x2F;</span><br></pre></td></tr></table></figure>
<p>这些新的模式可以继而用来搜索更多的元组。Bootstrapping系统还为新的元组分配了<u>置信度值</u>（confidence values），以避免<u>语义漂移</u>（semantic drift）。在语义漂移中，一个错误的模式会导致错误元组的引入，而这些元组又会导致不正确的模式的建立，使得抽取关系的意义发生漂移。</p>
<p>比如下面的例子。</p>
<blockquote>
<p>(17.9) Sydney has a ferry hub at Circular Quay.</p>
<p>(17.9) 悉尼在环形码头有一个轮渡枢纽。</p>
</blockquote>
<p>如果被接受为正例，这个表达式可能导致元组<span class="math inline">\(&lt;Sydney,CircularQuay&gt;\)</span>的错误引入。基于这个元组的模式可能会将更多的错误传播到数据库中。</p>
<p>模式的置信度值基于两个因素的平衡：模式相对于当前元组集的表现，以及模式就在文档集中产生的匹配数量而言的生产率。更正式的表达是，给定一个文档集 <span class="math inline">\(D\)</span>，一个当前的元组集 <span class="math inline">\(T\)</span>，以及一个引入的模式 <span class="math inline">\(p\)</span>，我们需要跟踪两个因素：</p>
<ul>
<li><p><span class="math inline">\(hits(p)\)</span>：在<span class="math inline">\(D\)</span>中查找时，<span class="math inline">\(p\)</span>所匹配的<span class="math inline">\(T\)</span>中的元组集</p></li>
<li><p><span class="math inline">\(finds(p)\)</span>：<span class="math inline">\(p\)</span>在<span class="math inline">\(D\)</span>中找到的元组的总集</p></li>
</ul>
<p>下面的公式平衡了这些考虑因素 (Riloff and Jones, 1999)： <span class="math display">\[
\operatorname{Conf}_{R \log F}(p)=\frac{|\operatorname{hits}(p)|}{|\operatorname{finds}(p)|} \log (|\operatorname{finds}(p)|)
\]</span> 这个度量标准一般经过标准化处理，产生一个概率。我们可以通过结合<span class="math inline">\(D\)</span>中与该元组相匹配的所有模式<span class="math inline">\(P&#39;\)</span>中支持该元组的证据来评估所提出的新元组的置信度 (Agichtein and Granoisy-or vano, 2000)。结合这种证据的一种方法是<u>noisy-or</u>技术。假设一个给定的元组是由<span class="math inline">\(P\)</span>中的一个模式子集支持的，每个模式都有自己的置信度，正如上面的度量所示。在noisy-or模型中，我们做了两个基本假设。首先，如果一个提出的元组是假的，它的所有支持模式一定是错误的；其次，它们各自失败的来源都是独立的。如果我们不严格地将我们的置信度视为概率，那么任何单个模式<span class="math inline">\(p\)</span>失败的概率为<span class="math inline">\(1-Conf(p)\)</span>；一个元组的所有支持模式出错的概率是它们单个失败概率的乘积，因此我们对一个新元组的置信度有以下公式： <span class="math display">\[
\operatorname{Conf}(t)=1-\prod_{p \in P^{\prime}}(1-\operatorname{Conf}(p))
\]</span> 在bootstrapping过程中，为接受新的模式和元组设置保守的置信度阈值，有助于防止系统偏离目标关系。</p>
<h2 id="远程监督式关系抽取">17.2.4 远程监督式关系抽取</h2>
<p>虽然手工标注文本的关系标注成本很高，但是有一些方法可以找到间接的训练数据来源。<u>远程监督</u>方法 <em>distant supervision</em> (Mintz et al., 2009) 结合了bootstrapping和监督学习的优点。远程监督不是只用少量的种子，而是使用一个大型数据库来获取大量的种子实例，从所有这些例子中创建大量的噪声模式特征（noisy pattern features），然后将它们结合在一个监督分类器中。例如，假设我们要学习人与出生城市之间的出生地关系。在基于种子的方法中，我们一开始可能只有5个例子。但基于维基百科的数据库，如<a href="https://en.wikipedia.org/wiki/DBpedia">DBPedia</a>或<a href="https://en.wikipedia.org/wiki/Freebase_(database)">Freebase</a>，有数万个许多关系的例子；包括超过10万个出生地的例子（<span class="math inline">\(&lt;Edwin Hubble，Marshfield&gt;\)</span>，<span class="math inline">\(&lt;Albert Einstein，Ulm&gt;\)</span>等）。下一步是在大量的文本上运行命名实体标注器-——Mintz et al. (2009) 使用了维基百科上的80万篇文章，并抽取出所有具有与元组相匹配的两个命名实体的句子，比如：</p>
<blockquote>
<p>...Hubble was born in Marshfield...</p>
<p>...Einstein, born (1879), Ulm...</p>
<p>...Hubble’s birthplace in Marshfield...</p>
</blockquote>
<p>现在可以从这些数据中抽取训练实例，每个元组&lt;关系，实体1，实体2&gt;都有一个相同的训练实例。因此，每个如下的元组都会有一个训练实例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;born-in, Edwin Hubble, Marshfield&gt;</span><br><span class="line">&lt;born-in, Albert Einstein, Ulm&gt;</span><br><span class="line">&lt;born-year, Albert Einstein, 1879&gt;</span><br></pre></td></tr></table></figure>
<p>然后我们可以应用基于特征或神经网络的分类。对于基于特征的分类，标准的监督关系抽取特征，比如两个提及的命名实体标注，提及之间的词和依存路径，以及相邻的词。每个元组都会有从许多训练实例中收集到的特征；像<span class="math inline">\(&lt;born-in,Albert Einstein,Ulm&gt;\)</span>这样的单个训练实例的特征向量将含有来自许多提到Einstein和Ulm的不同句子的词法和句法特征）。</p>
<p>因为远程监督有非常大的训练集，所以它也能够使用非常丰富的特征，这些特征是这些单个特征的联合体。所以我们会抽取出成千上万的模式，这些模式会将实体类型与中间的单词或依存路径结合在一起，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PER was born in LOC </span><br><span class="line">PER, born (XXXX), LOC </span><br><span class="line">PER’s birthplace in LOC</span><br></pre></td></tr></table></figure>
<p>回到我们的运行示例，对于这个句子：</p>
<blockquote>
<p>(17.12) American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said</p>
</blockquote>
<p>我们将学习丰富的连词（conjunction）特征，比如：</p>
<blockquote>
<p>M1 = ORG &amp; M2 = PER &amp; nextword=“said”&amp; path= <strong>NP ↑ NP ↑ S ↑ S ↓ NP</strong></p>
</blockquote>
<p>结果是一个监督分类器，它有大量丰富的特征集用于检测关系。由于并不是每个测试句子都会有一个训练关系，所以分类器还需要能够将一个例子标记为无关系。这个标注是通过随机选择在任何Freebase关系中都未出现的实体对，抽取它们的特征，并为每个这样的元组建立一个特征向量来训练的。最终的算法在如图17.9所示。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function DISTANT <span class="title">SUPERVISION</span><span class="params">(Database D, Text T)</span> returns relation classifier C</span></span><br><span class="line"><span class="function">	foreach relation R</span></span><br><span class="line"><span class="function">		foreach <span class="title">tuple</span> <span class="params">(e1,e2)</span> of entities with relation R in D</span></span><br><span class="line">			sentences←Sentences in T that contain e1 and e2</span><br><span class="line">			f←Frequent features in sentences</span><br><span class="line">			observations←observations + <span class="keyword">new</span> training tuple (e1, e2, f, R)</span><br><span class="line">		C←Train supervised classifier on observations</span><br><span class="line">	<span class="keyword">return</span> C</span><br></pre></td></tr></table></figure>
<p>远程监督享有我们所探讨的每一种方法的优势。像监督分类一样，远程监督使用一个具有大量特征的分类器，并通过详细的手工创建的知识进行监督。和基于模式的分类器一样，它可以利用高精度的证据来证明实体之间的关系。事实上，远程监督系统学习到的模式就像早期关系抽取器手工构建的模式。例如Snow et al. (2005)的is-a或hypernym抽取系统使用来自WordNet的hypernym/hyponym NP对作为远程监督，然后从大量的文本中学习新的模式。他们的系统精确地产生了Hearst（1992a）最初的5个模板模式，但也导致了包括这4个模式在内的7万个额外的模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NPH like NP Many hormones like leptin... </span><br><span class="line">NPH called NP ...using a markup language called XHTML </span><br><span class="line">NP is a NPH Ruby is a programming language... </span><br><span class="line">NP, a NPH IBM, a company with a long...</span><br></pre></td></tr></table></figure>
<p>这种同时使用大量特征的能力意味着，与基于种子的系统中模式的迭代扩展不同，没有语义漂移的发生。与无监督分类一样，它不使用有标注的训练语料库，而是依赖于非常大量的无标注数据，因此对训练语料库中的体裁（genre）问题不敏感。远程监督的另一个优势是，它可以创建训练元组，以便与神经网络分类器一起使用，而神经网络分类器不需要特征。远程监督的主要问题是，它往往会产生低精度的结果，因此目前的研究重点是如何提高精度。此外，远程监督只能帮助抽取已经存在的足够大的数据库的关系。如果要在没有数据集的情况下抽取新的关系，或者新领域的关系，就必须使用纯无监督的方法。</p>
<h2 id="关系抽取的评估">17.2.6 关系抽取的评估</h2>
<p><strong>有监督</strong>的关系抽取系统是通过使用带有人类标注的gold-standard关系的测试集并计算精度、召回率和 F-measure 来评估的。标注精度和召回率要求系统正确分类关系，而未标注的方法只是衡量系统检测相关实体的能力。</p>
<p><strong>半监督</strong>和<strong>无监督</strong>方法更难评估，因为它们是从网络或大型文本中抽取全新的关系。由于这些方法使用了非常多的文本，所以通常不可能只在一个小的标注测试集上运行它们，因此不可能预先标注出一个正确的关系实例的gold set。</p>
<p>对于这些方法，可以通过从输出中随机抽取一个关系样本来（仅仅是）近似精确性，并让人检查这些关系的准确性。通常，这种方法侧重于从文本中抽取的<strong>元组</strong>，而不是关系的<strong>提及</strong>；系统不需要检测每一个关系的提及来正确评分。相反，评估是基于系统完成任务时数据库中包含的元组集。也就是说，我们想知道系统是否能发现 Ryanair 在 Charleroi 有一个枢纽；我们并不关心它发现了多少次。那么，估计的精度<span class="math inline">\(\hat{P}\)</span> 就是： <span class="math display">\[
\hat{P}=\frac{\# \text { of correctly extracted relation tuples in the sample }}{\text { total } \# \text { of extracted relation tuples in the sample. }}
\]</span> 译：P = 样本中正确提取的关系元组 / 样本中提取的关系元组总数</p>
<p>另一种能给我们提供一点召回信息的方法是计算不同召回级别的精度。假设我们的系统能够对它所产生的关系进行排序（按照概率或置信度），我们可以分别计算前1000个新关系、前10000个新关系、前100000个新关系的精度等等。在每一种情况下，我们都会从该集合中随机抽取一个样本。这将向我们展示当我们抽取越来越多的元组时，精度曲线的表现。但是没有办法直接评估召回率。</p>
<hr />
<p><strong>本章剩余内容见：《自然语言处理综论》第17章-信息抽取（下）</strong></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>SemEval 2017任务10：ScienceIE--从科学出版物中提取关键词和关系</title>
    <url>/paper-0/</url>
    <content><![CDATA[<p>我们提出了一种方法，从研究工作的<strong>重点、应用领域和所使用的技术</strong>方面来描述其特征。我们展示了随着时间的推移对这些方面的追踪是如何提供一个新的衡量研究群体相互影响的方法。我们通过<strong>匹配语义提取模式</strong>来提取这些特征，语义提取模式是用<strong>bootstrapping</strong>学习的，与文章摘要中的句子的依存树相匹配。我们将这些信息与预先计算的文章到社区的分配（article-to-community assignments）结合起来，研究一个社区对其他社区的影响，即借用的技术和一些社区解决其他问题的 "成熟 "程度。作为一个案例研究，我们展示了计算语言学社区及其子领域多年来在其焦点、所用方法和领域问题方面的变化。例如，我们展示了POS标记和解析已经越来越多地被采用为解决其他领域问题的工具。我们还观察到，语音识别和概率论的影响最为深远。</p>
<span id="more"></span>
<p><img src="https://i.loli.net/2021/07/17/uBsvEqlp42G96tH.png"/></p>
<p>原文：https://aclanthology.org/I11-1001.pdf</p>
<h1 id="引言">1 引言</h1>
<p>思想的演变和一个研究社区的动态可以通过该社区发表的科学文章来研究。例如，我们可能对<strong>方法如何从一个社区传播到另一个社区</strong>感兴趣，或者对<strong>一个课题从研究重点到解决问题的工具的演变</strong>感兴趣。我们可能想在一个领域内找到技术驱动和领域驱动的研究之间的平衡。要建立对科学研究的发展和进步的如此丰富的洞察力，需要了解的不仅仅是讨论的 "话题 "或文章之间的引用链接，这些在过去的工作中被用来研究文章的趋势和影响。举例来说，为了确定技术驱动的研究人员的影响力是大还是小，我们需要能够识别工作的风格。为了达到这样的详细程度，并且能够将方法和思想联系在一起，就必须超越<strong>词袋主题模型</strong>（bagof-words topical models）。这需要对句子和<strong>论证结构</strong>（argument structure）的理解，因此是一种信息提取的形式。</p>
<p>为了研究应用<strong>领域</strong>、用于处理领域问题的<strong>技术</strong>以及社区中科学文章的重点，我们建议从文章中提取以下概念：</p>
<ul>
<li><strong>FOCUS</strong>：一篇文章的主要贡献</li>
<li><strong>TECHNIQUE</strong>：一篇文章中使用的方法或工具，例如期望最大化和条件随机场</li>
<li><strong>DOMAIN</strong>：一篇文章的应用领域，例如语音识别和文档的分类。</li>
</ul>
<p>例如，如果一篇文章专注于支持向量机中的正则化，并显示了解析准确性的提高，那么它的FOCUS和TECHNIQUE就是正则化和支持向量机，而它的DOMAIN就是解析。相反，如果一篇文章专注于词汇特征来提高解析的准确性，并使用支持向量机来训练模型，其FOCUS是词汇特征和解析，TECHNIQUE是词汇特征和支持向量机，而其DOMAIN仍然是解析。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>在这种情况下，尽管两篇文章的TECHNIQUEs和DOMAIN非常相似，<strong>但FOCUS短语将它们区分开来。</strong>请注意，一篇文章的DOMAIN可以是另一篇文章的TECHNIQUE，反之亦然。例如，一篇展示名实体识别（NER）上改进的文章，其DOMAIN是NER，然而，一篇使用命名实体作为中间工具来提取关系的文章，其NER是其TECHNIQUE之一。</p>
<p>我们的工作使用<strong>信息提取模式</strong>来从文章中提取上述三个类别的短语。这些短语是通过匹配句子的<strong>依存树中的语义模式</strong>来提取的。抽取系统的输入是一些<strong>种子模式</strong>（例子见表1），它使用<strong>boostrapping</strong>学习更多的模式。使用基于词袋的方法，比如<strong>主题模型，</strong>来解决这个问题并不简单；顾名思义，主题模型一般只识别论文的主题或领域（比如 "解析 "或 "语音识别"），既不提供也不标注不同的交叉方面，比如论文使用的技术或应用领域。</p>
<p>作为一个案例研究，我们考察了在计算语言学社区发表的文章。<strong>我们使用从文章中提取的FOCUS、TECHNIQUE和DOMAIN短语来研究该社区的子领域的影响</strong>，例如解析和机器翻译。我们使用ACL Anthology数据集<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>（Bird等人，2008；Radev等人，2009）中的文档集合，因为它有论文的全文。为了得到社区的子领域，我们使用latent Dirichlet allocation（Blei等人，2003）来寻找主题并手工标注。<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> 然而，我们的一般方法可以用来研究学术社区影响的任何情况，包括更广泛地研究统计学或经济学对整个社会科学的影响。我们研究社区如何在重复使用的技术方面相互影响，并展示一些社区如何 "成熟"，使他们产生的结果被采用为解决其他问题的工具。例如，词性标记（POS）社区的产品已经被其他许多使用POS标记作为中间步骤的社区所采用，这在我们的结果中也得到了证实。</p>
<p><strong>我们还展示了社区影响的时间轴。</strong>例如，我们的结果显示，正式的计算语义学和基于统一的语法在1980年代末有很大的影响。语音识别和概率论领域在90年代中期呈现出影响上升的趋势，尽管近年来有所下降，但它们对最近的论文仍有很大的影响，主要是由于期望最大化和隐马尔科夫模型等技术。因此，我们的结果显示，总体而言，它们是过去20年中最具影响力的领域。与语音识别不同，概率理论在传统上不是计算语言学的一个独立的子领域，但它是一个重要的话题，因为许多论文都使用概率方法并在其上工作。我们还表明，对影响力的研究不同于研究社区的流行度或热度，比如（Griffiths and Steyvers, 2004; Hall et al., 2008），后者是基于某一年在社区中发表的论文的预期数量。</p>
<h2 id="贡献">贡献</h2>
<p>我们对科学文章的关键方面进行了新的分类，即（1）焦点：主要贡献，（2）技术：使用的方法或工具，以及（3）领域：应用领域。我们通过将语义模式与依存树相匹配来提取这些方面，并使用boostraping学习这些模式。我们提出了一个新的定义，即一个研究社区的影响力是指它的关键方面被其他社区采用为技术。我们提出了一个关于计算语言学社区的案例研究，使用从其文章中提取的三个方面，既验证了我们系统的结果，也展示了计算语言学子领域的动态和整体影响力的新结果。<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<h1 id="相关工作">2 相关工作</h1>
<p>虽然与文本总结中的关键词选择有一些联系(Radev等人, 2002)，但是提取FOCUS、TECHNIQUE和DOMAIN短语从根本上说是一种信息提取的形式，并且在这个领域已经有各种各样的前期工作。一些工作，包括开创性的（Hearst, 1992），使用手写的规则来识别模式（ISA关系），而其他工作则通过依存图来学习模式（Bunescu and Mooney, 2005）。这项工作建立在以往在NLP中成功使用bootstrapping学习技术的基础上（Yarowsky，1995；Collins和Singer，1999；Riloff和Jones，1999）；在依存模式的使用方面，它也许特别接近（Yangarber等人，2000）。</p>
<p>主题模型已经被用来研究社区的受欢迎程度（Griffiths和Steyvers，2004），思想的历史（Hall等人，2008），以及论文的学术影响（Gerrish和Blei，2010）。然而，主题模型并不像我们这样从文本中提取详细信息。尽管如此，我们仍然使用主题模型中的主题到词的分布作为描述子领域的一种方式。</p>
<p>Demner-Fushman和Lin（2007）在他们的临床问题回答系统中使用手写知识提取器来提取信息，如人群和干预，以提高相关摘要的排名。我们对关键方面的分类适用于更广泛的社区，并且我们通过bootstrapping来学习这些模式。Li等人（2010）使用语义元数据创建了一个化学语义数字图书馆，并使用关键词特征识别了实验段落。Xu等人（2006）和Ruch等人（2007）分别在临床试验和生物医学领域提出了系统，通过使用结构化摘要[^5]或手工标记的句子，对摘要的句子进行分类，使其对应于介绍、目的、方法、结果和结论等类别，以提高文章检索率。一些总结系统也使用机器学习方法来寻找 "关键句子"。这些论文中建立的系统与我们的系统是相辅相成的，因为人们可以找到相关的段落或句子，然后从中提取关键的内容。请注意，一个句子可以有多个短语对应于我们的三个类别，因此对句子进行分类是不够的。</p>
<h1 id="方法">3 方法</h1>
<p>在本节中，我们将解释如何为三个类别（重点、技术和领域）中的每一个提取短语，以及如何计算社区的影响力。</p>
<h2 id="模式匹配和学习">3.1 模式匹配和学习</h2>
<p>从一篇文章的摘要和标题中，我们使用句子的<strong>依存树</strong>和一组<strong>语义提取模式</strong>来提取每个类别的短语，这些短语被一些期刊使用，有多个部分，如PURPOSE, METHOD, FOCUS、TECHNIQUE和DOMAIN类别。句子的依存树是一棵解析树，它给出了句子中词与词之间的依存关系（如直接-主语，主语）。图1显示了'我们致力于使用依存图提取信息' （We work on extracting information using dependency graphs）这句话的<strong>依存图</strong>。每个<strong>语义模式</strong>都是<span class="math inline">\(T→d\)</span>的形式，其中<span class="math inline">\(T\)</span>是一个触发词（如'使用'、'展示'），<span class="math inline">\(d\)</span>是一个依存关系（如'直接-宾语'）。我们从一些手写的模式开始（表1中的一些），并使用bootstrapping方法自动学习更多的模式。我们运行一个<strong>迭代算法</strong>，利用语义模式提取短语，然后从提取的短语中学习新的模式。每个步骤的细节将在下面描述。</p>
<p><img src="https://i.loli.net/2021/07/17/BrqQVJfDI8yaU5K.png" width="400"/></p>
<p><img src="https://i.loli.net/2021/07/17/QFT21zZiXes35pt.png" width="400"/></p>
<h3 id="从模式中提取短语"><strong>从模式中提取短语</strong></h3>
<p>一个依存树与一个模式<span class="math inline">\(T→（d）\)</span>相匹配，如果（1）它包含<span class="math inline">\(T\)</span>，并且（2）触发词的节点有一个successor（从属dependent or granddependent upto 4 levels），它与它的父节点（parent）的依存关系是<span class="math inline">\(d\)</span>。在本文的余下部分，我们把以successor为head的子树称为匹配的短语树。我们提取与匹配的短语树相对应的短语，并给它贴上模式的类别。例如，图1中的依存树与FOCUS模式<span class="math inline">\([work → (preposition\_on)]\)</span>和TECHNIQUE模式<span class="math inline">\([using → (direct-object)]\)</span>匹配。因此，系统将对应于以 ‘extracting’ 为head的短语树的短语，即 "使用依存图提取信息"，标记为FOCUS类别，并同样将 "dependency graphs "标记为TECHNIQUE类别。</p>
<p>我们对<strong>论文标题</strong>有特殊的规定，因为作者通常会把论文的主要贡献放在标题中。如果我们无法使用模式提取出一个FOCUS短语，我们就将整个标题标记为FOCUS。对于可以提取TECHNIQUE短语的标题，我们将其余的词（除了触发词）标记为DOMAIN。例如，对于标题 "Studying the history of ideas using topic models"，我们的系统使用模式<span class="math inline">\([using → (direct-object)]\)</span>将 "topic models" 提取为TECHNIQUE，然后将 "Studying the history of ideas" 标记为DOMAIN。</p>
<h3 id="从短语中学习模式"><strong>从短语中学习模式</strong></h3>
<p>在提取了具有模式的短语之后，我们希望能够<strong>构建和学习新的模式</strong>。对于每个句子，其依存树有一个对应于所提取的短语之一的<strong>子树</strong> (subtree)，我们通过考虑子树的祖先（父母或祖父母）作为触发词<span class="math inline">\(T\)</span>，以及子树的头部和其父母之间的依存关系作为依存关系<span class="math inline">\(d\)</span>，构建一个模式<span class="math inline">\(T→（d）\)</span>。对新构建的模式进行加权的方法如下。对于一组提取模式（<span class="math inline">\(q\)</span>）的短语（<span class="math inline">\(P\)</span>），模式<span class="math inline">\(q\)</span>在类别FOCUS中的权重是<span class="math inline">\(\sum_{p \in P} \frac{1}{z_{p}} \operatorname{count}(p \in FOCUS)\)</span>，其中<span class="math inline">\(z_p\)</span>是短语<span class="math inline">\(p\)</span>的总频率。 同样地，我们可以得到其他两个类别的模式的权重。请注意，我们不需要进行平滑处理，因为短语-类别比率是在构建模式的所有短语中汇总的（aggregated）。在对所有在之前的迭代中没有被选中的模式进行加权后，我们在每个类别中选择前<span class="math inline">\(k\)</span>个模式（在我们的实验中<span class="math inline">\(k\)</span>=2）。表3显示了通过迭代方法学到的一些模式。</p>
<p><img src="https://i.loli.net/2021/07/17/cD34mTlQZouvte9.png" width="400"/></p>
<h2 id="社区及其影响">3.2 社区及其影响</h2>
<p>我们将社区定义为人们希望研究的领域或子领域。为了利用发表的文章来研究社区，我们需要知道每篇文章属于哪个社群。文章到社区的分配可以通过几种方式来计算，比如通过手动分配、使用元数据，或者通过论文的文本分类。在我们的案例研究中，我们使用了通过应用<strong>潜在迪里切特分配</strong>（Blei等人，2003）来形成的主题，将每个主题视为一个社区。近年来，主题建模已被广泛用于从文本中获得 "概念"。它的优势在于以无监督的方式产生软性的、概率性的文章到社区的分配分数。我们将这些软分配分数与上一节提取的短语结合起来，为每个社区和每个类别的短语打分，具体如下。 从一篇文章中提取的短语<span class="math inline">\(p\)</span>，在社区<span class="math inline">\(c\)</span>和类别TECHNIQUE中的得分计算如下： <span class="math display">\[
\begin{array}{r}
t S \operatorname{core}(c, p, a)= 
\frac{1}{z_{p}} \operatorname{count}(p \in \text { TECHNIQUE } \mid a) P(c \mid a, \theta)
\end{array}
\]</span> 其中，函数<span class="math inline">\(P(c | a, θ)\)</span>给出了在主题建模参数<span class="math inline">\(θ\)</span>下，文章<span class="math inline">\(a\)</span>出现社区（即主题）的概率。 短语的标准化常数<span class="math inline">\(z_p\)</span>是该短语在所有摘要中的频率。</p>
<p>我们对影响力的定义是：如果社区比其他社区更早地使用技术，或者产生了用于解决其他问题的工具，那么它们就会得到更高的分数。例如，由于语音识别社区引入的隐马尔可夫模型和语音部分标记社区建立的语音部分标记工具已经作为技术被其他社区广泛使用，这些社区应该比新生的或不太常用的社区获得更高的分数。因此，我们根据一个社区的FOCUS、TECHNIQUE或DOMAIN短语在其他社区作为TECHNIQUE使用的次数来定义其影响力。为了计算一个社区对另一个社区的整体影响力，我们首先需要计算因为社区中的个别文章而产生的影响力，其计算方法如下。社区c1对另一个社区c2的影响，因为从文章a1中提取的短语p是： <span class="math display">\[
\begin{aligned}
&amp;\operatorname{tInfl}\left(c_{1}, c_{2}, p, a_{1}\right)= 
\text { allScore }\left(c_{1}, p, a_{1}\right) \sum_{a_{2} \in D \atop y_{a_{2}}&gt;y_{a_{1}}} t S \operatorname{core}\left(c_{2}, p, a_{2}\right) C\left(a_{2}, a_{1}\right)
\end{aligned}
\]</span> 其中，函数allScore(c, p, a)的计算方法与公式1相同，但使用了count(p∈ALL | a)，其中ALL表示所有三个类别中提取的短语的总和。变量D是所有文章的集合，ya2表示文章a2的出版年份。求和项计算了从文章a1中提取的短语p对社区c2中后来发表的所有文章的影响。函数C(a2, a1)是一个基于引用的加权函数，如果a2引用了a1，其值为1，否则为λ。如果λ为0，系统就只根据引文来计算影响力，而引文可能是有噪声的，不完整的。在我们的实验中，我们使用λ为0.5，因为我们想研究即使一篇文章没有明确引用另一篇文章的影响力。社区c1在y年对社区c2的技术影响力得分是通过对所有短语（P）和D中所有文章的前述等式进行加总计算的。它的计算公式为： tInf l(c1, c2, y) = P p∈P X a∈D ya1 =y tInf l(c1, c2, p, a) (3) 简单地说，社区c1对社区c2和所有其他社区的总体影响计算为： tInf l(c1, c2) = P y tInf l(c1, c2, y) (4) tInf l(c1) = P c26=c1 tInf l(c1, c2) (5) 接下来，我们用上述的影响力分数对计算语言学的各个子领域进行案例分析。</p>
<h1 id="实验设置">4 实验设置</h1>
<h2 id="数据集">数据集</h2>
<p>我们使用ACL文集网络和ACL文集参考语料库（Bird等人，2008；Radev等人，2009）中的15016篇文章的标题和摘要研究了1965年至2009年的计算语言学界。我们找到了52对摘要，它们之间有80%以上的共同词，因此在计算影响分数时，我们忽略了这些摘要中较早发表的论文对较晚发表的论文的影响。我们使用斯坦福分析器（Marneffe等人，2006）来生成句子的依存树。</p>
<p>在测试中，我们将474篇摘要用三个类别进行手工标注，以测量精度和召回率。对于每个摘要和每个类别，我们将从我们的算法中提取的独特的非停用词与手工标记的数据集进行比较。我们计算了每个摘要的精确性、召回率措施，并将其平均化以得到数据集的结果。</p>
<p>在从匹配的短语树中提取短语时，我们忽略了带有代词、数字、定语、标点或符号等语篇标签的标记，并删除了匹配的短语树中所有与父辈有相对词-修饰词或句子-补充词依存关系的子树，因为即使我们想要完整的短语，包括这些子树也会引入无关的短语和句子。我们还将匹配的短语树的子树上的短语加入到提取的短语集合中。</p>
<p><strong>我们对FOCUS使用了13个种子模式，对TECHNIQUE使用了7个，对DOMAIN使用了15个。</strong>在构建新的模式时，我们忽略了不是名词或动词的祖先，因为<strong>大多数触发词都是名词或动词（如使用、约束）</strong>。我们还忽略了连词、相对词-修饰词、从属词（最通用的从属关系）、量词-修饰词和缩写从属关系<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>，因为它们要么太通用，要么引入了不相干的短语和句子。</p>
<p>在对手工标注的测试集进行测试时，<strong>学习新的模式并没有帮助改善FOCUS类别的短语。</strong>当只使用种子模式和标题时，它得到了相对较高的分数，因此学习新的模式降低了精度，而召回率却没有任何明显的改善。<strong>因此，我们只为技术和领域类别学习新的模式。</strong>我们对这两个类别都进行了50次迭代，这是根据一些早期的试行实验，在模式精度和召回率之间选择的合理权衡。</p>
<p><strong>在提取了所有的短语之后，我们使用一个由3000个短语组成的停用词列表</strong>，删除了科学文章中经常使用的常见短语，例如 "这种技术 "和 "存在"。该列表是通过从ISI知识网络数据库<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>中随机抽取100,000篇有摘要的文章中出现次数最多的1至3个grams来创建的。</p>
<p>我们忽略了那些只有一个字符或超过15个字的短语。在寻找规范名称的过程中，我们通过搜索两个括号之间的文字，从论文全文中自动检测出缩写及其扩展形式，并<strong>将括号前的短语视为扩展形式</strong>（类似于（Schwartz and Hearst, 2003））。我们通过挑选出现次数最多的一对缩写和它们的扩展形式得到一个高精度的列表，并通过合并所有使用相同缩写的短语来创建短语组。然后，我们将提取的短语数据集中的<strong>所有短语改为它们的标准名称</strong>。我们还删除了 ‘model’, ‘approach’, ‘method’, ‘algorithm’, ‘based’, ‘style’ 等词以及它们在短语末尾出现的变体。</p>
<h2 id="基线">基线</h2>
<p>为了与基于非信息提取的基线进行比较，我们从摘要中提取了所有的名词短语，以及名词短语树的子树的短语，并给它们贴上了所有三个类别的标签。此外，我们给标题（及其子树）贴上了FOCUS类别的标签。然后，我们用tf-idf的激励措施对短语进行评分，即短语在摘要中的频率与单个词的总频率之比，并删除tf-idf措施小于0.001的短语（在众多实验中表现最好）。我们称这种方法为'Baseline tf-idf NPs'。<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>为了获得计算语言学文献中的社区，我们将Bethard和Jurafsky（2010）使用相同的ACL文集数据集产生的主题视为社区。他们对论文的全文进行了潜在的迪里切特分配，得到了100个主题。我们手工标记了这些主题，并在我们的研究中使用了其中的72个；其余的都是关于常见的词。在计算公式1中的分数时，如果<span class="math inline">\(P(c | a, θ)\)</span>的值小于0.1，我们认为它是0。</p>
<h1 id="结果与讨论">5 结果与讨论</h1>
<p>提取的短语总数在FOCUS为25,525个，TECHNIQUE为24,430个，DOMAIN为33,203个。在包括从匹配的短语树的子树上提取的短语之后，短语的总数分别为64,041个、38,220个和46,771个。从一些论文中提取的短语的例子见表2。</p>
<p>表4比较了我们使用三个类别的精确度、召回率和微平均F1分数。(1)只使用种子模式，(2)学习模式和种子模式的组合，(3)基线，以及(4)标注者之间的协议。我们计算了30篇摘要的标注者之间的一致性，其中每篇摘要由2名标注者标记，[^9]而精度-召回分数的计算是通过随机选择一个标注作为黄金标注，另一个作为预测的标注。我们[^9]第一作者标注了30篇摘要，两名计算语言学的博士生分别标注了15篇。</p>
<p>从表中可以看出，由于学习到的模式，TECHNIQUE的精度和召回率都有所提高，尽管DOMAIN的精度有所下降，但召回率有所提高。基准线的召回分数较高，但精度很低。三个可能的原因解释了我们的系统所犯的错误。(1) 作者有时使用通用短语来描述他们的系统，这些短语在测试集中没有被标注为三个类别中的任何一个，但却被系统提取了出来（如 "简单方法"、"更快的模型"、"新方法"）；(2) 一些句子的依存树是错误的；以及(3) 一些为TECHNIQUE和DOMAIN学习的模式是低精度但高召回率。图2显示了每5次迭代后TECHNIQUE和DOMAIN的F1得分。影响力表5显示了总体上最有影响力的社区（用公式5计算）和它们各自的有影响力的短语，这些短语已经被其他社区广泛采用为技术。我们可以看到，语音识别是最有影响力的社区，因为它在计算语言学文献中引入了隐马尔可夫模型等技术和其他随机方法，这表明尽管最近的影响有限，但它的长期种子影响仍然存在 (a) 各年社区的影响力。(b) 各年社区的受欢迎程度。图4：以与图3相同的方式比较机器翻译相关社区。统计机器翻译社区，是一个来自主题模型的主题，更多的是基于短语的。 流行度。概率论也得到了很高的分数，因为过去十年中许多论文都使用了随机方法。语篇标签和解析社区得到了高分，因为他们采用了一些在其他社区使用的技术，也因为其他社区在解决其他问题的中间步骤中使用了语篇标签和解析技术。图3(a)显示了一个社区的影响力随时间的变化，图3(b)显示了其受欢迎程度的变化。一个社区的受欢迎程度是该社区主题和某年发表的所有文章的文章-主题得分之和。10 两幅图中的得分都经过归一化处理，即一年中所有社区的总得分都是1。比较图3(a)中社区的相对分数和图3(b)中的相对分数。我们可以看到一个社区的影响力与一个社区在某年的受欢迎程度是不同的。如前所述，我们观察到虽然语音识别的影响力分数在最近几年有所下降，但它仍然有很大的影响力，尽管该社区在最近几年的流行度很低。机器学习分类在最近几年既受欢迎又有影响力。自2003年以来，命名实体识别的受欢迎程度有所下降，尽管其影响力有所上升或保持不变。图4比较了机器翻译社区，与我们在图3中比较其他社区的方式相同。我们可以看到，统计机器翻译（更多的是基于短语的）社区的受欢迎程度在过去5年里急剧上升，然而，其影响力10See (Hall et al., 2008) for more analysis. 请注意，这个分析只是使用了基于词包的主题模型。</p>
<p>影响力的增长速度较慢。另一方面，双语词汇对齐（2009年最有影响力的社区）的影响力在同一时期增加，主要是因为它对统计机器翻译的影响。非统计机器翻译的影响力最近一直在下降，尽管比它的流行速度慢。表6显示了对某一社区影响最大的社区（该列表按公式4的得分降序排列）。6 未来的方向 我们正在努力将文章的发表日期纳入其中，以学习更好的模式来提高系统的精确度和召回率。我们也在探索如何利用我们的系统来研究引文和合著网络。我们计划研究生物学、统计学和社会科学等更广泛社区的动态和影响。这种方法也可以用来研究跨学科研究中的创新，因为我们可以跟踪跨学科研究是否导致应用一个社区的旧技术来解决其他社区的问题，或者是否导致更好的合适技术的演变。</p>
<h1 id="结论">7 结论</h1>
<p>本文提出了一个框架，通过匹配依存树中的语义提取模式，从科学文章中提取详细信息，比如主要贡献、使用的工具和技术，以及解决的领域问题。我们从一些手写的种子模式开始，用引导的方法学习新的模式。我们利用从文章中提取的这些丰富的信息来研究研究社区的动态，并定义一种新的方法来衡量一个研究社区对另一个社区的影响。我们提出了一个关于计算语言学社区的案例研究，在那里我们发现了其子领域的影响，并观察到语音识别和概率论的影响最为深远。</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>A community与DOMAIN：社区可以像计算机科学或统计学一样广泛，而DOMAIN则是一个具体的应用，如中文词语分割。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>http://www.aclweb.org/anthology<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>在本文中，我们交替使用社区(communities)、子社区(subcommunities)和子领域(sub-fields)这些术语。<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>The dataset is available at http://cs.stanford.edu/people/sonal/fta for the research community<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>关于依存关系的细节见（Marneffe等人，2006）<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>www.isiknowledge.com<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>正如第1节所讨论的，使用无监督或弱监督的基于词包的方法对于识别文章的FOCUS、TECHNIQUE和DOMAIN来说并不直接，因此我们没有与一个人进行比较。]<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>期刊翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>一袋什么？用于文本分析的简单名词短语抽取</title>
    <url>/paper-6/</url>
    <content><![CDATA[<p>没有经过专门的自然语言处理培训的社会科学家在分析文本语料库时经常使用unigram词袋（BOW）表示。我们提供了一种新的基于短语的方法，即NPFST，用于丰富unigram词袋的内容。NPFST使用一个POS标记器和一个有限状态转换器来提取多词短语，并将其添加到unigram词袋中。我们将NPFST与Ngram和解析方法在产量（yield）、召回率（recall）和效率（efficiency）方面进行比较。然后我们演示了如何使用NPFST进行探索性分析；它在许多不同种类的英语文本上表现良好，无需配置。最后，我们介绍了一个使用NPFST分析美国国会法案(congressional bills ) 新语料库的案例研究。关于我们的开源实现，见http://slanglab.cs.umass.edu/phrases/。</p>
<span id="more"></span>
<h1 id="引言">1 引言</h1>
<p>社会科学家在分析文本语料库时通常使用unigram表示法；每个文档被表示为一个unigram词袋（BOW），而语料库本身被表示为一个计数（count）的文档-术语矩阵。例如，Quinn等人（2010）和Grimmer（2010）使用unigram词袋作为主题模型的输入，而Monroe等人（2008）使用unigram词袋来报告政治演说中最有党派性的术语。虽然unigram词袋的简单性很吸引人，但unigram分析不能保留有意义的多词短语，如 "医疗保健 "或 "社会保障"，也不能区分共享一个词的政治意义的短语，如 "非法移民 "和 "无证移民"。为了解决这些限制，我们引入了NPFST，它提取多词短语来充实unigram词袋，作为文档-术语矩阵的附加列。NPFST适用于许多不同种类的英语文本；它使用适度的计算资源，不需要任何专门的配置或标注。</p>
<h1 id="背景">2 背景</h1>
<p>我们将NPFST与其他几种方法在产量、召回率、效率和可解释性方面进行比较。产量指的是提取短语的数量--较低的产量需要较少的计算资源和人力资源来处理这些短语。召回率是指一种方法恢复最相关或最重要的短语的能力，由人决定。一个好的方法应该有较低的产量，但有较高的召回率。</p>
<h2 id="n-grams">2.1 n-grams</h2>
<p>我们最简单的基线是 <strong>AllNGrams（K）</strong>。这个方法从标记化的句子分段的文本中提取所有的n-grams，长度不超过K，但不包括跨越句子边界的ngrams。这种方法通常用于提取文本分类的特征（例如，Yogatama等人（2015）），但在社会科学背景下有几个缺点。首先，社会科学家经常想对单个短语进行实质性的解释，但跨越句子成分的零散短语可能没有意义。例如，《平价医疗法案》包括难以解释的4-gram，"the Internet website of"。其次，尽管AllNGrams(K)有很高的召回率（只要K足够大），<strong>但它的产量较高，因此需要大量资源来处理提取的短语。</strong></p>
<h2 id="解析">2.2 解析</h2>
<p>另一种方法<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>是使用语法将提取的短语限制在成分上，如名词短语（NPs）。与动词、介词或形容词短语不同，NPs即使从其周围的语境中剥离出来也常常是有意义的——例如，<span class="math inline">\([Barack Obama]_{NP}\)</span>与<span class="math inline">\([was inaugurated in 2008]_{V P}\)</span>。 有许多方法可以提取NPs。鉴于NLP中成分分析研究的悠久历史，一种明显的方法是运行一个现成的成分分析器 (constituent parser )，然后从树中检索所有的NP non-terminals <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>。我们称这种方法为 <strong>ConstitParse</strong>。不幸的是，英语训练数据的主要来源，如Penn Treebank（Marcus等人，1993），包括NP内的定语和 non-nested flat NP标注<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>，导致在我们的背景下召回率低（见§4）。由于现代解析器依赖于这些训练数据的来源，所以要改变这种行为是非常困难的。</p>
<h2 id="pos语法">2.3 POS语法</h2>
<p>另一种方法，由Justeson和Katz提出 (1995)提出的另一种方法是使用 POS 模式来寻找和提取NP--一种浅层部分解析的形式（Abney, 1997）。研究人员已经在各种不同的背景下使用这种方法（Benoit和 Nulty, 2015; Frantzi等人, 2000; Kim等人, 2010; Chuang等人，2012；Bamman和Smith，2014）。) 一个基于模式的方法可以用三组参数来指定：(G, K, M)，其中G是一个语法，K是一个最大长度，而M是一个匹配策略。语法G是一个非递归的正则表达式，定义了一个无限的POS标签序列（即正则语言）；最大长度K将提取的n-grams的长度限制在n≤K；而匹配策略M指定了如何提取符合语法的文本跨度。 我们考虑的最简单的语法是：</p>
<p><span class="math inline">\((A | N) ∗ N(P D ∗ (A | N) ∗ N)∗\)</span></p>
<p>在一个由形容词、名词（包括普通名词和专有名词）、介词和定语词组成的粗略的标签集上定义。我们把这种语法称为<strong>SimpleNP</strong>。与这种语法相匹配的成分是 bare NPs（ with optional PP attachments ）、N-bar和名字。我们在root NP上不包括任何定语。</p>
<p>我们还考虑了三个基线匹配策略，每个匹配策略可以（理论上）与任何G和K一起使用。第一个FilterEnum用正则语言枚举所有可能的字符串，最大长度为K，作为预处理步骤。然后，在运行时，它检查语料库中的每个n-gram是否存在于这个枚举中。这种匹配策略实现简单，可以提取长度为K的所有匹配，但是如果K较大，则在计算上是不可行的。第二种是filtersa，它将G编译成一个有限状态自动机（FSA）作为预处理步骤。然后，在运行时，它检查每个n-gram是否与这个FSA匹配。像FilterEnum一样，这种匹配策略提取所有长度为K的匹配；然而，如果K很大，它可能是低效的。第三个GreedyFSA也将G编译成FSA，但在运行时使用标准的贪婪匹配方法来提取与G匹配的ngram。与其他两种匹配策略不同，它不能提取重叠或嵌套的匹配，但可以提取很长的匹配。4</p>
<p>Justeson和Katz（1995）在他们最初的陈述中定义了一种与SimpleNP非常相似的语法，并建议使用2-gram和3-gram（即K=3）。在这种限制下，他们的语法由七种独特的模式组成。他们还建议使用FilterEnum来提取与这些模式匹配的文本跨度。我们将此方法称为JK=（SimpleNP，K=3，FilterEnum）。许多研究人员使用这种方法，也许是因为曼宁和舒茨（1999）在NLP教科书中描述了这种方法。</p>
<p>我们的贡献是一种新的基于模式的提取方法：NPFST =（FullNP，<span class="math inline">\(K=∞\)</span>, RewriteFST ）。在§3.1中，我们定义了FullNP语法，在§3.2中，我们定义了RewriteFST匹配策略。</p>
<h2 id="fullnp语法">3.1 FullNP语法</h2>
<p>FullNP扩展SimpleNP，通过添加具有相同标记的成对单词的coordination（例如，（VB CC VB）在 (cease and desist) order) ；名词短语的coordination ；parenthetical post-modifiers 插入式后置修饰语（例如<span class="math inline">\(401(k)\)</span>，由于通常的NLP标记化约定，它是4-gram）；数字修饰符和名词；支持Penn Treebank标签集， 粗糙的通用标签集（Petrov等人，2011）和Gimpel等人（2011）的Twitter特定粗糙标签集。我们在附录中提供了完整的定义 。</p>
<p><img src="https://i.loli.net/2021/07/28/VL1TDYsoCdqhuSa.png" width="400"/></p>
<h2 id="rewritefst匹配策略">3.2 RewriteFST匹配策略</h2>
<p>RewriteFST使用a finite-state transducer 有限状态转换器（FST）快速提取与G匹配的文本spans ，包括overlapping 和nested spans 。这种匹配策略是有限状态NLP的一种形式（Roche和Schabes，1997），因此建立在FST算法和工具的大量前期工作的基础上。</p>
<p>RewriteFST的输入是标记I的POS-tagged5序列，表示为FSA。对于一个简单的标记序列，这个FSA是一个线性链，但是，如果标记器的输出有不确定性，它可以是一个每个位置有多个标记的晶格。</p>
<p>语法G首先被编译成一个短语转换器P，6，它接受一个输入序列I并输出相同的序列，但是分别插入一对开始和结束符号-[S]和[E]，以指示可能的np（参见图1）。在运行时，RewriteFST计算输出晶格L=I◦ P采用FST合成；7因为它是不确定的，所以L包括所有重叠和嵌套的跨距，而不仅仅是最长的匹配。最后，FilterFST遍历L以找到带有[S]符号的所有边。从每一条路径开始，它都执行深度优先搜索，以找到指向带有[E]符号的边的所有路径，累加所有[S]和[E]分隔的跨距。8</p>
<p>在表1中，我们提供了FilterFST和§2.3中描述的三种匹配策略的比较。</p>
<p><img src="https://i.loli.net/2021/07/28/q6TMGZVjut4wOes.png" width="400"/></p>
<h1 id="实验结果">4 实验结果</h1>
<p>在本节中，我们提供了实验结果，将NPFST与第2节中描述的基线在产量、召回率、效率和可解释性方面进行比较。正如我们所期望的那样，NPFST具有较低的产量和较高的召回率，并且有效地提取了高度可解释的短语。</p>
<h2 id="产量和召回率">4.1 产量和召回率</h2>
<p>产量是指一种方法所提取的短语的数量，而召回率是指一种方法恢复最相关或最重要的短语的能力，由人类来决定。因为相关性和重要性是特定领域的概念，不容易定义，所以我们用三个命名实体识别（NER）数据集来比较这些方法：来自WNUT 2015共享任务（Baldwin等人，2015）的Twitter上提到的十种实体；来自BioNLP共享任务2011（Kim等人，2011）的生物医学文章中提到的蛋白质；以及使用斯坦福NER（Manning等人，2014）识别的纽约时报文章（Sandhaus，2008）中命名实体的合成数据集。命名实体无疑是这三个不同领域中的相关重要短语。9 对于每个数据集，我们将方法的收益率定义为其提取的跨度总数，将方法的召回率定义为其提取的跨度列表中存在的（标记的）命名实体跨度的百分比。</p>
<p>图2描述了NPFST和以下基线方法的召回率与收益率11。不同K值的AllNGrams(K)、ConstitParse、12 JK和(SimpleNP, K =∞, GreedyFSA)。因为(SimpleNP, K = 3, FilterFSA)的收益率和召回率与JK相同，我们在图2中省略了这些数值。我们还省略了(FullNP, K = ∞, FilterEnum)和(FullNP, K = ∞, FilterFSA)的产量和召回率，因为它们与NPFST的产量和召回率相同。最后，我们省略了(FullNP, K = ∞, GreedyFSA)的收益率和召回率，因为我们对GreedyFSA的实现（使用标准的Python库）太慢，无法与FullNP一起使用。</p>
<p>一个好的方法应该有较低的收益率，但有较高的召回率--也就是说，最好的方法是在每个图的左上角。基于模式的方法都取得了较高的召回率，但产量比AllNGrams(K)低得多。ConstitParse取得了比NPFST更低的产量，但也取得了更低的召回率。JK的表现比NPFST差，部分原因是它只能提取2-和3-grams，例如，BioNLP数据集中提到的蛋白质长达十一个tokens（例如，"Ca2+/钙调蛋白依赖性蛋白激酶（CaMK）类型IV/Gr"）。最后，（SimpleNP, K =∞, GreedyFSA）的表现比JK差很多，因为它不能提取重叠或嵌套的跨度。</p>
<p>对于WNUT的数据集，NPFST的召回率相对较低（91.8%）。为了测试它的一些假阴性是否是由于POS标签错误造成的，我们利用NPFST对每个位置有多个标签的输入格子的操作能力。具体来说，我们用每个位置的标签构建了一个输入网格I，其后验概率至少为t。我们试验了t=0.01和t=0.001。这些值分别将召回率提高到96.2%和98.3%，但只换来了稍高的产量（低于AllNGrams(2)的产量）。我们怀疑我们没有看到产量的更大增加，即使是t=0.001，也是因为后验校准的原因（Nguyen和O'Connor，2015；Kuleshov和Liang，2015）。</p>
<h2 id="效率">4.2 效率</h2>
<p>我们用BioNLP数据集中的十篇文章来比较这些方法的预处理和运行时间成本。表2包含了AllNGrams(∞), ConstitParse, JK, (SimpleNP, K = 3, FilterFSA), and (SimpleNP, K = ∞, GreedyFSA), 和NPFST的计时结果13。我们省略了（FullNP, K =∞, FilterEnum）、（FullNP, K =∞, FilterFSA）和（FullNP, K =∞, GreedyFSA）的结果，因为它们太慢了，无法与其他方法竞争。POS标签比解析快20倍左右，这对可能没有快速服务器的社会科学家很有帮助。NPFST比更简单的基于模式的方法稍慢；然而，它的80%的时间花在构建输入I和遍历输出格子L上，这两个过程都是用Python实现的，可以做得更快。</p>
<h2 id="可解释性">4.3 可解释性</h2>
<p>在分析文本语料库时，社会科学家经常检查术语的排名列表，其中每个术语根据一些分数进行排名。我们认为，当把多词短语从其周围的语境中剥离出来并以列表的形式呈现时，多词短语比单词短语更具有可解释性。在第4.3.1节中，我们解释了如何合并相关术语，在第4.3.2节中，我们提供了排名列表，证明NPFST比其他方法提取了更多可解释的短语。</p>
<h3 id="合并相关术语">4.3.1 合并相关术语</h3>
<p>如第3.2节所述，NPFST提取了重叠和嵌套的跨度。例如，当在一个关于犯罪的国会法案数据集上运行时，NPFST提取了 "综合犯罪控制和安全街道法案"，以及嵌套的短语 "犯罪控制 "和 "安全街道法案"。虽然这种行为通常是可取的，但它也会导致排名列表中的重复。因此，我们概述了一种高层次的算法，用于合并排名列表中排名最高的术语。我们的算法的输入是一个术语列表L。算法在列表中迭代，从排名最高的术语开始，根据一些用户定义的标准（例如，术语是否共享一个子串）聚合类似的术语，直到它产生了C个不同的术语集群。然后，该算法选择一个术语来代表每个集群。最后，算法13我们使用Python的timeit模块，对集群中的代表术语进行排序，形成一个长度为C的排名列表。</p>
<h3 id="排名列表">4.3.2 排名列表</h3>
<p>为了评估NPFST提取的短语的可解释性，我们使用了三个数据集：由（人工识别的）气候否认者撰写的关于气候变化的推文；18世纪伦敦老贝利法庭的刑事审判记录；15以及1993年9月的《纽约时报》文章。对于每个数据集，我们使用ConstitParse、JK和NPFST提取短语，并为每种方法生成一个术语列表，按计数排序。最后，我们用我们的术语合并算法合并了相关的术语，只有当一个术语是另一个术语的子串时才进行合并，以产生五个代表性术语的排名列表。表4.3包含这些列表，表明NPFST产生了高度可解释的短语。5 案例研究。寻找美国国会立法中的党派术语 许多政治学家研究了语言使用和党派之间的关系（Laver等人，2003；Monroe等人，2008；Slapin和Proksch，2008；Quinn等人，2010；Grimmer和Stewart，2013）。我们提出了一个案例研究，我们使用NPFST来探索美国国会关于法律和犯罪的立法中的党派差异。在第5.1节，我们描述了我们的数据集，在第5.2节，我们解释了我们的方法并介绍了我们的结果。</p>
<h2 id="国会法案语料库">5.1 国会法案语料库</h2>
<p>我们使用了一个新的数据集，其中包括1993年至2014年间在众议院和参议院提出的97,221项美国国会法案。我们通过刮取国会图书馆的网站来创建这个数据集。17 我们使用斯坦福大学的CoreNLP对法案进行标记和POS标签。我们删除了数字和标点符号，并丢弃了所有出现在少于五个法案中的术语。我们还从国会法案项目（Adler和Wilkerson，2014年）中为每个法案增加了其作者、最终结果（例如，它是否通过了委员会的审议，是否通过了参议院的投票）以及其主要议题领域（Purpura和Hillard，2006年）。在我们的案例研究中，我们专注于2013年和2014年之间提出的488项法案中的一个子集，这些法案主要是关于法律和犯罪。我们选择这个子集是因为我们预计它将清楚地突出党派的政策差异。例如，这些法案包括有关移民执法和监禁低级别罪犯的立法--在这两个领域，民主党人和共和党人往往有非常不同的政策偏好。</p>
<h2 id="党派术语">5.2 党派术语</h2>
<p>我们使用NPFST从法案中提取短语，然后使用信息量大的Dirichlet18特征选择17http://congress.gov/，为每个党派创建了术语的排名列表。18为了降低无信息量的高频术语的z-cores，我们将Dirichlet超参数设置为与我们完整的法案数据集的术语计数成比例。Monroe等人（2008）的方法。该方法为每个术语计算了一个z分数，反映了该术语与民主党人而非共和党人的关联程度--正z分数表示民主党人更有可能使用该术语，而负z分数表示共和党人更有可能使用该术语。我们合并了每个党派排名最高的术语，只有在一个术语是另一个术语的子串，以及这些术语很可能在一个法案中共同出现的情况下，才将这些术语汇总起来，19 以形成代表性术语的排名表。最后，为了进行比较，我们还使用了同样的方法来创建单字词的排名列表，每个党派都有一个。图3描述了z-score与术语数的关系，而表4列出了排名最高的20个术语。单词列表表明，民主党立法者更关注与精神健康、少年犯和可能的家庭暴力有关的立法，而共和党立法者更关注非法移民。然而，许多排名最高的单字在脱离其周围环境后是非常模糊的。例如，我们不知道 "国内 "是指 "家庭暴力"、"国内恐怖主义 "还是 "国内项目"，如果不对原始资料进行人工审查，我们就不知道这些术语是指什么。</p>
<p>https://aclanthology.org/W16-5615.pdf</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>统计学的collocation方法提供了另一种方法（例如Dunning（1993），Hannah和Wallach（2014））。这些方法专注于within-n-gram statistical dependence。在非正式分析中，我们发现它们对低频短语的召回率并不令人满意，但将全面比较推迟到未来的工作中。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>句法结构预测的另一种类型是NP chunking。这产生了一个较浅的、non-nested的表示法<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>英语网络树库（LDC2012T13）有一些更多的嵌套结构，OntoNotes（第5版，LDC2013T19）包括一个带有Vadas和Curran（2011）的nested NP标注的Penn树库变体。我们期待着在这些数据源上训练的成分分析器的出现。<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>用于从文本中半监督的网络论点语料库的清洗</title>
    <url>/paper-5/</url>
    <content><![CDATA[<p>辩论门户网站和类似的网络平台构成了计算论点研究及其应用的主要文本来源之一。虽然建立在这些来源上的语料库具有丰富的论点相关的内容和结构，但它们也包括与它们的目的不相关的，甚至是有害的文本。在本文中，我们提出了一种以精度为导向的方法，以半监督的方式检测这种不相关的文本。给出一些种子例子，该方法自动学习相关和不相关的基本词汇模式，然后从与这些模式匹配的句子中逐步引导出新的模式。在现有的有40万个争论性文本的args.me语料库中，我们的方法检测出了近87000个不相关的句子，根据人工评估，其精确度为0.97。只需较少的努力，该方法就可以适用于其他网络论点语料库，为提高语料库质量提供了一种通用方法。</p>
<h1 id="引言">1 引言</h1>
<p>计算论点研究为支持意见形成的应用奠定了基础，包括论点搜索引擎（Wachsmuth等人，2017b）、集体商议（Uszkoreit等人，2017）和辩论技术（Toledo等人，2019）。这类应用依赖于大量最新的论据库，除了在网络上几乎找不到其他地方。最常用的网络论据来源之一是辩论门户网站，人们在那里共同收集论据或就确定的问题相互辩论。辩论门户网站和类似的网络平台有丰富的论点相关内容和结构，包括论点以及事实、背景信息和类似内容。这使得研究人员能够以远距离监督的方式抓取大规模的论点语料（Al-Khatib等人，2016）。</p>
<p>然而，在辩论门户网站上发现的文本也包括辩论专用语言和模板文本，这些文本很可能与上述应用无关，甚至是有害的。例如，在图1的文本中，作者定义了辩论的问题（第2句），陈述了一个论点（第3-5句），并提出了两个论点（第6-8句，第9-13句）--所有这些都可以被认为是与辩论有关的。相比之下，#1、#14和#15句子没有增加任何重要内容，只是做了元评论和表达了感激之情。在其他情况下，不相关的文本包括敬语、侮辱、纯粹的修辞动作和垃圾邮件。正如第2节所详述的，寻找这类文本与寻找非论点文本片段不同，因为后者可能仍然与论点片段的背景有关，如图1中的第2句。许多现有的依靠辩论门户网站的方法并没有将抓取的论据从不相关的文本中清除。例如，到目前为止，论点搜索引擎args.me（Wachsmuth等人，2017b）只是将完整的显示文本作为查询 "同性恋婚姻 "的一个支持性论点返回。这至少损害了用户体验，在某些情况下，它甚至可能腐蚀意见形成的支持。</p>
<p>在本文中，我们研究了如何在网络论点中找到不相关的文本，如来自辩论门户网站的论点，以便在此基础上清理各自的语料库。特别是，我们开发了一种半监督的学习方法，旨在以非常高的精度检测尽可能多的不相关的句子，也就是说，几乎没有任何相关的句子应该被归类为不相关（第3节）。给定一个种子句子集，该方法学习基本的词汇n-gram模式，这些模式经常与相关或不相关的句子匹配文本，并保留所有具有某种最小精度的模式（根据所有匹配句子估计）。基于给定语料库中的所有匹配句子，它将引导新的模式，修改以前的模式，并逐步重复这个过程。最后一组不相关模式被用来清理语料库。</p>
<p>我们在args.me语料库（Ajjour等人，2019）上分析了我们的方法，该语料库由来自四个辩论门户网站的387,606个论据组成，比我们所知的任何其他可用语料库都多（第4节）。在探索不同类型的词汇模式时，我们发现忽略停止词的单词n-grams在区分相关句子和不相关句子方面作用最大。我们从最频繁的此类n-grams中，手动选择了一组种子句子。然后，我们运行引导过程，分析该方法在不同迭代中发现的模式，并以自动方式和与三位人类标注者对600个句子进行的人工标注研究来评估其精确度（第5节）。在Fleiss'κ协议为0.50的情况下，我们的方法以0.97的精度检测出不相关的句子，在来自args.me语料库的68,814个论据中共有86,916个。我们向社区提供一个经过清理的语料库版本。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>最后，我们讨论了如何采用我们的方法来提高网络论点语料库的质量，而不仅仅是所研究的那个（第6节）。总的来说，本文的贡献有三个方面：</p>
<ul>
<li>一种半监督的方法来检测网络论点中的不相关句子。</li>
<li>网络论点中的几个常见的相关和不相关的词汇模式。</li>
<li>一个最大的可用论点语料库的清洁版本，其中的不相关文本明显减少。</li>
</ul>
<h1 id="相关工作">2 相关工作</h1>
<p>最初，关于论点挖掘等任务的研究主要是在小型的、精心策划的文本集上进行的，包括维基百科文章（Aharoni等人，2014）、学生作文（Stab和Gurevych，2014）、纯论点（Peldszus和Stede，2015）和总统辩论（Lawrence和Reed，2017）。然而，计算论点的主要现实世界应用，需要扩展到网络背景下才能实现其目的。这包括在有争议的问题上反对正反双方论点的搜索引擎（Wachsmuth等人，2017b），与人类辩论的技术（Toledo等人，2019），等等。</p>
<p>为了获得网络论点，许多工作都依赖于抓取的辩论门户网站和类似的网络平台，通常是以远程监督的方式，论点结构和类似的标注直接来自可用的元信息（Al-Khatib等人，2016）。以这种方式建立的语料库基于几个辩论门户网站，包括4forums.com（Walker等人，2012）、idebate.org（Cabrio和Villata，2012）、createebate.com（Habernal和Gurevych，2016）、debate.org（Durmus和Cardie，2019）和reddit.com/r/changemyview（Egawa等人，2020）。自然，减少对所获得的网络文本的策划是以更多的噪音为代价的，这反过来要求对所获得的语料库进行清洗。</p>
<p>一些关于论点语料库的出版物描述了清理过程，但大多只提到了获得的标注（Habernal和Gurevych，2016；Toledo等人，2019；Gretz等人，2020）。相比之下，本文的目标是对语料库文本本身进行清理。只有少数作品详细描述了各自的清理步骤。其中，Al-Khatib等人（2016）从抓取的论据中删除了特殊符号和辩论中的特定短语，如 "this house"，Habernal和Gurevych（2017）删除了辩论帖中对以前帖子的引用。Wachsmuth等人（2017b）为争论搜索引擎args.me完全摒弃了某些类型的噪音实例，但原始相关语料库（Ajjour等人，2019）中的文本仍然包含许多不相关的文本，正如我们的实验将揭示的那样。应用我们的方法导致了该语料库的改进版本。在本文中，我们介绍了一种用于语料库清理的半监督学习方法。一般来说，我们遵循成功的模式挖掘方法的引导思想，比如<strong>DIPRE（Brin，1998）、Snowball（Agichtein和Gravano，2000）和Espresso（Pantel和Pennacchiotti，2006）</strong>。虽然这些方法的目标是语义上的相关信息，但我们在作者的论点话语中区分了实用性相关和不相关的文本。我们不知道在这个方向上有什么其他的方法。在这方面值得注意的是，目前的清理任务与论点性文本的单元分割有明显的不同（Ajjour等人，2017）。虽然所有的论点单元都符合这里考虑的相关性概念（在第3节中定义），但如果非论点单元提供事实、定义或其他背景信息作为论点单元的背景，也可以被视为相关。因此，我们的相关性概念涉及到与某些结论有关的局部相关性，而不是论点性陈述在讨论问题时的整体相关性（Wachsmuth等人，2017a）。 # 3 方法 本节介绍了我们<strong>检测网络论点中不相关文本的半监督学习方法</strong>，以及在此基础上清理各自的语料库。该方法旨在以超过阈值<span class="math inline">\(τ\)</span>（在第5节中，我们使用τ=0.95）的估计精度找到尽可能多的不相关文本单元。为此，<strong>它学习了经常出现在不相关单元中而很少出现在相关单元中的语言模式</strong>（或者相反）。<strong>之后，我们将每个句子视为一个单元</strong>，但其他粒度原则上也可以。图2概述了该方法的三个主要阶段，下面将详细介绍每个阶段：</p>
<p><strong>(a) 种子模式选择。</strong>给定一个语料库作为输入，从它的单元中挖掘出一个常见的语言模式库，从中手动选择表示不相关和相关的种子模式。</p>
<p><strong>(b) 模式引导。</strong>检索所有与任何种子不相关（相关）模式相匹配的单元，从这些单元中挖掘新的候选模式，并将其添加到池中。然后，只有高精度的无关性（相关性）模式被保留在池中，即那些几乎只在不相关（相关）单元中发现的模式。这个过程重复进行，直到没有发现新的模式或经过k次迭代。</p>
<p><strong>(c) 语料库清洗。</strong>最后的不相关模式库被用来自动从语料库中删除不相关的单元。重要的是要看到，相关性模式最终并不用于实际的清理工作。它们只是用来区分相关和不相关的单元，从而帮助识别高精度的不相关模式。虽然我们特别为网络论点设计了我们的方法，但请注意，我们所设计的方法在很大程度上是通用的，可以很容易地转移到其他可以区分相关和不相关单元的清理任务上。使我们的方法特别适用于网络论点的是我们所说的论点相关性和不相关性。</p>
<h2 id="论点的相关性和非相关性">3.1 论点的相关性和非相关性</h2>
<p>我们在这里从使用语料库中的单个论点来对人们如何论点进行实证分析或用于论点搜索和辩论技术等应用的角度考虑相关性。对于这样的用例，从论点序列中出现的特定门户辩论结构以及与基本辩论相关的纯粹修辞动作都不值得关注。因此，我们将无关性定义如下：</p>
<p><strong>论点的无关性</strong>。当且仅当网络论点的一个单元不代表任何主张、证据、事实、背景信息或与文本作者所讨论的问题相关的类似声明时，就可以说它是不相关的。不相关单位的例子包括对辩论的元评论、敬语、感谢的表达、个人侮辱、纯粹的修辞动作和垃圾邮件。任何不符合定义的单位都被认为是相关的。虽然我们也可以定义论点的相关性，但我们决定把重点放在不相关的单元上，因为它们构成了要检测的目标概念。换句话说，鉴于我们的目标是论点语料，我们希望不相关的单元是例外而不是默认。第4节将对我们实验中处理的数据的不相关单元的比例进行估计。</p>
<h2 id="种子模式的选择">3.2 种子模式的选择</h2>
<p>阶段（a）的目标是获得一个与文本单元相匹配的语言模式库，这些单元可以被认为是不相关或相关的。所有与这些种子模式相匹配的单元的集合就代表了模式引导开始的基础真实数据。在我们的方法中，种子模式的选择是唯一需要一定程度监督的步骤。为了最大限度地减少人工劳动，我们建议半自动地进行选择，<strong>也就是说，我们首先从样本数据中自动挖掘最有希望的候选模式（我们在第5节中使用给定语料库的10%的随机样本）</strong>。然后，我们<strong>手动将其中的一个子集分类为不相关或相关的种子模式</strong>。然而，要做到这一点，我们需要首先定义什么是候选模式。</p>
<p><strong>候选模式</strong>。一般来说，任何类型的语言模式都可以从语料库文本中挖掘出来，对于这些文本有相应的挖掘方法。由于我们希望给定的相关性概念在很大程度上可以只根据一个单元的词来评估，所以我们在这里把我们的观点限制在基本的词汇模式上。为了简单起见，我们只寻找n-grams，但我们探讨了在做出两种选择后出现的四种模式。</p>
<ul>
<li><p><strong>数值与TF-IDF。</strong>在计数的情况下，我们只是看到m个最频繁的n-grams作为每个n的候选人。在TF-IDF的情况下，我们采取那些在样本数据中具有最高TF-IDF得分的n-grams（每个单元是一个文档）。在我们的实验中，我们使用m=100，n∈{1, . . . , 5}.</p></li>
<li><p><strong>有/无停止词与有/无停止词</strong>。我们要么根据完整的单元文本确定n-grams（w/ stopwords），要么在之前应用停止词去除（w/o stopwords）。</p></li>
</ul>
<p>由于高的TF-IDF分数通常表示内容，各自的模式可能对相关句子比不相关句子更有用。不过，它们是否比基于计数的模式更有优势还很难预测。在第5节中，我们将这四种模式相互比较。给出每个n的首选模式类型的所有m个候选模式（例如，<span class="math inline">\(Counts \space w/o \space stopwords\)</span>），然后本文的作者就每个候选模式手动同意是否将其选为不相关模式、相关模式，或者两者都不选。</p>
<h2 id="模式引导">3.3 模式引导</h2>
<p>阶段（b）的目标是使用引导法逐步扩展无关性和相关性模式库，即通过从与池中当前模式相匹配的单元中推导出新的模式。这个全自动的过程一直持续到不再有新的模式被发现，或者直到最大的迭代次数<span class="math inline">\(k\)</span>过去，例如，如果运行时间是一个因素的话（在第5节，我们一直持续到最后）。</p>
<p>特别是，第一步是检索所有与任何无关模式相匹配的单元集，以及从语料库中检索所有与任何相关模式相匹配的单元集2，如图2所示，这些单元集用于两个目的。首先，从不相关（相关）单元的集合中挖掘新的候选不相关（相关）模式，并添加到模式库中。第二，只有那些表示不相关（相关）单元的模式被过滤并保留在池中，其估计精度p≥τ。我们对p的估计如下：</p>
<p><strong>估算的精确度</strong>。让tp是所有检索到的与特定不相关（相关）模式相匹配的不相关（相关）单元的数量，让f p是所有与该模式相匹配的相关（不相关）单元的数量。然后，该模式的精度被估计为p = tp / （tp + f p）。</p>
<p>对于挖掘步骤，需要决定的一个参数是一个模式的最小频率，以便将其视为一个候选人。我们建议从种子模式的频率中得出这个参数的值。例如，如果所有的种子模式在样本中至少有20个匹配，而完整的语料库有10倍的样本量，那么一个合理的值可能是20 - 10 = 200。对于过滤步骤，两个单元集的大小保持平衡是有利的，因为不平衡的大小会降低tp和f p值的可比性。因此，我们建议根据不相关单元的估计比例来调整最小数字。例如，如果相关单位的数量是不相关单位的10倍，那么合理的数值可能是200个不相关单位，200-10=2000个相关单位（这里给出的数字是我们在第4和第5节中使用的示例）。另一种方法是根据经验来测试和调整这些参数。</p>
<p>概述的引导过程的一个重要特点是，在以前的迭代中加入到池中的模式可能会在以后再次从池中删除。这是因为在这个过程中，检索到的相关单位和不相关单位的集合发生了变化，这反过来又可能改变模式的精度估计。这可以理解为我们的方法的内部修订机制，以优化最终池的精度。我们在第5节的实验中看到了这一机制的效果。</p>
<h2 id="语料库清理">3.4 语料库清理</h2>
<p>阶段（c）的目标是根据最终的非相关模式库，实际清理给定的语料库。相关性模式在这个阶段不再发挥任何作用；它们只是在之前被使用，以便能够帮助以高精确度识别不相关模式，如前所述。一个简单的清理方法是直接从语料库中删除所有与任何无关性模式相匹配的单元。然而，我们建议将删除的范围限制在第一个和最后一个相关单元之前的不相关单元。只要只删除那些真正不相关的单元，我们就可以避免对论点的连贯性产生负面影响。此外，就图1的例子而言，我们将看到大多数不相关的单元确实存在于文本的开头和结尾，也就是说，建议的限制只在一定程度上减少了回忆。请注意，这并不意味着开头和结尾的大多数单元是不相关的；根据我们上面的讨论，我们期望大多数文本根本不包含不相关的单元。下一节将证明，对于我们手头的语料库来说，这一点是真实的。</p>
<h1 id="数据">4 数据</h1>
<p>所提出的方法针对的是不同质量的争论性语言，正如在网络语料库中经常看到的那样。下面，我们评估它对args.me语料库（Ajjour等人，2019）的影响，据我们所知，这是迄今为止最大的可用论点语料库，文件大小约为7.3GB。该语料库代表了参数搜索引擎args.me（Wachsmuth等人，2017b）的基础数据库。它包含387,606个</p>
<p>这些论据是通过远程监督从四个辩论门户网站挖掘出来的：debate.org、debatewise.org、idebate.org和debatepedia.org。每个论点都由一个非常简短的结论和一个明显较长的前提组成，后者包含实际的论点文本。总的来说，这个语料库跨越了大约七百万个句子。在我们的方法中，我们将每个句子视为一个单元。args.me语料库中的许多文本包括与实际论点无关的句子，如图1中的例子。不用说，虽然没有给出关于无关性的基础真实信息。为了粗略估计不相关句子的比例，我们进行了一项试验性研究，本文的两位作者按照第3节的定义，独立决定一组句子的相关性。特别是，我们考虑了Alshomary等人（2020）之前使用的语料库样本，其中包含了前10个查询中的前5个正方和前5个反方论据。在这100个样本论据的1294个句子中，我们中的一个人将147个（11.3%）归为不相关，另一个人将139个（10.7%）归为不相关。就Cohen's κ而言，我们的标注者之间的一致性达到了0.75。总共有175个句子（13.5%）被我们中的任何一个人认为是不相关的，111个（8.5%）被我们两个人认为是不相关的。由于我们认为，在有疑问的情况下，一个句子应该被认为是相关的，我们把8.5%作为我们的估计。因此，在整个语料库中，我们预计大约有60万个句子是不相关的。这111个句子只来自100个论据中的39个。假设这个数字是有代表性的，那么语料库中大约有15万个论据应该包含不相关的句子。在下面的实验中，这些数字将使我们对我们的方法的召回率有一个大致的了解。在这里，我们使用所有语料库论据的10%的随机样本进行种子模式选择，并在所有后续步骤中使用整个语料库。</p>
<h1 id="评估">5 评估</h1>
<p>我们现在报告将第3节中的方法逐步应用于第4节中的语料库，并对获得的结果进行人工评估。我们的目标是评估该方法对基于网络的论点语料库的质量的影响。我们假设，该方法能够检测出大量的不相关的句子，其精度与内部精度阈值τ一样高。</p>
<h2 id="对种子模式选择的启示">5.1 对种子模式选择的启示</h2>
<p>为了了解哪种模式类型最适合检测不相关的句子，我们比较了第3节中讨论的两种选择（Counts vs. TF-IDF，w/ or w/o stopwords）中出现的所有四个候选模式。对于每种类型，我们检索了前100个n-grams，n∈{1, . . . , 5}，涵盖了基本论点中争论的大量问题。然后，本文的两位作者都对所有2000个结果模式进行了判断，以确定它们是否可能表示总是不相关的句子或总是相关的句子。根据我们都同意的模式，选择最有希望的类型作为种子模式。例如，表1列出了每个模式类型中分别表示相关或不相关的前1-5个词组。我们排除了诸如 "wonyou wonyou wonyou "和 "kfc kfc "这样的垃圾模式，因为它们会限制人们的洞察力，占据最重要的位置；每种模式类型的完整列表在补充材料中给出。对于这两种TF-IDF模式类型，我们发现相关性模式明显地达到了它们的目的，与论据的内容有关。在完整的列表中发现了许多这样的模式。然而，很少有TF-IDF模式似乎能可靠地表明不相关。这符合人们的直觉，即具有高TF-IDF分数的短语是针对文档内容的，而不是反映一般的语言。相比之下，两种计数模式产生了几种不相关模式，如表所示。我们决定采用Counts w/o Stopwords，因为它产生的模式澄清了许多Counts w/ Stopwords留下的模糊不清的情况。例如，"想感谢对手 "揭示了与源辩论门户（这里是debate.org）的无关性，而相应的带停止词的模式（"想感谢"，"想感谢我的"）则对相应句子的无关性留下了更多疑问。表2列出了38个相关类型和17个不相关类型的种子模式。如果一个模式是多余的，也就是说，如果它已经被一个更短的模式所覆盖，例如，"第一轮接受 "已经被 "第一轮 "所覆盖，则该模式不包括在内。我们观察到，没有一个1-gram进入不相关模式库；一个词似乎不足以确定不相关。然而，从长度2来看，我们判断有几个模式是足够可靠的无关性指标，最频繁的模式在语料库中出现了超过10,000次，即 "第一轮 "和 "感谢对手"。</p>
<h2 id="对模式引导的启示">5.2 对模式引导的启示</h2>
<p>如第3节所述，我们将τ设定为0.95，保留所有挖掘出来的至少有2000个匹配的相关模式作为候选，以及所有挖掘出来的至少有200个匹配的不相关模式。给出种子模式后，我们运行引导过程，直到不再有新的模式被发现，这发生在迭代6。在一台标准的计算机上（英特尔酷睿i7，2.7GHz，16GB内存），整个过程大约需要两个小时。表3显示了每个迭代（和种子模式选择）的关键统计数据。在相关性的情况下 这38个种子模式已经匹配了超过60万个不同的句子，平均估计精度为1.00，也就是说，它们几乎从未与为种子无关模式检索到的任何句子相匹配。在迭代2中，第3节中讨论的修订效应已经开始：57个相关句子被删除，因为它们也与新挖掘的不相关模式相匹配。尽管如此，相关模式的集合仍然是稳定的，而且这种行为在随后的迭代中继续。对于不相关模式，我们观察到在前五次迭代中模式库的单调增长，除了种子句子之外，在迭代1-5中还有超过1万个不同的句子被检测为不相关。总共发现了122个模式；它们的平均估计精度在所有迭代中至少保持在0.97。为了分析我们的方法在引导过程中的行为，我们随机选择了600个不相关的句子进行人工评估（见补充材料）：100个与种子不相关模式相匹配的句子，以及五个迭代中的不相关模式各100个。相关模式被忽略了，因为它们不需要用于语料库的清理。我们将所有句子的排序随机化，并将它们独立交给三位具有计算论点背景的标注者，他们都不是本文的作者（一位硕士生和两位博士生；两位男性，一位女性；欧洲、中东和东亚各一位）。我们要求标注者根据第3节的定义，将每个句子分类为相关或不相关。批注者得到了一些直观的指导（见补充材料），并可以事先提出问题。我们观察到，就Fleiss'κ而言，标注者之间的一致性为0.50，这似乎是合理的，因为相关性评估本身就是主观的（Croft等人，2009）。考虑到标注，我们计算了我们的方法在每个迭代中检测不相关句子的平均精度，一次是多数人同意（如果两个标注者这么说，则不相关正确），一次是完全同意（所有三个人都这么说）。表3的最右边一栏显示了结果，揭示了多数人同意的精确度在迭代2结束前是完美的。虽然接下来的两次迭代仍然很有希望，但在最后一次迭代中，精度下降到0.88（完全一致时为0.79），这表明模式会随着时间的推移而变差。因此，早期终止可能是有利的，但最佳时刻在实践中自然是未知的。52,849个不同的句子最终被检测出的无关性模式所匹配，总体精度为0.97。其中有些句子出现了多次，导致总共有86,916个不相关的句子，来自68,814个论点。根据第4节中粗略估计的不相关比例，不相关句子的召回率约为0.15，不相关句子的论据约为0.46。仅仅种子步骤就发现了71,926个不相关的句子，也就是说，召回率大约为0.12。如果我们将种子步骤视为完整方法的基线，我们会发现精确度下降了3%（1.00到0.97），但是召回率增加了20%（0.12到0.15）。虽然可以说还有优化的余地，但我们仍然得出结论，这些结果支持了我们的方法的影响，并由此支持了我们的假设。</p>
<h2 id="对语料库清洗的启示">5.3 对语料库清洗的启示</h2>
<p>基于最终的122个无关性模式，我们探索了对给定语料库的清洗潜力。图3(a)显示了具有一定数量被检测到的不相关句子的语料库文本的柱状图。我们看到，大多数文本只包含一个这样的句子，除了六种情况外，其余都是七个或更少。这些六个案例都有超过30个不相关的句子；人工检查发现，这些句子都含有相同词序重复的垃圾邮件。在图3(b)中，我们绘制了不相关句子在语料库文本中的位置。正如预期的那样，它们中的大多数都出现在开头或结尾。由于我们讨论过的只丢弃这些句子的限制，最终从args.me语料库中删除的句子数量达到了53,502个（在48,089个论据中发现）。除了原始的args.me语料库之外，我们现在还提供了经过清理的语料库版本，网址是：https://webis.de/data.html#args-me-corpus。</p>
<h1 id="结论">6 结论</h1>
<p>基于网络的论点语料库在计算论点研究及其应用中发挥了重要作用。不过，这种语料库中并非所有的文本都与论点有关。在本文中，我们提出了一种方法，可以在低监督下检测出论点文本中的不相关文本单元。该方法从匹配已知模式的单元中反复引导出不相关和相关的语言模式。在args.me语料库中的387k个论据中，该方法检测出87k个不相关的句子，精确度为0.97，其中至少有53k个句子可以被删除，而不会明显降低论据的一致性。这些结果证明了我们的方法在提高语料库质量方面的潜力。当然，该方法也有局限性。一方面，结果显示，在所采用的配置下，很大一部分检测到的句子来自于种子模式。为了获得好的种子模式，需要人工努力。另一方面，我们的方法的召回率似乎并不高，就我们从检查的数据中可以估计到。虽然不是所有的不相关单元都能被我们考虑的简单模式所捕获，但另一个原因可能在于我们的限制，即只发现新的候选模式出现在与先前模式相匹配的句子中。特别是那些只出现在短单元中的模式，如果它们没有被种子模式所覆盖，就可能会被忽略掉。改进措施可以考虑与不相关单元相邻的单元，但是这可能要以降低精确度为代价。在这方面，请注意我们的方法的影响在某种程度上取决于是否有可靠的单元边界检测器（例如，句子分割器），这对嘈杂的网络数据来说不是一个简单的要求。最后，一个产生的问题可能是将该方法应用于这里处理的数据之外的其他数据有多复杂。按照我们提出的自动获得频繁的候选种子模式的过程，主要的人工努力可以归结为在这些候选模式中找到可靠的种子模式。在我们的案例中，这只花了几个小时，鉴于对语料库质量的潜在影响，这似乎可以忽略不计。此外，只需要根据手头的数据对方法的参数进行一些初步的调整。因此，我们相信该方法可以很容易地被用于清理其他论点语料（包括口语论点语料的转录语料），以及其他许多清理任务，其中文本单元的无关性可以用一种可衡量的方式来定义。</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Both the original and the cleaned args.me corpus are found at: https://webis.de/data.html#args-me-corpus<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>期刊翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>语义学最终</title>
    <url>/semantics-final/</url>
    <content><![CDATA[<h2 id="vendler-classes"><strong>Vendler classes</strong></h2>
<p>The philosopher Zeno Vendler described types of verbs based on their inher ent <strong>ASPECTUAL</strong> differences. The classes he distinguished can be broken down <strong>COMPONENTIALLY</strong> according to three dichotomies, as shown in the table below. Vendler’s classification remains one of the most popular in describing types of <strong>SITUATIONS</strong>.</p>
<p>While Vendler intended his categories as categories of verb meanings, it has since been noted that <strong>TELICITY</strong> in particular is sensitive to the <strong>ARGUMENTS</strong> of the verb. For example, in the table the predicate sing is classified as an activity, but when it is joined with a <strong>BOUNDED</strong> noun phrase like three songs, it is an achievement, since the singing of three songs has a conclusion – at the end of the third song.</p>
<h2 id="denotational-versus-representational"><strong>Denotational versus representational</strong></h2>
<p>Semantic approaches can be divided into two main types. <strong>Denotational</strong> approaches attempt descriptions of the relation between language and the world – that is, between words or other expressions and the things or situations that they refer to (see <strong>DENOTATION</strong>). <strong>Representational</strong> approaches try to model how meaning is represented in the human mind – in other words, they are <strong>mentalistic</strong> in nature.</p>
<h2 id="sense"><strong>Sense</strong></h2>
<p>The term <strong>sense</strong> is one of two aspects of meaning, the other being <strong>REFERENCE</strong>. While reference is what an expression points to in the world, sense is the semantic aspect of meaning – the definitional properties that determine which things are referred to when an expression is used. <strong>Sense</strong> can also refer to a <strong>DEFINITION</strong> in a dictionary.</p>
<h2 id="semiotic-triangle"><strong>Semiotic triangle</strong></h2>
<p>For the purposes of linguistics, we can isolate three particularly important factors relevant to the study of meaning: the psychology of speakers, which creates and interprets language, the referent of the language expression as projected by the language user’s psychology, and the linguistic expression itself: these three points constitute the semiotic triangle.</p>
<h2 id="sense-reference-denotation-and-connotation">Sense, reference, denotation and connotation</h2>
<p>referent on any one occasion of use,</p>
<p>denotation: the set of all its referents</p>
<p>sense: the abstract, general meaning which can be translated from one language to another, paraphrased, or defined in a dictionary.</p>
<p>Connotation names those aspects of meaning which do not affect a word’s sense, reference or denotation, but which have to do with secondary factors such as its emotional force, its level of formality, its character as a euphemism, etc.</p>
<h2 id="explanations-of-meaning-in-terms-of-meanings-are-circular-and-four-ways-of-breaking-the-circle">Explanations of meaning in terms of meanings are circular and Four ways of breaking the circle</h2>
<p>All definitions of meaning in language, therefore, are ultimately circular because they use one kind of meaning to explain another.</p>
<p>There are four important answers to the question ‘what is meaning?’: the referential/denotational theory of meaning, the conceptual theory of meaning, the brain states theory and the use theory. We do not have to categorically choose between these theories. Instead, recognizing that the notion of meaning in linguistics is a way of talking about the factors which explain language use, we can see referents, concepts, brain states and uses as all relevant to this task.</p>
<h2 id="truth">Truth</h2>
<p>From this perspective, truth that is known before or without experience has traditionally been called <strong>a priori</strong>. This <strong>a priori truth</strong> is contrasted with <strong>a posteriori truth</strong>: truth which can only be known on the basis of empirical testing. Another related concept is Leibniz’s distinction between <strong>necessary truths</strong>, which cannot be denied without forcing a contradiction, for example the arithmetical statement Two and two make four, and <strong>contingent truths</strong> which can be contradicted, depending on the facts, for example the sentence The dodo is extinct. If someone unexpectedly found a dodo in a forest on Mauritius, this latter sentence would become false. It is difficult, on the other hand, to imagine circumstances in which Two and two make four would unexpectedly become false. This is similar to our a priori/a posteriori distinction but comes at truth from another viewpoint: not in terms of what the speaker knows but in terms of what the world is like. We can say that it is hard to think how our sentence about two and two making four could not be true without changing our view of the present facts of the world. From this perspective a sentence like 4.40 is also necessarily true and a contradiction like 4.41 is necessarily false. In another, related terminology tautologies like 4.39 are <strong>analytic</strong> while a sentence like My father is a sailor is <strong>synthetic</strong>. <strong>Analytic</strong> statements are those where the truth follows from the meaning relations within the sentence, regardless of any relationship with the world, while a <strong>synthetically</strong> true statement is true because it accords with the facts of the world.</p>
<h2 id="analytic">Analytic</h2>
<p>Analytic PROPOSITIONS are those that require no external verification, which is to say that their truth or falsity can be established by examining only their linguistic matter and internal logic, rather than appealing to our senses in order to verify the claims they describe. Consider, for example, the following: No dead person is alive.</p>
<h2 id="truth-1">Truth</h2>
<p>From this perspective, truth that is known before or without experience has traditionally been called a priori. This a priori truth is contrasted with a posteriori truth: truth which, as in our examples 4.17 and 4.18 earlier, can only be known on the basis of empirical testing. Another related concept is Leibniz’s distinction between necessary truths, which cannot be denied without forcing a contradiction, for example the arithmetical statement Two and two make four, and contingent truths which can be contradicted, depending on the facts, for example the sentence The dodo is extinct.</p>
<h2 id="presuppose-presupposition">Presuppose, presupposition</h2>
<p>A presupposition is a proposition that must be supposed to be true in order for another proposition to be judged true or false. For example, The king of France is bald presupposes the proposition that ‘there is a king of France.’ Unlike ENTAILMENTS, the presupposition remains the same when the sentence is negated. So, The king of France is not bald still presupposes that ‘there is a king of France.’</p>
<h2 id="anaphor-anaphora-anaphoric">Anaphor, anaphora, anaphoric</h2>
<p>In anaphora, a linguistic expression (called an anaphor or anaphoric pronoun) is understood to have the same REFERENCE as another linguistic expression (its antecedent), which typically precedes it in the same sentence or in the earlier discourse.</p>
<h2 id="anomalous-anomaly">Anomalous, anomaly</h2>
<p>A semantically anomalous linguistic expression is one that has an abnormal meaning or fails to make sense, despite being grammatically well-formed.</p>
<p>Semantic anomalies are sometimes accounted for as violations of SELECTIONAL RESTRICTIONS that words place on other words that they occur with. Thus The telephone ate my gingerbread is anomalous because the verb eat is restricted to only occur with subjects that refer to animate beings – unless the expression is to be interpreted FIGURATIVELY.</p>
<h2 id="antecedent">Antecedent</h2>
<p>An antecedent is something that comes before something else. In reference to ANAPHORA, it refers to an earlier expression to which a pronoun CO-REFERS.</p>
<h2 id="argument">Argument</h2>
<p>In a PROPOSITION or SENTENCE, (semantic) arguments are the participants in the EVENT or STATE expressed by the PREDICATE.</p>
<p>Argument In a PROPOSITION or SENTENCE, (semantic) arguments are the participants in the EVENT or STATE expressed by the PREDICATE. Within a sentence/proposition, arguments play particular roles, called SEMANTIC ROLES. The number and type of arguments that a predicate requires and their syntactic realization is known as argument structure.</p>
<h2 id="aspect-aspectual">Aspect, aspectual</h2>
<p>The SITUATIONS described by PROPOSITIONS can take place in time in different ways. The internal organization of a situation with relation to time – that is, how the situation unfolds in time or the temporal perspective taken on the situation – is its aspect as opposed to its TENSE, which is the when of the situation.</p>
<h2 id="causative-alternation">causative alternation</h2>
<p>In linguistics, causative alternation is a phenomenon in which certain verbs that express a change of state (or a change of degree) can be used transitively or intransitively. A causatively alternating verb, such as "open", has both a transitive meaning ("I opened the door") and an intransitive meaning ("The door opened"). When causatively alternating verbs are used transitively they are called causatives since, in the transitive use of the verb, the subject is causing the action denoted by the intransitive version. When causatively alternating verbs are used intransitively, they are referred to as anticausatives or inchoatives because the intransitive variant describes a situation in which the theme participant (in this case "the door") undergoes a change of state, becoming, for example, "opened".</p>
<h2 id="deictic-deixis">Deictic, deixis</h2>
<p>Deixis refers to the phenomenon where the meaning of some linguistic item relies inherently on the extralinguistic CONTEXT. Thus understanding deictic expressions like that book, here, yesterday or I depends on knowing where the utterance is spoken, when it is spoken and who is speaking.</p>
<h2 id="demonstrative">Demonstrative</h2>
<p>Demonstratives are expressions such as this, that, those and these. A distinction can be made between demonstrative determiners and demonstrative pronouns – the former occur with a noun in a noun phrase (this book, those boxes), while the latter substitute for a whole noun phrase and therefore stand alone (This is nice.).</p>
<h2 id="denotational-versus-representational-1">Denotational versus representational</h2>
<p>Semantic approaches can be divided into two main types. Denotational approaches attempt descriptions of the relation between language and the world – that is, between words or other expressions and the things or situations that they refer to (see DENOTATION). Representational approaches try to model how meaning is represented in the human mind – in other words, they are mentalistic in nature. Denotational approaches tend to derive from philosophy and mathematical logic (and thus FORMAL SEMANTIC approaches are often denotational), but much of the modern linguistic tradition in semantics is more representational than denotational – for example, COGNITIVE LINGUISTICS, CONCEPTUAL SEMANTICS and NATURAL SEMANTIC METALANGUAGE approaches are all representational.</p>
<h2 id="encyclopaedic-meaning">Encyclopaedic meaning</h2>
<p>Encyclopaedic meaning is general world knowledge, known by virtue of our experience of the world.</p>
<h2 id="entailment">Entailment</h2>
<p>Entailment is the PROPOSITIONAL RELATION in which if one PROPOSITION is true, then it is always the case that the related proposition is true. This can be stated as the MATERIAL CONDITIONAL</p>
<h2 id="evidential-evidentiality">Evidential, evidentiality</h2>
<p>Evidentiality is a type of MODALITY that indicates the speaker’s source of the knowledge that is expressed in a PROPOSITION or SENTENCE – for instance, through personal observation, by hearsay or by inferring it from other knowledge. Languages like Turkish have morphological markers of evidentiality, but in English it can only be marked by lexical means (e.g. apparently) or through constructions such as be supposed to: Daisy’s stew is supposed to be delicious (i.e. ‘I have heard that it is, but I do not have experience of it myself’).</p>
<h2 id="extension">Extension</h2>
<p>The extension of an expression is the set of all potential REFERENTS of the expression with respect to some world or MODEL.</p>
<h2 id="inchoative">Inchoative</h2>
<p>Inchoative Verbs (or PREDICATES) whose meanings involve ‘beginning’ or ‘becoming’ are referred to as inchoative (also inceptive) from the Latin verb for ‘begin’. Inchoative verbs describe a change of state or a beginning of having a state. For example redden means ‘become red’ and open denotes the change from being closed to not being closed. CAUSATIVE verbs can also be considered to be inchoative.</p>
<h2 id="inclusion-inclusive">Inclusion, inclusive</h2>
<p>The terms inclusion and inclusive can apply to a number of semantic phenomena. In SEMANTIC (particularly LEXICAL) RELATIONS, a relation of inclusion is when the EXTENSION of one term is a proper subset of the extension of another. For example, the things denoted by kitten are also among the things denoted by cat (see HYPONYM). Proper inclusion is when a SUBORDINATE CATEGORY is wholly contained within another, SUPERORDINATE category. The set of blue pens is properly included within the set of pens because there is nothing that is in the set of blue pens that is not also in the set of pens.</p>
<h2 id="lexeme">Lexeme</h2>
<p>A lexeme (cf. LEXICAL ITEM) is a unit of language that is represented in the LEXICON. Like other linguistic -eme terms, lexeme refers to an abstraction from the actual spoken or written language – that is, it refers to the word as it is represented in the mind, rather than in the mouth or on the page.</p>
<h2 id="lexicon">Lexicon</h2>
<p>Traditionally, the lexicon is a collection of information about a language’s LEXEMES, that is, the expressions that are learnt by the language’s users, rather than derived anew each time they are used. The term lexicon can refer to the following: a) a dictionary, especially a dictionary of a classical language; or b) the vocabulary of a language (also known as lexis); or c) a particular language user’s knowledge of her/his own vocabulary, as stored in her/his mind – the mental lexicon.</p>
<h2 id="meaning">Meaning</h2>
<p>Linguistic meaning is, of course, the object of study in semantics. However, meaning is rarely used as a technical term in semantic study because of its POLYSEMY and generality. For example, it may be used to refer to an expression’s DEFINITION or SENSE, but it may instead be used to include non-denotational aspects of meaning, such as CONNOTATION, or to the particular INTERPRETATION of the expression’s REFERENCE in a particular CONTEXT. Where it is used, it is usually because a distinction between sense and reference is not needed in the particular discussion or it is used as a synonym for sense or interpretation.</p>
<h2 id="metalanguage">Metalanguage</h2>
<p>A metalanguage is a system used for describing a language without using that language itself. A metalanguage resolves the inevitable circularity that arises if one, for instance, uses English to describe the semantics of English. One could, in principle, use one natural language to describe another (e.g. Finnish to describe Polish). However, this has the problem that the meanings of the object language (the language being described) would not necessarily translate in an equivalent way into the other natural language used as a metalanguage. An ideal metalanguage should provide a complete and unambiguous description of the object language.</p>
<h2 id="modality-modal-verbs">Modality, modal verbs</h2>
<p>In linguistics, modality refers to the expression of a speaker’s attitude towards a PROPOSITION. This involves notions such as obligation, permission, possibility, necessity and ability. In English these notions are typically expressed via the modal verbs may, must, can, will, shall, might, could and should, or semiGRAMMATICALIZED expressions such as have to, need to or had better. Expressions of MOOD may also indicate a type of modality distinction – that between the reality and irreality of a proposition.</p>
<p>Modality can be divided into different types and, within those types, into different degrees. The most established distinction of modality types is that between deontic and epistemic modality. Deontic modality involves a duty, obligation, permission or (when negated) prohibition being imposed on someone or something. Different degrees of deontic modality are shown below: Tim must take the dog out for a walk. (obligation) Tim should take the dog out for a walk. (weaker obligation) Tim may/can take the dog out for a walk. (permission) Epistemic modality, on the other hand, relates to the speaker’s judgement of how probable the truth of the proposition is, based on some available evidence.</p>
<h2 id="montague-grammar">Montague grammar</h2>
<p>The grammar presents a COMPOSITIONAL means of representing linguistic meaning through the use of a formal system of representation (LAMBDA CALCULUS) combined with set and type theories from mathematics.</p>
<h2 id="ontological-category-ontology">Ontological category, ontology</h2>
<p>Ontology is the philosophical field that attempts to organize everything that exists into a limited number of general CATEGORIES.</p>
<h2 id="polarity-item">Polarity item</h2>
<p>A polarity item is a LEXEME that is sensitive to the POLARITY of the constituent to which it belongs. For instance, the determiner any is a negative polarity item that can only occur in negated clauses, while the positive polarity item some must occur in affirmative clauses</p>
<h2 id="predicate">Predicate</h2>
<p>In semantics, predicate refers to the part of a PROPOSITION that expresses the relation or property that is being ascribed to some entities, ARGUMENTS.</p>
<h2 id="presuppose-presupposition-1">Presuppose, presupposition</h2>
<p>A presupposition is a proposition that must be supposed to be true in order for another proposition to be judged true or false. For example, The king of France is bald presupposes the proposition that ‘there is a king of France.’ Unlike ENTAILMENTS, the presupposition remains the same when the sentence is negated. So, The king of France is not bald still presupposes that ‘there is a king of France.’</p>
<h2 id="primitive-semantic">Primitive, semantic</h2>
<p>A primitive or atomic unit is one that cannot be broken down or defined further and thus forms the most basic unit of analysis. In semantics, the notion of primitives is particularly important in COMPONENTIAL ANALYSIS, which assumes that the meanings of linguistic items are built out of smaller units of meaning, meaning components.</p>
<h2 id="pronouns">Pronouns</h2>
<p>Personal pronouns like me, she and they have DEFINITE reference, as do the DEMONSTRATIVE pronouns this, that, these and those. In English, indefinite pronouns are often similar in form to indefinite QUANTIFIERS – for example, one or some in I want some.</p>
<p>Pronouns are often used ANAPHORICALLY, which is to say that they refer to something that was mentioned previously in the discourse. But pronouns are also used for DEIXIS, which involves reference to something in the extralinguistic context. An example of this would be when a speaker says That was there while pointing first to an object and then a location.</p>
<h2 id="proper-name-proper-noun">Proper name, proper noun</h2>
<p>A proper name, or proper noun, is a nominal expression that denotes the same individual (or particular set of individuals) every time it is used. This is opposed to a COMMON NOUN, which indicates a type of thing, and therefore can be used to denote different individuals each time it is used.</p>
<h2 id="proposition">Proposition</h2>
<p>A proposition may be defined as the meaning of a SENTENCE that makes a statement about some state of affairs. As such, a proposition has a TRUTH VALUE; it can be either true or false. A proposition is independent of the linguistic structure used to express it, which is to say that the same proposition can be expressed by different sentences.</p>
<h2 id="referring-expression">Referring expression</h2>
<p>A referring expression is an expression that REFERS in a context to an individual or set of individuals. In natural language, this would usually be expressed as a NOUN phrase, but not all uses of noun phrases refer. For example, in Noam Chomsky is a linguist, Noam Chomsky refers to a particular individual, but a linguist does not refer to any particular linguist, but rather describes the PROPERTY of being a linguist.</p>
<h2 id="selectional-restriction">Selectional restriction</h2>
<p>Selectional (or selection) restrictions are constraints that determine which co-occurrences of words or meanings of words are semantically well-formed, rather than ANOMALOUS or abnormal. Selectional restrictions are generally considered to be separate from grammatical constraints such as that transitive verbs must take noun phrases as their objects.</p>
<p>One meaning of vagueness refers to the underspecification or generality of SENSE. For example, the word cousin can refer to either a male or a female, or an infant or a pensioner and it therefore has a sense that is vague or underspecified with respect to gender and age. This meaning of vagueness contrasts with AMBIGUITY, where a linguistic form has multiple distinct senses (e.g. light ‘not dark’; ‘not heavy’ or bug ‘an insect’; ‘a listening device’).</p>
<h2 id="valency">Valency</h2>
<p>Valency refers to the number of ARGUMENTS a VERB requires in order to express a complete PROPOSITION. Valency corresponds to the classification of PREDICATES into one-, two- and three-place predicates.</p>
<h2 id="middle-voice">Middle voice</h2>
<p>Some languages also have a further grammatical category of middle voice. Middle voice typically serves to emphasize that the argument that occurs in the subject position is affected by the event expressed by the verb. In English, middle voice occurs in a very limited class of examples such as the ones below: These oranges peel very easily. The tickets aren’t selling that well. Like passives, these examples involve the promotion of a non-AGENT participant into the subject position, but English does not have any special morphological marking of middle voice.</p>
<h2 id="zeugma">Zeugma</h2>
<p>In semantics, a zeugma (or syllepsis) is a linguistic construction where a single constituent is related to two different semantic interpretations, in a way that gives rise to a semantic ANOMALY.</p>
<p>For the zeugmatic, anomalous reading to arise, the two interpretations that are evoked simultaneously need to be semantically distinct. Therefore, the possibility of constructing a zeugmatic sentence provides a way of testing whether particular readings of a linguistic form are AMBIGUOUS (POLYSEMOUS or HOMONYMOUS) or just VAGUE.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>语义</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>semantics</tag>
      </tags>
  </entry>
  <entry>
    <title>SemEval 2017任务10：ScienceIE--从科学出版物中提取关键词和关系</title>
    <url>/semeval/</url>
    <content><![CDATA[<p>我们描述了SemEval任务，即从科学文献中提取关键词和它们之间的关系，这对了解哪些出版物描述了哪些过程、任务和材料至关重要。尽管这是一项新的任务，但我们在3个评估方案中共收到26份提交的材料。我们希望这项任务和本文所报告的研究结果对从事理解科学内容的研究人员，以及更广泛的知识库群体和信息提取社区都有意义。</p>
<span id="more"></span>
<p><img src="https://i.loli.net/2021/07/17/mTzI62CiqE1ca9H.png"/></p>
<p>原文：https://aclanthology.org/S17-2091.pdf</p>
<h1 id="引言">1 引言</h1>
<p>经验性研究需要获得并保持对特定领域工作的理解。例如，研究人员面临的典型问题是哪些论文描述了哪些任务和过程，使用了哪些材料，以及这些材料之间的关系如何。虽然有些领域有综述论文，但如果不阅读大量的出版物，一般很难获得这样的信息。</p>
<p>目前解决这个问题的努力是搜索引擎，如谷歌学术<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>、Scopus<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>或语义学<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>，它们主要集中在浏览作者和引文图谱。</p>
<p>这里解决的任务是关键词的提及级识别和分类，如关键词提取（TASK），以及提取关键词之间的语义关系，如关键词提取HYPONYM-OF信息提取。这些任务与命名实体识别、命名实体分类和关系提取等任务相关。然而，关键词的识别要比人名等更具挑战性，因为它们在不同的领域有很大的不同，缺乏明确的标志物 (signifiers) 和语境，而且可能由许多标记 (tokens) 组成。为此，我们制作了一个由500个出版物组成的双标注语料库，其中包括计算机科学、材料科学和物理学领域的科学文章。</p>
<p>提取关键词和它们之间的关系对科学出版商来说是非常有意义的，因为它有助于向读者推荐文章，向作者强调缺失的引文，确定潜在的审稿人，并分析一段时间内的研究趋势。请注意，根据同义词和超近义词关系来组织关键词对搜索场景特别有用，例如，读者可能会搜索有关信息提取的文章，通过超近义词预测也会收到有关命名实体识别或关系提取的文章。</p>
<p>我们希望该任务的结果与更广泛的信息提取、知识库扩展（KBP, Knowledge base population）和知识库建设社区相关，因为它为该领域研究的方法提供了一个新的应用领域，同时还提供了与领域相关的挑战。由于该数据集被标注为三个相互依赖的任务，它也可以被用作联合学习（joint learning）或结构化预测信息提取方法的测试平台（Kate和Mooney，2010；Singh等人，2013；Augenstein等人，2015；Goyal和Dyer，2016）。此外，我们希望这项任务对研究旨在理解科学内容的任务的研究人员来说是有趣的，比如关键词提取（Kim等人，2010b；Hasan和Ng，2014；Sterckx等人，2016；Augenstein和Søgaard，2017），语义关系提取（Tateisi等人，2014年；Gupta和Manning，2011年；Marsi和Ozt ¨ urk ¨，2015年），科学文章的主题分类（O S ´ eaghdha和Teufel ´，2014年），引文背景提取（Teufel，2006年；Kaplan等人。2009），提取作者和引文图谱（Peng和McCallum，2006；Chaimongkol等人，2014；Sim等人，2015）或这些的组合（Radev和Abu-Jbara，2012；Gollapalli和Li，2015；Guo等人，2015）。</p>
<p>该任务的预期影响是，由于新语料库的发布，上述研究团体对该任务之外的兴趣，导致从科学文献中提取信息的新型研究方法。提出的语料库特别有用的是对提及层面的上下位和同义词关系的标注，因为现有的上下位和同义词关系资源是在类型层面的，例如WordNet。此外，我们预计这些方法将直接影响到使出版物有意义的工业解决方案，部分原因是任务组织者与Elsevier的合作。</p>
<h1 id="任务描述">2 任务描述</h1>
<p>该任务分为三个子任务。</p>
<ul>
<li><ol type="A">
<li>提及层面的关键短语识别</li>
</ol></li>
<li><ol start="2" type="A">
<li>提及层面的关键短语分类。关键词类型有PROCESS（包括方法、设备）、TASK和MATERIAL（包括语料库、物理材料）</li>
</ol></li>
<li>C）具有相同关键词类型的关键词之间的提及级语义关系提取。</li>
</ul>
<p>使用的关系类型是HYPONYM-OF和SYNONYM-OF。我们将上述子任务分别称为副任务A、副任务B和副任务C。例1中显示了计算机科学领域数据实例的一个简短的（人工）例子，材料科学和物理学的例子在附录中。第一部分是纯文本段落（为提高可读性，关键词用斜体表示），然后是基于字符偏移的远程（stand-off）关键词标注，接着是关系标注。</p>
<blockquote>
<p><strong>Example 1.</strong></p>
<p><strong>Text:</strong> <em>Information extraction</em> is the process of extracting structured data from unstructured text, which is relevant for several end-to-end tasks, including <em>question answering</em>. This paper addresses the tasks of <em>named entity recognition (NER)</em>, a subtask of <em>information extraction,</em> using <em>conditional random fields (CRF)</em>. Our method is evaluated on the <em>ConLL-2003 NER corpus</em>.</p>
</blockquote>
<p><img src="https://i.loli.net/2021/07/16/A4qHBYorOTtwDVz.png" width="300"/></p>
<p><img src="https://i.loli.net/2021/07/16/GRyNZjHQ2p9vKku.png" width="500"/></p>
<h1 id="semeval-2017任务的资源">3 SemEval-2017任务的资源</h1>
<h2 id="语料库">3.1 语料库</h2>
<p>该任务的语料库是由ScienceDirect<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>的开放性出版物建立的，参与者可以免费使用，无需签署版权协议。每个数据实例由一段文字组成，取自一篇科学论文。出版物以纯文本形式提供，此外还有xml格式，其中包括出版物的全文以及其他元数据。从计算机科学、材料科学和物理学等领域均匀分布的期刊文章中选择了500个段落。</p>
<p>语料库的训练数据部分包括350个文件，50个用于开发，100个用于测试。这与第5节中描述的试验任务类似，其中144篇文章被用于训练，40篇文章用于开发，100篇文章用于测试。</p>
<p>我们在表1中列出了关于该数据集的统计数据。值得注意的是，该数据集包含许多长的关键词。训练集中22%的关键词由5个或更多的词组成。这使得关键词识别的任务变得非常具有挑战性。然而，这些关键词中有93%是名词短语[^7]，这对于简单的启发式方法识别关键词候选来说是很有价值的信息。最后，训练数据集中包含的31%的关键词只出现过一次，系统必须对未见过的关键词进行良好的归纳。</p>
<p>[^7 ]: POS是自动确定的，使用nltk POS标记器</p>
<p><img src="https://i.loli.net/2021/07/16/ny6tr5ajlPvK9u4.png"/></p>
<h2 id="标注过程">3.2 标注过程</h2>
<p>提及层面的标注非常耗时，而且每份出版物中只能找到少量的语义关系，如hypernymy和synonymy。因此，我们只对可能包含关系的出版物的段落进行标注。</p>
<p>为了识别适合的文档，我们最初打算通过使用Hearst风格的模式（Hearst, 1991; Snow et al., 2005）从大型科学数据集中自动提取关系的知识图谱，然后使用这些知识图谱在一组不同的文档中寻找潜在的关系，类似于遥远的监督（Mintz et al., 2009; Snow et al., 2005）启发法。然后，含有大量这种潜在关系的文档将被选中。然而，这需要自动学习识别这些潜在关系之间的关键词，并要求关系在数据集中出现几次，这样的知识图谱才会有用。</p>
<p>最后，由于学习自动检测关键词的难度，以及不同文档中的关系只有少量的重叠，这种策略是不可行的。取而代之的是，使用一个粗略的无监督方法自动检测出关键词的段落（Mikolov等人，2013），并手动选择那些可能包含关系的段落进行标注。</p>
<p>在标注方面，学习计算机科学、材料科学或物理学的本科生志愿者通过UCL的学生通讯接受雇佣，该通讯覆盖所有的学生。我们向学生们展示了标注的例子和标注指南，如果他们仍然对参与标注活动感兴趣，就要求他们事先选择要标注的文件数量。大约50%的学生在看过标注的文件和阅读了标注指南后仍然感兴趣。然后，他们有两周的时间使用BRAT工具（Stenetorp等人，2012）来标注文件，该工具作为网络服务托管在亚马逊EC2实例上。学生们对每份文件的标注得到了补偿。每个文档和标注者的标注时间估计约为12分钟，在此基础上，他们的报酬约为每小时10英镑。他们只有在完成所有标注后才能得到补偿，也就是说，补偿是以标注所有文件为条件的。标注的费用由爱思唯尔公司承担。为了制定标注指南，我们用一位标注者对20份文献进行了小规模的试验性标注，之后对标注指南进行了完善。<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>我们原本打算让学生标注者对文献进行三重标注，并对标注进行多数投票，但由于难以招募到高质量的标注者，我们选择了对文献进行双重标注，其中第二位标注者是一位专家标注者。在标注不一致的情况下，我们选择了专家的标注。表2显示了用Cohen's kappa衡量的学生标注者和专家标注者之间的成对标注协议。表中的*表示标注质量随着时间的推移而下降，最后标注者没有完成对所有文件的标注。为了说明这一点，在计算标注者之间的一致性时，没有给出标注的文档被排除在外。在完成标注工作的标注者中，Cohen's kappa在0.45和0.85之间，其中一半的标注者有0.6或更高的实质性一致。对于这项任务的未来迭代，我们建议投入大量精力来招募高质量的标注者，或许可以进行更多的标注前质量筛选。</p>
<h1 id="评价">4 评价</h1>
<p>SemEval 2017任务10提供三种不同的评价方案。</p>
<ul>
<li>1）只给出纯文本（副任务A、B、C）。</li>
<li>2）给出带有人工标注的关键词边界的纯文本（副任务B、C）。</li>
<li>3）给出带有人工标注的关键词及其类型的纯文本（副任务C）。</li>
</ul>
<p>我们将上述情况分别称为情景1、情景2和情景3。</p>
<h2 id="衡量标准">4.1 衡量标准</h2>
<p>关键词识别（副任务A）传统上是通过计算与黄金标准的精确匹配来评估的。现有的工作是捕捉语义相似的关键词（Zesch和Gurevych，2009；Kim等人，2010a），但是由于这些关键词是用关系来捕捉的，与关键词提取的试验任务类似（第5节），我们用精确匹配标准来评估关键词、关键词类型和关系。系统的输出与黄金标准完全匹配。我们计算了传统上使用的精度、召回率和F1分数等指标，并计算了三种类型的出版物中这些指标的微观平均数。此外，对于副任务B和C，参与者可以选择使用人工标注的关键词提及和类型的文本。</p>
<h1 id="试行任务">5 试行任务</h1>
<p>在SemEval 2010上，其他组织者进行了一项从科学文献中提取关键词的试行任务（Kim等人，2010b）。该任务是要从科学文献中提取代表关键主题的关键词列表，即类似于我们提议的副任务A的第一部分，只是在类型层面。参与者最多可以提交3次，并被要求为每份文件提交一份由15个关键短语组成的清单，按照读者指定的短语的概率进行排序。数据是从ACM数字图书馆收集的，研究领域包括分布式系统、信息搜索和检索、分布式人工智能多代理系统以及社会和行为科学经济学。</p>
<p>参与者获得了144篇训练文章、40篇开发文章和100篇测试文章，每组文章都包含了不同研究领域的文章组合。数据以纯文本形式提供，用pdftotext从pdf转换。50名计算机科学专业的学生对出版物进行了关键词标注，并将其添加到作者提供的他们所发表的期刊所要求的关键词中。准则是关键词要准确出现在在论文文本中的任何地方，实际上15%的标注者提供的关键词没有，19%的作者提供的关键词也没有。作者指定的关键词的数量平均为4个，而标注者平均确定了12个。如果返回的短语与标注者或作者指定的关键词完全匹配，则被认为是正确的，允许有微小的句法变化（B的A→B的A；A的B→A的B）。精度、召回率和F1是针对前5名、前10名和所有关键词计算的。19个系统被提交到该任务中，最好的一个系统在作者指定和标注者指定的综合关键词上获得了27.5%的F1。</p>
<p>从这项任务中得到的教训是，性能的好坏取决于要提取多少个关键词，任务组织者建议不要为提取的关键词数量设定一个阈值。他们进一步建议采用一个更有语义的任务，考虑到关键词的同义词，而不是要求完全匹配。这两个建议都将在未来的任务设计中得到考虑。为了实现后者，我们将要求标注者为确定的关键词分配类型（过程、任务、材料），并确定它们之间的语义关系（上下位、同义词）。</p>
<h1 id="现有资源">6 现有资源</h1>
<p>作为与IARPA合作的FUSE项目的一部分，我们创建了一个由100个名词短语组成的小型标注语料库，这些名词短语是从物理学、计算机科学、化学和计算机科学等领域的Web Of Science语料库<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>中产生的标题和摘要。这些语料库不能公开发布，而是由IARPA资助机构提供的。标注是由3位标注者使用14种细粒度的类型进行的，包括PROCESS。</p>
<p>我们用Fleiss'Kappa来衡量这14种类型的三个标注者之间的一致性。发现K值为0.28，这意味着他们之间有相当的一致性，然而区分细粒度的类型大大增加了标注的时间。因此，我们在SemEval 2017任务10中只使用三种主要类型。</p>
<p>有一些现有的关键词提取语料库，然而，它们与提议的任务不够相似，无法证明重复使用。以下是对现有语料库的描述。</p>
<p>SemEval 2010关键词提取语料库（Kim等人，2010b）<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>由每篇文章的少数几个文档级关键词组成。与我们提出的任务相比，这些关键词是在类型层面上被标注的，没有被进一步分类为过程、任务或材料，也没有被标注为语义关系。此外，所考虑的领域是不同的，大多是计算机科学的子领域。</p>
<p>Tateisi等人（2014）<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>发布的语料库包含了230篇日文出版物摘要和400篇英文出版物摘要的句子级细粒度语义标注。与我们提出的相比，这些标注更加细化，而且标注只针对摘要。</p>
<p>Gupta和Manning（2011）研究了从ACL文集的文章中提取关键词的问题，应用了基于模式的引导方法，基于15016个文档，并分配了FOCUS、TECHNIQUE和DOMAIN类型。在30个人工标注的文档上评估了性能。尽管后者的语料库与我们提出的建议有关，但人工标注只适用于少量的文件，而且只适用于自然语言处理领域。</p>
<p>ACL RD-TEC 2.0数据集（QasemiZadeh和Schumann，2016年）由300个ACL文集摘要组成，在提及层面上有7种不同类型的关键词标注。与我们的数据集不同，它不包含关系标注。请注意，这个语料库是与SemEval 2017 Task 10数据集同时创建的，因此我们没有机会在其基础上进行研究。Augenstein和Søgaard（2017）对这两个数据集以及在其上评估的关键词识别和分类方法进行了更深入的比较。</p>
<h2 id="基线">6.1 基线</h2>
<p>我们将任务设定为一个序列到序列的预测任务。我们对文件进行预处理，将文件分割成句子，用nltk对其进行标记，然后将.ann文件的跨度标注与标记对齐。每个句子都被视为一个序列。</p>
<p>然后，我们将任务分成三个子任务，即关键词边界识别、关键词分类和关系分类，并增加三个输出层。我们对这三个子任务分别预测以下类型。副任务A：tA = O, B, I，表示标记在关键词之外、开始或内部；副任务B：tB = O, M, P, T，表示标记在关键词之外，或属于材料、过程或任务；副任务C：tC = O, S, H，表示同义词和次义词的关系。对于副任务A和B，我们为每个输入标记预测一个输出标签。对于副任务C，我们为每个标记预测一个向量，编码该标记与序列中其他每个标记之间的关系，即每个关键词中的第一个标记。在获得对标记的预测后，在后处理步骤中，这些标记又被转换为跨度和它们之间的关系。</p>
<p>我们报告了两个简单模型的结果：一个用于估计上界，将.ann文件转换为实例，如上所述，然后将其转换回.ann文件。接下来，估计一个下限，一个随机基线，对于每个标记为每个子任务分配一个随机标签。上限 span-token-span往返转换性能，F1为0.84，表明我们已经由于单独的句子分割和标记化而损失了大量的性能。随机基线进一步显示了关键词边界识别任务的艰难，尤其是整体任务，因为子任务之间相互依赖。对于副任务A，随机基线达到了0.03的F1。如果给出关键词边界，整体任务会变得更容易，关键词分类的F1为0.23，如果给出关键词类型，随机基线对副任务C的F1为0.04。</p>
<h1 id="参赛系统总结">7 参赛系统总结</h1>
<p>在这一部分，我们总结了比赛的结果。更多细节请参考各自的系统描述文件和任务网站https://scienceie. github.io/。我们有三个子任务，在第2节中描述，它们被归入三个评估场景，在第4节中描述。比赛分两个阶段在CodaLab<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>举办。(i) 开发阶段和(ii) 测试阶段。有54个团队参加了开发阶段，其中有26个团队参加了最后的比赛。比赛的主要成功之一是由于广泛的参与和各种不同技术的应用，从神经网络、带有仔细特征工程的监督分类到基于规则的简单方法。我们在下文中对任务参与者使用的方法进行了总结。</p>
<h2 id="评估方案1">7.1 评估方案1</h2>
<p>在这个方案中，参赛队需要解决所有三个子任务A、B和C；其中没有给出注释信息。有些团队只参加了副任务A或B；但各副任务的总体微观F1性能被考虑用于团队的排名。17个团队参加了这个方案。F1得分从0.04到0.43不等。完整的结果在表3中给出。不同的团队采用了各种不同类型的方法，有不同程度的监督。</p>
<p>最好的三个团队TTI COIN、TIAL UW和s2 end2end使用了基于循环神经网络（RNN）的方法，分别获得0.38、0.42和0.43的F1分数。然而，与TTI COIN相比，TIAL UW和s2 end2end通过在RNN之上使用条件随机场（CRF）层，在副任务A中取得了更高的F1分数。第四组PKU ICL的F1为0.37，他们发现基于随机森林和支持向量机（SVM）的分类模型非常有用，其特点是在一个非常大的外部语料库上使用TF-IDF，IDF加权的词缀等，同时还有一个现有的分类法。</p>
<p>另一方面，SciX使用名词短语分块，并在提供的训练数据上训练一个SVM分类器来对短语进行分类，并使用CRF来预测短语的标签。一些团队（NTNU、SZTE-NLP、WING-NUS）已经尝试了基于CRF的方法，包括语篇（POS）标记和正字法特征，如符号和大写字母的存在，它们导致了合理的性能（F1：0.23、0.26和0.27，分别）。HCC-NLP的带长度限制的名词短语提取，以及NITK IT PG的使用全局关键词列表，都没有令人满意的表现（F1：分别为0.16和0.14）。</p>
<p>前者令人惊讶，因为关键词绝大多数是名词短语，后者则不尽然，许多关键词在数据集中只出现一次（见表1）。GMBUAP进一步尝试使用通过观察副任务A的训练数据获得的经验规则，以及根据副任务B的训练数据训练的Naive Bayes分类器。这样的尝试让我们对问题的难易程度和简单方法对任务的适用性有了更多的了解。</p>
<h2 id="评价方案2">7.2 评价方案2</h2>
<p>在这个方案中，各小组需要解决子任务B和C。向各小组提供了部分注释，即子任务A的解决方案。完整的结果请参考表4。除了MayoNLP，其他三个团队只参加了B小任务。虽然排名是根据整体表现进行的，但在这个场景下，UKP/EELECTION团队在评估期结束后发现这些结果是基于开发集的训练。对于在训练集上的训练，他们的结果是。0.69 F1总分和0.72 F1的副任务B在每个类别中的排名是一致的。BUAP在副任务B中的F1得分最差（0.45），但仍比情景1中副任务B的最佳团队s2 end2end（0.44）好。</p>
<p>事实证明，副任务A的部分注释或准确性至关重要，再次强化了识别关键词边界是共享任务中最困难的部分。与情景1不同的是，在这种情况下，排名前两位的团队使用了带有词汇特征的分类器（F1：0.64）以及神经网络（F1：0.63）。第一支队伍MayoNLP使用了具有丰富特征集的SVM，如n-grams、词法特征、正字法特征，而第二支队伍UKP/EELECTION则使用了三种不同的神经网络方法，随后通过多数投票将它们结合起来。这两种方法的表现都很相似。然而，由LABDA和BUAP两个团队尝试的基于CRF的方法和具有更简单特征集的SVM在这种情况下被发现不太有效。MayoNLP应用了一个简单的基于规则的方法来提取同义词-关系，并应用Hearst模式来检测超义词-关系。同义词检测的规则是基于诸如in terms of, equivalently，which are called等在两个关键词之间的文本存在的短语。有趣的是，场景1中基于RNN的s2 end2end方法在不使用副任务A的部分注释的情况下比MayoNLP表现更好。</p>
<h2 id="评估场景3">7.3 评估场景3</h2>
<p>在这个场景中，团队只需要解决副任务C，副任务B和C的部分注释提供给团队。完整的结果请参考表5。在这个场景中，基于神经网络（NN）的模型被发现比其他方法表现更好。麻省理工学院的最佳方法使用卷积NN（CNN）。另一种方法使用了两个阶段的NN，并被认为是相当有效的（F1：0.54）。另一方面，应用五种不同的分类器（SVM、决策树、随机森林、多项式天真15A在评估期结束后，TTI COIN rel团队发现了预处理中的一个错误，导致了低结果。在纠正了这个错误后，他们的总体结果是Macro F1为0.48。Bayes和k-nearest neighbour）使用三种不同的特征选择技术（Chi square, decision tree, and recursive feature elimination）发现了与表现最好的技术接近的准确性（F1: 0.5）。LaBDA也使用了一个基于CNN的方法。然而，麻省理工学院应用的基于规则的后处理和参数排序策略似乎带来了额外的优势，他们也观察到了这一点。然而，这个场景中的大多数团队在关系预测方面超过了其他场景中的所有团队（他们没有获得B和C小任务的部分信息）。这也证明了要想在副任务C中表现准确，在副任务A和B中的准确性是非常重要的。</p>
<h1 id="结论">8 结论</h1>
<p>在本文中，我们介绍了SemEval 2017任务10的设置并讨论了参与系统，该任务是对科学文章中的关键词和它们之间的关系进行识别和分类，共有26个系统提交。成功的系统在其方法上有所不同。他们中的大多数使用RNN，通常与CRF以及CNN相结合，然而，在评估场景1中表现最好的系统使用SVM与精心设计的词汇特征集。识别关键词是最具挑战性的子任务，因为数据集包含许多长而不常出现的关键词，而依靠记忆这些关键词的系统表现不佳。</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>https://scholar.google.co.uk/<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>http://www.scopus.com/<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>https://www.semanticscholar.org/<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>http://www.sciencedirect.com/<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>我们向任务参与者提供了标注指南，它们可以在这里找到：https://scienceie. github.io/resources.html<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>http://thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/web-of-science.html<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>https://github.com/snkim/AutomaticKeyphraseExtraction<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>https://github.com/mynlp/raniscompetitions/15898<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>https://competitions.codalab.org/<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
  </entry>
  <entry>
    <title>二语习得最终</title>
    <url>/sla-4/</url>
    <content><![CDATA[<h1 id="theories">Theories</h1>
<h3 id="critical-period-hypothesis">Critical Period Hypothesis</h3>
<p>There is a specific and limited period for language acquisition. The language acquisition device, like other biological functions, works successfully only when it is stimulated at the right time called “critical period”.</p>
<h3 id="behaviorist-perspective">Behaviorist perspective</h3>
<p>Language learning is the result of imitation, practice, feedback on success, and habit formation. They view imitation and practice as primary processes in language development.</p>
<h3 id="what-is-the-logical-problem-of-language-acquisition">What is the “logical problem” of language acquisition?</h3>
<p>The question of how children achieve the final state of L1 development with ease and success when the linguistic system is very complex and their cognitive ability is not fully developed. Children come to know more about the structure of their language than they could reasonably be expected to learn on the basis of the samples of language they hear.</p>
<h3 id="what-is-contrastive-analysis-what-are-its-limitations">What is contrastive analysis? What are its limitations?</h3>
<p>Contrastive Analysis (CA) is an approach to the study of SLA which involves predicting and explaining learner problems based on a comparison of L1 and L2 to determine similarities and differences, influenced by Structuralism and Behaviorism.</p>
<h3 id="what-is-error-analysis-what-are-its-limitations">What is error analysis? What are its limitations?</h3>
<p>Error Analysis (EA) is the first approach to the study of SLA which includes an internal focus on learners’ creative ability to construct language. It is based on the description and analysis of actual learner errors in L2, rather than on idealized linguistic structures attributed to native speakers of L1 and L2 (as in CA).</p>
<h3 id="what-is-acquisition-learning-hypothesis-made-in-the-monitor-model">What is Acquisition-Learning Hypothesis made in the Monitor Model?</h3>
<p>Acquisition-Learning Hypothesis. There is a distinction to be made between acquisition and learning. Acquisition is subconscious, and involves the innate language acquisition device which accounts for children’s L1. Learning is conscious and is exemplified by the L2 learning which takes place in many classroom contexts.</p>
<h3 id="what-is-input-hypothesis">What is Input Hypothesis?</h3>
<p>Input Hypothesis. Language acquisition takes place because there is comprehensible input. If input is understood, and if there is enough of it, the necessary grammar is automatically provided.</p>
<h3 id="what-is-interlanguage">What is interlanguage?</h3>
<p>Interlanguage: the intermediate states (or interim grammars) of a learner’s language as it moves toward the target L2. </p>
<h3 id="define-the-following-terms-competence-performance-principles-parameters.">Define the following terms: competence, performance; principles, parameters.</h3>
<p>Linguists emphasize the characteristics of the differences and similarities in the languages that are being learned, and the linguistic competence (underlying knowledge) and linguistic performance (actual production) of learners at various stages of acquisition.</p>
<p>linguistic competence: The underlying knowledge that speakers/hearers have of a language. Chomsky distinguishes this from linguistic performance. linguistic performance: The use of language knowledge in actual production</p>
<p>Interlanguage: the intermediate states (or interim grammars) of a learner’s language as it moves toward the target L2. </p>
<h3 id="what-is-fossilization">What is fossilization?</h3>
<p>Fossilization: cessation of learning/IL development in some aspects before they reach target language norms, in spite of continuing L2 input and passage of time. Because age of learning, factors of social identity and communicative need.</p>
<h3 id="how-is-language-learning-viewed-in-the-connectionist-approaches-to-language-learning">How is language learning viewed in the Connectionist approaches to language learning?</h3>
<p>Connectionist approaches focus on the increasing strength of associations between stimuli and responses rather than on the inferred abstraction of “rules” or on restructuring. Indeed, from a connectionist perspective learning essentially is change in the strength of these connections.</p>
<h3 id="comprehension-of-written-or-spoken-language-involves-both-bottom-up-and-top-down-processing.">Comprehension of written or spoken language involves both bottom-up and top-down processing.</h3>
<p>Bottom-up processing requires prior knowledge of the language system (i.e. vocabulary, morphology, phonology, syntax, and discourse structure) and interpretation of physical (graphic and auditory) cues.</p>
<p>Top-down processing can compensate for linguistic limitations to some extent by allowing learners to guess the meaning of words they have not encountered before, and to make some sense out of larger chunks of written and oral text. For both L1 and L2 speakers, top-down processing utilizes prior knowledge of content, context, and culture, which were shown in 6.1 to be essential components of communicative competence.</p>
<h3 id="zpd-mediation-scaffolding">ZPD, mediation, scaffolding</h3>
<p>Zone of Proximal Development (ZPD). This is an area of potential development, where the learner can achieve that potential only with assistance. One way in which others help the learner in language development within the ZPD is through scaffolding. This includes the “vertical constructions” mentioned above as a type of modified interaction between NSs and NNSs, in which experts commonly provide learners with chunks of talk that the learners can then use to express concepts which are beyond their independent means. This According to S-C Theory, learning occurs when simple innate mental activities are transformed into “higher order,” more complex mental functions. This transformation typically involves symbolic mediation, which is a link between a person’s current mental state and higher order functions that is provided primarily by language. This is considered the usual route to learning, whether what is being learned is language itself or some other area of knowledge. The results of learning through mediation include learners’ having heightened awareness of their own mental abilities and more control over their thought processes.</p>
<h3 id="according-to-the-interaction-hypothesis-how-do-modifications-and-collaborative-efforts-facilitate-sla">According to the Interaction Hypothesis, how do modifications and collaborative  efforts facilitate SLA?</h3>
<p>According to claims made in the Interaction Hypothesis, the modifications and collaborative efforts that take place in social interaction facilitate SLA because they contribute to the accessibility of input for mental processing: “negotiation for meaning, and especially negotiation work that triggers interactional adjustments by the NS or more competent interlocutor, facilitates acquisition because it connects input, internal learner capacities, particularly selective attention, and output in productive ways”</p>
<h3 id="how-is-interaction-viewed-in-sociocultural-theory">How is interaction viewed in Sociocultural Theory?</h3>
<p>Sociocultural (S-C) Theory (Vygotsky 1962, 1978). interaction not only facilitates language learning but is a causative force in acquisition. considering interaction as an essential force rather than as merely a helpful condition for learning.</p>
<h3 id="output-hypothesis">output hypothesis</h3>
<p>The output hypothesis claims that the act of producing language (speaking or writing) constitutes, under certain circumstances, part of the process of second language learning. Three functions of output in second language learning:</p>
<p>Accessibility Hierarchy A continuum of relative clause types such that the presence of one type implies the presence of other types higher on the hierarchy.</p>
<p>Affective Filter Part of the Monitor Model. The claim is that affect is an important part of the learning process and that one has a “raised” or “lowered” affective filter. The latter leads to better learning.</p>
<p>attention The concentration of mental powers.</p>
<p>automaticity The degree of routinized control that one has over linguistic knowledge.</p>
<p>behaviorism A school of psychology that bases learning on a stimulus— response paradigm.</p>
<p>comprehensible input Originally formulated as part of the Monitor Model, this concept refers to the understandable input that learners need for learning. Input that is slightly more advanced than the learner’s current level of grammatical knowledge.</p>
<p>connectionism An approach that assumes that learning takes place based on the extraction of regularities from the input. (See also emergentism.)</p>
<p>contrastive analysis A way of comparing two languages to determine similarities and dissimilarities.</p>
<p>Contrastive Analysis Hypothesis The prediction that similarities between two languages do not require learning and that the differences are what need to be learned.</p>
<p>Creative Construction Hypothesis The proposal that child second language learners construct rules of the second language on the basis of innate mechanisms.</p>
<p>declarative knowledge Knowledge that learners have about something. This information is relatively accessible to conscious awareness. (See also procedural knowledge.)</p>
<p>error analysis A procedure for analyzing second language data that begins with the errors learners make and then attempts to explain them.</p>
<p>explicit knowledge Knowledge about language that involves awareness. (See also declarative knowledge, explicit learning, implicit knowledge, implicit learning, procedural knowledge.)</p>
<p>explicit learning Acquisition of language that involves deliberate hypothesis testing as learners search for structure. (See also declarative knowledge; explicit knowledge; implicit knowledge; implicit learning; procedural knowledge.)</p>
<p>feedback An intervention in which information is provided to a learner that a prior utterance is correct or incorrect. (See also corrective feedback.)</p>
<p>fossilization The cessation of learning. Permanent plateaus that learners reach resulting from no change in some or all of their interlanguage forms. (See also interlanguage; stabilization.)</p>
<p>implicit knowledge Knowledge about language that does not involve awareness of that knowledge. (See also declarative knowledge; explicit learning; explicit knowledge; implicit learning; procedural knowledge.)</p>
<p>implicit learning Acquisition of knowledge about the underlying structure of a complex stimulus environment without doing so consciously. (See also explicit knowledge; explicit learning; implicit knowledge.)</p>
<p>incidental vocabulary learning Learning that takes place with an explicit focus on meaning as opposed to having an explicit goal being the learning of new words.</p>
<p>input enhancement A technique that attempts to make parts of the input salient.</p>
<p>intake That part of the language input that is internalized by the learner.</p>
<p>interference The use of the first language (or other languages known) in a second language context when the resulting second language form is incorrect. (See also language transfer; negative transfer.)</p>
<p>interlanguage The language produced by a nonnative speaker of a 518 G LO S S A RY language (i.e., a learner’s output). Refers to the systematic knowledge underlying learners’ production. interlanguage transfer The influence of one L2 over another in instances where there are multiple languages acquired after the L1.</p>
<p>Language Acquisition Device (LAD) A language faculty that constrains and guides the acquisition process.</p>
<p>language transfer The use of the first language (or other languages known) in a second language context. (See also cross-linguistic influence; facilitation; interference; negative transfer; positive transfer.)</p>
<p>metalinguistic knowledge What one knows (or thinks one knows) about the language. It is to be differentiated from what one does in using language.</p>
<p>Monitor Model A model of second language acquisition based on the concept that learners have two systems (acquisition and learning) and that the learned system monitors the acquired system.</p>
<p>negative transfer The use of the first language (or other languages known) in a second language context resulting in a nontarget-like second language form. (See also interference; language transfer; positive transfer.)</p>
<p>negotiation of meaning The attempt made in conversation to clarify a lack of understanding.</p>
<p>Poverty of the Stimulus A proposal made within the confines of Universal Grammar that input alone is not sufficiently specific to allow a child to attain the complexities of the adult grammar. (See also Universal Grammar.)</p>
<p>procedural knowledge Knowledge that relates to cognitive skills that involve sequencing information. This information is relatively inaccessible. (See also declarative knowledge.) Processability Theory A theory that proposes that production and comprehension of second language forms only takes place to the extent that they can be handled by the linguistic processor. Understanding how the processor functions allows one to understand developmental paths.</p>
<p>recasts Reformulations of an incorrect utterance that maintain the original’s meaning. restructuring Changes or reorganization of one’s grammatical knowledge.</p>
<p>Sociocultural Theory A theory based on work by the Russian psychologist Vygotsky that considers knowledge/learning arises from a social context. Learning, being socially mediated, comes from faceto-face interaction. Knowledge is internalized from learners jointly constructing knowledge in dyadic interactions.</p>
<p>stabilization The plateaus that learners reach when there is little change in some or all of their interlanguage forms (See also fossilization; interlanguage.)</p>
<p>Universal Grammar A set of innate principles common to all languages.</p>
<p>U-shaped learning Learning whereby early forms appear to be correct, followed by a period of incorrect forms, with a final stage of correct forms.</p>
<p>working memory Memory that involves storage capacity and processing capacity.</p>
<h1 id="emergentism-and-usage-based-theories">Emergentism and usage-based theories</h1>
<p>Emergentism is a theory within cognitive psychology that attempts to account for human learning and knowledge. <strong>Central to the theory is that all human learning uses the same general architecture for knowledge and performance development be it language, tennis, or assembly-line work.</strong></p>
<p>In contrast to linguistic theory that posits a Universal Grammar that is unique to language and is responsible for the nature of language, emergentism would claim <strong>there is nothing special about language.</strong></p>
<p>What is more, the theory says that <strong>all learning and representation (knowledge) in the mind/brain is sensitive to frequency.</strong> So, those things that are more frequent in the environment or those things with which a person has more frequent experience are more likely to be learned before others.</p>
<p>At the same time, more frequent things are likely to have more robust representations in the mind/brain. Emergentism is aligned with what are called usage-based theories because such theories rely on people’s experience with their environment and how this is indicated in their performance. Performance is central to Emergentism in that observable behaviors are taken to be indicators of what exists in representation and how robustly it is represented. Applied to language, emergentism would suggest that something like sentence structure is a result of frequencies of occurrence in the input that people are exposed to. As the reader might surmise, <strong>a central idea within emergentism is that the human mind/brain is a statistical tabulator par excellence</strong>, and although other factors may come into play in how important a piece of linguistic data is, human knowledge and performance with that knowledge is a direct result of experience with the environment. There is nothing special in the mind/brain that constrains or regulates language.</p>
<p>Emergentism does have problems, however, in accounting for the poverty of the stimulus problem. Within the theory, there is no real way to explain how people come to know more than what they are exposed to. Because the theory relies heavily on frequency in the input and general learning architecture, it cannot explain why people know that certain kinds of sentence permutations are impossible.</p>
<p>The Declarative/Procedural Model</p>
<h2 id="complexity-theorydynamic-systems"><strong>Complexity Theory/Dynamic Systems</strong></h2>
<p>complexity (in any domain, not just language) arises as a result of the interaction of various individuals or components.</p>
<p><strong>Sociocultural Theory</strong></p>
<p><strong>The basics</strong></p>
<p>Sociocultural Theory, based on the work of the Russian psychologist Vygotsky, argues that the development of <strong>human cognitive functions derives from social interactions and that through participation in social activities individuals are drawn into the use of these functions.</strong> The theory focuses not only on how adults and peers influence individual learning, but also on how <strong>cultural beliefs and attitudes</strong> impact how instruction and learning take place. The central constructs of the theory are: mediation; the zone of proximal development (ZPD); and verbal thought. <strong>Language learning is a socially mediated process and language is a cultural artifact that mediates social and psychological activities.</strong> <strong>Mediation refers to the idea that humans possess certain cultural tools, such as language, literacy, numeracy, and others, that they purposefully use to control and interact with their environment.</strong></p>
<p>The ZPD is a difficult concept to articulate and is often subject to misinterpretation, but to summarize here, the ZPD refers to the distance between a learner’s current ability to independently solve problems and the level of potential development present when guided by more capable persons. According to Vygotsky, learning occurs in this zone and it is achieved through the cooperation of experts and novices working together. Applied to language, learning concerns the development of language function and mental function, along with the combination of language and thought. Generally speaking, acquisition includes the process in which the low level external or social speech develops into the highest level inner speech or verbal thought.</p>
<h1 id="the-monitor-model.">the Monitor Model.</h1>
<h1 id="mediation">Mediation</h1>
<p>Human activity (including cognitive activity) is mediated by what are known as symbolic artifacts (higher-level cultural tools) such as language and literacy and by material artifacts. These artifacts mediate the relationship between humans and the social and material world around us.</p>
<p>Within sociocultural theory, humans use symbols as tools to mediate psychological activity and to control our psychological processes. This control is voluntary and allows us to attend to certain things, to plan, and to think rationally. The primary tool that humans have available is language and it is a tool that allows us to connect to our environment (both physical and social). Language gives humans the power to go beyond the immediate environment and to think about and talk about events and objects that are far removed both physically and temporally.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>二语习得</category>
      </categories>
  </entry>
  <entry>
    <title>句法学主要内容</title>
    <url>/syntax-main/</url>
    <content><![CDATA[<h1 id="chapter-1">Chapter 1</h1>
<h2 id="learning-objectives">Learning Objectives</h2>
<p>After reading chapter 1 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Explain why Language is a psychological property of humans.</p></li>
<li><p>Distinguish between prescriptive and descriptive rules.</p></li>
<li><p>Explain the scientific method as it applies to syntax.</p></li>
<li><p>Explain the differences between the kinds of data gathering, including corpora and linguistic judgments.</p></li>
<li><p>Explain the difference between competence and performance.</p></li>
<li><p>Provide at least three arguments for Universal Grammar.</p></li>
<li><p>Explain the logical problem of language acquisition.</p></li>
<li><p>Distinguish between learning and acquisition.</p></li>
<li><p>Distinguish among observational, descriptive and explanatory adequacy.</p></li>
</ul>
<h2 id="conclusions">Conclusions</h2>
<p>In this chapter, we’ve done very little syntax but talked a lot about the assumptions underlying the approach we’re going to take to the study of sentence structure. The basic approach to syntax that we’ll be using here is generative grammar; we’ve seen that this approach is scientific in that it uses the scientific method. It is descriptive and rule-based. Further, it assumes that a certain amount of grammar is built in and the rest is acquired.</p>
<h1 id="chapter-2">Chapter 2</h1>
<h2 id="learning-objectives-1">Learning Objectives</h2>
<p>After reading chapter 2 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Distinguish between distributional and semantic definitions of parts of speech.</p></li>
<li><p>Identify a part of speech by its distribution.</p></li>
<li><p>Identify cases of complementary distribution.</p></li>
<li><p>Know the difference between an open-class and a closed-class part of speech.</p></li>
<li><p>Explain the difference between lexical and functional categories.</p></li>
<li><p>Identify different subcategories using feature notations.</p></li>
<li><p>Identify plural nouns, mass nouns and count nouns and distinguish them with features.</p></li>
<li><p>Explain the difference between predicates and arguments.</p></li>
<li><p>Categorize verbs according to their argument structure (intransitive, transitive, ditransitive) and represent this using features.</p></li>
</ul>
<h2 id="conclusions-1">Conclusions</h2>
<p>In this chapter, we’ve surveyed the parts of speech categories that we will use in this book. We have the lexical parts of speech N, V, Adj, and Adv, and the functional categories D, P, C, Conj, Neg, and T. Determining part of speech is done not by traditional semantic criteria, but by using morphological and syntactic distribution tests. We also looked at distributional evidence for various subcategories of nouns and verbs, and represented these distinctions as feature notations on the major categories.</p>
<h1 id="chapter-3">Chapter 3</h1>
<h2 id="learning-objectives-2">Learning Objectives</h2>
<p>After reading chapter 3 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Be able to explain what a constituent is.</p></li>
<li><p>Show whether a string of words is a constituent or not.</p></li>
<li><p>Using phrase structure rules, draw the trees for English sentences.</p></li>
<li><p>Explain and apply the Principle of Modification.</p></li>
<li><p>Produce paraphrases for ambiguous sentences and draw trees for each meaning.</p></li>
<li><p>Using data, be able to extract a set of phrase structure rules for another language.</p></li>
<li><p>Define recursion and give an example.</p></li>
</ul>
<h2 id="conclusions-2">Conclusions</h2>
<p>We’ve done a lot in this chapter. We looked at the idea that sentences are hierarchically organized into constituent structures. We represented these constituent structures in trees and bracketed diagrams. We also developed a set of rules to generate those structures. We looked at constituency tests that can be used to test the structures. And finally we looked at the way constituent structure can vary across languages.</p>
<h1 id="chapter-4">Chapter 4</h1>
<h2 id="learning-objectives-3">Learning Objectives</h2>
<p>After reading chapter 4 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify dominance in a tree.</p></li>
<li><p>Distinguish dominance from immediate dominance.</p></li>
<li><p>Understand the relationship between exhaustive domination and constituency.</p></li>
<li><p>Identify precedence in a tree.</p></li>
<li><p>Understand the constraint against crossing lines.</p></li>
<li><p>Identify c-command in a tree.</p></li>
<li><p>Distinguish symmetric from asymmetric c-command.</p></li>
<li><p>Identify different government relations.</p></li>
<li><p>Define structurally subject, object, oblique, object of a preposition and indirect object.</p></li>
</ul>
<h2 id="conclusions-3">Conclusions</h2>
<p>This chapter has been a bit different from the rest of this book. It hasn’t been about Language per se, but rather about the mathematical properties of the system we use to describe language. We looked at the various parts of a syntactic tree and then at the three relations that can hold between these parts: domination, precedence, and c-command. In all the subsequent chapters of this book, you’ll find much utility for the terms and the relations described here.</p>
<h1 id="chapter-5">Chapter 5</h1>
<h2 id="learning-objectives-4">Learning Objectives</h2>
<p>After reading chapter 5 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify and distinguish R-expressions, pronouns and anaphors.</p></li>
<li><p>Understand antecedent and anaphor.</p></li>
<li><p>Distinguish coindexing from binding.</p></li>
<li><p>Define and apply binding to a tree.</p></li>
<li><p>Apply principles A, B, C to a tree.</p></li>
<li><p>Identify binding domains.</p></li>
</ul>
<h2 id="conclusions-4">Conclusions</h2>
<p>In this chapter, we looked at a very complex set of data concerning the distribution of different kinds of NPs. We saw that these different kinds of NPs can appear in different syntactic positions. A simple set of binding</p>
<p>principles (A, B, and C) governs the distribution of NPs. This set of binding principles is built upon the structural relations developed in the last chapter.</p>
<p>　　In the next chapter, we are going to look at how we can develop a similarly simple set of revisions to the phrase structure rules. The constraints developed in this chapter have the shape of locality constraints (in that they require local, or nearness, relations between certain syntactic objects). In later chapters, we’ll see a trend towards using locality constraints in other parts of the grammar.</p>
<p>　　The constraints developed in this chapter account for a wide range of data, but there are many cases that don’t work. In particular there is a problem with our definition of binding domain. You can see some of these problems by trying some of the Challenge Problem Sets at the end of this chapter. We return to a more sophisticated version of the binding theory in chapter 17 in the last part of this book.</p>
<h1 id="chapter-6">Chapter 6</h1>
<h2 id="learning-objectives-5">Learning Objectives</h2>
<p>After reading chapter 6, you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Explain the motivation for simplifying the PSRs into X-bar theory.</p></li>
<li><p>Apply the notation of X-bar theory using variables.</p></li>
<li><p>Be able to draw a tree in X-bar theory.</p></li>
<li><p>Apply tests to distinguish complements from adjuncts.</p></li>
<li><p>Draw trees correctly placing modifiers as complements, adjuncts, and specifiers.</p></li>
<li><p>Describe the notion of a parameter.</p></li>
<li><p>Be able to correctly set the complement, adjunct, and specifier parameters for any foreign language data.</p></li>
</ul>
<h2 id="conclusions-5">Conclusions</h2>
<h1 id="chapter-7">Chapter 7</h1>
<h2 id="learning-objectives-6">Learning Objectives</h2>
<p>After reading chapter 7 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify and distinguish subjects from predicate phrases.</p></li>
<li><p>Identify various kinds of T and C nodes.</p></li>
<li><p>Distinguish finite from non-finite clauses, using tests.</p></li>
<li><p>Identify embedded and root clauses, and distinguish specifier, adjunct or complement clauses.</p></li>
<li><p>Correctly use X-bar format for DPs, TPs, and CPs in tree drawing.</p></li>
<li><p>Explain the arguments for DPs, TPs, and CPs.</p></li>
<li><p>Identify subjects in all types of clauses and correctly place them in the specifier position of TP.</p></li>
</ul>
<h2 id="conclusions-6">Conclusions</h2>
<h1 id="chapter-8">Chapter 8</h1>
<h2 id="learning-objectives-7">Learning Objectives</h2>
<p>After reading chapter 8 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Distinguish between thematic relation and theta role.</p></li>
<li><p>Identify the thematic relations agent, theme, goal, source, experiencer, location, instrument, recipient, benefactor.</p></li>
<li><p>Explain how X-bar theory over-generates.</p></li>
<li><p>Explain the structure of the lexicon.</p></li>
<li><p>Draw the theta grids for a predicate.</p></li>
<li><p>Apply the theta criterion to a sentence as a filter to X-bar theory.</p></li>
<li><p>Distinguish sentences with expletive subjects from ones with theta-role-bearing subjects.</p></li>
<li><p>Explain the Extended Projection Principle and how it accounts for expletives.</p></li>
<li><p>Explain the ordering of the EPP with the theta criterion in the context of the model we are developing.</p></li>
</ul>
<h2 id="conclusions-7">Conclusions</h2>
<p>We started this chapter off with the observation that while X-bar rules capture important facts about constituency and cross-categorial generalizations, they over-generate (that is, they generate ungrammatical sentences). One way of constraining X-bar theory is by invoking lexical restrictions on sentences, such that particular predicates have specific argument structures, in the form of theta grids. The theta criterion rules out</p>
<p>any sentence where the number and type of arguments don’t match up one to one with the number and type of theta roles in the theta grid.</p>
<p>　　We also looked at one apparent exception to the theta criterion: theta-role-less expletive pronouns. These pronouns only show up when there is no other subject, and are forced by the EPP. They escape the theta criterion by being inserted after the theta criterion has filtered out the output of X-bar rules.</p>
<p>　　By using lexical information (like theta roles) we’re able to stop the X-bar rules from generating sentences that are ungrammatical. Unfortunately, as we’ll see in the next chapter, there are also many sentences that the X-bar rules cannot generate. In order to account for these, we’ll introduce a further theoretical tool: the movement rule.</p>
<h1 id="chapter-9">Chapter 9</h1>
<h2 id="learning-objectives-8">Learning Objectives</h2>
<p>After reading chapter 9 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Using theta grids, explain the restrictions that various kinds of C, T, and D nodes impose on their complements.</p></li>
<li><p>Learn to distinguish the various tense, aspect, voice, and mood properties of English verbal constructions.</p></li>
<li><p>Learn to identify the modals and various auxiliaries.</p></li>
<li><p>Identify participles, gerunds, bare forms, preterites, and present tense forms of verbs.</p></li>
<li><p>Demonstrate the similarities and differences between main verbs, auxiliaries, and modals.</p></li>
<li><p>Draw trees showing stacked VPs.</p></li>
<li><p>Discuss the properties of do-support.</p></li>
</ul>
<h2 id="conclusions-8">Conclusions</h2>
<p>In this chapter, we’ve extended the use of theta grids to explain other kinds of restrictions on X-bar-theoretic trees. We saw that VPs not only select for specific theta roles, they also select for various features of CPs (such as finiteness). Complementizers themselves select for specific kinds of TPs. For example, that can’t have a TP headed by to as its complement. We also looked at several kinds of determiners, all of which place restrictions on the kinds of nouns they can take as complements. Finally we examined the scary world of English modals and auxiliaries. After breaking down all the parts of</p>
<p>a complex verb string in English into its component modal, tense, aspectual, and voice parts, we saw how various auxiliaries select for both the range of possible auxiliary and verbal complements (perfect, progressive, passive, main verbs) and for the form that complement takes (bare, preterite, present, participle, or gerund). This gives us an explanation for the ordering and realization of each of these auxiliaries in sentences like “the grand slam” given in section 4.7 above.</p>
<p>　　There are a number of issues that remain unaddressed here. The bulk of this chapter has been on English, but other languages function very differently in how they represent this kind of morphology. We also have left unexplained the situations where certain modals and tensed auxiliaries behave as a class. Both modals and tensed auxiliaries precede negation, as in the examples (79a and b), and both tensed auxiliaries and modals can undergo subject/auxiliary inversion to form yes/no questions as in (79c and d).</p>
<ul>
<li><ol type="a">
<li>Fiona must not eat the sautéed candy canes.</li>
</ol></li>
</ul>
<p>Fiona has not eaten the sautéed candy canes.</p>
<p>Can Fiona eat sautéed candy canes?</p>
<p>Has Fiona eaten sautéed candy canes?</p>
<p>The account of auxiliaries given in this chapter is completely silent on the behaviors in (79). In order to account for these facts as well as for patterns in other languages, we need the additional technology afforded us by movement. This is discussed in the next few chapters.</p>
<h1 id="chapter-10">Chapter 10</h1>
<h2 id="learning-objectives-9">Learning Objectives</h2>
<p>After reading chapter 10 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Understand the distinction between D-structure and S-structure.</p></li>
<li><p>Determine whether a language is verb-raising or not.</p></li>
<li><p>Discuss the interaction between V T and T C.</p></li>
<li><p>Explain the evidence for V T movement in French and Irish.</p></li>
<li><p>Discuss the position of tensed English auxiliaries as compared to main verbs.</p></li>
<li><p>Explain how the VP-internal subject hypothesis accounts for VSO languages.</p></li>
<li><p>Discuss the whens, wheres, and whys of do-support.</p></li>
</ul>
<h2 id="conclusions-9">Conclusions</h2>
<h1 id="chapter-11">Chapter 11</h1>
<h2 id="learning-objectives-10">Learning Objectives</h2>
<p>After reading chapter 11 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Draw the theta grids of raising predicates like is likely and seem.</p></li>
<li><p>Draw trees indicating DP movement of embedded subjects.</p></li>
<li><p>Explain how the Case filter motivates the movement of NPs out of infinitival clauses into main clauses.</p></li>
<li><p>Describe how the passive voice head affects the introduction of external arguments and Case assignment by verbs.</p></li>
<li><p>Show passive DP movement in a tree.</p></li>
</ul>
<h2 id="conclusions-10">Conclusions</h2>
<p>In this chapter, we’ve looked at situations where DPs don’t appear in the positions we expect them to (given our knowledge of theta theory). We have argued that these sentences involve movement of DPs to various specifier positions. The motivation for this comes from Case. The Case filter requires all DPs to check a Case in a specific structural position. We looked at two situations where DPs don’t get Case in their D-structure position. In raising structures, a DP is in the specifier of an embedded clause with non-finite T. In this position, it can’t receive Case so it raises to the specifier of the finite T in the higher clause. We also looked at passive structures. The passive morpheme does two things: it takes the role of external argument and it absorbs the verb’s ability to assign accusative Case. This results in a structure where there is no subject DP, and the object cannot receive Case in its base position. The DP must move to the specifier of T to get Case.</p>
<h1 id="chapter-12">Chapter 12</h1>
<h2 id="learning-objectives-11">Learning Objectives</h2>
<p>After reading chapter 12 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Explain the motivation for wh-movement.</p></li>
<li><p>Draw the tree indicating wh-movement.</p></li>
<li><p>Identify various complementizer types.</p></li>
<li><p>Draw a tree for a relative clause.</p></li>
<li><p>Identify various island types.</p></li>
<li><p>Explain why wh-island sentences are ungrammatical according to the Minimal Link Condition.</p></li>
<li><p>Explain, using the MLC, why certain instances of DP movement and head-to-head movement are ungrammatical.</p></li>
</ul>
<h2 id="conclusions-11">Conclusions</h2>
<p>In this chapter, we looked at a third kind of movement transformation: Wh-movement. This process targets wh-phrases and moves them to the specifier of CPs. This movement is triggered by the presence of a [+WH] feature in C. Wh-movement of a DP is always from a Case position to the specifier of CP. Wh-movement is not totally unrestricted; there is a locality constraint on the movement: the MLC. Movement must be local, where local is defined in terms of closest potential landing site. We saw further that the MLC might be extended to other types of movement.</p>
<p>　　In the next chapter, we’re going to continue this trend and look at movement processes in general and the similarities between them, as well as briefly delve into the interaction between the syntax and the formal interpretation (semantics) of the sentence.</p>
<p>　　</p>
<h1 id="chapter-13">Chapter 13</h1>
<h2 id="learning-objectives-12">Learning Objectives</h2>
<p>After reading chapter 13 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Understand and apply the Y-model of grammar.</p></li>
<li><p>Explain and apply the Principle of Full Interpretation.</p></li>
<li><p>Distinguish between overt and covert movement.</p></li>
<li><p>Identify whether a language has overt or covert movement for any given movement type.</p></li>
<li><p>Explain the difference between LF and PF.</p></li>
<li><p>Understand scope and how it applies to the universal and existential quantifiers.</p></li>
</ul>
<h2 id="conclusions-12">Conclusions</h2>
<p>In this chapter we made the big jump from three movement rules with different but similar motivations to a single rule with a single motivation (Full Interpretation). We also claimed that cross-linguistic variation in movement, when we assume a universal semantics, requires that movement can both be overt (before SPELLOUT) and covert (after SPELLOUT). The Y model with Saussurean interface levels (LF and PF) allows this to occur. We also looked very briefly at an example from quantifier scope that provides independent support for the notion of covert movement.</p>
<h1 id="chapter-14">Chapter 14</h1>
<h2 id="learning-objectives-13">Learning Objectives</h2>
<p>After reading chapter 14 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify shifted objects, and explain how split VPs account for them.</p></li>
<li><p>Draw the tree for particle constructions, prepositional ditransitives and double object ditransitives.</p></li>
<li><p>Explain how passives and actives have different selectional properties for the functional category that assigns accusative Case.</p></li>
</ul>
<h2 id="conclusions-13">Conclusions</h2>
<h1 id="chapter-15">Chapter 15</h1>
<h2 id="learning-objectives-14">Learning Objectives</h2>
<p>After reading chapter 15 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Distinguish between raising and control predicates.</p></li>
<li><p>Distinguish between subject-to-subject raising (SSR) and subject control constructions.</p></li>
<li><p>Distinguish between subject-to-object raising (SOR) and object control constructions.</p></li>
<li><p>Apply the idiom, clausal subject, and expletive tests.</p></li>
<li><p>Draw theta grids and trees of SSR, SOR, OC, and SC sentences.</p></li>
<li><p>Describe the restrictions on control of PRO.</p></li>
<li><p>Explain why PRO is null.</p></li>
<li><p>Distinguish between PRO and pro.</p></li>
<li><p>Determine how a language has set the null subject parameter.</p></li>
</ul>
<h2 id="conclusions-14">Conclusions</h2>
<p>　We started this chapter with the observation that certain sentences, even though they look alike on the surface, can actually have very different syntactic trees. We compared subject-to-subject raising constructions to subject control constructions, and subject-to-object raising constructions to object control constructions. You can test for these various construction types by working out their argument structure, and using the idiom test. Next under consideration was the issue of what kind of DP PRO is. We claimed that it only showed up in Caseless positions. We also saw that it didn’t meet any of the binding conditions, and suggested it is subject, instead, to control theory. Control theory is a bit of a mystery, but may involve syntactic, thematic, and pragmatic features. We closed the chapter by comparing two different kinds of null subject categories: PRO and pro. PRO is Caseless and is subject to the theory of control. On the other hand, pro takes Case and is often “licensed” by rich agreement morphology on the verb.</p>
<ul>
<li>This is not a universally true statement. Many Asian languages allow pro-drop even though they don’t have rich agreement systems. For discussion, see Huang (1989).</li>
</ul>
<h1 id="chapter-16">Chapter 16</h1>
<h2 id="learning-objectives-15">Learning Objectives</h2>
<p>After reading chapter 16 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify cases of VP ellipsis (including antecedent-contained deletion and pseudogapping) and sluicing.</p></li>
<li><p>Explain the licensing conditions on ellipsis and sluicing.</p></li>
<li><p>Provide arguments for and against the PF-deletion hypothesis and for and against the LF-copying analysis of ellipsis.</p></li>
<li><p>Explain the difference between strict and sloppy pronoun identity and why it occurs.</p></li>
<li><p>Demonstrate how movement of a DP through either QR or DP movement explains antecedent-contained deletion.</p></li>
<li><p>Show how DP movement of objects explains the non-constituent deletion effects in pseudogapping.</p></li>
</ul>
<h2 id="conclusions-15">Conclusions</h2>
<p>Ellipsis, the syntax of missing elements, is a tempting playground for syntacticians. What could be more appealing than trying to deduce the properties of something you know has to be there because you can interpret it, but at the same time has no overt expression in what we see or write? We’ve looked at a few different kinds of ellipsis (VP ellipsis, ACD, pseudogapping, and sluicing) and investigated what they have in common and what they don’t.</p>
<p>　　In the chapters leading up to this one, I’ve tried to give you a consistent set of analyses that follow from a sequence of hypothesis testing and hypothesis revision, building from phrase structure to X-bar theory to movement rules. This chapter has been deliberately different. Here, I’ve tried to give you a taste of what a working syntactician faces each day. We’ve looked at a series of phenomena and for each one posited a couple of conflicting hypotheses. So for example, looking at how elided material gets its meaning we considered two different hypotheses: LF copying and PF deletion. We looked at conflicting evidence that shows that either of them might be right or wrong. We did the same with ACD and pseudogapping. This kind of investigation is the bread and butter of what syntacticians do each day. They look at a puzzling set of data, consider different hypotheses and the predictions they make and weigh the evidence one way or another.</p>
<p>　　</p>
<h1 id="chapter-17">Chapter 17</h1>
<h2 id="learning-objectives-16">Learning Objectives</h2>
<p>After reading chapter 17 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Explain why the binding conditions appear to hold both before and after movement.</p></li>
<li><p>Describe the copy theory of movement.</p></li>
<li><p>Explain how the copy theory of movement explains the ordering paradox.</p></li>
<li><p>Explain the data that shows that pronouns and anaphors seem to have different binding domains.</p></li>
<li><p>Be able to identify the new binding domains for pronouns and anaphors making reference to “potential antecedents”.</p></li>
</ul>
<h2 id="conclusions-16">Conclusions</h2>
<h1 id="chapter-18">Chapter 18</h1>
<h2 id="learning-objectives-17">Learning Objectives</h2>
<p>After reading chapter 18 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Compare and contrast the syntax-free and radical pro-drop hypotheses approaches to polysynthesis.</p></li>
<li><p>Identify data that supports the movement analysis of incorporation.</p></li>
<li><p>Explain the movement approach to scrambling.</p></li>
<li><p>Compare and contrast the three approaches to non-configurationality (the dual-structure approach, the pronominal argument hypothesis, and the movement approach).</p></li>
</ul>
<h2 id="conclusions-17">Conclusions</h2>
<p>Polysynthesis, incorporation, scrambling, and non-configurationality are certainly serious challenges to innate generative grammar. But they aren’t insurmountable ones. A nuanced approach to investigating languages with these phenomena shows that they might have more in common with more familiar languages than we might think at first glance. For each phenomenon, I have given brief sketches of some common analyses within generative grammar. As in the previous few chapters, I’ve left it open as to whether these analyses are right or not. I encourage you to discuss these phenomena and the hypotheses about them with your fellow students and professors and try to figure out what parts of them are right and what are wrong. We started this book with the observation that syntax was a science. We propose hypotheses, test them, revise them in some cases and discard them in others. As in any other science there are plenty of open questions and unresolved issues. I hope the past few chapters have given you a taste for this.</p>
<p>　　In part 5 of this book – which is only available on the website for this book, I offer a brief description of two alternative approaches to syntax. Again like any science, we have competing approaches to difficult questions. As you work your way to becoming a syntactician, it’s worth taking your time to consider alternatives and test your hypotheses against new and challenging data like that found in this chapter.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>句法</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 BERT 对文本进行分类</title>
    <url>/tensorflow-1/</url>
    <content><![CDATA[<p>本教程包含微调 BERT 以对纯文本 IMDB 电影评论数据集执行情感分析的完整代码。除了训练模型之外，您还将学习如何将文本预处理为适当的格式。</p>
<span id="more"></span>
<p>在本笔记本中，您将：</p>
<ul>
<li>加载 IMDB 数据集</li>
<li>从 TensorFlow Hub 加载 BERT 模型</li>
<li>通过将 BERT 与分类器相结合来构建您自己的模型</li>
<li>训练你自己的模型，微调 BERT 作为其中的一部分</li>
<li>保存模型并使用它对句子进行分类</li>
</ul>
<p>如果您不熟悉 IMDB 数据集，请参阅<a href="https://tensorflow.google.cn/tutorials/keras/text_classification?hl=zh-cn">基本文本分类</a>以了解更多详细信息。</p>
<h2 id="关于bert">关于BERT</h2>
<p><a href="https://arxiv.org/abs/1810.04805">BERT</a>和其他 Transformer 编码器架构在 NLP（自然语言处理）中的各种任务上取得了巨大成功。他们计算适用于深度学习模型的自然语言的向量空间表示。BERT 系列模型使用 Transformer 编码器架构在前后所有标记的完整上下文中处理输入文本的每个标记，因此得名：Bidirectional Encoder Representations from Transformers。</p>
<p>BERT 模型通常在大型文本语料库上进行预训练，然后针对特定任务进行微调。</p>
<h2 id="设置">设置</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># A dependency of the preprocessing for BERT inputs</span></span><br><span class="line">pip install -q -U tensorflow-text</span><br></pre></td></tr></table></figure>
<p>您将使用<a href="https://github.com/tensorflow/models">tensorflow/models 中</a>的 AdamW 优化器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub</span><br><span class="line"><span class="keyword">import</span> tensorflow_text <span class="keyword">as</span> text</span><br><span class="line"><span class="keyword">from</span> official.nlp <span class="keyword">import</span> optimization  <span class="comment"># to create AdamW optimizer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">tf.get_logger().setLevel(<span class="string">&#x27;ERROR&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="情感分析">情感分析</h2>
<p>这个笔记本训练了一个情感分析模型，根据评论的文本将电影评论分为<em>正面</em>或<em>负面</em>。</p>
<p>您将使用包含来自<a href="https://www.imdb.com/">Internet 电影数据库</a>的 50,000 条电影评论的文本的<a href="https://ai.stanford.edu/~amaas/data/sentiment/">大型电影评论数据集</a>。</p>
<h3 id="下载-imdb-数据集">下载 IMDB 数据集</h3>
<p>让我们下载并提取数据集，然后探索目录结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url = <span class="string">&#x27;https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&#x27;</span></span><br><span class="line"></span><br><span class="line">dataset = tf.keras.utils.get_file(<span class="string">&#x27;aclImdb_v1.tar.gz&#x27;</span>, url,</span><br><span class="line">                                  untar=<span class="literal">True</span>, cache_dir=<span class="string">&#x27;.&#x27;</span>,</span><br><span class="line">                                  cache_subdir=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dataset_dir = os.path.join(os.path.dirname(dataset), <span class="string">&#x27;aclImdb&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_dir = os.path.join(dataset_dir, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove unused folders to make it easier to load the data</span></span><br><span class="line">remove_dir = os.path.join(train_dir, <span class="string">&#x27;unsup&#x27;</span>)</span><br><span class="line">shutil.rmtree(remove_dir)</span><br><span class="line">Downloading data <span class="keyword">from</span> https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</span><br><span class="line"><span class="number">84131840</span>/<span class="number">84125825</span> [==============================] - 7s 0us/step</span><br></pre></td></tr></table></figure>
<p>接下来，您将使用该<code>text_dataset_from_directory</code>实用程序创建一个带标签的<a href="https://tensorflow.google.cn/api_docs/python/tf/data/Dataset?hl=zh-cn"><code>tf.data.Dataset</code></a>.</p>
<p>IMDB 数据集已经分为训练和测试，但缺少验证集。让我们使用以下<code>validation_split</code>参数使用 80:20 的训练数据拆分创建验证集。</p>
<p><strong>注意：</strong> 使用<code>validation_split</code>和<code>subset</code>参数时，请确保指定随机种子或通过<code>shuffle=False</code>，以便验证和训练分割没有重叠。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">AUTOTUNE = tf.data.AUTOTUNE</span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">seed = <span class="number">42</span> <span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line">raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(</span><br><span class="line">    <span class="string">&#x27;aclImdb/train&#x27;</span>,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    validation_split=<span class="number">0.2</span>,</span><br><span class="line">    subset=<span class="string">&#x27;training&#x27;</span>,</span><br><span class="line">    seed=seed)</span><br><span class="line"></span><br><span class="line">class_names = raw_train_ds.class_names</span><br><span class="line">train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br><span class="line"></span><br><span class="line">val_ds = tf.keras.preprocessing.text_dataset_from_directory(</span><br><span class="line">    <span class="string">&#x27;aclImdb/train&#x27;</span>,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    validation_split=<span class="number">0.2</span>,</span><br><span class="line">    subset=<span class="string">&#x27;validation&#x27;</span>,</span><br><span class="line">    seed=seed)</span><br><span class="line"></span><br><span class="line">val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br><span class="line"></span><br><span class="line">test_ds = tf.keras.preprocessing.text_dataset_from_directory(</span><br><span class="line">    <span class="string">&#x27;aclImdb/test&#x27;</span>,</span><br><span class="line">    batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br><span class="line">Found <span class="number">25000</span> files belonging to <span class="number">2</span> classes.</span><br><span class="line">Using <span class="number">20000</span> files <span class="keyword">for</span> training.</span><br><span class="line">Found <span class="number">25000</span> files belonging to <span class="number">2</span> classes.</span><br><span class="line">Using <span class="number">5000</span> files <span class="keyword">for</span> validation.</span><br><span class="line">Found <span class="number">25000</span> files belonging to <span class="number">2</span> classes.</span><br></pre></td></tr></table></figure>
<p>让我们来看看一些评论。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> text_batch, label_batch <span class="keyword">in</span> train_ds.take(<span class="number">1</span>):</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    print(<span class="string">f&#x27;Review: <span class="subst">&#123;text_batch.numpy()[i]&#125;</span>&#x27;</span>)</span><br><span class="line">    label = label_batch.numpy()[i]</span><br><span class="line">    print(<span class="string">f&#x27;Label : <span class="subst">&#123;label&#125;</span> (<span class="subst">&#123;class_names[label]&#125;</span>)&#x27;</span>)</span><br><span class="line">Review: <span class="string">b&#x27;&quot;Pandemonium&quot; is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. &quot;Airplane&quot;, &quot;The Naked Gun&quot; trilogy, &quot;Blazing Saddles&quot;, &quot;High Anxiety&quot;, and &quot;Spaceballs&quot; are some of my favorite comedies that spoof a particular genre. &quot;Pandemonium&quot; is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\&#x27;t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\&#x27;s all this film has going for it. Geez, &quot;Scream&quot; had more laughs than this film and that was more of a horror film. How bizarre is that?&lt;br /&gt;&lt;br /&gt;*1/2 (out of four)&#x27;</span></span><br><span class="line">Label : <span class="number">0</span> (neg)</span><br><span class="line">Review: <span class="string">b&quot;David Mamet is a very interesting and a very un-equal director. His first movie &#x27;House of Games&#x27; was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.&lt;br /&gt;&lt;br /&gt;So is &#x27;Homicide&#x27; which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.&lt;br /&gt;&lt;br /&gt;This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.&lt;br /&gt;&lt;br /&gt;Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.&quot;</span></span><br><span class="line">Label : <span class="number">0</span> (neg)</span><br><span class="line">Review: <span class="string">b&#x27;Great documentary about the lives of NY firefighters during the worst terrorist attack of all time.. That reason alone is why this should be a must see collectors item.. What shocked me was not only the attacks, but the&quot;High Fat Diet&quot; and physical appearance of some of these firefighters. I think a lot of Doctors would agree with me that,in the physical shape they were in, some of these firefighters would NOT of made it to the 79th floor carrying over 60 lbs of gear. Having said that i now have a greater respect for firefighters and i realize becoming a firefighter is a life altering job. The French have a history of making great documentary\&#x27;s and that is what this is, a Great Documentary.....&#x27;</span></span><br><span class="line">Label : <span class="number">1</span> (pos)</span><br></pre></td></tr></table></figure>
<h2 id="从-tensorflow-hub-加载模型">从 TensorFlow Hub 加载模型</h2>
<p>您可以在此处选择将从 TensorFlow Hub 加载的 BERT 模型并进行微调。有多种 BERT 模型可用。</p>
<ul>
<li><a href="https://hub.tensorflow.google.cn/tensorflow/bert_en_uncased_L-12_H-768_A-12/3?hl=zh-cn">BERT-Base</a>、<a href="https://hub.tensorflow.google.cn/tensorflow/bert_en_uncased_L-12_H-768_A-12/3?hl=zh-cn">Uncased</a>和另外<a href="https://hub.tensorflow.google.cn/google/collections/bert/1?hl=zh-cn">七个</a>具有训练权重的<a href="https://hub.tensorflow.google.cn/google/collections/bert/1?hl=zh-cn">模型</a>，<a href="https://hub.tensorflow.google.cn/google/collections/bert/1?hl=zh-cn">这些模型</a>由 BERT 的原始作者发布。</li>
<li><a href="https://hub.tensorflow.google.cn/google/collections/bert/1?hl=zh-cn">小型 BERT</a>具有相同的通用架构，但具有更少和/或更小的 Transformer 块，这让您可以探索速度、大小和质量之间的权衡。</li>
<li><a href="https://hub.tensorflow.google.cn/google/collections/albert/1?hl=zh-cn">ALBERT</a>：四种不同大小的“A Lite BERT”，通过在层之间共享参数来减少模型大小（但不是计算时间）。</li>
<li><a href="https://hub.tensorflow.google.cn/google/collections/experts/bert/1?hl=zh-cn">BERT 专家</a>：八个模型都具有 BERT 基础架构，但提供不同预训练域之间的选择，以更紧密地与目标任务保持一致。</li>
<li><a href="https://hub.tensorflow.google.cn/google/collections/electra/1?hl=zh-cn">Electra</a>具有与 BERT 相同的架构（具有三种不同的大小），但在类似于生成对抗网络 (GAN) 的设置中作为鉴别器进行了预训练。</li>
<li>带有 Talking-Heads Attention 和 Gated GELU [ <a href="https://hub.tensorflow.google.cn/tensorflow/talkheads_ggelu_bert_en_base/1?hl=zh-cn">base</a> , <a href="https://hub.tensorflow.google.cn/tensorflow/talkheads_ggelu_bert_en_large/1?hl=zh-cn">large</a> ] 的BERT对 Transformer 架构的核心有两个改进。</li>
</ul>
<p>TensorFlow Hub 上的模型文档有更多详细信息和对研究文献的参考。按照上面的链接，或单击<a href="http://hub.tensorflow.google.cn/?hl=zh-cn"><code>hub.tensorflow.google.cn</code></a>下一个单元格执行后打印的URL。</p>
<p>建议从小型 BERT（参数较少）开始，因为它们可以更快地进行微调。如果您喜欢小模型但精度更高，ALBERT 可能是您的下一个选择。如果您想要更高的准确性，请选择经典的 BERT 大小之一或其最近的改进，例如 Electra、Talking Heads 或 BERT Expert。</p>
<p>除了下面可用的模型之外，还有<a href="https://hub.tensorflow.google.cn/google/collections/transformer_encoders_text/1?hl=zh-cn">多个版本</a>的模型更大，可以产生更高的精度，但它们太大而无法在单个 GPU 上进行微调。您将能够<a href="https://tensorflow.google.cn/text/tutorials/bert_glue?hl=zh-cn">在 TPU colab 上使用 BERT 在 Solve GLUE 任务</a>上做到这<a href="https://tensorflow.google.cn/text/tutorials/bert_glue?hl=zh-cn">一点</a>。</p>
<p>您将在下面的代码中看到，切换 hub.tensorflow.google.cn URL 足以尝试这些模型中的任何一个，因为它们之间的所有差异都封装在来自 TF Hub 的 SavedModels 中。</p>
<h3 id="选择一个-bert-模型进行微调">选择一个 BERT 模型进行微调</h3>
<p>切换代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BERT model selected           : https:&#x2F;&#x2F;hub.tensorflow.google.cn&#x2F;tensorflow&#x2F;small_bert&#x2F;bert_en_uncased_L-4_H-512_A-8&#x2F;1</span><br><span class="line">Preprocess model auto-selected: https:&#x2F;&#x2F;hub.tensorflow.google.cn&#x2F;tensorflow&#x2F;bert_en_uncased_preprocess&#x2F;3</span><br></pre></td></tr></table></figure>
<h2 id="预处理模型">预处理模型</h2>
<p>在输入到 BERT 之前，文本输入需要转换为数字标记 id 并排列在几个 Tensor 中。TensorFlow Hub 为上面讨论的每个 BERT 模型提供了一个匹配的预处理模型，它使用来自 TF.text 库的 TF ops 来实现这种转换。无需在 TensorFlow 模型之外运行纯 Python 代码来预处理文本。</p>
<p>预处理模型必须是 BERT 模型文档中引用的模型，您可以在上面打印的 URL 中阅读该文档。对于上面下拉列表中的 BERT 模型，会自动选择预处理模型。</p>
<p><strong>注意：</strong>您将预处理模型加载到<a href="https://tensorflow.google.cn/hub/api_docs/python/hub/KerasLayer?hl=zh-cn">hub.KerasLayer</a>以组成您的微调模型。这是将 TF2 样式的 SavedModel 从 TF Hub 加载到 Keras 模型的首选 API。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)</span><br></pre></td></tr></table></figure>
<p>让我们在一些文本上尝试预处理模型并查看输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text_test = [<span class="string">&#x27;this is such an amazing movie!&#x27;</span>]</span><br><span class="line">text_preprocessed = bert_preprocess_model(text_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;Keys       : <span class="subst">&#123;<span class="built_in">list</span>(text_preprocessed.keys())&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Shape      : <span class="subst">&#123;text_preprocessed[<span class="string">&quot;input_word_ids&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Word Ids   : <span class="subst">&#123;text_preprocessed[<span class="string">&quot;input_word_ids&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Input Mask : <span class="subst">&#123;text_preprocessed[<span class="string">&quot;input_mask&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Type Ids   : <span class="subst">&#123;text_preprocessed[<span class="string">&quot;input_type_ids&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">Keys       : [<span class="string">&#x27;input_type_ids&#x27;</span>, <span class="string">&#x27;input_mask&#x27;</span>, <span class="string">&#x27;input_word_ids&#x27;</span>]</span><br><span class="line">Shape      : (<span class="number">1</span>, <span class="number">128</span>)</span><br><span class="line">Word Ids   : [ <span class="number">101</span> <span class="number">2023</span> <span class="number">2003</span> <span class="number">2107</span> <span class="number">2019</span> <span class="number">6429</span> <span class="number">3185</span>  <span class="number">999</span>  <span class="number">102</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>]</span><br><span class="line">Input Mask : [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">Type Ids   : [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>如您所见，现在您拥有 BERT 模型将使用的预处理的 3 个输出（<code>input_words_id</code>、<code>input_mask</code>和<code>input_type_ids</code>）。</p>
<p>其他一些要点：</p>
<ul>
<li>输入被截断为 128 个标记。令牌的数量可以自定义，您可以<a href="https://tensorflow.google.cn/text/tutorials/bert_glue?hl=zh-cn">在 TPU colab 上</a>查看有关<a href="https://tensorflow.google.cn/text/tutorials/bert_glue?hl=zh-cn">使用 BERT 解决 GLUE 任务的</a>更多详细信息。</li>
<li>在<code>input_type_ids</code>仅具有一个值（0），因为这是一个简单的句子的输入。对于多句输入，每个输入都有一个数字。</li>
</ul>
<p>由于这个文本预处理器是一个 TensorFlow 模型，它可以直接包含在你的模型中。</p>
<h2 id="使用-bert-模型">使用 BERT 模型</h2>
<p>在将 BERT 放入您自己的模型之前，让我们先看看它的输出。您将从 TF Hub 加载它并查看返回值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bert_model = hub.KerasLayer(tfhub_handle_encoder)</span><br><span class="line">bert_results = bert_model(text_preprocessed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;Loaded BERT: <span class="subst">&#123;tfhub_handle_encoder&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Pooled Outputs Shape:<span class="subst">&#123;bert_results[<span class="string">&quot;pooled_output&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Pooled Outputs Values:<span class="subst">&#123;bert_results[<span class="string">&quot;pooled_output&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Sequence Outputs Shape:<span class="subst">&#123;bert_results[<span class="string">&quot;sequence_output&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Sequence Outputs Values:<span class="subst">&#123;bert_results[<span class="string">&quot;sequence_output&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">Loaded BERT: https://hub.tensorflow.google.cn/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-<span class="number">8</span>/<span class="number">1</span></span><br><span class="line">Pooled Outputs Shape:(<span class="number">1</span>, <span class="number">512</span>)</span><br><span class="line">Pooled Outputs Values:[ <span class="number">0.76262873</span>  <span class="number">0.99280983</span> -<span class="number">0.1861186</span>   <span class="number">0.36673835</span>  <span class="number">0.15233682</span>  <span class="number">0.65504444</span></span><br><span class="line">  <span class="number">0.9681154</span>  -<span class="number">0.9486272</span>   <span class="number">0.00216158</span> -<span class="number">0.9877732</span>   <span class="number">0.0684272</span>  -<span class="number">0.9763061</span> ]</span><br><span class="line">Sequence Outputs Shape:(<span class="number">1</span>, <span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line">Sequence Outputs Values:[[-<span class="number">0.28946388</span>  <span class="number">0.3432126</span>   <span class="number">0.33231565</span> ...  <span class="number">0.21300787</span>  <span class="number">0.7102078</span></span><br><span class="line">  -<span class="number">0.05771166</span>]</span><br><span class="line"> [-<span class="number">0.28742015</span>  <span class="number">0.31981024</span> -<span class="number">0.2301858</span>  ...  <span class="number">0.58455074</span> -<span class="number">0.21329722</span></span><br><span class="line">   <span class="number">0.7269209</span> ]</span><br><span class="line"> [-<span class="number">0.66157013</span>  <span class="number">0.6887685</span>  -<span class="number">0.87432927</span> ...  <span class="number">0.10877253</span> -<span class="number">0.26173282</span></span><br><span class="line">   <span class="number">0.47855264</span>]</span><br><span class="line"> ...</span><br><span class="line"> [-<span class="number">0.2256118</span>  -<span class="number">0.28925604</span> -<span class="number">0.07064401</span> ...  <span class="number">0.4756601</span>   <span class="number">0.8327715</span></span><br><span class="line">   <span class="number">0.40025353</span>]</span><br><span class="line"> [-<span class="number">0.29824278</span> -<span class="number">0.27473143</span> -<span class="number">0.05450511</span> ...  <span class="number">0.48849759</span>  <span class="number">1.0955356</span></span><br><span class="line">   <span class="number">0.18163344</span>]</span><br><span class="line"> [-<span class="number">0.44378197</span>  <span class="number">0.00930723</span>  <span class="number">0.07223766</span> ...  <span class="number">0.1729009</span>   <span class="number">1.1833246</span></span><br><span class="line">   <span class="number">0.07897988</span>]]</span><br></pre></td></tr></table></figure>
<p>BERT 模型返回一个带有 3 个重要键的映射：<code>pooled_output</code>、<code>sequence_output</code>、<code>encoder_outputs</code>：</p>
<ul>
<li><code>pooled_output</code>将每个输入序列表示为一个整体。形状是<code>[batch_size, H]</code>。您可以将其视为整个电影评论的嵌入。</li>
<li><code>sequence_output</code>表示上下文中的每个输入标记。形状是<code>[batch_size, seq_length, H]</code>。您可以将其视为电影评论中每个标记的上下文嵌入。</li>
<li><code>encoder_outputs</code>是<code>L</code>Transformer 模块的中间激活。<code>outputs["encoder_outputs"][i]</code>是具有<code>[batch_size, seq_length, 1024]</code>第 i 个 Transformer 模块输出的形状张量，对于<code>0 &lt;= i &lt; L</code>。列表的最后一个值等于<code>sequence_output</code>。</li>
</ul>
<p>对于微调，您将使用<code>pooled_output</code>数组。</p>
<h2 id="定义你的模型">定义你的模型</h2>
<p>您将创建一个非常简单的微调模型，其中包含预处理模型、选定的 BERT 模型、一个 Dense 层和一个 Dropout 层。</p>
<p><strong>注意：</strong>有关基本模型的输入和输出的更多信息，您可以按照模型的 URL 获取文档。在这里，您无需担心，因为预处理模型会为您处理好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_classifier_model</span>():</span></span><br><span class="line">  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=<span class="string">&#x27;text&#x27;</span>)</span><br><span class="line">  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=<span class="string">&#x27;preprocessing&#x27;</span>)</span><br><span class="line">  encoder_inputs = preprocessing_layer(text_input)</span><br><span class="line">  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=<span class="literal">True</span>, name=<span class="string">&#x27;BERT_encoder&#x27;</span>)</span><br><span class="line">  outputs = encoder(encoder_inputs)</span><br><span class="line">  net = outputs[<span class="string">&#x27;pooled_output&#x27;</span>]</span><br><span class="line">  net = tf.keras.layers.Dropout(<span class="number">0.1</span>)(net)</span><br><span class="line">  net = tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="literal">None</span>, name=<span class="string">&#x27;classifier&#x27;</span>)(net)</span><br><span class="line">  <span class="keyword">return</span> tf.keras.Model(text_input, net)</span><br></pre></td></tr></table></figure>
<p>让我们检查模型是否与预处理模型的输出一起运行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifier_model = build_classifier_model()</span><br><span class="line">bert_raw_result = classifier_model(tf.constant(text_test))</span><br><span class="line">print(tf.sigmoid(bert_raw_result))</span><br><span class="line">tf.Tensor([[<span class="number">0.50131935</span>]], shape=(<span class="number">1</span>, <span class="number">1</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>当然，输出是没有意义的，因为模型还没有经过训练。</p>
<p>让我们来看看模型的结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.utils.plot_model(classifier_model)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://www.tensorflow.org/text/tutorials/classify_text_with_bert_files/output_0EmzyHZXKIpm_0.png" alt="PNG" /><figcaption aria-hidden="true">PNG</figcaption>
</figure>
<h2 id="模型训练">模型训练</h2>
<p>您现在拥有训练模型的所有部分，包括预处理模块、BERT 编码器、数据和分类器。</p>
<h3 id="损失函数">损失函数</h3>
<p>由于这是一个二元分类问题，并且模型输出概率（单单元层），因此您将使用<a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy"><code>losses.BinaryCrossentropy</code></a>损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = tf.keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">metrics = tf.metrics.BinaryAccuracy()</span><br></pre></td></tr></table></figure>
<h3 id="优化器">优化器</h3>
<p>对于微调，让我们使用与 BERT 最初训练时相同的优化器：“自适应时刻”（Adam）。这个优化器最大限度地减少了预测损失，并通过权重衰减（不使用矩）进行正则化，这也称为<a href="https://arxiv.org/abs/1711.05101">AdamW</a>。</p>
<p>对于学习率 ( <code>init_lr</code>)，您将使用与 BERT 预训练相同的时间表：概念初始学习率的线性衰减，在前 10% 的训练步骤 ( <code>num_warmup_steps</code>) 中以线性预热阶段为前缀。与 BERT 论文一致，微调的初始学习率较小（5e-5、3e-5、2e-5 中的最佳）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="number">5</span></span><br><span class="line">steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()</span><br><span class="line">num_train_steps = steps_per_epoch * epochs</span><br><span class="line">num_warmup_steps = <span class="built_in">int</span>(<span class="number">0.1</span>*num_train_steps)</span><br><span class="line"></span><br><span class="line">init_lr = <span class="number">3e-5</span></span><br><span class="line">optimizer = optimization.create_optimizer(init_lr=init_lr,</span><br><span class="line">                                          num_train_steps=num_train_steps,</span><br><span class="line">                                          num_warmup_steps=num_warmup_steps,</span><br><span class="line">                                          optimizer_type=<span class="string">&#x27;adamw&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="加载-bert-模型和训练">加载 BERT 模型和训练</h3>
<p>使用<code>classifier_model</code>您之前创建的，您可以使用损失、度量和优化器编译模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifier_model.<span class="built_in">compile</span>(optimizer=optimizer,</span><br><span class="line">                         loss=loss,</span><br><span class="line">                         metrics=metrics)</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>训练时间会根据您选择的 BERT 模型的复杂程度而有所不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">f&#x27;Training model with <span class="subst">&#123;tfhub_handle_encoder&#125;</span>&#x27;</span>)</span><br><span class="line">history = classifier_model.fit(x=train_ds,</span><br><span class="line">                               validation_data=val_ds,</span><br><span class="line">                               epochs=epochs)</span><br><span class="line">Training model <span class="keyword">with</span> https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-<span class="number">8</span>/<span class="number">1</span></span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">5</span></span><br><span class="line"><span class="number">625</span>/<span class="number">625</span> [==============================] - 83s 124ms/step - loss: <span class="number">0.4881</span> - binary_accuracy: <span class="number">0.7403</span> - val_loss: <span class="number">0.3917</span> - val_binary_accuracy: <span class="number">0.8340</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">5</span></span><br><span class="line"><span class="number">625</span>/<span class="number">625</span> [==============================] - 77s 124ms/step - loss: <span class="number">0.3296</span> - binary_accuracy: <span class="number">0.8518</span> - val_loss: <span class="number">0.3714</span> - val_binary_accuracy: <span class="number">0.8450</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">5</span></span><br><span class="line"><span class="number">625</span>/<span class="number">625</span> [==============================] - 78s 124ms/step - loss: <span class="number">0.2530</span> - binary_accuracy: <span class="number">0.8939</span> - val_loss: <span class="number">0.4036</span> - val_binary_accuracy: <span class="number">0.8486</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">5</span></span><br><span class="line"><span class="number">625</span>/<span class="number">625</span> [==============================] - 78s 124ms/step - loss: <span class="number">0.1968</span> - binary_accuracy: <span class="number">0.9226</span> - val_loss: <span class="number">0.4468</span> - val_binary_accuracy: <span class="number">0.8502</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">5</span></span><br><span class="line"><span class="number">625</span>/<span class="number">625</span> [==============================] - 78s 124ms/step - loss: <span class="number">0.1604</span> - binary_accuracy: <span class="number">0.9392</span> - val_loss: <span class="number">0.4716</span> - val_binary_accuracy: <span class="number">0.8498</span></span><br></pre></td></tr></table></figure>
<h3 id="评估模型">评估模型</h3>
<p>让我们看看模型的表现如何。将返回两个值。损失（代表误差的数字，值越低越好）和准确度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss, accuracy = classifier_model.evaluate(test_ds)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;Loss: <span class="subst">&#123;loss&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="number">782</span>/<span class="number">782</span> [==============================] - 53s 67ms/step - loss: <span class="number">0.4476</span> - binary_accuracy: <span class="number">0.8554</span></span><br><span class="line">Loss: <span class="number">0.44761356711387634</span></span><br><span class="line">Accuracy: <span class="number">0.8554400205612183</span></span><br></pre></td></tr></table></figure>
<h3 id="绘制准确度和损失随时间的图">绘制准确度和损失随时间的图</h3>
<p>基于<code>History</code>返回的对象<code>model.fit()</code>。您可以绘制训练和验证损失以进行比较，以及训练和验证准确度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history_dict = history.history</span><br><span class="line">print(history_dict.keys())</span><br><span class="line"></span><br><span class="line">acc = history_dict[<span class="string">&#x27;binary_accuracy&#x27;</span>]</span><br><span class="line">val_acc = history_dict[<span class="string">&#x27;val_binary_accuracy&#x27;</span>]</span><br><span class="line">loss = history_dict[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history_dict[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">epochs = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(acc) + <span class="number">1</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">fig.tight_layout()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># &quot;bo&quot; is for &quot;blue dot&quot;</span></span><br><span class="line">plt.plot(epochs, loss, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line"><span class="comment"># b is for &quot;solid blue line&quot;</span></span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and validation loss&#x27;</span>)</span><br><span class="line"><span class="comment"># plt.xlabel(&#x27;Epochs&#x27;)</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(epochs, acc, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Training acc&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation acc&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">dict_keys([<span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;binary_accuracy&#x27;</span>, <span class="string">&#x27;val_loss&#x27;</span>, <span class="string">&#x27;val_binary_accuracy&#x27;</span>])</span><br><span class="line">&lt;matplotlib.legend.Legend at <span class="number">0x7f542fffc590</span>&gt;</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://www.tensorflow.org/text/tutorials/classify_text_with_bert_files/output_fiythcODf0xo_2.png" alt="PNG" /><figcaption aria-hidden="true">PNG</figcaption>
</figure>
<p>在这个图中，红线代表训练损失和准确率，蓝线代表验证损失和准确率。</p>
<h2 id="导出以进行推理">导出以进行推理</h2>
<p>现在您只需保存您的微调模型以备后用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset_name = <span class="string">&#x27;imdb&#x27;</span></span><br><span class="line">saved_model_path = <span class="string">&#x27;./&#123;&#125;_bert&#x27;</span>.<span class="built_in">format</span>(dataset_name.replace(<span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;_&#x27;</span>))</span><br><span class="line"></span><br><span class="line">classifier_model.save(saved_model_path, include_optimizer=<span class="literal">False</span>)</span><br><span class="line">WARNING:absl:Found untraced functions such <span class="keyword">as</span> restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body <span class="keyword">while</span> saving (showing <span class="number">5</span> of <span class="number">310</span>). These functions will <span class="keyword">not</span> be directly <span class="built_in">callable</span> after loading.</span><br></pre></td></tr></table></figure>
<p>让我们重新加载模型，以便您可以与仍在内存中的模型并排尝试。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reloaded_model = tf.saved_model.load(saved_model_path)</span><br></pre></td></tr></table></figure>
<p>在这里，您可以在您想要的任何句子上测试您的模型，只需添加到下面的示例变量即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_my_examples</span>(<span class="params">inputs, results</span>):</span></span><br><span class="line">  result_for_printing = \</span><br><span class="line">    [<span class="string">f&#x27;input: <span class="subst">&#123;inputs[i]:&lt;<span class="number">30</span>&#125;</span> : score: <span class="subst">&#123;results[i][<span class="number">0</span>]:<span class="number">.6</span>f&#125;</span>&#x27;</span></span><br><span class="line">                         <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(inputs))]</span><br><span class="line">  print(*result_for_printing, sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">  print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">examples = [</span><br><span class="line">    <span class="string">&#x27;this is such an amazing movie!&#x27;</span>,  <span class="comment"># this is the same sentence tried earlier</span></span><br><span class="line">    <span class="string">&#x27;The movie was great!&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;The movie was meh.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;The movie was okish.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;The movie was terrible...&#x27;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))</span><br><span class="line">original_results = tf.sigmoid(classifier_model(tf.constant(examples)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Results from the saved model:&#x27;</span>)</span><br><span class="line">print_my_examples(examples, reloaded_results)</span><br><span class="line">print(<span class="string">&#x27;Results from the model in memory:&#x27;</span>)</span><br><span class="line">print_my_examples(examples, original_results)</span><br><span class="line">Results <span class="keyword">from</span> the saved model:</span><br><span class="line"><span class="built_in">input</span>: this <span class="keyword">is</span> such an amazing movie! : score: <span class="number">0.998905</span></span><br><span class="line"><span class="built_in">input</span>: The movie was great!           : score: <span class="number">0.994330</span></span><br><span class="line"><span class="built_in">input</span>: The movie was meh.             : score: <span class="number">0.968163</span></span><br><span class="line"><span class="built_in">input</span>: The movie was okish.           : score: <span class="number">0.069656</span></span><br><span class="line"><span class="built_in">input</span>: The movie was terrible...      : score: <span class="number">0.000776</span></span><br><span class="line"></span><br><span class="line">Results <span class="keyword">from</span> the model <span class="keyword">in</span> memory:</span><br><span class="line"><span class="built_in">input</span>: this <span class="keyword">is</span> such an amazing movie! : score: <span class="number">0.998905</span></span><br><span class="line"><span class="built_in">input</span>: The movie was great!           : score: <span class="number">0.994330</span></span><br><span class="line"><span class="built_in">input</span>: The movie was meh.             : score: <span class="number">0.968163</span></span><br><span class="line"><span class="built_in">input</span>: The movie was okish.           : score: <span class="number">0.069656</span></span><br><span class="line"><span class="built_in">input</span>: The movie was terrible...      : score: <span class="number">0.000776</span></span><br></pre></td></tr></table></figure>
<p>如果您想在<a href="https://www.tensorflow.org/tfx/guide/serving">TF Serving</a>上使用您的模型，请记住它会通过其命名签名之一调用您的 SavedModel。在 Python 中，您可以按如下方式测试它们：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">serving_results = reloaded_model \</span><br><span class="line">            .signatures[<span class="string">&#x27;serving_default&#x27;</span>](tf.constant(examples))</span><br><span class="line"></span><br><span class="line">serving_results = tf.sigmoid(serving_results[<span class="string">&#x27;classifier&#x27;</span>])</span><br><span class="line"></span><br><span class="line">print_my_examples(examples, serving_results)</span><br><span class="line"><span class="built_in">input</span>: this <span class="keyword">is</span> such an amazing movie! : score: <span class="number">0.998905</span></span><br><span class="line"><span class="built_in">input</span>: The movie was great!           : score: <span class="number">0.994330</span></span><br><span class="line"><span class="built_in">input</span>: The movie was meh.             : score: <span class="number">0.968163</span></span><br><span class="line"><span class="built_in">input</span>: The movie was okish.           : score: <span class="number">0.069656</span></span><br><span class="line"><span class="built_in">input</span>: The movie was terrible...      : score: <span class="number">0.000776</span></span><br></pre></td></tr></table></figure>
<h2 id="下一步">下一步</h2>
<p>作为下一步，您可以<a href="https://www.tensorflow.org/text/tutorials/bert_glue">在 TPU 教程上</a>尝试<a href="https://www.tensorflow.org/text/tutorials/bert_glue">使用 BERT 解决 GLUE 任务</a>，该<a href="https://www.tensorflow.org/text/tutorials/bert_glue">教程在 TPU</a>上运行并向您展示如何处理多个输入。</p>
]]></content>
  </entry>
  <entry>
    <title>Transformer详解</title>
    <url>/transformer/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://jalammar.github.io/illustrated-transformer/</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<p>上一篇文章中，我们了解了<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">注意力</a>——现代深度学习模型中无处不用的方法，它有助于提高神经机器翻译应用性能。在这篇文章中，我们将介绍<strong>The Transformer</strong>——一个使用注意力来提升这些模型训练速度的模型，它在特定任务中的表现优于谷歌神经机器翻译模型，然而其最大的好处来自于并行化。事实上，谷歌云建议使用The Transformer作为参考模型来使用他们的<a href="https://cloud.google.com/tpu/">TPU云产品</a>。那么，让我们试着把这个模型拆开，看看它是如何运作的。</p>
<p>The Transformer是在论文<a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>中提出的，其TensorFlow实现作为<a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a>包的一部分提供。哈佛大学的NLP小组创建了一个<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">指南</a>，用PyTorch实现对该论文进行注释。在这篇文章中，我们将尝试把一切充分简化，并逐一介绍这些概念，希望能让没有深入了解这一主题的人更容易理解。</p>
<span id="more"></span>
<h1 id="高层视角">高层视角</h1>
<p>我们先把这个模型看成一个黑盒子，在机器翻译应用中，它将接受一种语言的句子，并输出另一种语言的翻译。打开盒子之后，我们看到一个<strong>编码组件</strong>（encoding component）、一个解码组件（decoding component）、以及它们之间的连接。编码组件是一摞<strong>编码器</strong>（encoders ），文中把六个编码器堆在一起——6这个数字并不神奇，人们绝对可以尝试其他的数字）；解码组件则是一摞相同数量的<strong>解码器</strong>（decoders）。</p>
<p>所有编码器（encoders）具有相同结构（但并不共享权重），每一个都被分解成两个子层（sub-layers）：编码器的输入（inputs）首先流经一个<strong>自注意力层</strong>（self-attention layer）——该层帮助编码器在对特定单词进行编码时查看输入句子中的其他单词（本文后面将详细研究自注意力）；然后，自注意力层的输出（outputs ）被送入<strong>前馈神经网络</strong>（feed-forward neural network），完全相同的前馈网络被独立地应用于每个位置。</p>
<p>所有解码器也有这两个层，但它们之间有一个<strong>注意力层</strong>（attention layer），帮助解码器专注于输入句子的相关部分（类似于<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">seq2seq</a>模型中注意力的作用）。</p>
<p><img src="https://i.loli.net/2021/07/14/Pg4KWoZxGwcY7ds.png"/></p>
<h1 id="引入张量">引入张量</h1>
<p>现在我们已经看到了模型的主要组成部分，让我们开始看看各种<strong>向量</strong>/<strong>张量</strong>（vectors/tensors），以及它们如何在这些组成部分之间流动，将训练过的模型的输入转化为输出。</p>
<p>正如NLP应用中的一般情况一样，我们首先使用<strong>嵌入算法</strong>（<a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">embedding algorithm</a>）将每个输入词变成一个向量。每个词都被嵌入到一个大小为512的向量中。</p>
<p>嵌入只发生在最底层的编码器中。所有编码器都有一个共同的抽象，那就是它们收到一个大小为512的向量的列表——在最底层的编码器中就是单词嵌入，但在其他编码器中，接收的是正下方编码器的输出。这个列表的大小是我们可以设置的超参数--基本上就是我们训练数据集中最长的句子的长度。</p>
<p>在输入序列中嵌入单词后，每个单词都流经编码器的两层中的每一层。</p>
<p><img src="https://i.loli.net/2021/07/14/M2rwFeYzuvLZgRy.png"/></p>
<p>在这里，我们开始看到Transformer的一个关键属性，即每个位置的词在编码器中流经它自己的路径。在自注意力层的这些路径之间存在着依赖关系。然而，前馈层没有这些依赖关系，因此在流经前馈层时，各种路径可以<strong>并行</strong>地执行。</p>
<p>接下来，我们将把这个例子换成一个较短的句子，看看在编码器的每个子层中会发生什么。</p>
<h1 id="开始编码">开始编码</h1>
<p>正如我们提到的，一个编码器接收一个向量列表作为输入。它通过将这些向量传入自注力意层，然后传入前馈神经网络（完全相同的网络）来处理这个列表，然后将输出向上发送到下一个编码器。</p>
<p><img src="https://i.loli.net/2021/07/14/LVtNTB1AlhmFnzQ.png"/></p>
<h2 id="self-attention初窥">Self-attention初窥</h2>
<p>不要被 <strong>self-attention</strong> 这个词所迷惑，好像它是一个人人都应该熟悉的概念。我个人从来没有接触过这个概念，直到读了 Attention is All You Need 这篇文章。让我们来提炼一下它的作用。</p>
<p>假设下面这个句子是我们要翻译的输入句：</p>
<p>”<code>The animal didn't cross the street because it was too tired</code>”</p>
<p>这句话中的 “it” 指的是什么？street 还是 animal ？对人来说，这是一个简单的问题，但对算法来说就不那么简单了。</p>
<p>当模型处理 “it” 这个词时，自注意使它将 “it” 与 animal 联系起来。</p>
<p>当模型处理每个词（输入序列中的每个位置）时，self-attention 使它能够查看输入序列中的其他位置，以寻找有助于更好地编码这个词的线索。</p>
<p>如果你熟悉<strong>RNN</strong>，就会想到维持一个<strong>隐藏状态</strong>（hidden state）是如何让RNN把它以前处理过的词/向量的表征与它现在处理的词/向量结合起来的。Self-attention 是 Transformer 用来将其他相关词语的“理解”融入我们当前处理的词语的方法。</p>
<p><img src="https://i.loli.net/2021/07/14/TEPi7XZChFIuMkv.png"/> 请务必查看<a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Tensor2Tensor Notebook</a>，在那里你可以加载一个Transformer模型，并使用这个交互式可视化来检查它。</p>
<h2 id="self-attention细节">Self-attention细节</h2>
<p>让我们先看看如何使用向量来计算自注意力，然后继续看看它实际是如何使用矩阵实现的。</p>
<h3 id="第一步">第一步</h3>
<p>计算自注意力的第一步是，从编码器的每个输入向量（在这种情况下，每个词的嵌入）中创建三个向量。因此，对于每个词，我们创建一个<strong>查询向量</strong>（Query vector）、一个<strong>键向量</strong>（Key vector）和一个<strong>值向量</strong>（Value vector）。这些向量是通过将嵌入与我们将要训练的三个矩阵相乘而创建的。</p>
<p>请注意，这些新向量的维度比嵌入向量小。它们的维数是64，而嵌入和编码器输入/输出向量的维数是512。它们不是非要小一点，这是一个架构上的选择，以使多头注意力的计算（大部分）恒定。</p>
<p><img src="https://i.loli.net/2021/07/14/sJ26uACO9WIYHoP.png"/></p>
<p>x1与WQ权重矩阵相乘产生q1，即与该词相关的 <strong>Query vector</strong> 。类似的，我们最终为输入句子中的每个词创建了一个 "query "、一个 "key" 和一个 "value" 的投影。</p>
<p>什么是 <strong>query</strong> 、 <strong>key</strong> 和 <strong>value</strong> 向量？</p>
<p>它们是抽象的，对计算和思考注意力很有用。一旦你继续阅读下面的注意力计算方法，你就会知道几乎所有你需要知道的这些向量中每一个所起的作用。</p>
<h3 id="第二步">第二步</h3>
<p>计算自注意力的第二步是计算一个分数。比如我们要计算本例中第一个词 "think" 的自注意力。我们需要对输入句子中的每个词与这个词进行评分，分数决定了当我们在某个位置对一个词进行编码时，要对输入句子的其他部分给予多大的关注。</p>
<p>分数的计算方法是取查询向量（query vector）与我们要打分的各个单词的键向量（key vector）的<strong>点积</strong>（dot product）。因此，如果我们正在处理1号位置的单词的自注意力，第一个分数将是q1和k1的点积。第二个分数将是q1和k2的点积。</p>
<p><img src="https://i.loli.net/2021/07/14/vP6BlLpwnWV8Qcy.png"/></p>
<h3 id="剩余步骤">剩余步骤</h3>
<ul>
<li><p>第三和第四步是将该分数除以8（论文中使用的键向量尺寸是64的平方根），这导致拥有更稳定的梯度（gradients）。可以设定其他值，但这是默认值。然后将结果通过softmax操作，将分数归一化，使得它们都是正数且和为1。Softmax分数决定了每个词在这个位置被表达出来的程度。显然，这个位置上的词将有最高的softmax分数，但有时关注另一个与当前词相关的词也是有用的。</p></li>
<li><p>第五步是将每个值向量乘以softmax得分（准备将它们加起来）。这里的直觉是保持我们想要关注的单词的完整价值，并淹没不相关的词（例如，通过将它们乘以0.001这样的微小数字）。</p></li>
<li><p>第六步是将加权的值向量相加。这就产生了自注意力层在这个位置上的输出（对于第一个词）。</p></li>
</ul>
<p><img src="https://i.loli.net/2021/07/14/uwKaQPAS98MLkv1.png"/></p>
<p>这就结束了自注意力的计算，得到的<strong>向量</strong>是可以发送给前馈神经网络的。然而在实际执行中，这个计算是以<strong>矩阵</strong>形式进行的，以加快处理速度。所以，现在我们来看看单词层面上的矩阵计算。</p>
<h2 id="self-attention-的矩阵计算">self-attention 的矩阵计算</h2>
<p>第一步是计算查询、键和值矩阵。我们将embeddings打包成一个矩阵X，并将其与我们训练的权重矩阵（WQ、WK、WV）相乘。</p>
<p><img src="https://i.loli.net/2021/07/14/CUYO9ZEsryHmFd7.png"/></p>
<p>X矩阵中的每一行都对应于输入句子中的一个词。我们再次看到嵌入向量（512，即图中的4个方框）和q/k/v向量（64，即图中的3个方框）的大小差异。</p>
<p>最后，由于我们在处理矩阵，我们可以将第二至第六步浓缩在一个公式中，以计算自注意层的输出。</p>
<p><img src="https://i.loli.net/2021/07/14/3MtVGoElqOB6NXF.png"/></p>
<h1 id="多头的猛兽">多头的猛兽</h1>
<p>该论文进一步完善了自注意力层，增加了一种叫做 <strong>multi-headed attention</strong> 的机制。这在两个方面提高了注意力层的性能：</p>
<ul>
<li><p>它扩大了模型对不同位置的关注能力。在上面的例子中，z1包含一点其他的编码，但它可能被真正的单词本身所支配。当我们翻译 “The animal didn’t cross the street because it was too tired” 这样的句子，就会想知道 it 指的是哪个词。</p></li>
<li><p>它给了注意力层多个“<strong>表征子空间</strong>”（representation subspaces）。正如我们接下来要看到的，在多头注意力下，我们不是只有一个，而是有多组查询/键/值权重矩阵（Transformer 使用了八个attention head，所以我们最终为每个编码器/解码器提供了八组矩阵），每一组都是随机初始化的。在训练之后，每一组都被用来将输入embedding（或来自下级编码器/解码器的向量）投射到不同的表征子空间。</p></li>
</ul>
<p>如果我们做上面概述的那样的自注意力计算，只是用不同的权重矩阵做了八次不同的计算，我们最终会得到八个不同的Z矩阵。</p>
<p><img src="https://i.loli.net/2021/07/14/A9nBYVEXt1jfpdQ.png"/></p>
<p>这给我们留下了一点挑战。前馈层不希望有八个矩阵——它希望有一个单一的矩阵（每个词的一个向量）。因此，我们需要一种方法，将这八个矩阵浓缩为一个单一的矩阵。</p>
<p>我们如何做到这一点呢？我们把这些矩阵连接起来，然后用一个额外的权重矩阵WO将它们相乘。</p>
<p><img src="https://i.loli.net/2021/07/14/SgGK3uz4lq8NMvL.png"/></p>
<p>这就是多头自注意力的几乎全部内容。看起来有相当多的矩阵，我试着把它们放在一张图片来看。</p>
<p><img src="https://i.loli.net/2021/07/14/mdZFGayVzXB4Ewn.png"/></p>
<p>现在我们已经接触到了注意力头，让我们重温一下之前的例子，看看在我们的例句中对 "it" 这个词进行编码时，不同的注意力头都集中在哪里（"animal"和"tired"）。</p>
<p><img src="https://i.loli.net/2021/07/14/HIOlYz3wKTNtdJZ.png"/></p>
<p>然而，如果我们把所有的注意力都展示出来上，解释起来会更难。</p>
<p><img src="https://i.loli.net/2021/07/14/j1LdSFCuciZ6DqY.png"/></p>
<h1 id="用位置编码表示序列的顺序">用位置编码表示序列的顺序</h1>
<p>目前为止，我们所描述的模型中缺少的是说明输入序列中的单词顺序的方法（因为每个词无论位置在哪里都跟其他词计算了attention）。</p>
<p>为了解决这个问题，Transformer 为每个输入嵌入添加一个向量。这些向量遵循模型学习的特定模式，这有助于它确定每个词的位置，或序列中不同词之间的距离。这里的直觉是，一旦嵌入向量被投射到Q/K/V向量中，在点乘 attention 时，将这些值添加到嵌入向量中，就能提供嵌入向量之间有意义的距离。</p>
<p><img src="https://i.loli.net/2021/07/14/N2ksImilp6vKQE8.png"/></p>
<p>为了让模型对单词的顺序有一个感觉，我们添加了位置编码向量——其值遵循一个特定的模式。</p>
<p>如果我们假设嵌入的维度为4，那么实际的位置编码看起来像这样：（一个玩具嵌入尺寸为4的位置编码的实际例子）</p>
<figure>
<img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这个模式可能是什么样子的呢？</p>
<p>在下图中，每一行都对应一个向量的位置编码。所以第一行是我们添加到输入序列中第一个词的嵌入的向量。每一行包含512个值——每个值在1和-1之间。我们对它们进行了颜色编码，所以模式是可见的。</p>
<p>一个位置编码的真实例子，20个词（行）的嵌入大小为512（列）。你可以看到，它在中心位置被分成了两半。这是因为左半部分的值是由一个函数（使用正弦）生成的，而右半部分是由另一个函数（使用余弦）生成的。然后将它们串联起来，形成每个位置编码向量。</p>
<p><img src="https://i.loli.net/2021/07/14/XM1JChujV3weSrZ.png"/></p>
<p>原论文中描述了位置编码的公式（3.5节）。你可以在<a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"><code>get_timing_signal_1d()</code></a>中看到生成位置编码的代码。这并不是唯一的位置编码的方法，然而它的优点是能够扩展成未见过的长度的序列（例如，如果我们的训练模型被要求翻译一个比我们训练集中的任何句子都长的句子）。</p>
<p>2020年7月更新：上面显示的位置编码是来自Tranformer2Transformer的实现。论文中显示的方法略有不同，它不是直接串联，而是将两个信号交织在一起。下图显示了这是什么样子的。<a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb">下面是生成它的代码</a>。</p>
<h1 id="残差连接">残差连接</h1>
<p>在继续之前，我们需要提到编码器架构中的一个细节，那就是每个编码器中的每个子层（自我注意，ffnn）都有一个围绕它的<strong>残差连接</strong> (residual connection)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>，并在其后有一个<a href="https://arxiv.org/abs/1607.06450">层规范化</a>的步骤。</p>
<p><img src="https://i.loli.net/2021/07/14/AGNjYMK3dzasCZn.png"/></p>
<p>如果我们要把向量和与自注意力相关的层规范化操作可视化，它看起来是这样的：</p>
<p><img src="https://i.loli.net/2021/07/14/u1eOTkfJDQAxzva.png"/></p>
<p>这对解码器的子层也是如此。如果我们要考虑一个由2个堆叠的编码器和解码器组成的Transformer，它看起来会是这样的：</p>
<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" /></p>
<h1 id="解码器">解码器</h1>
<p>现在我们已经涵盖了编码器方面的大部分概念，我们基本上也知道了解码器的组件如何工作。但让我们来看看它们是如何一起工作的。</p>
<p>编码器开始处理输入序列，然后顶部编码器的输出被转化为一组注意力向量K和V，这些将被每个解码器用于其编码器-解码器注意力层，帮助解码器关注输入序列中的适当位置：</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" /></p>
<p>在完成编码阶段后，我们开始解码阶段。解码阶段的每一步都从输出序列（本例中为英语翻译句子）中输出一个元素。</p>
<p>下面的步骤重复这个过程，直到达到一个特殊的符号，表明Transformer解码器已经完成了它的输出。每个步骤的输出都被送入下一个时间步骤中的底层解码器，解码器就像编码器那样冒出它们的解码结果。而且，就像我们对编码器输入所做的那样，我们在这些解码器输入中embed并添加位置编码，以表明每个词的位置。</p>
<p>解码器中的self-attention layer的操作方式与编码器中的略有不同。</p>
<p>在解码器中，self-attention layer只被允许关注输出序列中靠前的位置。这是通过在自注意力计算的softmax步骤之前屏蔽之后的位置（将它们设置为<code>-inf</code>）来实现的。</p>
<p>“Encoder-Decoder Attention” layer的工作方式与multiheaded self-attention一样，只是它从下面的层创建其查询矩阵，并从编码器栈的输出中获取键和值矩阵。</p>
<h1 id="最后的linear和softmax层">最后的Linear和Softmax层</h1>
<p>解码器栈输出一个浮点数的向量，我们如何把它变成一个词？这就是最后的线性层的工作，它的后面是一个Softmax层。</p>
<p>线性层是一个简单的全连接神经网络，它将解码器堆栈产生的向量投射到一个大得多的向量中，称为<strong>logits vector</strong>。</p>
<p>让我们假设我们的模型知道10,000个不同的英语单词（我们模型的 "输出词汇"），这是它从训练数据集中学到的。这将使logits向量有10,000个单元格宽——每个单元格对应于一个独特的单词的得分。这就是我们如何解释线性层之后的模型输出。</p>
<p>然后，softmax层将这些分数转化为概率（全部为正数，全部加起来为1.0），选择概率最高的单元，并产生与之相关的词作为这个时步的输出。</p>
<p><img src="https://i.loli.net/2021/07/14/A6N9nMfmJLDxGV8.png"/></p>
<p>该图从底部开始，产生的向量作为解码器栈的输出，并被转化为一个输出词。</p>
<h1 id="训练的总结">训练的总结</h1>
<p>现在，我们已经通过一个经过训练的Transformer涵盖了整个前向传递过程，有必要再看一下训练模型的直觉。</p>
<p>在训练过程中，一个未经训练的模型会经历完全相同的前向传递。但由于我们是在一个有标签的训练数据集上训练它，我们可以将其输出与实际的正确输出进行比较。</p>
<p>为了形象化这一点，我们假设我们的输出词汇只包含六个词（"a"、"am"、"i"、"thanks"、"student "和<code>"&lt;eos&gt;"</code>("句末 "的缩写)。</p>
<p>我们模型的输出词汇是在开始训练之前的预处理阶段创建的。</p>
<p>一旦我们定义了输出词汇，就可以用一个相同宽度的向量来表示我们词汇中的每个词，这也被称为独热编码 (one-hot encoding）。例如，我们可以用下面的向量来表示 "am "这个词：</p>
<p><img src="https://jalammar.github.io/images/t/one-hot-vocabulary-example.png" /></p>
<p>例子：对我们的输出词汇进行独热编码</p>
<p>在总结之后，让我们讨论一下模型的损失函数——我们在训练阶段要优化的指标，以形成一个经过训练的、有望惊人般准确的模型。</p>
<h1 id="损失函数">损失函数</h1>
<p>假设我们正在训练模型。假设这是训练阶段的第一步，我们在一个简单的例子上训练它——把 "merci "翻译成 "thanks"。</p>
<p>这意味着，我们希望输出是一个表示 "谢谢 "这个词的概率分布。但是，由于这个模型还没有被训练过，这不太现实。</p>
<p>由于模型的参数（权重）都是随机初始化的，（未经训练的）模型为每个单元格/单词产生一个具有任意值的概率分布。我们可以将其与实际输出进行比较，然后用<strong>反向传播法</strong>调整模型的所有权重，使输出更接近于期望的输出。</p>
<p>你如何比较两个概率分布？我们只需把它们相减。更多细节，请看<a href="https://colah.github.io/posts/2015-09-Visual-Information/">交叉熵</a>（cross-entropy）和<a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback–Leibler divergence</a>。</p>
<p>但请注意，这是一个过于简化的例子。更现实的是，我们将使用一个长于1个单词的句子。比如——输入"je suis étudiant"，预期输出 "我是一个学生"。这实际上意味着，我们希望模型能够连续地输出概率分布，其中每个概率分布由一个宽度为vocab_size的向量表示（在我们的玩具例子中为6，但更现实的是一个像30,000或50,000的数字）。</p>
<p>第一个概率分布在与单词 "i "相关的单元格中的概率最高。</p>
<p>第二个概率分布在与单词 "am "相关的单元格中的概率最高。</p>
<p>以此类推，直到第五个输出分布表示"<句末>"（<code>&lt;end of sentence&gt;</code>）符号，它也有一个与之相关的10,000元素词汇的单元。</p>
<p><img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png" /></p>
<p>我们将在训练实例中针对一个样本句子的目标概率分布来训练我们的模型。</p>
<p>在一个足够大的数据集上训练模型足够长的时间后，我们希望产生的概率分布看起来像这样:</p>
<p><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png" /></p>
<p>希望在训练后，模型能输出我们期望的正确翻译。当然，如果这个短语是训练数据集的一部分，那就没有真正的迹象（见：<a href="https://www.youtube.com/watch?v=TIgfjmp-4BA">交叉验证</a>）。请注意，每个位置都有一点概率，即使它不可能成为该时步的输出——这是softmax的一个非常有用的属性，有助于训练过程。</p>
<p>现在，由于该模型一次产生一个输出，我们可以假设该模型是从该概率分布中选择概率最高的词，并丢弃其余的。这是一种方法（称为贪婪解码greedy decoding）。另一种方法是保留，比如说，前两个词（比如说，'I'和'a'），然后在下一步，运行模型两次：一次假设第一个输出位置是'I'，另一次假设第一个输出位置是'a'，考虑到1号和2号位置，哪个版本产生的误差小就保留。我们对2号和3号位置重复这样做......等等。这种方法被称为 "波束搜索" （beam search），在我们的例子中，beam_size是两个（意味着在任何时候，两个部分假设（未完成的翻译）都被保留在内存中），top_beams也是两个（意味着我们会返回两个翻译）。这两个都是超参数，你可以进行实验。</p>
<h2 id="go-forth-and-transform">Go Forth And Transform</h2>
<p>I hope you've found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I'd suggest these next steps:</p>
<ul>
<li>Read the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> paper, the Transformer blog post (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a>), and the <a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">Tensor2Tensor announcement</a>.</li>
<li>Watch <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">Łukasz Kaiser’s talk</a> walking through the model and its details</li>
<li>Play with the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></li>
<li>Explore the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor repo</a>.</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><em>Residual</em> neural networks do this by utilizing skip <em>connections</em>, or shortcuts to jump over some layers.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语言模型</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Reclaiming Concepts，Eleanor Rosch</title>
    <url>/concepts/</url>
    <content><![CDATA[<blockquote>
<p>故事说的是一位物理学家应奶农协会的邀请，告诉他们如何从奶牛身上获得更多的牛奶。物理学家开始说："首先我们从球形的奶牛开始。"这被当作一个笑话来讲! 然而，更奇怪的是，认知主义对本应是研究人类思想和人类生活的东西做了什么。本章是关于概念的，即认知主义理论的核心构件。我将首先说明认知主义如何必然不能对概念进行充分的处理，然后，更重要的是（谁会注意批评呢？），我将概述一种新的非表征性的概念观的基础，它应该把对概念的研究放在真实（而不是球状牛）的基础上。</p>
</blockquote>
<span id="more"></span>
<p>如果说有一个领域你认为认知主义可以把它搞对的话，那就是概念。人们不一定期望认知主义能照亮生物学、艺术、质感、直觉或灵性。但概念是认知主义理论的基本组成部分。因此，概念是本卷的一个特别恰当的案例研究，因为研究概念的案例就是在认知主义的本土上穿透它。</p>
<p>为什么概念应该被期望成为认知主义的必要条件？要回答这个问题需要一些背景：我们所说的概念以及认知主义是什么意思？</p>
<p>概念是分类研究的一个方面。生物最基本的功能之一就是分类，也就是把可区分的物体和事件当作等同物。人类生活在一个分类的世界里；从家庭用品到情感到性别到民主，物体和事件虽然是独特的，但却被当作类别的成员来对待。一些理论会说，如果没有这种能力，就不可能从经验中学习，因此，分类是生命的基本功能之一。至少从十九世纪开始，人们就普遍把类别的认知或心理方面称为概念。</p>
<p>概念有哪些认知主义无法探究的东西呢？在本文中，我将论证。概念是心灵与世界之间的自然桥梁，以至于它们要求我们改变我们认为是心灵的东西和我们认为是世界的东西；概念只发生在实际情境中，在这种情境中它们作为参与的部分发挥作用，而不是作为表征或作为识别对象的机制；概念是开放的系统，生物可以通过它来学习新的东西，可以进行发明创造；概念存在于一个更大的背景中——它们不是生物认识和行动的唯一形式。 另一方面，在认知主义的机器中，概念本质上是唯我论的；概念的性质只能是封闭的分析性定义，既不允许有参与性功能，也不允许有新颖性；不可能有其他形式的认识的背景，而概念的认识可以位于其中。那么，认知主义是什么，它从哪里来，以至于它有这些局限性？</p>
<p>在处理概念和范畴方面有一个长期的哲学传统，被称为 "普遍性问题"，认知科学也继承了这个传统。问题在于，当感官提供的东西看起来如此多变和不可靠的时候，如何能有一个坚实的知识基础。但认知主义还受到另一组限制。正如本卷中的其他论文所指出的(也见Johnson和Erneling, 1997)，实际上有两种相当不同的努力被称为认知科学：一种只是人工智能、心理学、哲学、语言学、神经生理学和人类学之间跨学科合作的尝试。另一种形式的认知科学则额外假定了现在被称为认知主义的特殊哲学立场。</p>
<p>认知主义中概念的中心地位与它在哲学和心理学传统中的起源有些不同。认知主义把心当做一台机器，更准确地说，是一个计算机程序，更准确地说，是那种在符号表征上作为一系列计算(即规则控制的变化)发挥作用的程序。心灵被认为是一个心理表征的集合，与计算机的符号表征完全相似。我们可以对这样的模型或机器提出的唯一问题，即对它的唯一适当的测试，只是经典的图灵测试--它的输出与人的输出无法区分。Fodor(1998)称这种模型为 "心灵的表征理论(RTM)"，并指出它对认知主义的重要性："没有表征就没有认知"(第26页)。</p>
<p>表征在这里有一个技术含义；它不是指心灵和世界事物之间的关系，而是指机器的封闭系统中的符号。认知论者（Carnap, Chomsky, Fodor等）长期以来一直认为，虽然语言和思想可能有语义内容（有意义的现实世界的参考），但认知科学所能接触到的只是语法（形式操作）。Fodor(1982)称这为 "方法论的孤独主义"。<strong>概念是RTM的核心，因为它们是制定规则的单位，也是规则运作的基础。</strong>因此，在某种意义上，概念已经从哲学上的角色转移到技术上的角色--一种深层的技术，其中在机器的符号表征上设置机器的计算的要求成为我们的心智理论。</p>
<p>上述模式可能被称为哲学的认知主义或严格的认知主义。实践者研究者们习惯于忽略认知主义的方法论上的唯我论方面，而<strong>把认知表征当作是对世界的镜像，或者至少是对世界的信息的恢复。</strong>在其他方面，这些研究者可以保持认知主义模型的其余部分。我们将称它为工作中的认知主义。由于概念是认知主义心智表征模型的核心，认知主义非常需要一个关于概念的理论。Fodor(1998)又说："如果没有一个可行的概念理论，RTM就根本没有用"(第39页)。那么，有哪些可能的概念理论呢？</p>
<p>人们普遍认为，目前对概念有三种主要的方法。(1）所谓的经典观点（2）分级结构和/或原型观点，以及（3）各种理论观点。这些都像许多牛人的道路一样，从哲学和心理学传统中生长出来。尽管没有任何一种观点起源于认知主义的立场本身，但认知主义已经详细地采用或批判了这些观点，因为它需要一个关于其核心构件的理论。本文的一个论点是，认知主义因其自身的信条而被迫严重误解和曲解了这些概念的每一种方法，因此，它对这些传统的批判实际上是对其自身架构的尖锐批判。在下面的章节中，我将从每一种观点自身的传统出发，说明认知主义是如何被迫曲解这一传统的，指出每一种观点的局限性和（有时并不明显的）智慧，并将在最后提出一种非认知主义的替代性观点。</p>
<h1 id="i-目前对概念和类别的方法">I: 目前对概念和类别的方法</h1>
<h2 id="a.-古典观点">A. 古典观点</h2>
<p>古典观点是源自西方哲学史的对概念的方法。当人类开始用理性来观察他们的经验时，关于感官的可靠性和知识的基础的问题就自然而然地产生了，关于范畴如何具有一般性，词语如何具有意义，以及头脑中的概念如何与世界中的范畴相联系的更具体的问题也自然而然地产生了。希腊人和此后的大多数西方哲学家都认为，通过感官一刻不停地体验特定事物是不可靠的，因此，只有稳定的、抽象的、逻辑的、普遍的范畴才能作为知识的对象和词语意义的参照对象。这在历史上是一个重要的选择。希腊并不是唯一做出这种转向本质主义思维的文化；印度的梵语传统也是如此（Chakrabarti, 1975; Radhakrishnan and Moore, 1957）。然而，中国的辩证法思维却采取了相当不同的方向（Nisbett等人，正在审查；Peng和Nisbett，1999）。为了给知识提供一个适当的基础，柏拉图认为他必须引入形而上学的实体，即形式，它们是经验的基础，人们通过回忆来认识它们。对亚里士多德来说，普遍性存在于特定的物体中，并且是通过对物体的经验来了解。普遍性是特定事物中的共同要素；也就是说，一个普遍性的X是所有X所共有的，或共享的。单个物体将被分为具有相同属性的种类，而种类将根据它们之间的属性差异被细分为属和种。自然科学的主要任务之一是将自然对象按属和种划分为它们在本质上属于的真正的种类，并将其分类。亚里士多德的系统对我们来说应该听起来非常熟悉。我们的字典就是这样组织的，更不用说我们的大部分思维了。(作为对比，早期的汉语词典只包含同义词和对汉语表意文字的分析性剖析--贝克，未发表）。) 关于类别的下一个主要历史学派被称为概念主义，归因于从约翰-洛克开始的英国经验主义者。实际上，概念主义并不是希腊现实主义的对手理论，而是一个重点不同的问题--心灵究竟是如何形成一般概念的？洛克认为，这是通过从特定概念中抽象出来的过程发生的；每当人们遇到一个类别的成员时，就会从自己对该类别的概念中减去该类别所有成员所不具备的特征，直到只剩下对该类别成员来说必要且充分的特征。洛克、伯克利和休谟之间随后的争论涉及所产生的心理抽象的性质。洛克被认为（有点不公平），他声称这是一个一般的、抽象的观念或形象；伯克利的论点是，这是一个特殊的观念，通过代表同类事物而成为一般的观念，而休谟的反对意见（一般被忽视）是，这根本不是一个观念，只是习惯或习俗。请注意，在对概念和范畴的总体描述方面，经验主义思想与亚里士多德是多么的相似。</p>
<p>心理学从哲学史中继承了对范畴的一种特殊看法。为了作为知识的适当的本质主义基础，类别被要求做到 (1) 是准确的，而不是模糊的--即有明确定义的边界，以及(2) 有共同的属性，这些属性是类别成员的必要和充分条件。由此可见，（3）一个类别的所有成员在成员资格方面必须是同样好的；要么他们有必要的共同特征，要么他们没有。因此，类别和概念被看作是逻辑集合。哲学家的类别观在20世纪50年代以概念学习研究的形式明确地进入心理学。在Jerome Bruner和他的同事们（Bruner et al., 1956）的带领下，受试者被要求学习类别，这些类别是由明确的属性定义的逻辑集合，比如红色和方形，由逻辑规则结合，比如和。理论兴趣集中在受试者如何学习哪些属性是相关的以及哪些规则将它们结合起来。在发展心理学中，皮亚杰和维果茨基的理论与概念学习范式相结合，研究儿童的非结构化、主题化的概念如何发展为成人的逻辑模式。人工刺激通常被用于各级实验研究，结构化的微观世界中，关于类别性质的普遍信念已经建立起来。如Fodor（1998）所述，经典观点的认知主义版本是，概念是定义，这意味着等同和可替换的符号串。单身汉是一个未婚的人 "就是一个典型的例子。认知主义机器的确是通过可替代的符号串来工作的，但是对于任何一个古典观点的支持者来说，这真的是概念的意义吗？当亚里士多德用属相和区别来定义物体时，他明确地打算描述经验性的内容。在学习理论中的标准概念实现范式中，心理学家试图找出生物如何学习哪些属性与概念相关，以及哪些规则将它们连接起来；受试者并没有被训练去替代彼此的符号串。类别可能被视为逻辑集合，但它们是旨在反映心灵和世界之间的映射的集合。因此，认知主义者对经典观点的解释有一个问题，即类别是基于可替代的符号串的定义，它实际上并没有描述经典观点。但是，认知论者是否可以争辩说这就是经典观点应该是的样子？当我们说某人拥有或理解一个概念时，我们是否意味着他所拥有的只是一串可替换的符号？这个问题类似于一直流行的关于计算机是否能思考的辩论。计算机是通过可替换的符号串来工作的。但考虑到约翰-塞尔现在著名的中文64 E. ROSCH 版权(c) Imprint Academic 2013 仅供个人使用 -- 不得复制房间的例子：一个不会说或不懂中文的人坐在一个孤立的房间里。有人把中文句子传给他，他在对照表上查找这些句子，然后把列出的答案传出去。这就是他所做的一切。他的回答通过了图灵测试，也就是说，与懂中文的人的回答没有区别。他懂中文吗？他是在用中文的概念和类别工作吗？没有！没有 然而，这个人所做的正是用符号串相互替代。中国房间的例子在几个方面唤起了我们对概念的直觉。其中一个方面似乎是固有的错误，即唯我论；另一个方面是交流的贫乏性。概念似乎不仅需要我们的字典，还需要有经验的人类知识百科全书的全部内容，更不用说与物质和社会世界的互动了（关于这些问题的背景见Dreyfus and Hall, 1982）。类别的经典观点可能没有提供足以满足这些要求的说明（正如将在后面的章节中显示的那样），但它至少在西方思想史所寻求答案的领域中提出了问题。</p>
<p>事实上，即使是认知论者也不接受通过可替换的符号串来定义，当它被直截了当地说成是一种充分的概念理论。这种说法实际上是唯名论的一种形式，这种哲学立场断言一般性只存在于词语之中。名词主义从来没有被哲学认真对待过，在那里它被视为一个不连贯的立场，很容易被拆穿(Woozley, 1967)。即使是Fodor，这位老牌的认知主义者，也不接受对概念的充分的定义性说明；事实上，他觉得所有现存的概念理论都是 "从根本上和实践上看都是不真实的，对此需要采取一些激烈的行动"(Fodor, 1998, p. viii)。那么，认知论者，比如Fodor，为什么可以简单地认为经典观点的定义解释是理所当然的呢？也许这是因为机器的概念是可替换的符号串；这就是它们唯一可以成为的东西。这在心灵表征理论的基于规则的计算系统中尤其明显。就福多的批判成功而言，他的批判可以被看作是对认知主义本身而不是对概念的经典观点的影响。</p>
<h2 id="b.-分级结构原型观点">B. 分级结构/原型观点</h2>
<p>考虑红色：红色的头发和红色的消防车一样是你对红色的想法或形象的一个好例子吗？牙医的椅子和餐厅的椅子一样是椅子的一个好例子吗？这样的问题在经典的范畴观中是无稽之谈，因为在经典的范畴观中，某物要么是范畴成员，要么不是，所有成员都是等同的。对经典观点的挑战来自心理学内部，首先是关于颜色类别的工作（Rosch, 1973）。颜色类别没有任何明显的可分析的标准属性、形式结构或明确的边界，它们有一个内部结构，根据人们判断颜色在其类别中的典范程度进行分级。此外，那些被判断为颜色类别的最佳范例的颜色空间区域似乎起着特殊的作用；它们是最稳定的、跨文化认同的，甚至可能是颜色类别的生理起源（Berlin and Kay, 1969; Hardin and Maffi, 1997; Rosch, 1977）。其他种类的类别是否也是以类似的方式结构的？重新定义概念 65 Copyright (c) Imprint Academic 2013 For personal use only - not for reproduction 一项广泛的研究计划建立了一个经验性的核心发现（Rosch, 1978; 1994）。首先，所有的类别都显示出成员的梯度；也就是说，受试者很容易地、迅速地、有意义地评价一个特定的项目在多大程度上符合他们对该项目所属类别的想法或印象。这种判断是分级结构/原型观点的标志。请注意，这些不是概率判断，而是成员程度的判断。成员的梯度判断适用于最多样化的类别：如红色的感知类别，如家具的语义类别，如女人的生物类别，如职业的社会类别，如民主的政治类别，有经典定义的形式类别，如奇数，以及临时的、源于目标的类别，如火灾中要带出屋外的东西。成员的梯度必须被视为心理学上的重要因素，因为这种措施已被证明会影响心理学研究中使用的几乎所有主要的研究和测量方法。我将对这一点进行一些赘述，作为与认知主义对待分级结构/原型观点的方式的对比，这一点将在不久后描述。(除非另有说明，以下研究都是在Markman, 1989; Mervis and Rosch, 1981; Rosch, 1973; 1978; 1987; Rosch and Lloyd, 1978; or Smith and Medin, 1981中报道或引用的）。) 学习：类别的好例子在实验中被受试者学习，儿童在自然中获得的时间比差的例子早，而且当先呈现更好的例子时，类别可以更容易被学习--这些发现对教育有影响。处理速度：一个例子在其类别中越好，受试者就越能迅速判断该物品是否属于该类别。这很重要，因为反应时间通常被认为是学习心理过程的皇家道路。期待：当受试者在对该类别进行一些快速判断之前被告知一个类别名称时，其表现对该类别中的好成员有帮助，对坏成员有阻碍。这一发现在心理学中被称为 "启动 "或 "设定"，它被用来论证（并非无可争辩地）该类别的心理表征在某些方面更像好的例子而不是差的例子。联想：当被要求列出该类别的成员时，受试者比较差的例子更早、更频繁地产生更好的例子。推断：受试者从更有代表性的类别成员推断到较无代表性的类别成员，比反过来更容易，而且项目的代表性影响了形式逻辑任务中的判断，如Syllogisms。概率判断：代表性强烈影响概率判断（Kahneman等人，1982），这很重要，因为概率被认为是归纳推理的基础，因此也是我们学习世界的方式。分级结构的自然语言指标：自然语言本身包含了各种承认和指向分级结构的装置，比如像technically和really这样的对冲词（Lakoff, 1987; Rosch, 1975）。对相似性的判断：类别中不太好的例子被判断为与好的例子更相似，反之亦然。这违反了逻辑学中处理相似性的方式，在逻辑学中相似性关系是对称的和可逆的。其他研究直接挑战了经典观点的要求，即类别具有定义性特征。Rosch和Mervis(1975)发现，当被试被要求列出类别成员的属性时，许多类别显示出很少或没有66 E. ROSCH Copyright (c) Imprint Academic 2013 For personal use only - not for reproduction attributes in common. 属性似乎具有家族相似性（Wittgenstein, 1953），而不是必要和充分的结构。</p>
<p><strong>判断出的概念类别的最佳例子被称为原型。</strong>虽然有些可能是基于统计学上的频率，如各种属性的平均值或模式（或家族相似性结构），但其他似乎是由生理学（好的颜色、好的形式）、社会结构（总统、教师）、文化（圣人）、目标（饮食中的理想食物）等因素使之突出的理想。形式结构（十进制系统中十的倍数），因果理论（"看起来 "随机的序列），以及个人经验（最初学到的或最近遇到的项目，或者因为它们有感情、生动、具体、有意义或有趣而变得特别突出的项目）。</p>
<p>原型在哲学上最有说服力的一个方面是，它们远不是几个定义属性的抽象，它们似乎是丰富的、想象的、感觉的、完整的心理事件，在上述所有种类的研究效果中作为参考点。关于原型和分级结构的一个非常重要的发现是它们对背景有多敏感。例如，狗或猫可能被作为宠物动物的原型，而狮子或大象则更有可能被作为马戏团动物的原型。在默认的语境中（没有指定语境），咖啡或茶或可乐可能被列为典型的饮料，但葡萄酒更有可能在晚宴的背景下被选中。此外，人们对特设的目标衍生的类别（如躲避黑手党的好地方）表现出完全良好的类别效应，并具有分级结构。</p>
<p>事实上，背景对分级结构的影响是无处不在的（Barsalou, 1987）。在古典观点中，从其起源于希腊思想的时候起，如果一个知识的对象随着环境的每一次心血来潮而改变，那么它就不是一个知识的对象，一个词的意义一定不会随着它的使用条件而改变。对其支持者来说，标准属性假设的一个最大优点是，假设的标准属性就是不随语境变化的东西。Barsalou认为，语境效应表明，类别原型和分级结构并不是预先储存在头脑中的，而是每次从更基本的特征或其他心理结构中 "临时 "创造出来的。类别对环境影响的极端灵活性可能有更根本的影响。认知论者通常将分级结构/原型观点定义为一种统计理论。Smith和Medin（1981）称其为概率论观点。Fodor(1998)将其定义为一种基于 "概念外延中的东西具有[某个]特征所表达的属性的可能性有多大 "的统计理论（第92页）。但是，这样定义它就错过了分级结构在哲学上最有预见性的一面。对于所有的概念类别，所有的人都判断类别的成员具有不同程度的成员资格，即使在没有关于项目是否被判断为类别成员的问题或统计学上的变化。(例如，判断8421是明确的奇数的人也判断7是他们对奇数类别概念的一个更好的例子)。事实上，判断不同程度的成员的现象是如此普遍，以至于通过一个奇怪的逻辑转折，它被用作对分级结构影响的反驳（Gleitman等人，1983）。此外，正如前面所指出的，许多原型来自于统计频率以外的来源。仅仅从统计学的角度来定义分级结构是在逃避解决分级结构和原型效应给经典观点和认知主义带来的基本挑战之一，67 Copyright (c) Imprint Academic 2013 For personal use only - not for reproduction. 我们将在后面讨论为什么认知主义必须这样定义它。</p>
<p>严格的和工作的认知论者都对原型和分级结构作为概念或分类的说明持高度批评态度，即使他们接受了实验结果。在认知主义中，原型能够代表概念的唯一合法方式是，原型可以替代定义并进行相应操作。原型能否替代正式语义模型中的定义特征，从而说明同义词、矛盾和联结类别等逻辑和语言功能？这就是所谓的成分性问题。在一篇有影响力的论文中，Osherson和Smith（1981）使用Zadeh（1965）的模糊集合逻辑对原型理论进行了建模，其中共轭类别是通过最大化规则来计算的，并表明原型并不遵循这一规则；例如，guppy既不是宠物类别的一个很好的例子，也不是鱼类别的一个很好的例子，而是宠物鱼类别的一个很好的例子。(当然，这实际上是对一个特定的形式模型，即Zadeh的模糊集合逻辑的批评，而不是对原型和分级结构的批评。目前还不清楚哪一个（如果有的话）形式模型适合于原型和分级结构，甚至不清楚原则上是否有任何经验证据可以区分对立的模型（因为每一个存储模型总是以互补的处理假设呈现，使其可以匹配任何一种实验数据；Barsalou, 1990）。</p>
<p>为什么认知主义要求概念在形式模型中是构成性的？在严格的认知主义的符号系统中，人们必须能够通过纯粹的句法手段--即通过形式规则对形式符号的操作--从宠物和鱼到宠物鱼，因为不可能有相应的现实世界语义可以依赖。但是我们不能通过对宠物和鱼的符号的任何操作来推导出宠物鱼的原型。因此，我们不能把分级结构变成一个形式化的定义。认知主义认为这是对原型和分级结构的破坏性证据；但同样地，它也可以被视为对认知主义的批判。工作认知主义试图与原型和分级结构的经验证据达成一致的一种方式是将分类分为核心概念和处理启发式。这体现在一类模型中，其中类别术语的实际意义是一个经典的定义，在此基础上加上一个处理启发式或识别程序，以说明分级结构的方面（Osherson and Smith, 1981; Smith et al., 1974）。这样一来，奇数可以同时 "拥有 "一个经典定义和一个原型。这是一个危险的举动，因为它将理论与任何经验性的参照物脱钩。这样一来，一个范畴术语的 "实际意义 "就可以成为一种仅由逻辑知道的形而上学实体，这种实体也许适合于方法论上的唯我论、人工智能、认知主义系统，但不是我们的经验科学可能想要接纳的实体。总而言之：分级结构和原型的证据违反了经典观点和严格的、有效的认知主义的信条。(1) 分级结构类别没有明确的界限。这不仅仅是物品被归类为类别成员的概率问题，因为对于许多类别，如颜色，受试者会断言一些物品确实在类别之间。(2) 许多类别没有，而且任何类别也不需要有任何必要和充分的. 相反，类别原型包含大量丰富的信息，如完整的图像和关于特定情况的许多细节，这些信息对类别的所有成员来说并不常见。简而言之，当概念被用于执行大多数活动时，骨架式的必要和充分属性的想法似乎与概念无关。(3) 一个类别中的项目在成员资格方面是不等价的，而是拥有不同等级的成员资格。这也不仅仅是一个概率问题，因为人们会直接断言一个类别的一个成员比另一个成员是该类别更好的例子。(所有的曲线不一定是概率分布。) (4) 分级结构不是形式系统，分级结构中的任何项目也不一定是结构中任何其他项目的暗示或生产，分级结构中的任何东西也不需要填补可替代的符号串的作用。分级结构和原型不需要被形式化系统（如Zadeh的模糊逻辑）所模拟，就可以存在。(5) 分级结构和原型，尽管它们有默认的上下文，但在其他方面都是灵活的，因为它们可以不断地进行调整。 它们不能被方法论或其他方面的唯心主义所模拟，因为它们必须考虑到那些不断变化的现实世界的情况。出于同样的原因，不清楚它们如何能在工作认知主义的意义上成为表征性的；现实世界中什么稳定的东西，无论是底层的还是表面的，它们能代表什么？</p>
<p>C. 理论</p>
<p>当今对概念的第三种方法是以理论为基础的。实际上有两组理论家，很多人把它们混为一谈：以认知主义为导向的认知心理学家，他们主要解决分类问题；"理论学派 "的发展心理学家，他们解决概念的变化。第一组（Medin，1989；Medin和Wattenmaker，1987；Murphy和Medin，1985）把理论的概念主要作为对以前分类研究的批评。通常，他们在所谓的基于相似性的分类理论和他们的新理论观点之间建立了对立，他们把分级结构/原型观点（以及可能的其他一切）置于其中。相似性观点被批评的依据是 (1)他们依赖于计算属性，但不能解释属性，(2)相似性不能被现有的形式模型（即Tversky, 1977）所解释，(3)关于原型和分级结构的各种实验显示了背景效应。然后他们断言，属性、相似性和所有关于分类的实验结果都来自于 "理论"，并由其解释。的确，我们现在的说法都没有对属性、相似性或语境的性质提供充分的理解，这些都是老生常谈的问题。但理论的观点是什么，它是如何解释这些问题的？值得注意的是，第一组理论家从来没有定义或描述他们的理论是什么意思，而且在他们所有的文献中，从来没有提供过一个实际理论的例子，分类研究中的发现，哪怕是一个发现，都可以从中得出。也没有试图说明属性、相似性或背景是如何从理论中得出的，无论是抽象的还是具体的理论。这种作为理论的类别理论是一种新的实质主张，还是仅仅是一种战斗口号？理论的含义是什么？可以被带入意识的明确的陈述？世界知识的任何项目？完整的字典和百科全书？重新定义概念 69 版权(c) Imprint Academic 2013 仅供个人使用 -- 不得复制 任何期望或习惯？脑海中或书中的任何东西？任何背景？很难摆脱这样的印象：对于认知心理学将类别作为理论的方法来说，目前绝对任何东西都可以算作一个理论，而且理论这个词可以被援引来解释任何关于相似性、属性或背景的发现。如果有知觉限制，人们就会谈论知觉理论（并援引进化论）--有点像早期心理学中的本能和驱动力的扩散。有趣的是，许多用来支持理论观点的论据（例如要求人们把世界知识带入解释的例子）正是海德格尔现象学传统中用来反对理论的问题，并支持有必要提出一个非理论基础的习惯和技能的背景，它是人类生活类别和活动的基础。为什么一个如此依赖挥手的观点被证明如此具有诱惑力呢？我相信这是因为理论这个词能够唤起并给人以满足对世界的两种有问题的矛盾的理解的印象。一方面，人们（甚至是心理学家）非常清楚地知道，生命活动发生在一个比我们的实验室实验所提供的大得多的背景中。经验是在相互依存的有意义的整体中呈现的，而不是在孤立的单元中。有意义的整体包括：世界知识、信仰（通常不会被组织成任何类似于连贯的理论）、期望、欲望、习惯、技能、直觉、身体、感官的运作、隐性知识、一切无意识或非意识的东西、习俗、价值观、环境等等。另一方面，我们觉得科学知识必须以有界限的、明确的（最好是正式的）结构出现。理论这个词既暗示了背景，也暗示了形式主义。但这是一个幻觉。有意义的整体的大背景并不是由理论组成的，而我们称之为理论的东西本身（正如科学哲学家之间的辩论所说明的那样）远不是无可争辩的可规定性。如果试图执行理论方案，就会遇到同样的限制，并受到它对其他说法的同样批评。理论观点之所以能保持可行的假象，只是因为它没有内容。因此，它具有严重的误导性。然而，理论的流行是一个信号，表明人们对概念和背景有重要的直觉，这些直觉应该被进一步探索，这项任务将在本文的第二部分进行。</p>
<p>与认知心理学家对理论的含义缺乏具体说明相比，发展心理学的 "理论学派"(见Gopnik和Meltzoff, 1997)明确地将理论定义为类似于科学理论，很像Kuhnian的范式，并认为认知发展应该被看作是儿童持有的一个范式理论被另一个范式理论的连续替换。对概念的兴趣倾向于从儿童（而不是研究者）对概念是什么的理论变化的角度出发。当研究具体的概念时（如生物类型--Carey，1985；Keil，1979），其主旨是将它们作为更大的理论单位的一部分。代表严格的认知主义的Fodor(1998)将 "理论 "理论定义为概念整体主义，并以这样的指控来否定它：如果一个概念是根据它所处的理论来定义的，那么心理学家所讨论的那种概念变化是不可能的，因为在一个系统中引入一个新的术语或者改变一个已经纳入的术语的意义，将改变这个系统中所有其他术语的意义。事实上，用这样一个版本的认知主义机器编程，70 E. ROSCH Copyright (c) Imprint Academic 2013 For personal use only -- not for reproduction conceptual holism会有这个困难。在现实中，根据实验的要求，发展论者的理论被以许多不同的方式处理--例如，作为结构松散的暗示性信念陈述的集合，其中一个信念的变化，例如活着意味着什么，会导致某些其他信念的变化，例如死亡意味着什么（Slaughter等人，在出版中）。Fodor会称这是对 "理论 "理论的不连贯的表述。Fodor在最后的分析中所做的，以及任何严格的认知论者都必须做的，就是把分级结构/原型和理论都还原为以可替换的符号串为单位的概念的定义性说明。认知主义必须这样做，因为那是认知主义机器所能处理的全部问题。然而，正如我们以前所看到的，这完全不能令人满意，甚至对认知主义者来说也是如此。(Fodor, 1998, 认识到了这个问题，并试图在不改变认知主义世界观的任何信条的情况下，通过......是的......一套新的定义来补救它--关于批评，见Rosch, 1999。) 要从认知主义手中夺回概念，乃至整个认知科学，需要比这更激进的东西。这需要对心灵、世界、概念及其关系进行真正的重新思考。</p>
<p>二：新概念观的基础</p>
<p>A. 概念是心灵和世界之间的桥梁</p>
<p>我们把心灵和世界看成是独立的东西1。这是因为我们以某种方式来看待它们。从另一个角度看（Rosch, in press），这显然是错误的。用Skarda（本卷）的术语来说，被感知或分类的世界是一个无缝的整体或无缝的网络，其中感知者/分类者和被感知/分类者只是同一事件的两极。在意识中，这些极点似乎被划分为实际的独立事物。概念的第一个功能是把这些对立的两极重新连接起来，成为有效的、即使是表面上独立的单元。以这种方式来看，概念部分地恢复了一种状态，这种状态比我们通常看世界的方式更真实，因此可能更科学。第一个阐明类似这种观点的认知科学家是J.J. Gibson。对世界的感知就是对自己的感知......。指明自我的光学信息......伴随着指明环境的光学信息。. . . 一个不可能没有另一个而存在。主观和客观的所谓独立领域实际上只是注意力的两极。观察者和环境的二元论是不必要的。对 "这里 "的感知的信息与对 "那里 "的感知的信息是同类的，而且一个连续的表面布局从一个延伸到另一个（Gibson, 1979, p. 116）。这种东西在感知自己的位置时可能是最直接明显的。一个人只有相对于其他事物的位置才知道自己在哪里，反之亦然；也就是说，一个人对自己的位置的判断和对相关环境参考点的位置的判断是同一个包容性的方面，RECLAIMING CONCEPTS 71 [1] 请注意，我是从通常使用的心智和世界开始的，并试图引导读者对这些事情有一个不同（可以说是更好）的理解。如果读者觉得有必要从心智、世界、有机体、环境、意识等的经典定义开始，可以在这里暂停一下，思考一下本文第一部分的重要性。Copyright (c) Imprint Academic 2013 仅供个人使用 -- 不可用于复制信息领域。新兴的生态心理学学科包含了许多关于这一事实如何影响行为的实验证明（见Neisser, 1993）。同样，人与人之间的互动也是共同定义、体验和行动的；这被称为主体间性，也是目前许多研究的动力（Trevarthen, 1993 - 也见Rosch, 1996）。在微观层面上，Skarda（本卷）详细而清晰地展示了二元知觉是如何从一个知觉事件的不间断领域中产生的。Jarvilehto（1998年a，1998年b）认为有必要在微观（神经）和宏观（行为）层面上将有机体和环境之间的关系重新概念化为一个单一系统。然而，在有意识的感知水平上，非常明显的是，感知者和被感知的世界被体验为不同的和独立的。因此概念。前面讨论的所有关于概念和类别的方法都在努力解决如何把感知者/认知者和被感知/认知的世界放在一起的问题。看看这些说法：世界包含认识者回忆或发现的真正的普遍性；或者世界包含被分类的对象（类别的延伸），而认识者的头脑包含概念；或者世界和认识者一起产生了突出物，这些突出物成为类别形成的原型；或者世界和认识者一起产生了发生灵活分类的情况；或者世界以某种方式运作，认识者通过理论映射；等等。所有这些说法都有一个共同的基本直觉，即是概念跨越了心智-世界的鸿沟。只有严格的认知主义及其方法论上的唯我论否认了这一点，或者至少否认了科学可以在此基础上进行。推论A：概念不是表征 由于概念和范畴的主观和客观方面是作为同一认知行为的不同两极一起出现的，并且是同一信息场的一部分，所以它们在开始时就已经结合在一起了。它们不需要通过心智的表征理论，如工作认知主义的理论，来进一步结合，它们也不能被严格认知主义的心智的唯物主义表征理论所分离。概念和范畴并不代表心智中的世界；它们是心智世界整体的一个参与部分，心智的感觉（有一个正在看或正在想的心智）是其中的一极，心智的对象（如可见的物体、声音、思想、情绪等等）是另一极。概念--红色、椅子、害怕、美味、犰狳等等--以许多不同的运作方式，将这种存在或拥有心灵的感觉与心灵对象的感觉不可分割地结合起来。</p>
<p>B. 概念和分类只存在于具体的复杂情况中</p>
<p>无论一个概念看起来多么抽象和普遍（例如平方根），该概念实际上只出现在具体的，具体的情况中。真实情境是信息丰富的完整事件。</p>
<p>人们不会像在哲学例子中那样站在稀薄的空气中对着一棵树张望；总是有一个丰富的背景，丰富到有人认为它永远无法被完全指定（Searle, 1983）。情境/背景是整个生命形式的心智世界的结合部分。在心理学研究中，情境效应往往只被当作负面因素来研究，即使某人的实验或理论无效的人工制品。但可能背景或情境才是分类研究真正需要研究的单位。72 E. ROSCH Copyright (c) Imprint Academic 2013 For personal use -- not for reproduction</p>
<p>推论B-1：概念和分类永远不会孤立地出现</p>
<p>概念只会作为由其他概念和相互关联的生活活动提供的意义网络的一部分出现。</p>
<p>想想大这个概念；它只有在与小相关的情况下才有意义。此外，我们知道一只大跳蚤比一只小象小。建立实际的人工智能系统和故事理解器的人知道，教机器一个词的意义意味着必然要教它许多其他词的意义；同样，对于那些其他的词，也是一个指数级的爆炸，只能通过人工手段来限制。仅仅有单词是不够的。在一本外国词典中查找一个外国单词；如果你不了解这门语言，其定义对你没有好处。教会一个词的意思也意味着要教会机器许多事实，这就是问题的百科全书部分。正如前面所指出的，正是概念和世界理解的相互关联方面使理论观点具有明显的合理性。但是，理解的相互关联的网络并没有被可指定的理论所限定。事实，如词语，需要无数的其他事实。苏尔的等价交换论证指出，这些事实确实是无数的：当你订购汉堡包时，你不会规定它不能被包裹在透明材料中，不能有一英里长。这是对意义和语境的无限多的假设，而这些假设永远不可能被正式地规定。此外，对于所有提供给他们的信息，机器故事理解者仍然做得很蹩脚；他们不理解故事的意义。这是因为概念（以及人类其他的思维方式）本身并不是抽象的信息性的；它们是参与性的。推论B-2：概念在各种情况下具有参与性而非识别性的功能 一件事是什么，已经预先被赋予了它所发生的整个心灵世界的一部分。</p>
<p>概念的基本功能不是识别事物（正如它不是代表）；概念的功能不是我们不断地对自己说：'瞧，那边的物体有四条腿，会叫；它一定是条狗！' 相反，概念以无数种灵活的方式参与到情境中。许多关于概念和类别的实验研究，当然还有大多数模型，都假定要解释、争议或建模的是识别功能。可以肯定的是，我们可以参与专门的识别活动（比如参加植物学考试，玩20个问题，或者成为概念学习实验的主体），但这些活动最好被视为特殊的语言游戏（如维特根斯坦，1953），而不是原型的概念活动。(类似地，还有一些特殊的表征形式，如建筑图纸等。甚至创造形式系统也可以被看作是某些文化在某些情况下所珍视和奖励的活动）。) 由于概念是基于情境和参与性的，而不是识别功能，所以可以从新的角度来看待定义。在我们的文化中，人们发现自己所处的情况之一是被问及一个概念的定义。在这种情况下，我们从小到大的整个实践背景、理解和明确的教义都会发挥作用，而正确的答案通常是亚里士多德式的经典范畴定义。这是另一种形式的语言游戏。通过原型和分级结构的 "定义 "可以起到不同的作用。原型以其丰富的非标示性信息和意象，可以在许多不同的层面上表明在复杂情境中自我定位和导航的可能方式。</p>
<p>情境的重要性超出了分类研究。因为心智和世界只有在实际的复杂情境中才会出现，所以情境是RECLAIMING CONCEPTS 73 版权(c) Imprint Academic 2013 仅供个人使用--不得复制，其中人类的解释、情感和动机起着支配作用。(例如，令人费解的所谓意志薄弱的情况就是动机不能跨情况转移的情况）。) 情境也是行动的领域（人格心理学中的一个老问题，见Buss和Cantor, 1989）。因此，情境肯定是一般心理学需要研究的单位。如何研究？什么是情境？有哪些类型的情境？情境是如何关联和分类的？它们是如何被组织成生活形式的？- 以及概念和类别可能与所有这些有什么关系？由于概念和分类在连接心智世界单元和揭示情境背景方面发挥着重要作用，它们或许能够为情境研究提供一个切入点。与其问类别如何能够普遍，或者概念如何能够在内部思维中代表外部世界，我们不如问类别和类别系统首先来自哪里？(为什么椅子与桌子或沙发是不同的类别？为什么椅子看起来更像是这个物体的真实名字，而不是家具、物质物体或办公椅？为什么体重在1.3到2.9磅之间的袋鼠这个类别看起来既不基本，也不连贯，更不可能？) 在严格的认知主义和经典观点中，类别也可以是任意的属性集合，事实上，在传统的实验室概念学习任务中也是如此。就工作认知主义只关注表征问题而言，它不能提供关于现实世界范畴形成的生态条件的线索（除了对一般的进化理论进行必要的挥手之外）。解决这些问题的一个尝试（Rosch等人，1976）提出，在自然条件下，感知、行动和生活活动之间存在着大量的关系结构，而类别的形成是为了最大限度地映射这种结构。2对于普通物体的分类法，他们把这个层次称为基本层次，表明它是解释类别的默认层次，并提供证据表明它具有感知、语言和发展的优先权。</p>
<p>由于物体是情境中的 "道具"，这样的研究能否为情境的基本分类水平提供线索？(见Cantor等人，1982；Rifkin，1985；Rosch，1978。)富含信息的概念是否可以作为情境的模拟器(Barsalou, in press; Kahneman等人，1982)？是否有其他的情境方法可以将概念置于其自然环境中？认知主义不能处理这样的问题；它甚至不能提出这些问题。C. 心灵-世界非表征单位的 "因果 "规律 当心灵和世界被认为是分开的时候，因果或解释的效力就被归于心灵或世界。世界上发生的事情可能被认为是有机体或人所回应的刺激，或者心灵或人可能被视为欲望、意图、理论或对世界事物的行动的来源。这些极端的两极分化导致了理论和研究项目的流行和淘汰，但却没有进展。最多只能说心灵和世界是相互作用的。新的观点要求重新思考我们要如何完全建立因果关系和预测的模型。心灵和世界在一连串的情况下一起发生，这些情况在某种程度上是合法的和可预测的。我们希望能够找到这些规律和74 E. ROSCH [2] 将某物视为椅子或阿斯雷德，同样是一种感知和分类的行为。更广泛地说，认知和感知可能比我们目前的理论，特别是认知主义，愿意承认的要密切得多（例如，见Barsalou，出版中）。Copyright (c) Imprint Academic 2013 For personal use only -- not for reproduction to find a level of description which neither turns human action into something mechanical like engineering nor something mental like fantasy. 吉布森的生态心理学也面临类似的问题。他的生态光学被指责为将环境置于生物体内部的一种隐蔽方式。然而，观察他在讨论人的手的动作时是如何避免掉下二元论的钢丝的任何一边的。手的运动并不包括对刺激的反应......。. . . 难道唯一的选择就是把手看作是心灵的工具吗？例如，皮亚杰有时似乎在暗示，手是儿童智力的工具。但这就好比说，手是内在儿童的工具....。这无疑是一个错误。另一个选择不是回到精神主义。我们应该认为手既不是被触发的也不是被命令的，而是被控制的（Gibson, 1979, p. 235）。吉布森正试图发展一种对感知和行动的分析，而这种分析恰恰抓住了主体和客体之间功能描述的那个层面。在这一点上，他没有被追随。虽然研究者们蜂拥而至，发展他关于感知中高阶不变性的想法，这些想法可以在不改变人的思维方式的情况下进行，但在实施一种新的描述形式方面却没有什么作为。</p>
<p>D. 新颖性</p>
<p>认知主义的机器是确定的。当一台机器只是简单地操纵符号串，用一个符号替换另一个符号时，任何出来的东西都是以某种形式放进去的。归根结底，它是由人类程序员放进去的。在严格认知主义的规则驱动系统中对概念和类别的描述不允许有任何新意。概念和类别运作的实际情况是独特的，而且彼此之间有无限的不同。工作认知主义只能描绘出概念在各种情况下对一些共同点的反应，以产生经典观点中的骨架式抽象概念。在新的观点中，概念不是表征，而是运作的、参与整个情境的部分。</p>
<p>因此，概念允许情况本身的所有新颖性，而且可以有真正的惊喜、学习和发明。</p>
<p>E. 非概念背景</p>
<p>概念只存在于非概念的背景中。如果不对它们不是什么进行一些对比，我们甚至无法想到要谈论概念和概念化。除了认知主义之外，所有的系统都有某种方式来承认并至少试图接近非概念性。一些例子："如何知道 "与 "如何知道"，海德格尔的习惯和实践背景，私人经验及其所谓的质点，基于身体的认识，塞尔的等价交换，直觉，所有艺术的经验，以及在爱、悲伤、做数学、宗教和其他一切方面的不可言说的经验。但在认知主义中，一路都是概念! - 认知主义系统不可能处理非概念性的东西。然而，正是在这些经验中，人们找到了他们生活的意义和完整性。在认知主义中，这些东西必须被归入一个单独的领域，在那里它们要么被否认存在，要么被从根本上置于认知科学的范围之外。在这里提出的新观点中，非概念性的东西本来就是心灵世界情境的一部分，也许是每个情境的一部分。任何关于人类存在的科学都不能直截了当地排除对人最有意义的东西。重新认识概念 75 版权(c) Imprint Academic 2013 仅供个人使用--不得转载 结论 认知主义系统是由概念构成的。如果认知主义不能充分说明它的基本构件，它就有麻烦了。我试图说明认知主义如何必然误解了以前关于概念的理论和研究，以及它如何无法应对关于概念的基本事实。我已经概述了对概念和范畴的新观点的基础，这应该把它们的研究放在真实的（而不是幻想的球形牛）基础上。概念和范畴的逻辑必须是一个开放的系统，而不是一个封闭的逻辑。只有这样，对概念的研究才能从定义和可替代的符号串的无意义的范围内重新获得，成为人类认识的真正科学。</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>认知语言学</category>
      </categories>
  </entry>
  <entry>
    <title>信息抽取</title>
    <url>/information-retrieval-4/</url>
    <content><![CDATA[<h3 id="知识库-knowledge-base">11.1.1 知识库 (knowledge base)</h3>
<p>本章旨在教会你的机器人理解它所阅读的内容，并将知识存储在一个灵活的数据结构中，即将信息记录在一个知识库中，用于后期查询。除了识别文本中的数字和日期等简单任务之外，机器人还要提取更多关于世界的一般信息。</p>
<p>例如，它能够从自然语言文档中（比如维基百科）学习这句话：</p>
<blockquote>
<p>In 1983, Stanislav Petrov, a lieutenant colonel of the Soviet Air Defense Forces, saved the world from nuclear war.</p>
<p>1983年，斯坦尼斯拉夫·彼得罗夫，苏联防空部队的一名中校从核战争中拯救了世界。</p>
</blockquote>
<p>如果你在历史课上读完或听完这样的内容后做笔记，你可能会对事情进行解读，并在大脑中建立概念或词语之间的联系。你可能会把它还原成一个知识，那个你“从中得到的东西”。你希望你的机器人也能做同样的事情， “记下”它所学到的任何东西，比如斯坦尼斯洛夫·彼得罗夫是一名中校的事实或知识。</p>
<p>它可以存储在一个类似这样的数据结构中：</p>
<blockquote>
<p>('Stanislav Petrov', 'is-a', 'lieutenant colonel')</p>
</blockquote>
<p>这是知识图谱或知识库中两个<u>命名实体节点</u>('Stanislav Petrov'和'lieutenant colonel')和它们之间的<u>关系</u>或连接('is a')的例子。当这样的关系以符合知识图谱的<u>RDF标准</u>（关系描述格式）的形式存储时，它被称为<u>RDF三元模型(triplet)</u>。历史上，这些RDF triplet存储在XML文件中，但也可以存储在任何文件格式或数据库中，这些文件格式或数据库以 <u>(subject, relation, object)</u> 的形式保存，这些三元组的集合就是一个知识图谱。这有时也被语言学家称为<u>本体</u>（ontology），因为它存储的是关于词的结构化信息。但当图谱的目的是为了表示关于世界的事实而不仅仅是单词时，它就被称为<u>知识图谱</u>或<u>知识库</u>。</p>
<p>图11.1是你想从这样的句子中提取的知识图谱的图形表示。 图11.1顶部的“is-a”关系代表了一个不能直接从斯坦尼斯拉夫的陈述中提取的事实。但从一个军事组织成员的头衔是军衔这一事实可以推断出，“中校”是一个军衔。这种从知识图谱中推导出事实的逻辑操作叫做<u>知识推理</u>。也可以称为查询知识库，类似于查询关系型数据库。</p>
<p><img src="https://i.loli.net/2021/04/13/NoW1JAQhvMOaCTX.png"/></p>
<p>对于斯坦尼斯洛夫的军衔这一特殊推断或查询，你的知识图谱必须已经包含了关于军队和军衔的事实。也许你现在可以看到知识库是如何帮助机器理解一个语句的。如果没有这个知识基础，像这样简单的语句中的许多事实都会让你的聊天机器人“摸不着头脑”。你甚至可以说，关于职业等级的问题对于一个只知道如何根据随机分配的主题对文件进行分类的机器人来说，是“超乎寻常的"。如果你曾经与一个不懂“哪条路是向上的“的聊天机器人进行过互动，你就会明白。在人工智能研究中，最令人生畏的挑战之一是如何编译和高效查询常识性知识的知识图谱。</p>
<p><strong>机器很难找到常识性知识的语料库来阅读和学习。</strong>没有常识性知识的维基百科文章存在，你的机器就无法对其进行信息抽取。而有些知识是本能，是硬编码在我们的DNA中的，事物和人之间存在各种事实关系，比如“kindof"、"is-used-for"、"has-a"、"is-famous-for"、"was-born“和“has-profession"。</p>
<p>卡耐基梅隆大学永无止境的语言学习机器人NELL，几乎完全专注于提取“kind-of”关系信息的任务。大多数知识库都会对定义这些关系的字符串进行归一化处理，这样“kind of”和“type of”就会被分配一个归一化的字符串或ID来表示这个特定的关系。而有些知识库也会对知识库中代表obejct的名词进行解析。所以“Stanislav Petrov”这个bigram可能会被分配一个特定的ID。"Stanislav Petrov”的同义词，比如“S. Petrov”和“Lt Col Petrov"，也会被分配给同一个ID，如果NLP管道怀疑它们指的是同一个人。 知识库可以用来构建一种实用型的聊天机器人，称为问答系统（QA系统）。客服聊天机器人，包括大学TA机器人，几乎完全依靠知识库来生成他们的回答。问答系统对于帮助人类发现事实信息非常有用，它可以让人类的大脑自由地做自己擅长的事情，比如试图从这些事实中归纳总结。<strong>人类不善于准确地记住事实，但善于发现这些事实之间的联系和模式，这是机器尚未掌握的。</strong></p>
<h3 id="信息抽取">11.1.2 信息抽取</h3>
<p>所以你已经了解到“信息抽取“是将非结构化文本转换为存储在知识库或知识图谱中的结构化信息。信息抽取是被称为自然语言理解（NLU）的研究领域的一部分，尽管这个术语经常与自然语言处理同义使用。 信息抽取和NLU是一种不同于你在研究数据科学时可能想到的学习。它不仅仅是无监督的学习，甚至连“模型“本身，即关于世界如何运作的逻辑，都可以在没有人类干预的情况下组成。与其说是给机器钓鱼（事实），不如说是教它如何钓鱼（提取信息）。尽管如此，机器学习技术还是经常被用来训练信息抽取器。</p>
<h3 id="信息抽取作为ml特征提取">11.2.2 信息抽取作为ML特征提取</h3>
<p>模式匹配（和正则表达式）仍然是信息抽取的最先进的方法。即使使用机器学习方法来处理自然语言，你也需要进行特征工程。你需要创建词袋或词嵌入，以尝试将自然语言文本中近乎无限的意义可能性减少到机器可以轻松处理的向量中。信息抽取只是机器学习从非结构化自然语言数据中提取特征的另一种形式，比如创建一个词袋，或者在这个词袋上做PCA。而这些模式和特征即使在最先进的自然语言机器学习管道中仍然被采用，比如谷歌的Assistant、Siri、亚马逊Alexa和其他最先进的机器人。</p>
<p>信息抽取可以事先完成，以填充事实的知识库。或者，当聊天机器人被问到一个问题或搜索引擎被查询时，可以按需找到所需的语句和信息。当提前建立知识库时，可以优化数据结构，以便在更大的知识领域内进行更快的查询。预先构建的知识库可以让聊天机器人快速响应关于更广泛信息的问题。如果在聊天机器人被查询时实时检索信息，这通常被称为“搜索"。</p>
<p>Google和其他搜索引擎结合了这两种技术，查询知识库（knowledge base），如果没有找到必要的事实，就回落到文本搜索。你在学校里学到的许多自然语言语法规则可以被编码在正式的语法中，设计成对代表语言部分的单词或符号进行操作。而英语语言可以被认为是构成语言的单词和语法规则，或者你也可以把它看成是你可以说的所有可能的事情的集合，这些事情会被英语语言使用者认可为有效的语句。 而这就引出了<u>形式化语法</u>和<u>有限状态机</u>的另一个特点，它将在NLP中派上用场。</p>
<p>任何形式化语法都可以被机器以两种方式使用：</p>
<ul>
<li><p>识别与该语法相匹配的内容</p></li>
<li><p>生成新的符号序列</p></li>
</ul>
<p>你不仅可以使用模式（正则表达式）从自然语言中提取信息，还可以在聊天机器人中使用它们，让它“说“出与该模式相匹配的东西！我们将向你展示如何用有限状态机来实现这一点。我们在这里向你展示如何使用一个名为rstr4的包来实现这个功能，用于你的一些信息抽取模式。 这种形式化语法和有限状态机的模式匹配方法还有其他一些很棒的特性。</p>
<p>一个真正的有限状态机可以保证总是在有限时间内运行（停止）。它将始终告诉你是否在你的字符串中找到了匹配。它永远不会陷入永久循环......只要你不使用正则表达式引擎的一些高级功能，这些功能允许你“欺骗“并将循环纳入你的有限状态机。 所以，你要坚持使用不需要这些“回看“或“前瞻“作弊的正则表达式。你将确保你的正则表达式匹配器会处理每个字符，并且只有当它匹配时才会前进到下一个字符--有点像一个严格的列车员在座位上走动检查车票。如果你没有，列车员就会停下来，并宣布有问题，不匹配，他拒绝继续前进，或查看你的前面或后面，直到他解决这个问题。火车上的乘客没有“走回头路“或“重来"，也没有严格的正则表达式。</p>
<h2 id="值得提取的信息">11.3 值得提取的信息</h2>
<p>一些关键的定量信息值得“手工制作”正则表达式的努力：</p>
<blockquote>
<p>GPS位置｜日期｜价格｜数字</p>
</blockquote>
<p>其他重要的自然语言信息需要比正则表达式更复杂的模式：</p>
<blockquote>
<p>问题触发词｜问题目标词｜命名实体</p>
</blockquote>
<h3 id="提取gps位置">11.3.1 提取GPS位置</h3>
<p>GPS位置是典型的数字数据，你需要使用正则表达式从文本中提取。GPS位置是以一对经纬度的数字值来表示的。它们有时还包括第三个数字，即海拔高度，或海平面以上的高度，但你现在将忽略它。我们只提取十进制的经纬度对，用度数表示。这将适用于许多谷歌地图的URL。虽然URL在技术上不是自然语言，但它们通常是非结构化文本数据的一部分，你想提取这一点信息，这样你的聊天机器人就可以知道地方以及事物。 让我们使用之前例子中的十进制数字模式，但让我们更具限制性，确保该值在纬度（+/- 90度）和经度（+/- 180度）的有效范围内。你不能比北极（+90度）更北，也不能比南极（-90度）更南。而如果你从英国格林威治向东航行180度（+180度经度），你就会到达日期线，在那里你也是从格林威治向西180度（-180度）。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lat = <span class="string">r&#x27;([-]?[0-9]?[0-9][.][0-9]&#123;2,10&#125;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lon = <span class="string">r&#x27;([-]?1?[0-9]?[0-9][.][0-9]&#123;2,10&#125;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sep = <span class="string">r&#x27;[,/ ]&#123;1,3&#125;&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps = re.<span class="built_in">compile</span>(lat + sep + lon)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&#x27;http://...maps/@34.0551066,-118.2496763...&#x27;</span>)</span><br><span class="line">[(<span class="number">34.0551066</span>, -<span class="number">118.2496763</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&quot;https://www.openstreetmap.org/#map=10/5.9666/116.0566&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;5.9666&#x27;</span>, <span class="string">&#x27;116.0566&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&quot;Zig Zag Cafe is at 45.344, -121.9431 on my GPS.&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;45.3440&#x27;</span>, <span class="string">&#x27;-121.9431&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>数字数据很容易提取，特别是如果数字是机器可读字符串的一部分。URL和其他机器可读的字符串将经纬度等数字以可预测的顺序、格式和单位排列，以方便我们。这种模式仍然会接受一些不属于这个世界的经纬度值，但它可以满足你从OpenStreetMap等地图网络应用中复制的大多数URL的要求。 但是日期呢？正则表达式对日期有用吗？如果你想让你的日期提取器能在欧洲和美国工作，那该怎么办，因为那里的日/月顺序经常是相反的。</p>
<h3 id="提取日期">11.3.2 提取日期</h3>
<p>日期比 GPS 坐标更难提取。日期是一种比较自然的语言，类似的事情有不同的方言来表达。在美国，2017年的圣诞节是“12/25/17"。在欧洲，2017年的圣诞节是“25/12/17"。你可以检查你的用户的所在地，并假设他们和他们所在地区的其他人一样写日期。但这个假设可能是错误的。 因此，大多数日期和时间提取器都会尝试使用这两种日/月顺序，并检查以确保这是一个有效的日期。当我们读到这样的日期时，人脑就是这样工作的。即使你是一个美式英语的人，你在圣诞节前后在布鲁塞尔，你也可能会认出“25/12/17“是一个节日，因为一年只有12个月。</p>
<p><strong>这种在计算机编程中行之有效的“鸭子打字“ (duck-typing) 法<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>也可以用于自然语言。如果它看起来像一只鸭子，行为又像鸭子，那它可能就是一只鸭子。</strong>如果它看起来像个日期，行为又像个日期，那可能就是个日期。你会把这种“先试后买“的方法也用在其他自然语言处理任务上。你会尝试一堆选项，然后接受一个有效的选项。你会尝试你的提取器或生成器，然后你会在上面运行一个验证器，看看它是否合理。 对于聊天机器人来说，这是一个特别强大的方法，允许你结合多个自然语言生成器的优点。在第10章中，你使用LSTMs生成了一些聊天机器人的回复。为了改善用户体验，你可以生成很多回复，然后选择拼写、语法和情感最好的那个。我们将在第12章中详细讨论这个问题。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>us = <span class="string">r&#x27;((([01]?\d)[-/]([0123]?\d))([-/]([0123]\d)\d\d)?)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mdy = re.findall(us, <span class="string">&#x27;Santa came 12/25/2017. An elf appeared 12/12.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mdy</span><br><span class="line">[(<span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;25&#x27;</span>, <span class="string">&#x27;/2017&#x27;</span>, <span class="string">&#x27;20&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>列表理解可以用来为提取的数据提供一点结构，方法是将月份、日期和年份转换为整数，并用有意义的名称标记这些数字信息，如下面的列表所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates = [&#123;<span class="string">&#x27;mdy&#x27;</span>: x[<span class="number">0</span>], <span class="string">&#x27;my&#x27;</span>: x[<span class="number">1</span>], <span class="string">&#x27;m&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">2</span>]), <span class="string">&#x27;d&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">3</span>]),</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;y&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">4</span>].lstrip(<span class="string">&#x27;/&#x27;</span>) <span class="keyword">or</span> <span class="number">0</span>), <span class="string">&#x27;c&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">5</span>] <span class="keyword">or</span> <span class="number">0</span>)&#125; <span class="keyword">for</span> x <span class="keyword">in</span> mdy]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates</span><br><span class="line">[&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">25</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">0</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>即使对于这些简单的日期，也不可能设计出一个能够解决第二个日期“12/12”中所有歧义的regex。在日期的语言中，有一些含糊不清的地方，只有人类能够利用圣诞节等知识和文本作者的意图来猜测解决。</p>
<p>比如“12/12“可能意味着：</p>
<blockquote>
<p>December 12th, 2017—month/day in the estimated year based on anaphora resolution</p>
<p>December 12th, 2018—month/day in the current year at time of publishing</p>
<p>December 2012—month/year in the year 2012</p>
</blockquote>
<p>因为在美国日期和我们的regex中，month/day在年份之前，"12/12“被假定为未知年份的12月12日。你可以使用内存中的结构化数据中的上下文来填充任何缺失的数字字段，如下面的列表所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(dates):</span><br><span class="line"><span class="meta">... </span><span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line"><span class="meta">... </span><span class="keyword">if</span> <span class="keyword">not</span> v:</span><br><span class="line"><span class="meta">... </span>d[k] = dates[<span class="built_in">max</span>(i - <span class="number">1</span>, <span class="number">0</span>)][k]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates</span><br><span class="line">[&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">25</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> date</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>datetimes = [date(d[<span class="string">&#x27;y&#x27;</span>], d[<span class="string">&#x27;m&#x27;</span>], d[<span class="string">&#x27;d&#x27;</span>]) <span class="keyword">for</span> d <span class="keyword">in</span> dates]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>datetimes</span><br><span class="line">[datetime.date(<span class="number">2017</span>, <span class="number">12</span>, <span class="number">25</span>), datetime.date(<span class="number">2017</span>, <span class="number">12</span>, <span class="number">12</span>)]</span><br></pre></td></tr></table></figure>
<p>这是一种从自然语言文本中提取日期信息的基本但相当健壮的方法。要把它变成一个生产型的日期提取器，剩下的主要任务就是添加一些适合你的应用的异常捕获和上下文维护。如果你通过拉取请求将其添加到nlpia包(http://github.com/ totalgood/nlpia)中，我相信你的读者朋友们会很感激。而如果你为时间添加了一些提取器，好吧，那你就是相当的英雄了。 一些手工制作的逻辑有机会处理几个月甚至几天的边缘情况和自然语言名称。但是再复杂也无法解决“12/11“这个日期的歧义。这可能是12月11日，无论你在哪一年读到或听到它11月12日，如果你在伦敦或塔斯马尼亚州朗塞斯顿（一个联邦领土）听到它2011年12月，如果你在美国报纸上读到它2012年11月，如果你在欧盟报纸上读到它有些自然语言的歧义是无法解决的，即使是人脑。但是，让我们确保你的日期提取器可以通过在你的regex中颠倒月份和日期来处理欧洲的日/月顺序。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>eu = <span class="string">r&#x27;((([0123]?\d)[-/]([01]?\d))([-/]([0123]\d)?\d\d)?)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy = re.findall(eu, <span class="string">&#x27;Alan Mathison Turing OBE FRS (23/6/1912-7/6/1954) \</span></span><br><span class="line"><span class="string">... was an English computer scientist.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy</span><br><span class="line">[(<span class="string">&#x27;23/6/1912&#x27;</span>, <span class="string">&#x27;23/6&#x27;</span>, <span class="string">&#x27;23&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/1912&#x27;</span>, <span class="string">&#x27;19&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;7/6/1954&#x27;</span>, <span class="string">&#x27;7/6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/1954&#x27;</span>, <span class="string">&#x27;19&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy = re.findall(eu, <span class="string">&#x27;Alan Mathison Turing OBE FRS (23/6/12-7/6/54) \</span></span><br><span class="line"><span class="string">... was an English computer scientist.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy</span><br><span class="line">[(<span class="string">&#x27;23/6/12&#x27;</span>, <span class="string">&#x27;23/6&#x27;</span>, <span class="string">&#x27;23&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/12&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;7/6/54&#x27;</span>, <span class="string">&#x27;7/6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/54&#x27;</span>, <span class="string">&#x27;&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>这个正则表达式正确地从维基百科的摘录中提取了图灵的生卒日期。但我作弊了，我把“6月”这个月份转换成了数字6，然后才在维基百科的那句话上测试正则表达式。所以这不是一个现实的例子。而且如果不指定世纪的话，你对年份的解析还是会有一些歧义的。54年是指1954年还是指2054年？你希望你的聊天机器人能够从未经修改的维基百科文章中提取日期，这样它就可以阅读名人的资料，学习导入日期。为了让您的正则表达式能够在更多的自然语言日期上发挥作用，例如维基百科文章中的日期，您需要在您的日期提取正则表达式中添加诸如“June"（及其所有缩写）这样的单词。 你不需要任何特殊符号来表示单词（按顺序排列的字符）。你可以在正则表达式中完全按照你希望它们在输入中的拼写来输入，包括大写。你所要做的就是在它们之间的正则表达式中加上一个OR符号(|)。你需要确保它可以处理美国的月/日顺序以及欧洲的顺序。你将把这两种可供选择的日期“拼法“添加到你的正则表达式中，在它们之间加上一个“大“的 OR (|)，作为正则表达式中决策树的分叉。 让我们使用一些命名组来帮助你识别年份，比如"'84'"是1984，"08“是2008。而且让我们尝试更精确地匹配4位数的年份，只匹配未来到2399年的年份和过去到0.6年的年份，请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_19xx = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr_19xx&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>, <span class="number">100</span>)) +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_20xx = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr_20xx&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;:02d&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)) + <span class="string">&#x27;|&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>, <span class="number">30</span>)) +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_cent = <span class="string">r&#x27;\b(?P&lt;yr_cent&gt;&#x27;</span> + <span class="string">&#x27;|&#x27;</span>.join(</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">40</span>)) + <span class="string">r&#x27;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_ccxx = <span class="string">r&#x27;(?P&lt;yr_ccxx&gt;&#x27;</span> + <span class="string">&#x27;|&#x27;</span>.join(</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;&#123;:02d&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>)) + <span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_xxxx = <span class="string">r&#x27;\b(?P&lt;yr_xxxx&gt;(&#x27;</span> + yr_cent + <span class="string">&#x27;)(&#x27;</span> + yr_ccxx + <span class="string">r&#x27;))\b&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span>yr_19xx + <span class="string">&#x27;|&#x27;</span> + yr_20xx + <span class="string">&#x27;|&#x27;</span> + yr_xxxx +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>groups = <span class="built_in">list</span>(re.finditer(</span><br><span class="line"><span class="meta">... </span>yr,“<span class="number">0</span>, <span class="number">2000</span>, 01, <span class="string">&#x27;08, 99, 1984, 2030/1970 85 47 `66&quot;))</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; full_years = [g[&#x27;</span>y<span class="string">r&#x27;] for g in groups]</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; full_years</span></span><br><span class="line"><span class="string">[&#x27;</span><span class="number">2000</span><span class="string">&#x27;, &#x27;</span>01<span class="string">&#x27;, &#x27;</span>08<span class="string">&#x27;, &#x27;</span><span class="number">99</span><span class="string">&#x27;, &#x27;</span><span class="number">1984</span><span class="string">&#x27;, &#x27;</span><span class="number">2030</span><span class="string">&#x27;, &#x27;</span><span class="number">1970</span><span class="string">&#x27;, &#x27;</span><span class="number">85</span><span class="string">&#x27;, &#x27;</span><span class="number">47</span><span class="string">&#x27;, &#x27;</span><span class="number">66</span><span class="string">&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>哇！这可真够费劲的。这是一个很大的工作，只是为了在regex中而不是在Python中处理一些简单的年份规则。别担心，有一些包可以用来识别常见的日期格式。它们更精确 (更少的错误匹配) 和更通用 (更少的失误)。所以，你不需要自己能够编写复杂的正则表达式，比如这样。这个例子只是给你提供了一个模式，以备将来你需要使用正则表达式提取某种特定的数字。货币值和 IP 地址是一些例子，在这些例子中，一个更复杂的正则表达式（带有命名组）可能会派上用场。 让我们完成你的提取日期的正则表达式，为维基百科上的日期添加月份名称的模式，比如图灵生日中的“June“或“Jun"，如下面的列表所示。你能看到如何将这些正则表达式组合成一个更大的可以处理欧盟和美国日期格式的正则表达式吗？一个复杂的问题是，你不能为一个组重复使用相同的名称（正则表达式的括号部分）。所以你不能在月份和年份的命名正则表达式的美国和欧盟排序之间放一个OR。而且你需要在日、月、年之间包含一些可选的分隔符的模式。最后，你需要验证这些日期，看看它们是否可以变成有效的Python日期时间对象，如下面的列表所示。想想像Python-dateutil和datefinder这样的包是如何解决歧义和处理更多“自然“语言的日期，比如“今天“和“下周一"。如果你认为你能比这些包做得更好，就给他们发个拉请求吧! 如果你只是想要一个最先进的日期提取器，统计（机器学习）方法会让你更快地达到目的。Stanford Core NLP SUTime库(https:// nlp.stanford.edu/software/sutime.html)和Google的dateutil.parser.parse是最先进的。</p>
<h2 id="提取关系relation">11.4 提取关系（relation）</h2>
<p>到目前为止，你只研究了提取棘手的名词实例，如日期和GPS经纬度值。而且你主要是处理数字模式。现在是时候解决从自然语言中提取知识这个更难的问题了。</p>
<p>你想让你的机器人从阅读知识百科全书（如维基百科）中了解关于世界的事实。你希望它能够将这些日期和GPS坐标与它所阅读的实体联系起来。你的大脑可以从维基百科的这句话中提取什么知识呢？</p>
<blockquote>
<p>On March 15, 1554, Desoto wrote in his journal that the Pascagoula people ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5.</p>
<p>1554年3月15日，德索托在他的日记中写道，帕斯卡古拉人的范围是位于30.4，-88.5的最北边的利夫河和奇卡索河的交汇处。</p>
</blockquote>
<p>提取日期和GPS坐标也许能让你把这个日期和地点、德索托、帕斯卡古拉人以及这两条你念不出名字的河流联系起来。你希望你的机器（和你的大脑）能够将这些事实与更大的事实联系起来——例如，德索托是一个西班牙征服者，而帕斯卡古拉人是一个和平的美国土著部落。而且你希望日期和地点能与正确的“东西”联系起来：分别是德索托和两条河流的交汇处。</p>
<p>这就是大多数人听到自然语言理解这个词时想到的。要理解一个语句，你需要能够提取关键的信息，并将其与相关知识关联起来。对于机器来说，你将这些知识存储在一个图谱中，也称为知识库。你的知识图谱的边就是事物之间的关系。而你的知识图谱的节点就是在你的语料库中找到的名词或对象。 你要用来提取这些关系（或关系）的模式是一种subject - verb - object这样的模式。为了识别这些模式，你需要你的NLP管道知道句子中每个词的词性。</p>
<h3 id="词性pos标记">11.4.1 词性（POS）标记</h3>
<p>POS标记可以通过语言模型来完成，这些语言模型具有包含所有可能词性的单词字典。然后，他们可以对正确标记的句子进行训练，以识别新句子和字典中的其他单词中的词性。NLTK和spaCy都实现了POS标记功能。你在这里会使用spaCy，因为它更快、更准确。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> spacy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>en_model = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = (<span class="string">&quot;In 1541 Desoto wrote in his journal that the Pascagoula people&quot;</span> + <span class="string">&quot;ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent = en_model(sentence)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent.ents</span><br><span class="line">(<span class="number">1541</span>, Desoto, Pascagoula, Leaf, Chickasawhay, <span class="number">30.4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27; &#x27;</span>.join([<span class="string">&#x27;&#123;&#125;_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tok, tok.tag_) <span class="keyword">for</span> tok <span class="keyword">in</span> parsed_sent])</span><br><span class="line"><span class="string">&#x27;In_IN 1541_CD Desoto_NNP wrote_VBD in_IN his_PRP$ journal_NN that_IN the_DT Pascagoula_NNP people_NNS ranged_VBD as_RB far_RB north_RB as_IN the_DT confluence_NN of_IN the_DT Lea f_NNP and_CC Chickasawhay_NNP rivers_VBZ at_IN 30.4_CD ,_, -88.5_NFP ._.&#x27;</span></span><br></pre></td></tr></table></figure>
<p>因此，为了建立你的知识图谱，你需要弄清楚哪些对象（名词短语）应该配对。你想把日期“1554年3月15日“与命名实体Desoto配对。然后你可以将这两个字符串（名词短语）解析为指向你知识库中的对象。1554年3月15日可以转换为具有归一化表示的datetime.date对象。</p>
<p>spaCy-parsed句子还包含嵌套字典中的依存树。而spacy.displacy可以生成一个可扩展的矢量图形SVG字符串（或一个完整的HTML页面），它可以在浏览器中作为图像查看。这种可视化可以帮助您找到使用该树创建标签模式以进行关系提取的方法。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> spacy.displacy <span class="keyword">import</span> render</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = <span class="string">&quot;In 1541 Desoto wrote in his journal about the Pascagoula.&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent = en_model(sentence)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;pascagoula.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="meta">... </span>f.write(render(docs=parsed_sent, page=<span class="literal">True</span>, options=<span class="built_in">dict</span>(compact=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>
<p>这个短句的依存树显示名词短语“The Pascagoula”是主语“Desoto”关系“met”的宾语（见图11.2）。两个名词都被标记为专有名词。</p>
<p><img src="https://i.loli.net/2021/04/13/oF3AcQ9CBVrps1X.png"/></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">token_dict</span>(<span class="params">token</span>):</span></span><br><span class="line"><span class="meta">... </span>	<span class="keyword">return</span> OrderedDict(ORTH=token.orth_, LEMMA=token.lemma_, POS=token.pos_, TAG=token.tag_, DEP=token.dep_)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">doc_dataframe</span>(<span class="params">doc</span>):</span></span><br><span class="line"><span class="meta">... </span>	<span class="keyword">return</span> pd.DataFrame([token_dict(tok) <span class="keyword">for</span> tok <span class="keyword">in</span> doc])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc_dataframe(en_model(<span class="string">&quot;In 1541 Desoto met the Pascagoula.&quot;</span>))</span><br><span class="line">         ORTH       LEMMA    POS  TAG    DEP</span><br><span class="line"><span class="number">0</span>          In          <span class="keyword">in</span>    ADP   IN   prep</span><br><span class="line"><span class="number">1</span>        <span class="number">1541</span>        <span class="number">1541</span>    NUM   CD   pobj</span><br><span class="line"><span class="number">2</span>      Desoto      desoto   NOUN   NN  nsubj</span><br><span class="line"><span class="number">3</span>         met        meet   VERB  VBD   ROOT</span><br><span class="line"><span class="number">4</span>         the         the    DET   DT    det</span><br><span class="line"><span class="number">5</span>  Pascagoula  Pascagoula  PROPN  NNP   dobj</span><br><span class="line"><span class="number">6</span>           .           .  PUNCT    .  punct</span><br></pre></td></tr></table></figure>
<p>现在您可以看到POS或TAG特性的序列，它们将构成一个良好的模式。如果您正在查找人员和组织之间的“has meet”关系，您可能希望允许使用诸如“PROPN met PROPN”、“PROPN met the PROPN”、“PROPN met the PROPN”、“PROPN met with the PROPN”和“PROPN frequency meeting with PROPN”之类的模式。您可以分别指定这些模式中的每一个，或者尝试使用一些*或者？专有名词之间的“任意词”模式的运算符：</p>
<blockquote>
<p>'PROPN ANYWORD? met ANYWORD? ANYWORD? PROPN'</p>
</blockquote>
<p>spaCy中的模式比前面的伪代码更强大、更灵活，因此您必须更详细地解释您想要匹配的word特性。在spaCy模式规范中，您可以使用字典来捕获要为每个标记或单词匹配的所有标记，如下面的清单所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = [[&#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, [&#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;],</span><br><span class="line"><span class="meta">... </span>	[&#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;meet&#x27;</span>&#125;],</span><br><span class="line"><span class="meta">... </span>	[&#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;, &#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;]]</span><br></pre></td></tr></table></figure>
<p>然后可以从解析的句子中提取所需的标记标记，如下面的清单所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;In 1541 Desoto met the Pascagoula.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher = Matcher(en_model.vocab)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher.add(<span class="string">&#x27;met&#x27;</span>, [pattern])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">[(<span class="number">12280034159272152371</span>, <span class="number">2</span>, <span class="number">6</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[<span class="number">0</span>][<span class="number">1</span>]:m[<span class="number">0</span>][<span class="number">2</span>]]</span><br><span class="line">Desoto met the Pascagoula</span><br></pre></td></tr></table></figure>
<p>所以你从创建pattern的原始句子中提取了一个匹配项，但是维基百科中类似的句子呢？请参见下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;October 24: Lewis and Clark met their first Mandan Chief, Big White.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">(<span class="number">12280034159272152371</span>, <span class="number">3</span>, <span class="number">11</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[<span class="number">1</span>]:m[<span class="number">2</span>]]</span><br><span class="line">Lewis <span class="keyword">and</span> Clark met their first Mandan Chief</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;On 11 October 1986, Gorbachev and Reagan met at a house&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher(doc)</span><br><span class="line">[]</span><br></pre></td></tr></table></figure>
<p>您需要添加第二个模式，以允许动词出现在主语和宾语名词之后，如下面的清单所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;On 11 October 1986, Gorbachev and Reagan met at a house&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pattern = [&#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, &#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;and&#x27;</span>&#125;, &#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, &#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;, &#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;meet&#x27;</span>&#125;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher.add(<span class="string">&#x27;met&#x27;</span>, <span class="literal">None</span>, pattern)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">[(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">9</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">11</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">7</span>, <span class="number">11</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">12</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[-<span class="number">1</span>][<span class="number">1</span>]:m[-<span class="number">1</span>][<span class="number">2</span>]]</span><br><span class="line">Gorbachev <span class="keyword">and</span> Reagan met at a house</span><br></pre></td></tr></table></figure>
<p>所以现在你有了你的实体和关系。您甚至可以构建一个模式，该模式对中间的动词（“met”）限制较少，对两边的人和组的名称限制更多。这样做可能会让你识别出暗示一个人或一组人遇到了另一个人或另一组人的其他动词，比如动词“知道”，甚至是被动短语，比如“有过一次谈话”或“变得熟悉”。然后你可以使用这些新动词来为两边的新专有名词添加关系。但你可以看到你是如何偏离你的种子关系模式的最初含义的。这就是所谓的语义漂移。</p>
<p>幸运的是，spaCy不仅用词性和依存关系树信息来标记解析文档中的单词，而且还用词性模式匹配器（如清单11.18所示）将多个模式结合起来，以获得更健壮的模式匹配器该模式不匹配Wikipedia中句子的任何子字符串。添加附加图案而不删除以前的图案。这里的“met”是一个任意键。随便你怎么命名你的图案。“+”运算符增加了重叠替代匹配的数量。最长的匹配是匹配列表中的最后一个。357提取关系（relations）还提供Word2vec单词向量。您可以使用此向量来防止连接动词和两边的专有名词偏离种子模式的原始含义太远。</p>
<h3 id="实体名称规范化">11.4.2实体名称规范化</h3>
<p>实体的规范化表示通常是字符串，即使对于日期等数字信息也是如此。此日期的标准化ISO格式为“1541-01-01”。实体的标准化表示使您的知识库能够将世界上在同一日期发生的所有不同事件连接到图形中的同一节点（实体）。对其他命名实体也会这样做。</p>
<p>你应该纠正单词的拼写，并尝试解决物体、动物、人、地方等名称的歧义。规范化命名实体和解决歧义通常被称为共指消解或回指消解，特别是对于依存上下文的代词或其他“名称”。这类似于我们在第2章讨论的lemmatization。命名实体的规范化可确保拼写和命名变体不会因混淆、冗余的名称而污染实体名称的词汇表。例如，“Desoto”在一个特定的文档中至少可以用五种不同的方式来表达：</p>
<ul>
<li>“de Soto”</li>
<li>“Hernando de Soto”</li>
<li>“Hernando de Soto (c. 1496/1497–1542), Spanish conquistador”</li>
<li>https://en.wikipedia.org/wiki/Hernando_de_Soto (a URI)</li>
<li>A numerical ID for a database of famous and historical people</li>
</ul>
<p>类似地，您的规范化算法可以选择这些形式中的任何一种。知识图应该以相同的方式规范化每种实体，以防止同一类型的多个不同实体共享相同的名称。您不希望多个人名引用同一个自然人。更重要的是，无论是在向知识库中写入新事实时，还是在阅读或查询知识库时，规范化都应该始终如一地应用。如果在填充数据库之后决定更改规范化方法，则应该“迁移”或更改知识中现有实体的数据，以遵循新的规范化方案。无模式数据库（键值存储）与用于存储知识图或知识库的数据库一样，不能免除关系数据库的迁移责任。毕竟，无模式数据库是关系数据库的接口包装器。您的规范化实体还需要“is-a”关系来将它们连接到定义实体类型或类别的实体类别。这些“is-a”关系可以看作是标记，因为每个实体可以有多个“is-a”关系。就像7，这是积极研究的主题：https://nlp.stanford.edu/pubs/structuredVS.pdf。如果您想将人名或POS标签、日期和其他离散的数字对象合并到您的知识库中，则需要对它们进行规范化。实体之间的关系需要以正常的方式存储吗？</p>
<h3 id="关系规范化和提取">11.4.3关系规范化和提取</h3>
<p>现在需要一种方法来规范关系，以识别实体之间的关系类型。这样做可以让你找到日期和人之间的所有生日关系，或者历史事件发生的日期，例如“Hernando de Soto”和“Pascagola people”之间的遭遇。您需要编写一个算法来为您的关系选择正确的标签。这些关系可以有一个层次名称，例如“发生在/大约”和“发生在/确切地”以允许您找到特定的关系或关系类别。您还可以使用该关系的“置信度”、“概率、权重或标准化频率”（术语/词的ANLO  GOU到TF-IDF）的数值属性标记这些关系。每次从新文本中提取的事实证实或与数据库中存在的事实相矛盾时，都可以调整这些置信值。现在，您需要一种方法来匹配可以找到这些关系的模式。</p>
<h3 id="字型字模式">11.4.4字型字模式</h3>
<p>与正则表达式一样，而非字符。您没有字符类，而是有单词类。例如，您可能会有一个单词模式决定来匹配所有的单数名词（“NN”POS标记），8这通常是通过机器学习完成的。有些种子句子被标记了一些正确的关系（事实）从这些句子中提取。POS模式可以用来找到类似的句子，在这些句子中主语和宾语，甚至关系都可能发生变化。无论您希望匹配多少模式，您都可以使用spaCy包两种不同的方法来匹配O（1）（恒定时间）中的这些模式：”“。任何单词/标记序列模式的phrasether 9™POS标记序列模式匹配器10，以确保新句子中找到的新关系与原始种子（例如）真正类似（示例）关系，你经常需要约束主语，关系，和宾语的意思，以类似于种子句。最好的方法是用一些向量表示单词的意思。这个响吗？第4章讨论的词向量是目前应用最广泛的词义表示方法之一。它们有助于最小化语义漂移。</p>
<p>参考：</p>
<p><a href="https://livebook.manning.com/book/natural-language-processing-in-action/chapter-11/1">Chapter 11. Information extraction (named entity extraction and question answering) - Natural Language Processing in Action: Understanding, analyzing, and generating text with Python (manning.com)</a></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>https://www.pythonf.cn/read/129220<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>语义学术语大全</title>
    <url>/semantics-terms/</url>
    <content><![CDATA[<h1 id="chapter-1">Chapter 1</h1>
<h3 id="the-meaningfulness-of-language-is-an-instance-of-the-meaningfulness-of-behaviour">The meaningfulness of language is an instance of the meaningfulness of behaviour</h3>
<p>The meaningfulness of language can be seen as just one instance of the meaningfulness of human behaviour and communication in general, and is one of the systems of structured meaningfulness studied in semiotics.</p>
<h3 id="meaning-is-a-very-vague-term">‘Meaning’ is a very vague term</h3>
<p>‘Meaning’ is a very vague term: in English it refers to a variety of different relations between the world, language and speakers. Most languages do not have precise equivalents for the English term ‘meaning’, and some use a very different stock of lexical resources to talk about meaning-like phenomena.</p>
<h3 id="the-semiotic-triangle">The semiotic triangle</h3>
<p>For the purposes of linguistics, we can isolate three particularly important factors relevant to the study of meaning: the psychology of speakers, which creates and interprets language, the referent of the language expression as projected by the language user’s psychology, and the linguistic expression itself: these three points constitute the semiotic triangle.</p>
<h3 id="lexemes">Lexemes</h3>
<p>In providing a semantic description of a language, we do not need to treat all the variant morphological forms of a single word separately. Instead, we describe the meanings of a language’s lexemes, or the abstract units which unite all the morphological variants of a single word.</p>
<h3 id="sense-reference-denotation-and-connotation">Sense, reference, denotation and connotation</h3>
<p>There are several different aspects of the meaning of a lexeme: its referent on any one occasion of use, its denotation, which is the set of all its referents, and its sense, or the abstract, general meaning which can be translated from one language to another, paraphrased, or defined in a dictionary. Connotation names those aspects of meaning which do not affect a word’s sense, reference or denotation, but which have to do with secondary factors such as its emotional force, its level of formality, its character as a euphemism, etc.</p>
<h3 id="compositionality">Compositionality</h3>
<p>Meaning is often compositional, which means that the meanings of sentences are made up, or composed, of the meanings of their constituent lexemes.</p>
<h3 id="sentence-and-utterance-meaning">Sentence and utterance meaning</h3>
<p>Sentence meaning is the compositional meaning of the sentence as constructed out of the meanings of its individual component lexemes. Utterance meaning is the meaning which the words have on a particular occasion of use in the particular context in which they occur. Semantics studies sentence meaning, whereas pragmatics studies utterance meaning and other aspects of language use.</p>
<h3 id="object-language-and-metalanguage">Object language and metalanguage</h3>
<p>In analysing meaning we distinguish the object language, or the language whose meanings are being described, from the metalanguage, the language in which we describe these meanings.</p>
<h3 id="explanations-of-meaning-in-terms-of-meanings-are-circular">Explanations of meaning in terms of meanings are circular</h3>
<p>When we propose a definition in a metalanguage as an analysis of the meaning of an object language term, the more basic questions, ‘what is meaning?’ and ‘what is it to understand a meaning?’ are left unanswered. All definitions of meaning in language, therefore, are ultimately circular because they use one kind of meaning to explain another.</p>
<h3 id="four-ways-of-breaking-the-circle">Four ways of breaking the circle</h3>
<p>　There are four important answers to the question ‘what is meaning?’: the referential/denotational theory of meaning, the conceptual theory of meaning, the brain states theory and the use theory. We do not have to categorically choose between these theories. Instead, recognizing that the notion of meaning in linguistics is a way of talking about the factors which explain language use, we can see referents, concepts, brain states and uses as all relevant to this task.</p>
<h1 id="chapter-2">Chapter 2</h1>
<h3 id="meaning-definition-and-the-mental-lexicon">Meaning, definition and the mental lexicon</h3>
<p>The concept of a word’s meaning is closely linked to the concept of definition. Many linguists identify the task of linguistic semantics with the task of describing the entries stored in the mental lexicon, a stock of words and meanings stored in long-term memory: the definition</p>
<p>of a word is part of its entry in the mental lexicon, and the process of matching a meaning with a word-form is assumed to be analogous to that involved in consulting a dictionary. In order to serve the purposes of serious linguistic description, the definitions in the lexicon must be much more detailed than is usual in ordinary dictionaries.</p>
<h3 id="what-units-need-to-receive-definition">What units need to receive definition?</h3>
<p>Any attempt to analyse the meanings of language must specify what the meaning-bearing units are. Individual lexemes are the central examples of units with individually describable meanings. Morphemes also have meanings, as do phrasal verbs and compounds.</p>
<p>　Ambiguities about the level of grammatical structure to which meaning is correctly attributed are not infrequent: sound symbolism and idioms exemplify cases where the correct level for the analysis of a meaning may not be clear.</p>
<h3 id="real-and-nominal-definition">Real and nominal definition</h3>
<h3 id="we-can-distinguish-two-types-of-definition">We can distinguish two types of definition:</h3>
<p>definition of the essence of a thing (real definition), or definition of the meaning of a word (nominal definition).</p>
<p>Most linguists take nominal definition to be the type that is of interest to linguistic semantic research.</p>
<h3 id="cognitive-and-extensional-definition">Cognitive and extensional definition</h3>
<p>A nominal definition may be of two types:</p>
<p>cognitive (aimed to inculcate an understanding of the word’s correct use), or extensional (aimed at delimiting the denotation of the word).</p>
<p>​</p>
<h1 id="chapter-3">Chapter 3</h1>
<h3 id="the-basic-question-meaning-and-context">The basic question: meaning and context</h3>
<p>One of the main questions to be answered by any theory of meaning concerns the scope of an expression’s meaning: how much of the total effect of an expression is to be attributed to its meaning, and how much to the context in which it occurs?</p>
<p>We can distinguish two essential types of context:</p>
<p>the external or real-world context to which linguistic expressions refer, and the interpersonal context of linguistic action in which any utterance is placed.</p>
<h3 id="external-context-sense-and-reference">External context: sense and reference</h3>
<p>Frege distinguished an expression’s reference, which concerns the entities which the expression is about, from its sense, which is the way in which we grasp or understand its referent. In the Fregean view, two crucial features of sense are as follows:</p>
<p>sense is what our minds ‘grasp’ when we understand the meaning of a word; sense determines reference; words’ referents are identified through their senses.</p>
<p>Truth has a central place in Frege’s semantics. To know the sense of a sentence is, for Frege, to know how the sentence could be assigned a value as true or false: to know what the conditions are that would make it true or false. Knowledge of a sentence’s truth conditions allows us to determine, by looking at the sentence’s referents, whether the world actually is the way the sentence represents it, and thus whether or not the sentence is therefore true.</p>
<h3 id="predication-and-deixis">Predication and deixis</h3>
<p>As well as referring, linguistic expressions can often be used to predicate (attribute properties). Verbs, for example, are characteristically limited to this function. Deictic expressions (otherwise known as deictics or indexicals) are defined as those which make reference to some aspect of the context of utterance as an essential part of their meaning. Examples of deictics in English include the words I, you and here. The languages of the world show a large variety of deictic systems.</p>
<h3 id="knowledge-of-meaning-and-knowledge-of-facts">Knowledge of meaning and knowledge of facts</h3>
<p>Since reference is an important part of the meaning of many words, many linguists have wanted to distinguish knowledge we have of a word’s meaning (sense) from knowledge we might have about its referent. This is the distinction between lexical (‘dictionary’) knowledge and factual (‘encyclopaedic’) knowledge.</p>
<p>　The distinction enables an economical description of word meanings, but is often criticized: the boundary between dictionary and encyclopaedia seems to be so highly permeable as to be nonexistent.</p>
<h1 id="chapter-4">Chapter 4</h1>
<h3 id="interpersonal-context">Interpersonal context</h3>
<p>The relations between language and context are not limited to those in which a linguistic expression describes a preexisting world. The assertion of facts about the world is just one of the acts which we can use language to perform: we also ask questions, issue orders and make requests. In these types of speech act, truth is not a relevant parameter in the appreciation of meaning.</p>
<h3 id="austin-and-searle-on-speech-acts">Austin and Searle on speech acts</h3>
<p>Austin’s theory of speech acts distinguished three types of act we perform in any utterance:</p>
<p>the locutionary act is the act of saying something, i.e. the act of expressing the basic, literal meanings of the words chosen</p>
<p>the illocutionary act is the act performed in saying something, i.e. the act of using words to achieve such goals as warning, promising, guaranteeing, etc.</p>
<p>the perlocutionary act is the act performed by saying something, i.e. the act of producing an effect in the hearer by means of the utterance.</p>
<p>Considerations of truth and falsity are simply irrelevant for many types of illocutionary act. Austin distinguished constative utterances like snow is white, which have the illocutionary force of simply stating something, from performative utterances like I apologize, which themselves bring about the state of affairs they mention. Fregean truth conditions are relevant to constatives but not to performatives. Instead of truth conditions, performative utterances have felicity conditions. Typical felicity conditions for many types of constative and performative utterance were described by Searle.</p>
<h3 id="grice-on-implicature">Grice on implicature</h3>
<p>Grice recast the study of the relations between language and context by highlighting the central role of intention to meaning, and developed a theory of implicature and conversational maxims which described the relation between sentence and utterance (speaker) meaning. Grice’s main contribution is the four conversational maxims of Quality, Quantity, Manner and Relevance. Many implied meanings result from speakers’ deliberate infringement of these maxims.</p>
<h3 id="relevance-theory">Relevance theory</h3>
<p>Relevance Theory, finally, represents a third tradition which challenges some of the central presuppositions of the study of meaning. According to Relevance Theorists, the production and understanding of utterances is explained as the result of a universal comprehension procedure which consists in selecting the most relevant aspects of a word’s meaning in a given situation. There is no distinction between literal and non-literal meaning, and what meanings are activated by a word is highly dependent on the particular context in which it is uttered.</p>
<h1 id="chapter-5">Chapter 5</h1>
<p>As well as knowing a word’s definitional meaning, a competent speaker knows how it relates to other words of the language. Five important types of lexical relation have been identified.</p>
<h3 id="antonymy">Antonymy</h3>
<p>Antonymy (oppositeness) may be characterized as a relationship of incompatibility between two terms with respect to some given dimension of contrast. The principal distinction to be made in discussion of antonymy is between gradable (e.g. hot–cold) and non-gradable (e.g. married–unmarried) antonyms, i.e. antonyms which do and do not admit a midpoint.</p>
<h3 id="meronymy">Meronymy</h3>
<p>Meronymy is the relation of part to whole: hand is a meronym of arm, seed is a meronym of fruit, blade is a meronym of knife. Not all languages seem to have an unambiguous means of lexicalizing the concept PART OF, but meronymy is often at the origin of various polysemy patterns in languages.</p>
<h3 id="hyponymy-and-taxonomy">Hyponymy and taxonomy</h3>
<p>Hyponymy and taxonomy (kind of-ness) define different types of class-inclusion hierarchies; hyponymy is an important structural principle in many languages with classifiers, while taxonomy has been argued to be basic to the classification and naming of biological species.</p>
<h3 id="synonymy">Synonymy</h3>
<p>Synonymy is frequently claimed to exist between different expressions of the same language, but genuine lexical synonyms prove extremely hard to find: once their combinatorial environments have been fully explored, proposed lexical synonyms often prove not to be such.</p>
<h3 id="componential-analysis">Componential analysis</h3>
<p>The importance of appreciating a lexeme’s semantic relations in order to understand its meaning is one of the motivations for a componential approach to semantic analysis. Componential analysis analyses meaning in terms of binary features (i.e. features with only two possible values, + or –), and represents a translation into semantics of the principles of structuralist phonological analysis. As a type of definitional analysis, componential analysis inherits the failings of traditional</p>
<p>definitions, and words for which it proves hard to couch definitions are also hard to analyse componentially.</p>
<h3 id="polysemy-and-monosemy">Polysemy and monosemy</h3>
<p>Theoretical and ordinary description of meaning would both be impossible without the recognition of separate senses within the same word. Words with several related senses are described as polysemous. Polysemy contrasts simultaneously with monosemy, the case where a word has a single meaning, and homonymy, the case where two unrelated words happen to share the same phonological form. In spite of the intuitive obviousness of these distinctions, there are many instances where it is not clear whether a word should be analysed as polysemous or monosemous, and no absolute criteria have ever been proposed which will successfully discriminate them.</p>
<h1 id="chapter-6">Chapter 6</h1>
<h3 id="the-nature-and-importance-of-logic">The nature and importance of logic</h3>
<p>Logic investigates the properties of valid arguments and chains of reasoning, and specifies the conditions which arguments must meet in order to be valid. It is important to linguists for three principal reasons:</p>
<ul>
<li><p>it constitutes one of the oldest and most developed traditions of the study of meaning</p></li>
<li><p>it is at the heart of formal and computational theories of semantics</p></li>
<li><p>certain logical concepts, like ¬ or ), provide an interesting point of contrast with their natural language equivalents.</p></li>
</ul>
<h3 id="logical-form-validity-and-soundness">Logical form, validity and soundness</h3>
<p>Logic analyses the underlying logical structure of arguments, known as their logical form. This is independent of the way in which the argument happens to be phrased in any given language. We distinguished between valid and sound arguments:</p>
<ul>
<li><p>Valid arguments are ones in which, if the premises are true, the conclusion must also be true.</p></li>
<li><p>Sound arguments are valid arguments which have true premises.</p></li>
</ul>
<h3 id="propositional-logic">Propositional logic</h3>
<p>　Propositional logic is the branch of logic that studies relations between propositions. A proposition is something which serves as the premise or conclusion of an argument. In propositional logic, special importance is given to the four propositional connectives or operators not, and, or and if . . . then. These connectives are truth-functional. This means that whether the propositions to which they are added are true or not depends solely on the truth of the original propositions. The values or meanings of the operators can be specified in the form of truth tables, which display the way in which logical connectives affect the truth of the propositions in which they appear.</p>
<h3 id="logic-as-representation-and-as-perfection-of-meaning">Logic as representation and as perfection of meaning</h3>
<p>The truth-functional definitions of the propositional connectives are quite often counterintuitive and unnatural. None of the operators corresponds perfectly with any English equivalent. The clash between the meanings of the logical connectives and their ordinary language equivalents reveals a contrast between two different interpretations of the nature of logic: logic as a representation and as a perfection of meaning.</p>
<h3 id="predicate-logic">Predicate logic</h3>
<p>‘Some’ and ‘all’ are the basic notions of predicate logic. Predicate logic studies the logical form of propositions involving three kinds of expression:</p>
<ul>
<li><p>singular terms or individual constants, which refer to individuals (whether things or people). Singular terms are symbolized by lower case letters.</p></li>
<li><p>predicates, which represent properties or relations, such as ‘primate’, ‘hairy’ or ‘adore’. Predicates are symbolized by upper case letters.</p></li>
<li><p>quantifiers, like ‘some’ ( ) and ‘all’ ( ).</p></li>
</ul>
<p>　Predicates have a certain number of arguments. An argument is the individual or individuals to which the property or relation expressed by the predicate is attributed.</p>
<p>　　is called the existential quantifier. ( x) is read as ‘there is at least one x, such that’. is called the universal quantifier. ( x) is read ‘for every x, it is the case that’. Quantification may be single or multiple. A singly quantified proposition contains only a single quantifier. A multiply quantified proposition contains several. Propositions with both the universal and the existential quantifiers allow for the disambiguation of sentences like everyone loves someone.</p>
<h3 id="reference-truth-and-models">Reference, truth and models</h3>
<p>For logical approaches to semantics, reference and truth are the principal semantic facts: the most important thing about the meaning of a word is what it refers to, and the most important thing about a sentence is whether or not it is true. The model of a set of logical formulae is a set of statements showing what each expression of the formula refers to in some possible world (6.5). The referent of a logical expression is called its extension:</p>
<ul>
<li><p>The extension of an individual constant (singular term) is simply the individual entity which the constant picks out.</p></li>
<li><p>The extension of a one-place predicate is the entire set of individuals to which the predicate applies. The predicate ‘tall’, for instance, applies to all tall entities.</p></li>
<li><p>The extension of a two-place predicate like ‘respect’ will be the set of all pairs of individuals such that the first respects the second.</p></li>
</ul>
<p>In general, we can say that the extension of an n-place predicate is an ordered n-tuple of entities.</p>
<h3 id="relations-between-propositions">Relations between propositions</h3>
<p>Entailment is the relation between propositions where the truth of the first guarantees the truth of the second. Presupposition is the relation between two propositions p and q, such that both p and ¬p entail q. A contradictory is a pair of propositions which always have opposite truth values. Pairs of propositions which cannot both be true but can both be false are called contraries. Pairs of propositions which cannot be simultaneously false, but can be simultaneously true are called subcontraries.</p>
<h3 id="meaning-postulates">Meaning postulates</h3>
<p>The theory of meaning postulates uses logical notions to describe the relations which a word has with other members of the same vocabulary, and constitutes a possible alternative to the decompositional modes of meaning analysis.</p>
<h3 id="russell-on-definite-descriptions">Russell on definite descriptions</h3>
<p>Russell’s theory of definite descriptions offers an analysis in logical terms of the meaning of propositions involving the English determiner the, according to which such propositions contain disguised quantifications.</p>
<h3 id="is-logic-relevant-to-the-semantics-of-natural-language">Is logic relevant to the semantics of natural language?</h3>
<p>In the course of the chapter we saw a number of reasons to doubt that logical tools provide an appropriate model of the meanings involved in natural language. These include: - the existence of incompatibilities between logical operators and their natural language equivalents</p>
<ul>
<li>the orientation of logic to reference and truth, which are only some of the considerations relevant to natural language.</li>
</ul>
<p>However, many linguists in favour of logical approaches to semantics would claim that:</p>
<ul>
<li><p>in its attention to declarative sentences a logical approach promises a formalization of an important subset of natural language sentences, and that</p></li>
<li><p>a logical approach permits a degree of rigour and formalization which entirely outstrips that of the more descriptive approaches to meaning.</p></li>
</ul>
<h1 id="chapter-7">Chapter 7</h1>
<h3 id="two-views-of-categorization">Two views of categorization</h3>
<p>Categorization is a fundamental psychological process: the human mind can class different things in the world in the same category, and denote them with a single term. Since words can be seen as the names of categories, categorization has been a major focus of investigations of word meaning. Linguists and psychologists typically contrast two views of categorization:</p>
<ul>
<li><p>the classical view, on which membership of a given category is an either-or property, with no in-between cases. For example, on the classical view, something either is or is not a flower, or a lie, or red.</p></li>
<li><p>the prototype view, on which a category is structured in terms of a central tendency. On this view, categories like FLOWER, LIE or RED each have more and less central (prototypical) members.</p></li>
</ul>
<h3 id="problems-with-classical-categorization">Problems with classical categorization</h3>
<p>The classical view of categorization is rejected by many semanticists since it seems unable to account for basic semantic phenomena, such as the following:</p>
<ul>
<li><p>There are categories in which some members are better exemplars of the category than others.</p></li>
<li><p>There are categories in which the boundaries of membership are not clearcut: it is not always possible to say whether or not something is a member of the category.</p></li>
</ul>
<h3 id="problems-with-prototype-categorization">Problems with prototype categorization</h3>
<p>The prototype view is certainly able to account for these facts, but is open to several questions and problems:</p>
<ul>
<li><p>how do we identify the relevant attributes in a category?</p></li>
<li><p>how can we account for the boundaries of prototype categories?</p></li>
<li><p>how much of the vocabulary is structured according to prototype categories? - prototype semantics may simply absolve the semanticist from the serious effort of lexical description</p></li>
<li><p>the evidence for prototypes may be metalinguistic in nature</p></li>
</ul>
<h3 id="cognitivist-approaches-to-semantics">Cognitivist approaches to semantics</h3>
<p>Cognitivist approaches to semantics are directly inspired by prototype theory. These approaches have four important commitments:</p>
<ul>
<li><p>an identification between meaning and conceptual structure</p></li>
<li><p>a rejection of the syntax–semantics distinction</p></li>
<li><p>a rejection of the semantics–pragmatics distinction</p></li>
<li><p>a rejection of a modular approach to language</p></li>
</ul>
<h3 id="icms-and-image-schemas">ICMs and image schemas</h3>
<p>A central notion in cognitive semantics is that linguistic meaning depends on encyclopaedic knowledge structures stored in long-term memory. Lakoff (1987) calls these idealized cognitive models (ICMs), and sees prototype effects as explained by them. ICMs can be thought of as theories of particular subjects – the implicit knowledge we have about the objects, relations and processes named in language. The knowledge structures typically involve image schemas, such as CONTAINMENT, SOURCE-PATH-GOAL, FORCE, BALANCE and so on. These are organizing structures of our experience and understanding at the level of bodily perception and movement. They are usually represented diagrammatically. Image schemas are particularly useful as representations of the meanings of prepositions.</p>
<h3 id="metaphor">Metaphor</h3>
<p>Metaphor is stressed in much cognitive semantics as an inherent aspect of language structure. Cognitive semantics shows that metaphor is not the exception in language: metaphorical ways of talking are just as widespread as ‘literal’ ones. The normal way of referring to many domains of meaning, such as that of obligation in English, is metaphorical. Lakoff and Johnson’s conceptual theory of metaphor proposes that metaphor is a cognitive process which helps us to conceptualize our experience by setting up correspondences between easily understood things like burdens and hard to understand things like obligations. A metaphorical mapping allows knowledge about the metaphor’s source or vehicle domain (burdens) to be applied to the target (obligations) in a way that fundamentally determines or influences the conceptualization of the target.</p>
<h3 id="metonymy">Metonymy</h3>
<p>Another important cognitive process is metonymy: the concepts related by a metonymy can be understood as contiguous to (neighbouring) each other, either conceptually or in the real world.</p>
<h3 id="semantic-extension-and-radial-categories">Semantic extension and radial categories</h3>
<p>Metaphor and metonymy constitute the principal mechanisms of semantic extension, as seen in expressions like head of a queue, head of cattle and so on. This type of structure, where there is a central case and conventionalized variations on it which cannot be predicted by general rules, is called a radial structure. For head, the central case is represented by uses consistent with the ICM described above, and the variations are the metaphoric and metonymic extensions.</p>
<h3 id="problems-with-cognitive-semantics">Problems with cognitive semantics</h3>
<p>Cognitive approaches to semantics have proven very popular, but can be criticized for three main reasons:</p>
<ul>
<li><p>the ambiguity of diagrammatic representations</p></li>
<li><p>the problem of determining a lexical item’s core meaning, and</p></li>
<li><p>the indeterminate and speculative nature of the analyses.</p></li>
</ul>
<h1 id="chapter-8">Chapter 8</h1>
<h3 id="jackendoff-conceptual-semantics">Jackendoff: Conceptual Semantics</h3>
<p>Jackendoff’s Conceptual Semantics shares with cognitive semantics a commitment to analysing meaning as inherently linked to conceptualization. Its most important difference from cognitive semantics is that it uses a formalism.</p>
<h3 id="decomposition-and-conceptual-primitives">Decomposition and conceptual primitives</h3>
<p>Jackendoff claims that a decompositional method is necessary to explore conceptual structure, in which the concepts underlying word meaning are broken down into their smallest elements: conceptual primitives envisaged as the semantic equivalents of phonological features. Conceptual Semantics posits ‘a finite set of mental primitives and a finite set of principles of mental combination’ governing their interaction. The conceptual structure of a lexical item is an element with zero or more open argument slots, which are filled by the syntactic complements of the lexical item. Jackendoff’s system permits interesting connections to be made between apparently unrelated meanings, but can be criticized for the apparently somewhat arbitrary nature of the conceptual constituents it recognizes.</p>
<h3 id="modelling-meaning-computationally">Modelling meaning computationally</h3>
<p>Computers come closer than any other artificial system to matching the complexity and interconnectedness of the human brain, and it has often been assumed that the attempt to simulate human linguistic ability computationally will teach us important lessons about the way language is processed in real-life minds/brains. This is particularly true of studies of words and their meanings, though it is important not to push the mind–computer analogy too far.</p>
<h3 id="wordnet-and-the-lexicon">WordNet and the lexicon</h3>
<p>The most comprehensive attempt to model lexical knowledge on a computer is WordNet, an online lexical database which sets out to represent and organize lexical semantic information in a psychologically realistic form that facilitates maximally efficient digital manipulation. The main organizational unit in WordNet is the synset. Synsets are groupings of near-synonyms, arranged into hyponymic/taxonomic trees called inheritance hierarchies. Each term in an inheritance hierarchy ‘inherits’ the information associated with its hypernyms: this gives the user immediate access to the full range of information associated with a lexical item.</p>
<h3 id="word-sense-disambiguation">Word sense disambiguation</h3>
<p>One of the hardest problems in computer simulations of natural language processing is the problem of word-sense disambiguation. Computers must know how to distinguish between the different senses of ambiguous words like bank if they are to be able to process language correctly. We discussed two main approaches to this task:</p>
<ul>
<li><p>selectional restriction approaches, which use selectional restrictions to weed out improperly formed semantic representations; and</p></li>
<li><p>the contextual approach, in which the computer assesses the words surrounding the target word, and chooses the appropriate sense on the basis of the other words in this immediate context.</p></li>
</ul>
<p>Both approaches are in their infancy and programs using them significantly underperform humans.</p>
<h3 id="pustejovsky-and-qualia-structure">Pustejovsky and qualia structure</h3>
<p>　Pustejovsky attempts to solve a number of problems in word-sense disambiguation by proposing a richer structure for nominal entries in the lexicon. He claims that the meaning of nouns is best modelled by the notion of qualia structure. A noun’s qualia structure consists of four roles, the constitutive, formal, telic and agentive roles. The key element of Pustejovsky’s theory is that each of these roles can operate independently within the semantics of a clause. For example, we know that a fast car is one that moves quickly, and a fast motorway is one on which cars can move quickly, since fast applies to the telic role of the noun: the role that refers to the function or purpose which the referent fulfils.</p>
<h1 id="chapter-9">Chapter 9</h1>
<h3 id="what-parts-of-speech-a-language-has-is-a-matter-of-interpretation">What parts of speech a language has is a matter of interpretation</h3>
<p>There are usually several different plausible interpretations of the part of speech categories (lexical categories, grammatical categories) of any language.</p>
<h3 id="morphological-and-distributional-criteria-for-parts-of-speech">Morphological and distributional criteria for parts of speech</h3>
<p>Some languages have clear morphological criteria for assigning words to parts of speech: we can divide up the classes on the basis of what affixes they appear with. In languages without clear morphology, a distributional approach to parts of speech can sometimes be used to classify parts of speech on the basis of the way they pattern in sentences. Both morphological and distributional methods for part of speech classification are unreliable. In any case, both presuppose a preexisting decision about how the parts of speech are to be defined.</p>
<h3 id="semantic-definitions-of-parts-of-speech">Semantic definitions of parts of speech</h3>
<p>The same problem affects semantic definitions of parts of speech: we cannot appeal to thingness, eventhood and so on as criteria for grammatical category, since they are not known independently of the very grammatical features which they are supposed to establish.</p>
<h3 id="multicategoriality">Multicategoriality</h3>
<p>Many languages show widespread multicategoriality (roots which may appear as different parts of speech). We can think of nouns and verbs as ‘slots’ or contexts available in each clause, each of which comes associated with the appropriate grammatical machinery. The grammatical slots themselves can be seen as the carriers of the nounhood or verbhood which the word ends up acquiring.</p>
<h3 id="hopper-and-thompson-parts-of-speech-and-discourse-function">Hopper and Thompson: parts of speech and discourse function</h3>
<p>Hopper and Thompson suggest that parts of speech can be understood as prototype categories defined by their discourse functions. The difference in the grammatical options available to a given occurrence of a noun or verb correlates with its discourse function in a given context – the closer the noun or verb is to playing its prototypical discourse role, the closer it comes to exhibiting the full range of grammatical possibilities of its class. It is open to question whether the discourse function definition of parts of speech is any less problematic than the semantic definitions it replaces.</p>
<h3 id="tense">Tense</h3>
<p>Tense is the name of the class of grammatical markers used to signal the location of situations in time. Three basic temporal divisions are relevant to the representation of time in language: what is happening now, what will happen afterwards, and what has already happened. Some languages display a three-way division between past, present and future, with each tense marked separately on the verb. Others have a two-way distinction; either between past and non-past, or (more rarely) future and non-future. Perfect tenses are often described in terms of relevance to the speech situation, but this definition is problematic.</p>
<h3 id="aspect">Aspect</h3>
<p>Aspect is the grammatical category which expresses differences in the way time is presented in events. Aspectual categories express the internal temporal constituency of an event; whether the event is viewed from the distance, as a single unanalysable whole (perfective aspect), or from closeup, so that the distinct stages of the event can be seen individually (imperfective aspect). The perfective/imperfective distinction has nothing to do with the actual nature of the event, but is all about how the event is construed by the speaker. In particular, it is independent of the actual duration of the event in question.</p>
<h3 id="tense-is-deictic-aspect-isnt">Tense is deictic, aspect isn’t</h3>
<p>A major difference between tense and aspect is that tense is deictic and aspect isn’t.</p>
<p>　The particular time reference of any tense therefore has to be anchored deictically in the moment of utterance.</p>
<h3 id="aktionsart">Aktionsart</h3>
<p>Aktionsart is the term for an event’s inherent aspectual classification.</p>
<h3 id="many-researchers-claim-that-events-can-be-classified-into-five-basic">Many researchers claim that events can be classified into five basic</h3>
<p>Aktionsart classes:</p>
<ul>
<li><p>states</p></li>
<li><p>activities</p></li>
<li><p>accomplishments</p></li>
<li><p>achievements, and</p></li>
<li><p>semelfactives</p></li>
</ul>
<p>These classes show significant interaction effects with perfective and imperfective meanings. The five Aktionsart classes can be summarized along the dimensions of whether they are static (whether they refer to unchanging states or to occurrences), telicity (whether they have an inherent endpoint) and punctuality (whether they are conceived of as consisting of internal temporal parts):</p>
<p>State [+static], [−telic], [−punctual]</p>
<p>Activity [−static], [−telic], [−punctual]</p>
<p>Achievement [−static], [+telic], [+punctual]</p>
<p>Accomplishment [−static], [+telic], [−punctual]</p>
<p>Semelfactive [−static], [−telic], [+punctual]</p>
<h3 id="the-internal-structure-of-achievements">The internal structure of achievements</h3>
<p>Botne showed that achievement verbs have a more complex temporal-ity than was originally assumed. What is punctual about achievement verbs like find, die, notice, or recognize is their nucleus; in addition to this nucleus, a verb may contain a preceding onset phase or a subsequent coda phase. Languages differ in the temporal phases surrounding the central nucleus.</p>
<h3 id="tense-and-aspect-less-languages">Tense and aspect-less languages</h3>
<p>Some languages lack any grammatical means of expressing tense– aspect contrasts. In such languages, the relevant contrasts will be achieved through non-grammatical (lexical and pragmatic) means. The complete absence of grammatical coding of tense and aspect is not uncommon in the languages of the world.</p>
<h1 id="chapter-10">Chapter 10</h1>
<h3 id="the-linking-problem">The linking problem</h3>
<p>The linking problem is the problem of accounting for the relations between a verb and its associated noun phrases. According to the traditional generative understanding, the lexical entries for verbs include a specification of the types of argument they have associated with them. It was assumed that the possible arguments of all verbs could be classified into a small number of classes, called thematic roles, theta-roles, participant roles or semantic roles. Typical roles include agent, patient/ theme, goal, source, location, instrument, beneficiary and experiencer. Some roles are more likely to be coded as subject, and others as object. It was suggested that it is possible to rank the different roles in an order which shows their relative accessibility to subject position.</p>
<h3 id="problems-with-thematic-roles">Problems with thematic roles</h3>
<p>There are three main problems with thematic roles:</p>
<ul>
<li><p>The arguments of many verbs seem hard to assign to any of the conventional thematic roles.</p></li>
<li><p>There are also many occasions where an argument could be assigned to several thematic roles.</p></li>
<li><p>It has not proven possible to formulate a universal thematic hierarchy ranking these roles.</p></li>
</ul>
<h3 id="proto-roles">Proto-roles</h3>
<p>Dowty (1991) suggested that the different participant roles are cluster concepts, like Roschean prototypes, and that thematic roles are based on entailments of verb-meanings. The argument with the most Proto-Agent entailments will be coded as subject, and the one with the most Proto-Patient entailments as object.</p>
<h3 id="thematic-roles-and-conceptual-structure">Thematic roles and conceptual structure</h3>
<p>Jackendoff’s theory of semantic representation dispenses completely with theta-roles, and derives argument structure directly from the semantics of the verb. This means that the thematic hierarchy can be completely restated in terms of underlying conceptual configurations. In Jackendoff’s theory of conceptual structure, selectional restrictions are also specified directly by the conceptual structure: they are not extra information which needs to be learnt in addition to the meaning of the verbs themselves.</p>
<h3 id="verb-classes-and-alternations">Verb classes and alternations</h3>
<p>Many verbs show several different argument structures. These different types of argument structure are known as alternations. They include the causative, middle, resultative, conative and others. Levin and Hovav proposed that which alternations a verb participates in is explained by its underlying semantic structure. On their theory, verbs fall into semantically defined classes, which all show similar syntactic behaviour with respect to their alternations.</p>
<h3 id="the-meaning-of-constructions">The meaning of constructions</h3>
<p>Words are not the only meaning-bearing units in grammar. Semantic representations are also associated with constructions. Goldberg put forward a constructional account of the syntax–semantics interface, in which arguments can be subcategorized by the construction itself. Not all verbs can appear in all constructions. A verb’s meaning determines whether it is compatible with a given construction. The constructional account reduces the proliferation of verb-senses.</p>
<h1 id="chapter-11">Chapter 11</h1>
<h3 id="diachronic-and-cross-linguistic-meaning-comparison-presupposes-a-correct-metalanguage">Diachronic and cross-linguistic meaning comparison presupposes a correct metalanguage</h3>
<p>Claims about the variation or change of given meanings necessitate a particular metalanguage in which the meanings can be described, a situation which immediately introduces complications since there is not yet any agreement about what the correct metalanguage for semantic description is. Linguistics in general, and semantic theory in particular, assume that languages are mutually translatable in a way that preserves important meaning components.</p>
<h3 id="importance-of-polysemy-in-meaning-change">Importance of polysemy in meaning change</h3>
<p>　Meaning change crucially involves polysemy. A word does not suddenly change from meaning A to meaning B in a single move; instead, the change happens via an intermediate stage in which the word has both A and B among its meanings.</p>
<h3 id="the-traditional-classification-of-semantic-change">The traditional classification of semantic change</h3>
<p>The traditional classification of semantic change recognized the following six types:</p>
<ul>
<li><p>Specialization (narrowing), in which a word narrows its range of reference</p></li>
<li><p>Generalization (broadening), in which a word’s meaning changes to encompass a wider class of referents</p></li>
<li><p>Pejorization, in which a word takes on a meaning with a less favourable evaluative force</p></li>
<li><p>Ameliorization, in which a word takes on a meaning with a more favourable evaluative force</p></li>
<li><p>Metonymy, the process of sense-extension in which a word shifts to a contiguous meaning</p></li>
<li><p>Metaphor, changes based on similarity or analogy</p></li>
</ul>
<h3 id="conventionalization-of-implicature">Conventionalization of implicature</h3>
<p>Much modern work on semantic change examines pathways and regularities of semantic change, stressing the role of the conventionalization of implicature. This is the theory that semantic change occurs through the progressive strengthening of the implicatures of expressions in particular contexts, until the implicated meaning becomes part of the expression’s literal meaning. These explanations can supersede ones based on the traditional categories.</p>
<h3 id="subjectification">Subjectification</h3>
<p>　An important tendency in semantic change is subjectification. This is the tendency for meanings to ‘become increasingly based in the speaker’s subjective belief state/attitude toward the proposition’.</p>
<p>　 Perception verbs and the mind-as-body metaphor</p>
<p>Viberg (1984) found a strong cross-linguistic hierarchy governed the polysemies of perception verbs. The visual modality of perception is always the source but never the target of processes of polysemy. The mind-as-body metaphor is a possible explanation of the extension of ‘see’ verbs to ‘know/understand’ in Indo-European languages, but is not universal: many languages draw their ‘know’ verb from verbs for hearing.</p>
<h3 id="grammaticalization">Grammaticalization</h3>
<p>One particular context for semantic change is grammaticalization, the process by which open-class content words turn into closed-class function forms. They do this by losing elements of their meaning, and by a restriction in their possible grammatical contexts. Study of these processes has revealed a number of regular pathways which recur again and again in the world’s languages linking particular open-class lexemes with particular grammaticalized functions.</p>
<h3 id="corpus-studies-of-meaning-variation">Corpus studies of meaning variation</h3>
<p>The seeds of semantic change are found in synchronic meaning variation in everyday discourse. Corpora are useful for semantic analysis because they can reveal unsuspected patterns of collocation (regular word combination). Many words cluster in predictable collocational patterns. Studies of collocation can give surprising results: for example, corpus investigation reveals that cause is not used neutrally, but has a strong tendency to be associated with negative events.</p>
<h3 id="semantic-typology">Semantic typology</h3>
<p>Because of the problems of determining universals of sense, semantic typology concentrates on the question of cross-linguistic regularities in denotation or extension (11.4).</p>
<h3 id="typology-of-body-part-reference">Typology of body-part reference</h3>
<p>The human body is a basic and universal aspect of our experience, but there are remarkably few cross-linguistic generalizations that hold about the semantics of body-part terms.</p>
<h3 id="typology-of-colour-reference">Typology of colour-reference</h3>
<p>Colour terms have been an important site of cross-linguistic investigation. Berlin and Kay hypothesized that each language has a set of basic colour terms (BCTs). Basic colour terms in all languages target a restricted range of colours, but the boundaries between these targets vary widely. The number of BCTs in a language makes it possible to predict exactly what the basic colour terms are, and Berlin and Kay proposed seven types of language, classified according to the number of BCTs. Berlin and Kay’s findings have been broadly confirmed, but</p>
<p>there are some significant counterexamples to their typology, as well as fundamental criticisms of their methodology which cast doubt on the significance of their approach.</p>
<h3 id="deictic-motion-typology">Deictic motion typology</h3>
<p>Meanings in the domain of deictic motion, the actions expressed in</p>
<p>English as ‘coming’ and ‘going’, also vary widely cross-linguistically.</p>
<h3 id="typology-of-motion-path-and-manner-lexicalization">Typology of motion, path and manner lexicalization</h3>
<p>　Talmy (1985) compared how different languages lexicalize the four elements of motion, path, figure and manner in motion verbs. Talmy proposed a major typological division between verb-framed and satellite-framed languages. In verb-framed languages the path component is lexicalized in the verb root itself. In satellite-framed languages it is lexicalized in a satellite element. As well as verb- and satellite-framed languages, some linguists claim that there is a third type, equipollent languages, in which both path and manner are treated in the same way. Talmy’s typology has been widely discussed, and the distinction between verb- and satellite-framing is often invoked as a way of characterizing how different languages distribute motion information in the clause. It has been challenged, however, on the grounds that it artificially targets an abstract motion component in verbs whose meaning is actually much less abstract.</p>
<h3 id="the-typology-of-spatial-reference">The typology of spatial reference</h3>
<p>A frame of reference is ‘the internally consistent system of projecting regions of space onto a figure-ground relationship in order to establish specification of location’ (Pederson et al. 1998: 571). Languages with a relative frame of reference use spatial expressions with meanings like ‘in front of me/behind me’ and ‘to my left/right’. Other languages contain an absolute frame of reference. This is a system of spatial location which does not depend on the position of a speech participant, but is anchored instead in unchanging features of the geography, like uphill/ downhill distinctions, or in the cardinal directions (north, south, east, west). The least common frame of reference in the languages of the world is the intrinsic frame of reference. This system only makes reference to intrinsic features of figure and ground: ‘the man is at the side of the tree, the tree is at the chest/face/back of the man’ and so on.</p>
<h3 id="the-relation-of-language-and-thought">The relation of language and thought</h3>
<p>Differences in semantic typology raise the question of the influence between language and thought. Whorf believed that the grammatical categories of one’s language determine the categories of broader cognition. This idea is known as linguistic determinism or the linguistic relativity hypothesis. Thinking in general must be distinguished from thinking for speaking. The grammatical categories of a language must determine thinking for speaking. The interesting question is whether they also determine general cognition. Research has found a statistically very highly reliable correlation between the prevailing frame of spatial reference used in a language and the types of response in non-linguistic cognitive tasks. These suggest a limited influence of language on general cognition. There are many other domains, however, where such an effect is not observed.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>语义</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>semantics</tag>
      </tags>
  </entry>
  <entry>
    <title>句法学框架及术语大全</title>
    <url>/syntax-terms/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2021/04/29/5UCyu3mlzBNXD8Y.png"/></p>
<p>See <a href="https://share.mubu.com/doc/2aerL3sh4SV">mubu</a> for more details.</p>
<h1 id="chapter-1">Chapter 1</h1>
<ul>
<li><p><strong>Syntax:</strong> The level of linguistic organization that mediates between sounds and meaning, where words are organized into phrases and sentences.</p></li>
<li><p><strong>Language (capital L):</strong> The psychological ability of humans to produce and understand a particular language. Also called the Human Language Capacity or i-Language. This is the object of study in this book.</p></li>
<li><p><strong>language (lowercase l)</strong>: A language like English or French. These are the particular instances of the human Language. The data sources we use to examine Language are languages. Also called e-language.</p></li>
<li><p><strong>Generative Grammar:</strong> A theory of linguistics in which grammar is viewed as a cognitive faculty. Language is generated by a set of rules or procedures. The version of generative grammar we are looking at here is primarily the Principles and Parameters approach (P&amp;P) touching occasionally on Minimalism.</p></li>
<li><p><strong>The Scientific Method:</strong> Observe some data, make generalizations about that data, draw a hypothesis, test the hypothesis against more data.</p></li>
<li><p><strong>Falsifiable Prediction:</strong> To prove that a hypothesis is correct you have to look for the data that would prove it wrong. The prediction that might prove a hypothesis wrong is said to be falsifiable.</p></li>
<li><p><strong>Grammar:</strong> Not what you learned in school. This is the set of rules that generate a language.</p></li>
<li><p><strong>Prescriptive Grammar:</strong> The grammar rules as taught by so-called “language experts”. These rules, often inaccurate descriptively, prescribe how people should talk/write, rather than describe what they actually do.</p></li>
<li><p><strong>Descriptive Grammar:</strong> A scientific grammar that describes, rather than prescribes, how people talk/write.</p></li>
<li><p><strong>Anaphor:</strong> A word that ends in -self or -selves (a better definition will be given in chapter 5).</p></li>
<li><p><strong>Antecedent:</strong> The noun an anaphor refers to.</p></li>
<li><p>**Asterisk (*)**: The mark used to mark syntactically ill-formed (unacceptable or ungrammatical) sentences. The hash mark, pound, or number sign (#) is used to mark semantically strange, but syntactically well-formed, sentences.</p></li>
<li><p><strong>Gender (grammatical):</strong> Masculine vs. Feminine vs. Neuter. Does not have to be identical to the actual sex of the referent. For example, a dog might be female, but we can refer to it with the neuter pronoun it. Similarly, boats don’t have a sex, but are grammatically feminine.</p></li>
<li><p><strong>Number:</strong> The quantity of individuals or things described by a noun. English distinguishes singular (e.g., a cat) from plural (e.g., cats). Other languages have more or less complicated number systems.</p></li>
<li><p><strong>Person:</strong> The perspective of the participants in the conversation. The speaker or speakers (I, me, we, us) are called the first person. The addressee(s) (you) is called the second person. Anyone else (those not involved in the conversation) (he, him, she, her, it, they, them) is referred to as the third person.</p></li>
<li><p><strong>Case:</strong> The form a noun takes depending upon its position in the sentence. We discuss this more in chapter 11.</p></li>
<li><p><strong>Nominative:</strong> The form of a noun in subject position (I, you, he, she, it, we, they).</p></li>
<li><p><strong>Accusative:</strong> The form of a noun in object position (me, you, him, her, it, us, them).</p></li>
<li><p><strong>Corpus</strong> (pl. Corpora): A collection of real-world language data.</p></li>
<li><p><strong>Native Speaker Judgments (Intuitions):</strong> Information about the subconscious knowledge of a language. This information is tapped by means of the grammaticality judgment task.</p></li>
<li><p><strong>Semantic Judgment:</strong> A judgment about the meaning of a sentence, often relying on our knowledge of the context in which the sentence was uttered.</p></li>
<li><p><strong>Syntactic Judgment:</strong> A judgment about the form or structure of a sentence.</p></li>
<li><p><strong>Garden Path Sentence:</strong> A sentence with a strong ambiguity in structure that makes it hard to understand.</p></li>
<li><p><strong>Center Embedding:</strong> A sentence in which a relative clause consisting of a subject and a verb is placed between the main clause subject and verb. E.g., The house [Bill built] leans to the left.</p></li>
<li><p><strong>Parsing:</strong> The mental tools a listener uses to process and understand a sentence.</p></li>
<li><p><strong>Competence:</strong> What you know about your language.</p></li>
<li><p><strong>Performance:</strong> The real-world behaviors that are a consequence of what you know about your language.</p></li>
<li><p><strong>Learning:</strong> The gathering of conscious knowledge (like linguistics or chemistry).</p></li>
<li><p><strong>Acquisition:</strong> The gathering of subconscious information (like language).</p></li>
<li><p><strong>Innate:</strong> Hardwired or builtin, an instinct.</p></li>
<li><p><strong>Recursion:</strong> The ability to embed structures iteratively inside one another. Allows us to produce sentences we’ve never heard before.</p></li>
<li><p><strong>Universal Grammar (UG):</strong> The innate (or instinctual) part of each language’s grammar.</p></li>
<li><p><strong>The Logical Problem of Language Acquisition:</strong> The proof that an infinite system like human language cannot be learned on the basis of observed data – an argument for UG.</p></li>
<li><p><strong>Underdetermination of the Data:</strong> The idea that we know things about our language that we could not have possibly learned – an argument for UG.</p></li>
<li><p><strong>Universal:</strong> A property found in all the languages of the world.</p></li>
<li><p><strong>Bioprogram Hypothesis:</strong> The idea that creole languages share similar features because of an innate basic setting for language.</p></li>
<li><p><strong>Observationally Adequate Grammar:</strong> A grammar that accounts for observed real-world data (such as corpora).</p></li>
<li><p><strong>Descriptively Adequate Grammar:</strong> A grammar that accounts for observed real-world data and native speaker judgments.</p></li>
<li><p><strong>Explanatorily Adequate Grammar:</strong> A grammar that accounts for observed real-world data and native speaker judgments and offers an explanation for the facts of language acquisition.</p></li>
</ul>
<p>　</p>
<h1 id="chapter-2">Chapter 2</h1>
<ul>
<li><p>Parts of Speech (a.k.a. Word Class, Syntactic Categories): The labels we give to constituents (N, V, Adj, Adv, D, P, C, T, Neg, Conj). These determine the position of the word in the sentence.</p></li>
<li><p><strong>Distribution:</strong> Parts of speech are determined based on their distribution. We have both morphological distribution (what affixes are found on the word) and syntactic distribution (what other words are nearby).</p></li>
<li><p><strong>Complementary Distribution:</strong> When you have two categories and they never appear in the same environment (context), you have complementary distribution. Typically complementary distribution means that the two categories are subtypes of a larger class.</p></li>
<li><p><strong>Parts of speech that are Open Class can take new members or coinages</strong>: N, V, Adj, Adv.</p></li>
<li><p><strong>Parts of speech that are Closed Class don’t allow new coinages:</strong> D, P, Conj, C, T, Neg, and the pronoun and anaphor subcategories of N.</p></li>
<li><p>Lexical Categories express the content of the sentence. N (including pronouns), V, Adj, Adv.</p></li>
<li><p><strong>Functional Categories contain the grammatical information in a sentence:</strong> D, P, Conj, T, Neg, C.</p></li>
<li><p><strong>Subcategories:</strong> The major parts of speech can often be divided up into subtypes. These are called subcategories.</p></li>
<li><p>Feature Notations on major categories are a mechanism for indicating subcategories.</p></li>
<li><p>Plurality refers to the number of nouns. It is usually indicated in English with an -s suffix. Plural nouns in English do not require a determiner.</p></li>
<li><p>Count vs. Mass: Count nouns can appear with determiners and the quantifier many. Mass nouns appear with much and usually don’t have articles.</p></li>
<li><p>The Predicate defines the relation between the individuals being talked about and some fact about them, as well as relations among the arguments.</p></li>
<li><p><strong>Argument Structure:</strong> The number of arguments that a predicate takes.</p></li>
<li><p>The Arguments are the entities that are participating in the predicate relation.</p></li>
<li><p><strong>Intransitive:</strong> A predicate that takes only one argument.</p></li>
<li><p><strong>Transitive:</strong> A predicate that takes two arguments.</p></li>
<li><p><strong>Ditransitive:</strong> A predicate that takes three arguments.</p></li>
</ul>
<h1 id="chapter-3">Chapter 3</h1>
<ul>
<li><p><strong>Constituent:</strong> A group of words that function together as a unit.</p></li>
<li><p><strong>Hierarchical Structure:</strong> Constituents in a sentence are embedded inside of other constituents.</p></li>
<li><p><strong>Syntactic Trees and Bracketed Diagrams:</strong> These are means of representing constituency. They are generated by rules.</p></li>
<li><p>English Phrase Structure Rules</p></li>
</ul>
<ol type="a">
<li>CP</li>
</ol>
<ol start="3" type="A">
<li>TP</li>
</ol>
<ol start="2" type="a">
<li>TP</li>
</ol>
<p>{NP/CP} (T) VP</p>
<p>​</p>
<ol start="3" type="a">
<li><p>VP (AdvP+) V (NP) ({NP/CP}) (AdvP+) (PP+) (AdvP+)</p></li>
<li><p>NP (D) (AdjP+) N (PP+) (CP)</p></li>
</ol>
<ul>
<li>PP P (NP)</li>
</ul>
<ol start="6" type="a">
<li><p>AdjP (AdvP) Adj</p></li>
<li><p>AdvP (AdvP) Adv</p></li>
<li><p>XP XP conj XP</p></li>
</ol>
<ul>
<li><p>**X X conj X</p></li>
<li><p><strong>Head:</strong>** The word that gives its category to the phrase.</p></li>
<li><p><strong>Recursion:</strong> The possibility of loops in the phrase structure rules</p></li>
</ul>
<p>that allow infinitely long sentences, and explain the creativity of language.</p>
<ul>
<li><p><strong>The Principle of Modification:</strong> If an XP (that is, a phrase with some category X) modifies some head Y, then XP must be a sister to Y (i.e., a daughter of YP).</p></li>
<li><p><strong>Constituency Tests:</strong> Tests that show that a group of words functions as a unit. There are four major constituency tests given here: movement, coordination, standalone, and replacement.</p></li>
</ul>
<h1 id="chapter-4">Chapter 4</h1>
<ul>
<li><p><strong>Branch:</strong> A line connecting two parts of a tree.</p></li>
<li><p><strong>Node:</strong> The end of a branch.</p></li>
<li><p><strong>Label:</strong> The name given to a node (e.g., N, NP, TP, etc.).</p></li>
<li><p><strong>(Proper) Domination:</strong> Node A dominates node B if and only if A is higher up in the tree than B and if you can trace a branch from A to B going only downwards.</p></li>
<li><p><strong>Immediate Domination:</strong> Node A immediately dominates node B if there is no intervening node G that is dominated by A, but dominates B. (In other words, A is the first node that dominates B.)</p></li>
<li><p>A is the Mother of B if A immediately dominates B.</p></li>
<li><p>B is the Daughter of A if B is immediately dominated by A.</p></li>
<li><p><strong>Sisters:</strong> Two nodes that share the same mother.</p></li>
<li><p><strong>Root Node (revised):</strong> The node that dominates everything, but is dominated by nothing. (The node that is no node’s daughter.)</p></li>
<li><p><strong>Terminal Node (revised):</strong> A node that dominates nothing. (A node that is not a mother.)</p></li>
<li><p><strong>Non-terminal Node (revised):</strong> A node that dominates something. (A node that is a mother.)</p></li>
<li><p><strong>Exhaustive Domination:</strong> Node A exhaustively dominates a set of terminal nodes {B, C, ..., D} provided it dominates all the members of the set (so that there is no member of the set that is not dominated by and there is no terminal node G dominated by A that is not a member of the set.</p></li>
<li><p><strong>Constituent:</strong> A set of terminal nodes exhaustively dominated by a particular node.</p></li>
<li><p><strong>Constituent of:</strong> A is a constituent of B if and only if B dominates A.</p></li>
<li><p><strong>Immediate Constituent of:</strong> A is an immediate constituent of B if and only if B immediately dominates A.</p></li>
<li><p><strong>Sister Precedence:</strong> Node A sister-precedes node B if and only if both are immediately dominated by the same node, and A appears to the left of B.</p></li>
<li><p><strong>Precedence:</strong> Node A precedes node B if and only if neither A dominates B nor B dominates A and A or some node dominating A sister-precedes B or some node dominating B.</p></li>
<li><p><strong>No Crossing Branches Constraint:</strong> If node X precedes another node Y then X and all nodes dominated by X must precede Y and all nodes dominated by Y.</p></li>
<li><p><strong>Immediate Precedence:</strong> A immediately precedes B if there is no node G that follows A but precedes B.</p></li>
<li><p><strong>C-command (informal):</strong> A node c-commands its sisters and all the daughters (and granddaughters, and great-granddaughters, etc.) of its sisters.</p></li>
<li><p><strong>C-command (formal):</strong> Node A c-commands node B if every node dominating A also dominates B and neither A nor B dominates the other.</p></li>
<li><p><strong>Symmetric C-command:</strong> A symmetrically c-commands B if A c-commands B and B c-commands A.</p></li>
<li><p><strong>Asymmetric C-command:</strong> A asymmetrically c-commands B if A c-commands B but B does not c-command A.</p></li>
<li><p><strong>Government:</strong> Node A governs node B if A c-commands B, and there is no node G such that G is c-commanded by A and G asymmetrically c-commands B.</p></li>
<li><p><strong>Phrase-government:</strong> If A is a phrase, then the categories that count for G in the above definition must also be phrases.</p></li>
<li><p><strong>Head-government:</strong> If A is a head (word), then the categories that count for G in the above definition must also be heads.</p></li>
<li><p><strong>Subject (preliminary):</strong> NP or CP daughter of TP.</p></li>
<li><p><strong>Object of Preposition (preliminary):</strong> NP daughter of PP.</p></li>
<li><p><strong>Direct Object:</strong></p></li>
</ul>
<p>With verbs of type V[NP__NP], V[NP__ CP] and V[NP__ NP PP], the NP or CP daughter of VP.</p>
<p>With verbs of type V[NP __ NP {NP/CP}], an NP or CP daughter of VP that is preceded by an NP daughter of VP.</p>
<ul>
<li><strong>Indirect Object (preliminary):</strong></li>
</ul>
<p>With verbs of type V[NP__ NP PP], the PP daughter of VP immediately preceded by an NP daughter of VP.</p>
<p>With verbs of type V[NP __ NP {NP/CP}], the NP daughter of VP immediately preceded by V (i.e., the first NP daughter of VP).</p>
<ul>
<li><strong>Oblique:</strong> any NP/PP in the sentence that is not a subject, direct object of a preposition, direct object, or indirect object.</li>
</ul>
<h1 id="chapter-5">Chapter 5</h1>
<ul>
<li><p><strong>R-expression:</strong> An NP that gets its meaning by referring to an entity in the world.</p></li>
<li><p><strong>Anaphor:</strong> An NP that obligatorily gets its meaning from another NP in the sentence.</p></li>
<li><p><strong>Pronoun:</strong> An NP that may (but need not) get its meaning from another NP in the sentence.</p></li>
<li><p><strong>Antecedent:</strong> The element that binds a pronoun, anaphor or R-expression. When this element c-commands another coindexed NP, it is a binder of that NP.</p></li>
<li><p><strong>Index:</strong> A subscript mark that indicates what an NP refers to.</p></li>
<li><p><strong>Coindexed:</strong> Two NPs that have the same index (i, j, k, etc.) are said to be coindexed.</p></li>
<li><p><strong>Corefer:</strong> Two NPs that are coindexed are said to corefer (refer to the same entity in the world).</p></li>
<li><p><strong>Binding:</strong> A binds B if and only if A c-commands B and A and B are coindexed. A is the binder, B is the bindee.</p></li>
<li><p><strong>Locality Constraint:</strong> A constraint on the grammar, such that two syntactic entities must be “local” or near to one another.</p></li>
<li><p><strong>Binding Domain:</strong> The clause (for our purposes).</p></li>
<li><p><strong>Free:</strong> Not bound.</p></li>
<li><p>The Binding Principles</p></li>
<li><p><strong>Principle A:</strong> An anaphor must be bound in its binding domain.</p></li>
<li><p><strong>Principle B:</strong> A pronoun must be free in its binding domain.</p></li>
<li><p><strong>Principle C:</strong> An R-expression must be free.</p></li>
</ul>
<h1 id="chapter-6">Chapter 6</h1>
<ul>
<li><p><strong>Specifier:</strong> Sister to X', daughter of XP.</p></li>
<li><p><strong>Adjunct:</strong> Sister to X', daughter of X'.</p></li>
<li><p><strong>Complement:</strong> Sister to X, daughter of X'.</p></li>
<li><p><strong>Head:</strong> The word that gives its category to the phrase.</p></li>
<li><p><strong>Projection:</strong> The string of elements associated with a head that bear the same category as the head (N, N', N', N', NP, etc.).</p></li>
<li><p><strong>Maximal Projection:</strong> The topmost projection in a phrase (XP).</p></li>
<li><p><strong>Intermediate Projection:</strong> Any projection that is neither the head nor the phrase (i.e., all the X' levels).</p></li>
<li><p><strong>One-replacement:</strong> Replacement of an N' node with one.</p></li>
<li><p><strong>Do-so-replacement:</strong> Replacement of a V' with do so.</p></li>
</ul>
<ol start="24" type="a">
<li>Specifier Rule: XP (YP) X' or XP X' (YP)</li>
</ol>
<ol start="11" type="i">
<li><p>Adjunct Rule: X' X' (ZP) or X' (ZP) X'</p></li>
<li><p>Complement Rule: X' X (WP) or X' (WP) X</p></li>
</ol>
<ul>
<li><p><strong>Additional Rules:</strong></p></li>
<li><p>**CP (C) TP</p></li>
</ul>
<p>XP XP Conj XP</p>
<p>TP</p>
<p>X'</p>
<p>NP (T) VP</p>
<p>X' Conj X'</p>
<p>X X Conj X</p>
<ul>
<li><p><strong>Parameterization:</strong>** The idea that there is a fixed set of possibilities in terms of structure (such as the options in the X-bar framework), and people acquiring a language choose from among those possibilities.</p></li>
<li><p><strong>Principle of Modification (revised):</strong> If a YP modifies some head X, then YP must be a sister to X or a projection of X (i.e., X’ or XP).</p></li>
</ul>
<h1 id="chapter-7">Chapter 7</h1>
<ul>
<li><strong>Determiner Phrase (DP):</strong> D is not in the specifier of NP. D heads its</li>
</ul>
<p>own phrase: [DP [D' D NP]].</p>
<ul>
<li><strong>Complementizer Phrase (CP):</strong> C is the head of CP and is obligatory in all clauses, although sometimes phonologically null:</li>
</ul>
<p>[CP [C' C TP ]].</p>
<ul>
<li><p><strong>Tense Phrase (TP):</strong> T is the head of TP and is obligatory in all clauses. Sometimes it involves lowering of the affix to the V. The subject DP occupies the specifier position: [TP DPsubject [T' T VP ]].</p></li>
<li><p><strong>Free Genitive/of-Genitive:</strong> Possessed of the possessor.</p></li>
<li><p><strong>Construct Genitive/’s-Genitive:</strong> Possessor ’s possessed.</p></li>
<li><p><strong>Subject:</strong> A DP that has the property indicated by the predicate phrase. What the sentence is about. In most sentences, this surfaces in the specifier of TP.</p></li>
<li><p><strong>Predicate Phrase:</strong> A group of words that attributes a property to the subject. (In most sentences this is the VP, although not necessarily so.)</p></li>
<li><p><strong>Clause:</strong> A subject and a predicate phrase (always a CP in our system).</p></li>
<li><p><strong>Root, Matrix, or Main Clause:</strong> A clause (CP) that isn’t dominated by anything.</p></li>
<li><p><strong>Embedded Clause/Subordinate Clause:</strong> A clause inside of another.</p></li>
<li><p><strong>Specifier Clause:</strong> An embedded clause in a specifier position.</p></li>
<li><p><strong>Adjunct Clause:</strong> An embedded clause in an adjunct position.</p></li>
<li><p><strong>Complement Clause:</strong> An embedded clause in a complement position.</p></li>
<li><p><strong>Tenseless or Non-finite Clause:</strong> A clause that isn’t tensed (e.g., I want [Mary to leave]).</p></li>
<li><p><strong>Tensed or Finite Clause:</strong> A clause that is tensed.</p></li>
<li><p><strong>Yes/No Question:</strong> A question that can be answered with a yes, a no or a maybe.</p></li>
<li><p><strong>Subject-Aux Inversion:</strong> A means of indicating a yes/no question. Involves movement of T to Ø[+Q] complementizer for morpho-phonological reasons.</p></li>
<li><p><strong>Affix Lowering:</strong> An old analysis of how past and present tense</p></li>
</ul>
<p>suffixes get on the verb: The lowering of inflectional suffixes to attach to their verb. Now largely replaced by an analysis where T is null and selects for a VP complement that is correctly inflected.</p>
<h1 id="chapter-8">Chapter 8</h1>
<ul>
<li><p><strong>Selectional Restrictions:</strong> Semantic restrictions on arguments.</p></li>
<li><p><strong>Thematic Relations:</strong> Semantic relations between a predicate and an argument – used as a means of encoding selectional restrictions.</p></li>
<li><p><strong>Agent:</strong> The doer of an action (under some definitions must be capable of volition).</p></li>
<li><p><strong>Experiencer:</strong> The argument that perceives an event or state.</p></li>
<li><p><strong>Theme:</strong> The element that is perceived, experienced, or undergoing the action or change of state</p></li>
<li><p><strong>Goal:</strong> The end point of a movement.</p></li>
<li><p><strong>Recipient:</strong> A special kind of goal, found with verbs of possession</p></li>
<li><p><strong>Source:</strong> The starting point of a movement.</p></li>
<li><p><strong>Location:</strong> The place where an action or state occurs.</p></li>
<li><p><strong>Instrument:</strong> A tool with which an action is performed.</p></li>
<li><p><strong>Beneficiary:</strong> The entity for whose benefit the action is performed.</p></li>
<li><p><strong>Proposition:</strong> The thematic relation assigned to clauses.</p></li>
<li><p><strong>Theta Role:</strong> A bundle of thematic relations associated with a particular argument (DPs, PPs, or CPs).</p></li>
<li><p><strong>Theta Grid:</strong> The schematic representation of the argument structure of a predicate, where the theta roles are listed.</p></li>
<li><p><strong>External Theta Role:</strong> The theta role associated with subjects.</p></li>
<li><p><strong>Internal Theta Role:</strong> The theta role associated with other arguments.</p></li>
<li><p><strong>The Theta Criterion:</strong></p></li>
</ul>
<p>Each argument is assigned one and only one theta role.</p>
<p>Each theta role is assigned to one and only one argument.</p>
<ul>
<li><p><strong>Lexical Item:</strong> Another way of saying “word”. A lexical item is an entry in the mental dictionary.</p></li>
<li><p><strong>The Projection Principle:</strong> Lexical information (like theta roles) is syntactically represented at all levels.</p></li>
<li><p><strong>Expletive (or Pleonastic) Pronoun:</strong> A pronoun (usually it or there) without a theta role. Usually found in subject position.</p></li>
<li><p><strong>Extended Projection Principle (EPP):</strong> All clauses must have subjects. Lexical information is syntactically represented.</p></li>
<li><p><strong>Expletive Insertion:</strong> Insert an expletive pronoun into the specifier of TP.</p></li>
<li><p><strong>The Lexicon:</strong> The mental dictionary or list of words. Contains all irregular and memorized information about language, including the argument structure (theta grid) of predicates.</p></li>
<li><p><strong>The Computational Component:</strong> The combinatorial, rule-based part of the mind. Where the rules and filters are found.</p></li>
</ul>
<h1 id="chapter-9">Chapter 9</h1>
<ul>
<li><p>Theta Grids can contain material other than theta roles, such as features.</p></li>
<li><p><strong>[±FINITE]:</strong> A feature of complementizers that indicates if the clause is finite or not. That is [+FINITE].</p></li>
<li><p><strong>[±Q]:</strong> A feature of complementizers that indicates if the clause is a question or not. If and whether are [+Q].</p></li>
<li><p><strong>[±INFINITIVE]:</strong> Not to be confused with [±FINITE], this is a feature of T nodes. To is [+INFINITIVE].</p></li>
<li><p><strong>[±PLURAL]:</strong> A feature of N heads indicating number.</p></li>
<li><p><strong>[±PROPER]:</strong> The feature associated with proper names.</p></li>
<li><p><strong>[±PRONOUN]:</strong> The feature associated with pronouns.</p></li>
<li><p><strong>[±COUNT]:</strong> This feature distinguishes count nouns from mass nouns.</p></li>
<li><p>Tense refers to the time of an event relative to the time at which the sentence is either spoken or written.</p></li>
<li><p><strong>The Event Time:</strong> The time at which the event described by the predicate occurs.</p></li>
<li><p><strong>The Assertion Time:</strong> The time at which the sentence is said.</p></li>
<li><p><strong>Past Tense:</strong> The event time happened before the assertion time.</p></li>
<li><p><strong>Present Tense:</strong> The event time is the same as the assertion time.</p></li>
<li><p><strong>Future Tense:</strong> The event time happens after the assertion time.</p></li>
<li><p><strong>Preterite:</strong> the special form of verbs in the past tense.</p></li>
<li><p><strong>Futurates:</strong> the future tense usage of a present tense verb.</p></li>
<li><p><strong>Aspect:</strong> a temporal relation that makes reference to some point other than the speech time, then looking at when the event happens relative to that reference point.</p></li>
<li><p><strong>Perfect:</strong> the aspect when the time of the event occurs before some reference point. Haveperf + participle.</p></li>
<li><p><strong>Participle:</strong> A particular form of the verb used in perfects and passives. It is often formed by suffixing –en or –ed, although other irregular methods are found too. Same thing as past participle.</p></li>
<li><p><strong>Gerund:</strong> A particular form of the verb used in progressives. It is normally formed by suffixing –ing. Traditionally called the present participle.</p></li>
<li><p><strong>Progressive:</strong> An aspect where the event time and the reference time overlap and the event is ongoing. beprog + gerund.</p></li>
<li><p><strong>Voice:</strong> An inflection that indicates the number of arguments and position of arguments that a verb uses.</p></li>
<li><p><strong>Active:</strong> A type of voice where the agent or experiencer of the sentence is in subject position and the theme is in the object position. Actives in English are unmarked morphologically.</p></li>
<li><p><strong>Passive:</strong> A type of voice where the theme of the sentence is in subject position. Passives are always marked in English by the combination of a be auxiliary and a participle.</p></li>
<li><p><strong>Mood:</strong> An inflectional category that refers to the speaker’s perspective on the event, indicating possibility, probability, necessity, or obligation.</p></li>
<li><p><strong>Possessive Have:</strong> A main verb use of have, which indicates possession.</p></li>
<li><p><strong>Copular Be:</strong> A main verb use of be, where the subject is attributed a certain property or is identified with a particular role.</p></li>
<li><p><strong>Main Verb Do:</strong> The use of the verb do to indicate accomplishments.</p></li>
<li><p><strong>Modals:</strong> Verbs that can only appear before negation and never take tense inflection. Auxiliaries, by contrast, can follow negation and can bear tense inflection.</p></li>
<li><p>[FORM] Features indicate the form of complements. Possible values include bare, participle, gerund, preterite, and present.</p></li>
<li><p><strong>Do-support:</strong> The use of the auxiliary do to bear tense features in the context of negation. This do is of category T.</p></li>
<li><p><strong>Affix Hopping:</strong> An alternative analysis of multiple auxiliary constructions, where affixes associated with particular tenses, aspects, and voice are generated as part of the same word as the relevant auxiliary, but then “hop” one verbal element to the right.</p></li>
</ul>
<h1 id="chapter-10">Chapter 10</h1>
<ul>
<li><p><strong>Transformation:</strong> A rule that takes an X-bar-generated structure and changes it in restricted ways.</p></li>
<li><p><strong>D-structure:</strong> The level of the derivation created by the base. No transformations have yet applied.</p></li>
<li><p><strong>S-structure:</strong> The output of transformations. The form you perform judgments on.</p></li>
<li><p><strong>V T Movement:</strong> Move the head V to the head T (motivated by morphology).</p></li>
<li><p><strong>Verb Movement Parameter:</strong> All verbs raise (French) or only auxiliaries raise (English).</p></li>
<li><p><strong>The VP-internal Subject Hypothesis:</strong> Subjects are generated in the specifier of the voice-headed VP.</p></li>
<li><p><strong>T C Movement:</strong> Move T to C when there is a phonologically empty Ø[+Q] complementizer.</p></li>
<li><p><strong>Do-support:</strong> When there is no other option for supporting inflectional affixes, insert the dummy verb do into T.</p></li>
</ul>
<h1 id="chapter-11">Chapter 11</h1>
<ul>
<li><p><strong>The Locality Constraint on Theta Role Assignment:</strong> Theta roles are assigned within the clause containing the predicate that introduces them (i.e., the VP or other predicate).</p></li>
<li><p><strong>DP Movement:</strong> Move a DP to a specifier position.</p></li>
<li><p><strong>Raising:</strong> A specific instance of DP movement. The DP moves from the specifier of an embedded non-finite T to the specifier of a finite T in the main clause where it can get Case.</p></li>
<li><p><strong>case (lowercase c):</strong> The special form DPs get depending upon their place in the sentence.</p></li>
<li><p><strong>Case (capital C):</strong> Licensing for DPs: NOM is found on the specifier of finite T. ACC is found on the complement of transitive V.</p></li>
<li><p><strong>The Case Filter:</strong> All DPs must be marked with Case.</p></li>
<li><p><strong>Passives:</strong> A particular verb form where the external argument (often the agent or experiencer) is suppressed and the theme appears in subject position. The movement of the theme is also an instance of DP movement.</p></li>
<li><p><strong>Burzio’s Generalization:</strong> If a verb does not have an external argument (i.e., is passive or unaccusative), then it can’t assign accusative Case.</p></li>
<li><p><strong>Unaccusatives:</strong> Inherently passive verbs like arrive.</p></li>
</ul>
<h1 id="chapter-12">Chapter 12</h1>
<ul>
<li><p><strong>Wh-movement:</strong> Move a wh-phrase to the specifier of CP to check a [+WH] feature in C.</p></li>
<li><p><strong>Wanna-contraction:</strong> The contraction of want and to, which does not apply across a wh-trace.</p></li>
<li><p><strong>That-trace Effect:</strong> Movement of a wh-phrase from subject position in English is disallowed when that trace is preceded by the complementizer that.</p></li>
<li><p><strong>That-trace Filter:</strong> *[CP that t …]</p></li>
<li><p><strong>Relative Clause:</strong> A CP that modifies a noun. These always have a “missing” element in them that corresponds to some kind of wh-element.</p></li>
<li><p><strong>Factive Clause:</strong> A clause that is the complement to a factive verb (know, claim, recall, etc.), or to a factive noun (knowledge, claim, fact, recollection, etc.).</p></li>
<li><p><strong>Operator (Op):</strong> The wh-element in relative clauses without an overt wh-phrase.</p></li>
<li><p><strong>Restrictive Relative Clause:</strong> A relative clause that restricts the meaning of a noun as a modifier. Adjoined to N’.</p></li>
<li><p><strong>Nonrestrictive Relative Clause:</strong> A relative clause that adds additional parenthetical commentary about a noun. Adjoined to D’.</p></li>
<li><p><strong>Island:</strong> A phrase that contains (dominates) the wh-phrase, and that you may not move out of.</p></li>
<li><p><strong>The Complex DP Constraint:</strong> *whi [ … [DP … ti … ] …]</p></li>
<li><p><strong>Wh-island Constraint:</strong> *whi [ … [CP whk [ … ti … ] … ] …]</p></li>
<li><p><strong>The Subject Condition:</strong> *whi … [TP [CP … ti … ] T …]</p></li>
<li><p><strong>Coordinate Structure Constraint:</strong> <em>whi … [XP [XP … ti … ] conj [XP … ]] … or </em>whi … [XP [XP … ] conj [XP … ti … ]] … or *whi … [XP [XP … ] conj ti] …</p></li>
</ul>
<p>or *whi … [XP ti conj [XP … ]] …</p>
<ul>
<li><p><strong>Minimal Link Condition (MLC) (intuitive version):</strong> Move to the closest potential landing site.</p></li>
<li><p><strong>Whin-situ:</strong> When a wh-phrase does not move.</p></li>
<li><p><strong>Echo Questions and Intonational Questions:</strong> Question forms that are licensed by the phonology (intonation and stress) and not by the syntax, although they may involve a special C.</p></li>
</ul>
<h1 id="chapter-13">Chapter 13</h1>
<ul>
<li><p><strong>Move (very informal version):</strong> Move something somewhere.</p></li>
<li><p><strong>Full Interpretation:</strong> Features must be checked in a local configuration.</p></li>
<li><p><strong>Local Configuration:</strong></p></li>
</ul>
<p>[WH], [NOM] features: specifier–head configuration.</p>
<p>[ACC] features: head–complement configuration.</p>
<p>[PAST] etc., [Q] features: head–head configuration.</p>
<ul>
<li><p><strong>Logical Form (LF):</strong> The semantic/interpretive system.</p></li>
<li><p><strong>Phonetic Form (PF):</strong> The overt component of grammar.</p></li>
<li><p><strong>SPELLOUT:</strong> The point at which the derivation divides into form (PF) and meaning structures (LF).</p></li>
<li><p><strong>Overt Movement:</strong> Movement between D-structure and SPELLOUT.</p></li>
<li><p><strong>Covert Movement:</strong> Movement between SPELLOUT and LF.</p></li>
<li><p><strong>Universal Quantifier ( ):</strong> A word such as every, each, all, any. Identifies all the members of a set.</p></li>
<li><p><strong>Existential Quantifier ( ):</strong> A word like some, or a. Identifies at least one member of a set.</p></li>
<li><p><strong>Scope:</strong> A quantifier’s scope is the range of material it c-commands.</p></li>
<li><p>Wide vs. Narrow Scope: Wide scope is when one particular</p></li>
</ul>
<p>quantifier c-commands another quantifier. Narrow scope is the opposite.</p>
<ul>
<li><strong>Quantifier Raising (QR):</strong> A covert instance of Move that moves quantifiers.</li>
</ul>
<h1 id="chapter-14">Chapter 14</h1>
<ul>
<li><p><strong>Light Verbs (Little v):</strong> the higher part of a complex verb, usually meaning CAUSE (or LOCATE, in the case of ditransitive double object verbs).</p></li>
<li><p><strong>Object Shift:</strong> the phenomenon where accusatively marked objects shift leftwards.</p></li>
<li><p><strong>AgrO:</strong> the head that checks accusative Case in the split VP system.</p></li>
</ul>
<p>　</p>
<h1 id="chapter-15">Chapter 15</h1>
<ul>
<li><p><strong>PRO (Big PRO):</strong> A null (silent) DP found in Caseless positions.</p></li>
<li><p><strong>pro (Little pro or Baby pro):</strong> A null (silent) DP often found in languages with “rich” agreement. pro does get Case.</p></li>
<li><p><strong>Clausal Subject Construction:</strong> A sentence where a clause appears in the specifier of TP.</p></li>
<li><p><strong>Extraposition (Expletive Subject):</strong> A sentence where there is an expletive in the subject position and a clausal complement.</p></li>
<li><p><strong>Subject-to-subject Raising:</strong> A kind of DP movement where the subject of an embedded non-finite clause moves to the specifier of TP of the main clause to get nominative Case.</p></li>
<li><p><strong>Subject-to-object Raising (also called Exceptional Case Marking or ECM):</strong> A kind of DP movement where the subject of an embedded non-finite clause moves to the specifier of AgrO in the main clause to get accusative Case.</p></li>
<li><p><strong>Control Theory:</strong> The theory that governs how PRO gets its meaning.</p></li>
<li><p><strong>Pragmatics:</strong> The science that looks at how language and knowledge of the world interact.</p></li>
<li><p><strong>Subject Control (also called Equi):</strong> A sentence where there is a PRO in the embedded non-finite clause that is controlled by the subject argument of the main clause.</p></li>
<li><p><strong>Object Control:</strong> A sentence where there is a PRO in the embedded</p></li>
</ul>
<p>non-finite clause that is controlled by the object argument of the main clause.</p>
<ul>
<li><p>Obligatory vs. Optional Control: Obligatory control is when the PRO must be controlled. Optional control is when the DP can be controlled or not.</p></li>
<li><p><strong>PROarb:</strong> Uncontrolled PRO takes an “arbitrary” reference.</p></li>
<li><p><strong>Null Subject Parameter:</strong> The parameter switch that distinguishes languages like English, which require an overt subject, from languages like Italian that don’t, and allow pro.</p></li>
</ul>
<h1 id="chapter-16">Chapter 16</h1>
<ul>
<li><p><strong>Ellipsis:</strong> A construction that omits a constituent when it is identical to a string that has previously been uttered.</p></li>
<li><p><strong>VP Ellipsis:</strong> A process that omits a VP (or vP) under identity with a previously uttered identical VP, normally in a conjunction. (E.g., I will eat a squid sandwich and you will too.)</p></li>
<li><p><strong>Antecedent-Contained Deletion (ACD):</strong> A kind of ellipsis where the antecedent of the ellipsis contains the ellipsis site. (E.g., She read every book that I did.)</p></li>
<li><p><strong>Pseudogapping:</strong> A variety of ellipsis where the accusative object is not omitted, but the rest of the VP is. (E.g., Dan can’t prove Paul innocent but he can prove Della innocent.)</p></li>
<li><p><strong>Comparative Deletion:</strong> The deletion in a comparative construction; often more than just a VP is missing. We did not attempt an account of comparative deletion in this chapter. (E.g., I’ve read more books than you [have read books].)</p></li>
<li><p><strong>Comparative Subdeletion:</strong> A kind of comparative deletion that is effectively equivalent to one kind of pseudogapping. (E.g., I’ve eaten more popcorn than you have eaten fries.)</p></li>
<li><p><strong>Stripping:</strong> An ellipsis process where only one argument remains and the rest of the clause is elided. (E.g., Frank read the Times last night, or maybe the Post.)</p></li>
<li><p><strong>N-ellipsis:</strong> The deletion of some part of a DP, typically including the N head. (E.g., I read these three books not those two ____.)</p></li>
<li><p><strong>Sluicing:</strong> A kind of ellipsis, where a TP is elided after a wh-phrase (E.g., I saw someone come into the room, but I don’t remember who ____.)</p></li>
<li><p><strong>LF-copying hypothesis:</strong> The idea that VP ellipsis consists of a null pronominal VP that is replaced by a copy of its antecedent after SPELLOUT and before LF.</p></li>
<li><p><strong>PF-deletion hypothesis:</strong> The idea that VP ellipsis targets a fully structured VP, which is deleted under identity with an antecedent after SPELLOUT and before PF.</p></li>
<li><p><strong>Sloppy Identity:</strong> In an ellipsis structure an elided pronoun or anaphor takes its reference from a local subject (e.g., where John loves his father and Bill does too has an interpretation where Bill loves Bill’s father).</p></li>
<li><p><strong>Strict Identity:</strong> In an ellipsis structure an elided pronoun or anaphor takes its reference from the subject in the antecedent clause (e.g., where John loves his father and Bill does too has an interpretation where Bill loves John’s father).</p></li>
<li><p><strong>Preposition Stranding:</strong> The phenomenon in English and related languages where prepositions do not move with the wh-phrase. (E.g., Who did you take a picture of?)</p></li>
<li><p><strong>Sag’s Analysis of ACD:</strong> To avoid infinite regress, the quantified DP undergoes covert QR to the top of the clause, which places it outside of the VP that is its antecedent.</p></li>
<li><p><strong>Hornstein’s Analysis of ACD:</strong> To avoid infinite regress, the DP undergoes DP movement to the specifier of AgrOP, which places it outside of the (lowest) VP that is its antecedent.</p></li>
<li><p><strong>Lasnik’s Analysis of Pseudogapping (Ellipsis Analysis):</strong> To explain why accusative objects survive ellipsis in pseudogapping structures, the accusative object moves to the specifier of AgrOP, which places it outside the VP that is elided. Normal VP ellipsis is really vP ellipsis.</p></li>
<li><p><strong>Across-the-Board Movement (ATB):</strong> The movement, typically of a wh-phrase, that appears to originate in two different conjoined VPs or clauses (e.g., Who did [Evan despise ti] and [Calvin adore ti?])</p></li>
<li><p><strong>Agbayani and Zoerner’s Analysis of Pseudogapping (ATB Analysis):</strong> Pseudogapping is not VP ellipsis; instead, it is the overt ATB head movement of a V head in two different phrases into a single little v node. The second trace of this movement corresponds to the “ellipsis” site in pseudogapping.</p></li>
</ul>
<h1 id="chapter-17">Chapter 17</h1>
<ul>
<li><p><strong>The Copy Theory of Movement:</strong> Movement is a two-part operation. First, the moved element is copied and put into the surface position; second, the original is made silent.</p></li>
<li><p><strong>Chain:</strong> The moved copy and all its traces.</p></li>
<li><p><strong>Potential Antecedent:</strong> A DP in the specifier of TP or another DP. The potential antecedent cannot be the anaphor or pronoun itself, nor can it be a DP that contains the anaphor or pronoun.</p></li>
<li><p><strong>Binding Principle A (final):</strong> One copy of an anaphor in a chain must be bound within the smallest CP or DP containing it and a potential antecedent.</p></li>
<li><p><strong>Binding Principle B (Dialect 1):</strong> A pronoun must be free within the smallest CP or DP containing it.</p></li>
<li><p><strong>Binding Principle B (Dialect 2):</strong> A pronoun must be free within the smallest DP (with a filled specifier) or CP containing it.</p></li>
</ul>
<h1 id="chapter-18">Chapter 18</h1>
<ul>
<li><p><strong>Polysynthesis:</strong> The phenomenon where all the required arguments of a verb surface as morphemes on that verb.</p></li>
<li><p><strong>Syntax-free Hypothesis:</strong> A hypothesis where polysynthetic languages are said to lack a syntactic component. All the work of the grammar is done by the morphology instead.</p></li>
<li><p><strong>Polysynthesis Parameter:</strong> Every argument of a head must be related to a morpheme in the word that contains the head.</p></li>
<li><p><strong>Radical Pro-drop Hypothesis:</strong> A hypothesis about polysynthetic languages, where the morphemes on a verb are agreement morphemes related to null pro arguments.</p></li>
<li><p><strong>Incorporation:</strong> A phenomenon where the direct object appears as part of the inflected verb.</p></li>
<li><p><strong>The Empty Category Principle:</strong> Traces must be c-commanded by the moved element (their antecedents).</p></li>
<li><p><strong>Scrambling:</strong> A movement rule that positions DPs in specifier of FocusP or TopicP.</p></li>
<li><p><strong>Topics:</strong> DPs that represent given (old) information in the discourse.</p></li>
<li><p><strong>Information Focus:</strong> The new information in a discourse.</p></li>
<li><p><strong>Contrastive Focus:</strong> A phrase that is presented in contrast to a previously expressed idea.</p></li>
<li><p><strong>Non-configurationality:</strong> Non-configurational languages exhibit very free word order, discontinuous constituents, and missing DP arguments.</p></li>
<li><p><strong>The Dual-Structure Hypothesis:</strong> The idea that syntax is divided into two structures.</p></li>
<li><p><strong>Hale’s Phrase Structure Rule for Non-configurational Languages:</strong> TP XTZ+</p></li>
<li><p><strong>The Pronominal Argument Parameter:</strong> Only pronouns can function as the arguments of predicates.</p></li>
</ul>
]]></content>
      <categories>
        <category>语言学</category>
        <category>句法</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title>心理语言学：资源和知识整理</title>
    <url>/psycho-linguistics/</url>
    <content><![CDATA[<h1 id="心理语言学术语glossary">心理语言学术语（Glossary）</h1>
<p>Absolute terms Spatial terms that refer to the location of an object in space irrespective of the location of a person (for example, north/south). Accommodation A phonological process in which elements that are shifted or deleted are adapted to their errorinduced environments. Acoustic phonetics The branch of phonetics that specifies the acoustic characteristics associated with each speech sound. Acquired dyslexia A form of reading disability in a previously literate person who has sustained brain damage. Active processing A collection of activities that includes relating new information to information we have in permanent memory, asking questions of the material, and writing summaries or outlines of the material. Active voice A sentence in which the surface structure subject is also the deep structure or logical subject of the sentence, such as The woman scolded the child. Addition A speech error in which linguistic material is added, as in I didn’t explain this clarefully enough [carefully enough]. Affricate A consonant that begins with complete closure of the vocal tract followed by gradual release of air pressure, such as the ch in church. Agent The thematic or semantic role corresponding to an individual who performs a given action, such as the manager in The manager opened the store. Agrammatic speech Speech in which there is a lack of grammatical structure, such as the absence of grammatical morphemes and function words. Agrammatism See agrammatic speech. Agraphia An aphasia characterized by the inability to write. Alexia An aphasia characterized by the inability to comprehend written or printed words. Alphabet A writing system in which each letter is supposed to represent a phoneme. Alveolar A consonant articulated at the alveolar ridge, such as the d in dog. Ambiguity A property of language in which a word or sentence may be interpreted in more than one way. See also deepstructure ambiguity, lexical ambiguity, and phrase ambiguity. American Sign Language (ASL) The form of sign language used in the United States. It is a complete language distinct from oral languages. Anaphor A linguistic expression that refers back to prior information in discourse. Anaphoric reference A form of reference cohesion in which one linguistic expression refers back to prior information in discourse. Angular gyrus Believed to serve as an association area in the brain that connects one region with another; particularly important for the association of visual stimuli with linguistic symbols. Damage to the angular gyrus leads to both alexia and agraphia. Animacy A semantic feature denoting whether an object is alive. Anomalous suspense In narrative comprehension, the experience of suspense when the reader already knows how a story will turn out. Antecedent Prior information in discourse. Anticipation A speech error in which a later word or sound takes the place of an earlier one. Anticipatory coarticulation Type of coarticulation in which the shape of the vocal tract for a given speech sound is influenced by upcoming sounds. Anticipatory retracing Selfrepair in which the speaker traces back to some point prior to an error. Previously correct material is repeated along with the corrected material. See also fresh start and instant repair. Aphasia A language or speech disorder caused by brain damage. Arbitrariness A feature of language in which there is no direct resemblance between words and their referents. Arcuate fasciculus The primary pathway in the brain between Wernicke’s area and Broca’s area. Articulatory phonetics The branch of phonetics that specifies the articulatory gestures associated with each speech sound. Aspiration A puff of air that accompanies the production of certain speech sounds. Aspiration is phonemic in some languages but not in English. Assertion A communicative act in which a person draws the attention of another person to a particular object—for example, a child showing a toy to an adult as if to say This is mine. Assertions may be made through words or gestures. Assimilation A phonological process in which one speech sound is replaced by another that is similar to sounds elsewhere in the utterance. Associative chain theory A theory favored by behaviorists that explains the formulation of a sentence as a chain of associations between the individual words in the sentence. Attemptsuppressing signal A cue given by a speaker to indicate to a listener that he or she is not finished. Attributive relations Relations between words that indicate the attributes of a given word, such as round as an attribute for ball. Auditory level A level of speech perception in which the speech signal is represented in terms of frequency, intensity, and temporal attributes. Automaticity A property of cognitive processes that do not require any processing capacity. Automatic process An activity that does not require any processing capacity. Autonoetic consciousness A form of consciousness in which one experiences time, as past, present, or future. Auxiliary verb A ‘‘helping verb.’’ A verb such as is, do, or can used in conjunction with the main verb in a sentence, such as Kim is gardening this afternoon. Baby talk See motherese. Background In discourse processing, information that was introduced or discussed earlier and is no longer the focus of discussion. Basic child grammar The grammatical characteristics of early child language, such as telegraphic speech, found in numerous languages. Basic color term A term that refers to color and that is only one morpheme, not contained within another color, and not restricted to a small number of referents. Basiclevel term A term that refers to a category in which there are broad similarities among exemplars. Behaviorism The doctrine that states that the proper concern of psychology should be the objective study of behavior rather than the study of the mind. Bilabial A consonant articulated at the mouth such as the b in big. Bilingual firstlanguage acquisition When children acquire two languages at the same time. Binaural perception A procedure in which the same stimulus is presented to the two ears. Blend A speech error in which two or more words are combined. Bottomup processing A process in which lowerlevel processes are carried out without influence from higherlevel processes (for example, perception of phonemes being uninfluenced by the words in which they appear). Bound morpheme A unit of meaning that exists only when combined or bound to a free morpheme. Bridging A process in which the listener or reader draws inferences to build a ‘‘bridge’’ between the current utterance and preceding utterances. Broca’s aphasia An aphasia characterized by deficits in language production. Also called expressive aphasia. Broca’s area A brain region in the frontal lobe of the left hemisphere. Damage to this region leads to Broca’s aphasia. Bystanders Individuals who are openly present but not part of a conversation. Cataphoric reference A form of reference cohesion in which one linguistic expression refers to information yet to be introduced in discourse. Categorical perception The inability to discriminate sounds within a phonemic category. Categorysize effect The fact that it takes longer to semantically verify a statement of the form An A is a B if B is a larger semantic category. Categoryspecific dissociations In aphasia, the selective inability to retrieve certain categories of words, such as fruits or vegetables, while retaining the ability to recognize and use other word categories. Childdirected speech Speech addressed to children. See also motherese. Childhood amnesia The inability of adults to remember the first few years of life. Also called infantile amnesia. Chunking Grouping individual pieces of information into larger units. A shortterm memory strategy. Closedclass words See function words. Coalescence A phonological process in which phonemes from different syllables are combined into a single syllable. Coarticulation The process of articulating more than one speech sound at a time. Codability The length of a verbal expression. Cognitive constraint A bias that children are assumed to use to infer the meanings of words. Cognitive economy A characteristic of semantic memory in which information is only represented once within a semantic network. Cognitive science The branch of science devoted to the study of the mind; consists of the fields of psychology, artificial intelligence, neuroscience, linguistics, philosophy, and adjacent disciplines. Coherence The degree to which different parts of a text are connected to one another. Coherence exists at both local and global levels of discourse. Cohesion Local coherence relations between adjacent sentences in discourse. GLOSSARY 423 Cohort model A model of auditory word recognition in which listeners are assumed to develop a group of candidates, a word initial cohort, and then determine which member of that cohort corresponds to the presented word. Common ground The shared understanding of those involved in the conversation. Communicative competence The skill associated with using a language appropriately and effectively in various social situations. Complement A noun phrase that includes a verb—for example, you sat down in I see you sat down. Complex sentence A sentence that expresses more than one proposition. Conceptual complexity See semantic complexity. Conceptual metaphor theory In figurative language comprehension, the position that we comprehend figurative language in terms of underlying conceptual metaphors. For example, we might comprehend the metaphor We’re spinning our wheels in terms of the conceptual metaphor LOVE IS A JOURNEY. Conduction aphasia An aphasia characterized by the inability to repeat what one has heard. Conjunctive cohesion A form of cohesion in which we express a relationship between sentences or phrases by using conjunctions such as and, or, and but. Connected discourse See discourse. Connectionist model A model of cognitive/linguistic processes that assumes (1) a vast interconnected network of information nodes in which each node influences and is influenced by a large number of adjacent nodes and 　parallel processing of information. Also called parallel distributed processing. Connotation The aspect of meaning suggested by a word but not strictly part of the word’s dictionary definition. See also denotation. Consonant A speech sound in which the vocal tract is partially or fully closed during production. Constituent A grammatical unit such as a noun or verb phrase. Constraintbased model A model of sentence comprehension in which we simultaneously use all available information (semantic, syntactic, contextual, and so forth) in our initial parsing of a sentence. Content word A word (such as a noun, a verb, or an adjective) that plays a primary role in the meaning of a sentence. See also function word. Contextconditioned variation The fact that the acoustic parameters associated with a given speech sound vary with its phonetic context. Contextualized language Language that is related to the immediate context. Contralateral The arrangement in the nervous system in which one half of the brain controls the other half of the body. Controlled process An activity that requires processing capacity. Convention A shared assumption about communication. Coordination A sentence in which two or more simple sentences are linked by a coordinating expression such as and, or, or but—for example, Lake Superior is beautiful, but it is cold. Also refers to words in the lexicon that are at the same level in a hierarchy, such as sparrow and robin. Copula The verb to be used as the main verb in a sentence such as Miguel is wonderful. Corpus callosum A band of fibers that connects the two cerebral hemispheres. Counterfactual reasoning The ability to reason about an event that is contrary to fact. Count noun A noun that takes the plural morpheme and refers to an object with clear boundaries, such as a stick. Also called an individual noun. Creole The language developed by children who have been exposed to a pidgin as their native language. Critical period hypothesis The view that there is a period early in life in which we are especially prepared to acquire a language. Decontextualized language Language that is separated in time or place from its referent. Deep structure The level of linguistic structure assumed in transformational grammar that expresses the underlying semantic meaning of a sentence. Deepstructure ambiguity A form of ambiguity in which a sentence may be derived from two different deep structures. Default value The value of a parameter that a child is hypothesized to be born with. Deferred imitation Imitation of a behavior that was observed some time earlier. De´ja` vu The erroneous feeling that one has experienced a particular event before. Deletion A speech error in which something is left out. Denotation The dictionary definition of a word. See also connotation. Dental A consonant articulated at the teeth, such as the th in thin. Derivation The series of linguistic rules needed to generate a sentence. Derivational morpheme A bound morpheme that is added to a free morpheme to create a new word. For example, ness turns good (an adjective) into goodness (a noun). Derivational theory of complexity The theory that states that the psychological complexity of a sentence is directly proportional to the length of its derivation. Descriptive adequacy The extent to which a grammar can provide a structural description of a sentence. See also explanatory adequacy and observational adequacy. Determiner A part of speech that quantifies or specifies a count noun, such as the in The cat ate the plant. Dichotic listening task An experimental task in which different stimuli are simultaneously presented to the two ears. Differentiation The number of words in a semantic domain. Discontinuous constituent A grammatical constituent in which some elements are separated, such as picked and up in George picked the baby up. Discourse A group of sentences combined in a meaningful manner. Discreteness A semantic feature denoting whether an object has definite outlines or boundaries. For example, a tree is + discrete, whereas air is – discrete. Dishabituation The recovery of the strength of a habituated response when a novel stimulus is presented. Displacement A feature of language in which words are separated in space and time from their referents. Distinctive feature The specification of the differences between speech sounds in terms of individual contrasts. Duality of patterning A feature of a communication system in which a small number of meaningless units can be combined into a large number of meaningful units. Duplex perception An experimental technique in which formant transitions are presented to one ear and steady states to the other. Eavesdroppers Individuals who listen in on conversations without the participants’ awareness. Elaboration The process of relating incoming information to information already stored in permanent memory. Ellipsis A form of cohesion in which a previous item is dropped from subsequent sentences but its presence is assumed. Empiricism The branch of philosophy that emphasizes the use of controlled observation and the belief that experience shapes human behavior. Episode A component of a story grammar. Episodic memory The division of permanent memory in which personally experienced information is stored. Evoked potential Measurement of electrical activity in a region of the brain following presentation of a stimulus. Exaptation Evolutionary process in which preexisting physical structures are used for new functions. Exchange A speech error in which two sounds or words change places with one another. Excitatory interaction In a connectionist model, the tendency for one unit’s activation to increase the activation of other units. Explanatory adequacy The extent to which a grammar can explain the facts of language acquisition. See also descriptive adequacy and observational adequacy. Explicit knowledge Knowledge of how to perform various acts. See also tacit knowledge. Expository discourse A type of discourse in which the writer’s goal is to convey information about the subject matter. Expressive aphasia See Broca’s aphasia. Expressive strategy A style of child language characterized by low noun/pronoun ratio, poor articulation, clear intonation, and relatively long utterances. Eye–voice span The lag between eye position and voice when reading aloud, about six or seven words. False recognition error When a subject believes that an item was presented during a study although it was not. Fast mapping The process of acquiring new words rapidly. Feature level A level of written language perception in which a visual stimulus is represented in terms of the GLOSSARY 425 physical features that comprise a letter of the alphabet, such as a vertical line, a curved line, and so on. Felicity condition A condition that must be present for a speech act to be understood as sincere or valid. Feral children Children who have grown up without human companionship in the wild. Figurative language Language that means one thing literally but is taken to mean something different. Fis phenomenon When a child mispronounces a word yet correctly distinguishes between child and adult versions of that word. Fixation The time spent focused at a given location during reading; the time between eye movements. Focal color The most representative example of a basic color. Foreground In discourse processing, information that is currently being discussed or explained. Formal complexity See syntactic complexity. Formant A concentrated band of energy found in the sound spectrograms of phonemes. Formant transition A rapid increase or decrease in frequency at the beginning of a formant. Free morpheme A unit of meaning that can stand alone. Fresh start A form of Selfrepair in which the speaker replaces the original syntactic structure with a new one. See also anticipatory retracing and instant repair. Fricative A consonant in which the vocal tract is partially closed during articulation, such as the f in fat. Functional magnetic resonance imaging (f MRI) A method of imaging brain structure and brain activity. Functional relations Relations among words that indicate what can be done with the referent of a word. For example, words such as sitting, rest, and rocking indicate what can be done with a chair. Function word A word such as an article, preposition, or conjunction that plays a secondary role in the meaning of a sentence. See also content word. Garden path sentence A sentence in which the comprehender assumes a particular meaning of a word or phrase but discovers later that the assumption was incorrect, forcing the comprehender to backtrack and reinterpret the sentence. Genre A category of discourse characterized by a particular form or content, such as the genre for fairy tales. Given information Information that the speaker assumes the listener already knows. Given/new strategy A comprehension strategy in which utterances are analyzed into given and new components and the new information is stored in memory with previously received information. Global structure See macrostructure. Glottis The opening between the vocal cords. Grammar In linguistics, a theory of language or set of hypotheses about how language is organized. Grammatical gender The grammatical property in which languages identify objects as masculine, feminine, and sometimes neuter. Grammatical morpheme See bound morpheme. Grapheme A printed letter of the alphabet. Ground In metaphor, the implied similarity between tenor and vehicle. Habituation The decline in a response to a stimulus following repeated presentation of the stimulus. Head parameter A grammatical feature that specifies the position of the head of a phrase (noun in noun phrase, verb in verb phrase, and so on). Hemispherectomy A surgical procedure in which one of the cerebral hemispheres is removed. Holistic processing A style of processing, associated with the right cerebral hemisphere, that is global in nature. Holophrase A oneword utterance used by a child to express more than the meaning attributed to the word by adults. Homesign A form of gestural communication invented by deaf children who are not exposed to a sign language. Hominids The family of species that includes modernday human beings. Also called hominins. Homophone A word that is pronounced the same as another word but means something different, such as to and two. Hypernymy A semantic relationship in which a word is a subordinate of another. For example, animal is a hypernym of dog. Hyponymy A semantic relationship in which a word is a superordinate of another. For example, collie is a hyponym of dog. Iconicity A characteristic of language in which words resemble their referents. Idiomorph A sound or sound sequence used consistently by a child to refer to someone or something even though it is not the sound sequence conventionally used in the language for that purpose. Illocutionary force In speech act theory, the action that is performed by a speaker in uttering a sentence. Immediacy principle The principle that we immediately interpret words as we encounter them. Incremental processing The notion that we are planning one portion of our utterance as we articulate another portion. Indirect speech act A speech act in which the literal utterance meaning is not the same as the speaker’s meaning. Induction A process of reasoning from the specific to the general. For instance, if all of the specific horses we have seen are brown, then we might induce that all horses are brown. Inference A proposition drawn by the listener or reader. Inflectional morpheme A bound morpheme that is added to a free morpheme to express grammatical contrasts in sentences. English examples include the plural and past tense morphemes. Inhibitory interaction In a connectionist model, the tendency for one unit’s activation to decrease the activation of other units. Initiationreplyevaluation sequence A form of discourse used in classrooms in which the teacher asks a student a question, the student answers, and the teacher evaluates the answer. Instantiation Identifying a general term with a specific meaning. Instant repair A form of Selfrepair in which the speaker traces back to an error that is then replaced with the correct word. See also anticipatory retracing and fresh start. Institutional setting A conversational setting in which participants engage in speech exchanges that resemble ordinary conversations but are limited by institutional rules. Interactional content Content of a sentence that conveys the speaker’s attitude toward the listener. Utterances that are high in interactional content include jokes, insults, and excessively polite speech. Interactive gesture A form of gesture used in conversation to convey interactional content, such as holding up one’s hands to indicate that one’s turn is not finished. Internal lexicon The storage of lexical information in memory. Interruption A period of simultaneous speech more than one word prior to the speaker’s projected completion point. See also overlap. Intersection search The process of retrieving information from a semantic network. Intonation The use of pitch to signal meaning. Intonational contour A pattern of pitch changes characteristic of an utterance as a whole, such as the rising intonation often found in questions. Intrinsic terms Spatial terms that refer to objects in relation to various object coordinates (such as behind the house, at the tip of the post). Ipsalateral The arrangement in the nervous system in which one half of the brain controls the same side of the body. Isolated children Children who have grown up without normal human interactions. Joint action An action carried out by an ensemble of people acting in coordination with one another. Examples include dancing and conversing. Kana Japanese syllabic symbols. Kanji Japanese logographic characters borrowed from Chinese. Lack of invariance The fact that there is no onetoone correspondence between speech cues and perception. Language Within linguistic theory, an infinite set of wellformed sentences. Language bioprogram A hypothesized innate grammar that is used by children whose environmental exposure to language is limited. The bioprogram is assumed to be suppressed in children whose language environment is normal. Language bioprogram hypothesis The hypothesis that children whose environmental exposure to language is limited use a backup linguistic system. Language transfer In secondlanguage acquisition, the process in which the first language influences the acquisition of a subsequent language. Laryngeal system The system of muscles that determines whether a speech sound is voiced or voiceless. GLOSSARY 427 Late closure strategy A strategy used in parsing that states that wherever possible we prefer to attach new items to the current constituent. Lateralization The extent to which a given psychological function is served by one hemisphere of the brain. Functions primarily served by one hemisphere are said to be lateralized to that hemisphere. Lemma Syntactic aspects of word knowledge. Letter level The level of written perception in which a visual stimulus is represented as a letter of the alphabet. Lexeme Phonological aspects of word knowledge. Lexical access The process of activating lexical items from semantic memory. Lexical ambiguity A form of ambiguity in which a word has more than one meaning. Lexical bias effect The finding that speech errors more commonly result in true words than would be expected by chance. Lexical cohesion The use of reiteration, synonymy, hyponymy, and other semantic relationships to link successive sentences in discourse. Lexical decision task An experimental task in which a subject sees a string of letters and must rapidly decide whether the string is a word. Lexicalfunctional grammar A grammar in which structural relationships are built into enriched lexical entries rather than with transformational rules. Lexicalinsertion rule A rule that governs how lexical entries are inserted into a tree structure during the derivation of a sentence. Lexicon The vocabulary of a language. See also internal lexicon. Linguistic creativity See linguistic productivity. Linguistic determinism The hypothesis that languages determine nonlinguistic cognitive processes such as the perception of shapes. Linguistic productivity The ability to create or comprehend an infinite number of new sentences that are grammatically correct; also called linguistic creativity. Linguistic relativity The hypothesis that the cognitive processes determined by language vary from language to language. Linguistics The branch of science that studies the origin, structure, and use of language. Local structure See microstructure. Locutionary act In speech act theory, the act of saying something. Logogen Structure in the internal lexicon that specifies the various attributes (semantic, orthographic, and so on) of a word. Logography An orthography in which spoken words are represented by visual symbols. Longitudinal investigation A method of studying child development in which a small number of children are studied over a period of years. Longterm memory See permanent memory. Macrostructure The global coherence relationships in discourse. Manner of articulation How a speech sound is articulated (for example, stop, fricative, and so on). Manual English A manual version of English, as in fingerspelling the letters of the English alphabet. See also American Sign Language. Mass noun A noun that does not take the plural morpheme and refers to objects without clear boundaries, such as air. Mean length of utterance in morphemes (MLU) An index of children’s language growth. It is computed by dividing the number of morphemes by the number of utterances. Mental model A mental representation of some aspect of the world. Meronymy A semantic relationship that pertains to the parts of an object referred to by a word; for example, for the word car, both engine and wheels are meronyms because they refer to parts of a car. Metalinguistic awareness The ability to think of language as an object. Metaphor A form of language in which a word or phrase that literally denotes one idea is interpreted to mean a different one and suggests a similarity between the two—for example, My head is an apple without a core. Microstructure The local coherence relationships in discourse. Minimal attachment strategy A principle used in parsing. It states that we prefer attaching new items into the phrase marker being constructed using the fewest syntactic nodes consistent with the rules of the language. Minimal response An utterance such as uhhuh or umhmm made by a listener during a conversation. Ordinarily minimal responses are taken as displays of interest in a speaker’s topic. Mispronunciation detection An experimental task in which subjects are presented auditorily with tapes that occasionally include mispronounced words. The subject’s task is to detect the mispronunciations. Modularity The degree to which language processing is independent of general cognitive processes such as memory and reasoning. Also refers to the degree to which an aspect of language is independent of other aspects of language. For example, parsing may be thought of as modular if there is a syntactic processor that operates independently of semantic and discourse processes. Morpheme The smallest unit of meaning in a language. Morphology The system of wordforming elements and processes in a language. Motherese A form of adulttochild speech characterized by relatively simple utterances, concrete referents, exaggerated intonation patterns, and a high proportion of directive utterances. Mutual exclusivity bias A cognitive constraint in which children assume that an object is ordinarily not given two different names. Narrative discourse A form of discourse in which settings, characters, and plot play a central role. Nasal A consonant in which air flows through the nasal cavity as in the n in nail. Nativism An approach to language acquisition that emphasizes the innate organization of language. Necessary condition A condition that must be present in order for a specified event to occur. Negative evidence Evidence that a particular linguistic expression (a word or sentence) is inappropriate or unacceptable. Negative evidence may be presented explicitly (No, that’s not a cow; that’s a dog) or implicitly (such as when adults repeat child utterances with corrections). Neurolinguistics The study of how linguistic information is processed in the brain. New information Information that the comprehender (reader or listener) is assumed not to know. Nonnutritive sucking A procedure used in research with infants in which sucks on a pacifier are recorded. Nullsubject parameter A grammatical feature that specifies whether a language permits sentences without subjects. Also called the prodrop parameter. Object permanence The awareness that objects that can no longer be seen still exist. Object relative clause A relative clause in which a wh clause modifies the object of a sentence. Observational adequacy The extent to which a grammar can distinguish between acceptable and unacceptable strings of words. See also descriptive adequacy and explanatory adequacy. Occipital lobe The visual center at the back of the brain. Openclass word See content word. Operating principle A preferred way of taking in or operating on information. Original word game A game in which adults teach children the names of words. Children point to an object and say, ‘‘What’s that?’’ and the adult supplies the name. Orthography The representation of a sound by written or printed symbols. Ostensive definition The process of defining a word by pointing to its referent. Overextension When a child uses a word to refer to a larger set of referents than an adult would—for example, calling a round clock a moon. Overhearers Individuals who are not part of a conversation. May be bystanders or eavesdroppers. Overlap A period of simultaneous speech during the last word of a speaker’s projected closing. See also interruption. Overregularization When a child applies a linguistic rule to cases that are exceptions to the rule—for example, saying goed instead of went. Paragrammatic speech Speech that is fluent but not coherent and that contains many irrelevant associations. Parallel distributed processing See connectionist model. Parallel processing When two or more processes take place at the same time. Parallel transmission The notion that different phonemes of the same syllable are encoded into the speech signal simultaneously. Parameter (1) In grammatical theory, a grammatical feature that is set in different ways in different languages. See also head parameter and nullsubject parameter. GLOSSARY 429 In American Sign Language, a dimension along which signs may differ, such as hand configuration, movement, and location. Parameter setting In grammatical theory, the notion that children are born with grammatical parameters that are preset to certain values. Language acquisition is seen as a matter of resetting these parameters to the values of one’s native language. Parietal lobe Middle brain region containing motor centers that control facial and speech muscles. Parsing The process of assigning words into grammatical categories. Partial report technique A method for studying the sensory stores. Subjects are briefly presented with an array of stimuli and asked to report only a portion of the array. Participants Individuals who are taking part in a conversation. Particlemovement transformation A transformational rule that accounts for the movement of particles such as up around noun phrases. Passive transformation A transformational rule that transforms the deep structure of an active sentence into the passive voice. Passive voice A sentence in which the surface structure subject is the deep structure or logical object of the action, such as in The child was scolded by the mother. Patient A thematic or semantic role corresponding to the individual acted on, such as the elderly man in The neighborhood frightened the elderly man. Pattern recognition A process of matching information in the sensory stores with information retrieved from permanent memory. Perceptual span The size of the area from which a reader picks up visual information. Perlocutionary effect In speech act theory, the effect of a speech act on a listener. Permanent memory Memory that is essentially permanent (also called longterm memory). Includes semantic and episodic memory. Perseveration A speech error in which an earlier word or sound intrudes on a later one. Perseveratory coarticulation The type of coarticulation in which the shape of the vocal tract for a given speech sound is influenced by previous sounds. Personal settings A conversational setting in which there is a free exchange of turns among two or more participants. Phone The minimal unit of sound. Phoneme The minimal unit of sound that contributes to meaning. Phoneme monitoring An experimental task in which subjects listen for a particular phoneme while comprehending a passage and being timed for how long it takes them to monitor the phoneme. Phonemic restoration A topdown process in which the listener uses the context to restore phonemes missing from the speech signal. Phonemic similarity effect The observation that speech errors and targets are phonemically similar. Phonetic level A level of speech perception in which the speech signal is represented in terms of acoustic cues, such as formant transitions. Phonetics The study of speech sounds. Phonetic trading relations The notion that different acoustic cues have tradeoff effects on speech perception. Phonological bias technique A method of inducing speech errors by having a subject read a series of words with similar phonological patterns. Phonological dyslexia A form of reading disability in which a person’s ability to read words aloud is disrupted. Phonological level A level of speech perception in which the speech signal is converted into a phoneme and phonological rules are applied to the sound sequence. Phonology The sound system of a language, including the rules determining how different phonemes may be arranged in a word. Phrase marker A tree diagram that represents the phrase structure of a sentence. Phrase structure The hierarchical organization of sentences into phrases. Phrasestructure ambiguity A form of ambiguity in which a sentence has multiple meanings that may be revealed by regrouping the sentence constituents. Phrasestructure rule A rule that rewrites one constituent into one or more constituents. For example, a verb phrase may be rewritten as a verb and a noun phrase. Pidgin An auxiliary language that is created when speakers of mutually unintelligible languages are in close contact. Place of articulation The location within the vocal tract where articulation of a speech sound is produced (for example, bilabial, alveolar, and so on). Planum temporale An area in the temporal lobe known to be related to language functioning. Positive evidence Evidence that a particular linguistic expression (a word or sentence) is appropriate or acceptable. Positive evidence may be presented explicitly (when someone approves of another’s word or utterance) or implicitly (for example, when a person responds to another’s utterance without explicitly commenting on its appropriateness). Poverty of stimulus argument The argument made by followers of nativism that the environmental input presented to children is too weak and degenerate to account for the child’s language acquisition. Pragmatics The social rules underlying language use. Pragmatic theory In figurative language comprehension, the position that we comprehend figurative language by considering the literal meaning, then rejecting it. Preemption principle The principle that the speech of a child’s linguistic environment preempts or suppresses the language bioprogram. Preoperational period The second of Piaget’s periods of cognitive development. Pretend play The use of an object in a playful or unconventional manner, such as using a toy rake to comb a doll’s hair. Processing capacity The overall amount of mental capacity available for various tasks or activities. Prodrop parameter See nullsubject parameter. Proposition A unit of meaning consisting of a predicate (verb, adjective, or conjunction) plus one or more arguments (noun or pronoun). Simple sentences express a single proposition, whereas complex sentences express more than one proposition. Propositional representation In sentence or discourse memory, memory for the meaning apart from the exact words used. Prosodic factors Factors such as intonation and stress that are superimposed on speech segments. Also called suprasegmentals. Psycholinguistics The study of the comprehension, production, and acquisition of language. Psychologically realistic grammar A grammar or theory of language that takes psychological or processing considerations into account. Pure word deafness An aphasia in which a person is unable to comprehend language in the auditory modality. Comprehension of visual language and production in both modalities are normal. Radical In a logography, a group of strokes related to meaning. Rate The speed at which speech is articulated. Rate normalization The process of taking the rate of speech into consideration when using acoustic cues during speech perception. Rationalism The philosophical tradition that emphasizes the use of argument and the belief that innate knowledge guides human behavior. Reading span task A measure of working memory capacity during reading. Subjects read aloud a series of sentences and then try to recall the last word in each sentence. The number of words recalled is the measure of the subject’s reading span. Receptive aphasia See Wernicke’s aphasia. Recipient A semantic or thematic role referring to the person to whom something is given—for example, Susan in John gave the flower to Susan. Reciprocity In American Sign Language, the distinction between whether the subject is the agent of the action and the object is the recipient (they pinched them) and whether there is mutual interchange between the subject and object (they pinched each other). Recognition point In auditory word recognition, the point at which a word diverges from other possible words. Recursive rule A rule that applies to its own output, such as a rule for selfembedded sentences. Reduction A phonological process in child language in which one or more phonemes are deleted. Also called cluster reduction because consonant clusters are often reduced, such as saying take for steak. Reduplicated babbling A form of babbling in which infants use the same sounds over and over, as in gagagaga. Reduplication A phonological process in which the repetition of one syllable is used to mark a multisyllabic word (for example, dada for daddy). Reference The relationship between a linguistic expression and a person, object, or event in the world. GLOSSARY 431 Reference cohesion A form of cohesion in which the information needed to interpret a linguistic expression is found elsewhere in the text. See also anaphoric reference and cataphoric reference. Referent The person, object, or event to which a linguistic expression refers. Referential communication task An experimental task in which the subject must formulate a message about an object in the environment (as opposed to one’s thoughts or feelings). Referential gesture A form of gesture used in conversation to refer to some aspect of the content of a conversation. Referential strategy The style of child language that emphasizes a high ratio of nouns to pronouns, clear articulation, and an emphasis on naming. Regression Backward eye movement during reading. Reinstatement The timeconsuming process in which antecedents are retrieved from permanent memory into working memory to comprehend a current sentence. Relational processing A style of processing, associated with the left hemisphere, which emphasizes the analysis of whole units into parts. Relative clause A wh clause that modifies a noun—for example, that you found in Show me the book that you found. Relative terms Spatial terms that indicate the relationship between an object in space and a person (for example, in front of me, to the left of her). Request A communicative act in which a person attempts to influence the behavior of another—for example, a child pointing at a milk bottle in order to be given some. Requests may occur in words or gestures. Respiratory system The system of muscles that regulates the flow of air from the lungs to the vocal tract. Retention interval The time between when information is presented and when it is to be recalled. Saccade An eye movement during reading. Sapir–Whorf hypothesis See Whorf hypothesis. Schema (plural, schemata) A structure in semantic memory that specifies the expected sequence of events. Secondlanguage acquisition When an individual (child or adult) acquires a second language after already acquiring a native language. Also called sequential bilingualism. Selfreference effect The tendency to remember information better when one relates it to oneself. Selfrepairs Selfcorrection of speech errors. Semantic bootstrapping The process of using semantics to acquire syntax. Semantic complexity The complexity of the ideas expressed in a sentence or phrase (also called conceptual complexity). See also syntactic complexity. Semantic differential A tool for measuring the associative meanings of words by asking people to rate words on dimensions such as good/bad and strong/weak. Semantic memory The portion of permanent memory that contains organized knowledge of words, concepts, symbols, and objects. See also internal lexicon. Semantic network A model of semantic memory in which words are represented as nodes and connected to other nodes by various semantic relationships. Semantic priming An experimental procedure in which one word is presented in advance of another, target word, which reduces the time needed to retrieve or activate the target word. Semantics The domain of language that pertains to the meanings of words and sentences. Semantic verification task An experimental task in which subjects view sentences of the form An A is a B and rapidly decide whether the sentence is true or false. Sense The relationship a word has with other words in the lexicon. Sensorimotor period The first of Piaget’s periods of cognitive development, characterized by sensory and motor development and the inability to fully represent objects symbolically. See object permanence. Sensory stores The initial memory system for sensory stimuli. There is a separate store for each sense (vision, audition, and so on). Sequential bilingualism See secondlanguage acquisition. Serial processing Processes that occur one at a time. Shadowing An experimental task in which subjects repeat what they hear. Shift A speech error in which a speech sound or word moves from one location to another. Shortterm memory The memory system that holds information for about 30 seconds. See also working memory. Side participant An individual who is taking part in a conversation but is not currently being addressed. Simultaneous bilingualism See bilingual firstlanguage acquisition. Situational model A mental model of discourse. Sociolinguistics The study of how language functions in social situations. Somatosensory regions Areas in the brain’s parietal lobe controlling the sense of touch. Sound spectrogram A visual representation of the speech signal. Sound spectrograph A device used to create a sound spectrogram. Speaker normalization The process of taking the pitch of the speaker into account when using acoustic cues during speech perception. Speech act An utterance with an illocutionary force. Speech perception The process of using acoustic information to arrive at a recognition of the speech sounds in a message. Spreading activation The process by which one node in a semantic network, when active, activates related nodes. Steady state The portion of a formant that is of relatively constant frequency. Stop A consonant in which the vocal tract is completely closed, building up air pressure, which is then abruptly released, such as in the b in bat. Story grammar The mental representation (schema) of an expected series of events in a story. Stress The emphasis given to a word or syllable during the articulation of a sentence (for example, blackBIRD versus BLACKbird ). Structure dependence The fact that linguistic rules apply to grammatical structures (or constituents) rather than to individual words. Subset principle The notion that languages may be considered as subsets of one another. Substitution A form of cohesion in which one word is replaced by another as an alternative to repeating the first word. Also, a speech error or phonological process in which one sound or word replaces another. Sufficient condition A condition that, if present, ensures that a specified event will occur. Supralaryngeal system The system of muscles that manipulates the size and shape of the vocal tract. Suprasegmentals Prosodic factors such as stress and intonational patterns that lie ‘‘on top of ’’ speech segments. Surface dyslexia A form of reading disability in which a person retains the ability to name nonwords but not words. Surface representation In sentence or discourse memory, representation of the exact words that were presented. Surface structure The level of syntactic structure assumed in transformational grammar that is closer to the phonetic specification of an utterance. Syllabary An orthography in which syllables are represented by visual symbols. Synonymy A semantic relationship in which two or more words have a similar meaning. Syntactic category Another term for part of speech, such as noun, verb, and so forth. Syntactic complexity The complexity of the grammatical operations required to express an idea in a given language; also called formal complexity. See also semantic complexity. Syntax The domain of language that pertains to the grammatical arrangement of words in a sentence. Syrinx The major structure in the vocal system of the chaffinch. Tachistoscope A machine that presents visual stimuli for very brief periods of time. Tacit knowledge Knowledge of how to perform an act. See also explicit knowledge. Tag question A question that is ‘‘tagged’’ onto a declarative sentence such as isn’t it in It sure is cold in here, isn’t it? Task specificity The notion that certain cognitive processes are restricted to language and are not employed in other intellectual domains. Taxonomic bias A cognitive constraint in which children assume that a word refers to a class of individuals rather than to a single person or animal. Taxonomic relations Relations among words that indicate the position of words in a taxonomy. For example, for the word dog, mammal is a superordinate term, cat is a coordinate term, and collie is a subordinate term. Temporal lobes Auditory regions at each side of the brain. GLOSSARY 433 Tenor The topic of a metaphor. Theory of mind The ability to view another person as an intentional being. For example, interpreting a person’s bumping into you as an intentional rather than an accidental act is an example of a theory of mind. Tipofthetongue (TOT) phenomenon When we know a word but are temporarily unable to retrieve it. Topdown processing A process in which higher levels influence lower levels of processing. For example, the perception of phonemes may be influenced by the words in which they appear. TRACE model A connectionist model of speech perception. Transformational rule A rule that transforms one phrase structure into another by adding, deleting, or moving grammatical constituents. Also called transformation. Truth conditions The conditions that need to be present in the world in order for a sentence to be true. Turnyielding signal A set of cues given by a speaker to indicate that he or she is ready to yield the floor. Typicality effect The fact that it takes longer to verify a statement of the form An A is a B when A is not typical or characteristic of B. Underextension When a child uses a word in a more limited way than adults do (for example, refusing to call a taxi a car). Undershooting In speech production, the tendency for articulators to fall short of target locations for different speech sounds, owing to coarticulation. Variegated babbling A form of babbling consisting of syllable strings with varying consonants and vowels. Vehicle What is predicated of the topic in a metaphor. Velar A consonant articulated at the velum, such as the c in collar. Visual field task An experimental task in which visual stimuli are presented to either the right or the left visual field. Vocal cords Two bands of muscular tissue in the larynx that vibrate during the production of speech sounds; also called vocal folds. Vocal tract The structures above the larynx that participate in speech production, principally the mouth (oral cavity) and nose (nasal cavity) regions. Voiced Speech sound in which the vocal cords are vibrating during the production of sound. Voiceless Speech sound in which the vocal cords are not vibrating during the production of sound. Voice onset time The period of time from when a consonant is released until the vocal cords vibrate. Voicing Whether or not the vocal cords are vibrating when air from the lungs passes over them. If the cords are vibrating, the speech sound is called voiced; if not, voiceless. Vowel A speech sound in which the vocal tract is open during production. Wernicke’s aphasia An aphasia characterized by fluent speech that is not informational and by disorders of comprehension. Also called receptive aphasia. Wernicke’s area A brain region in the temporal lobe of the left hemisphere. Damage to this region leads to Wernicke’s aphasia. Whole object bias A cognitive constraint in which children assume that a word refers to an entire object, not a part of it. Whorf hypothesis The hypothesis that languages shape thought processes; also called the Sapir–Whorf hypothesis. See also linguistic determinism and linguistic relativity. Whquestion A question beginning with a whword, such as who, what, where, or when. Word association test A test in which a person is presented with a word and asked to respond with the first word that comes to mind. Word initial cohort In auditory word recognition, the initial set of lexical candidates activated by the comprehender. Word level A level of written language perception in which a visual stimulus is represented as a familiar word. Wordsuperiority effect An experimental finding that it is easier to perceive a letter in a word context than in isolation. Working memory A form of memory with both storage and processing functions. Working memory is used to hold information for a short period of time as well as to perform various operations on the stored information. Yes/no question A question that can be answered with a yes or no answer. Zipf ’s law The fact that the length of a word is negatively correlated with its frequency of use.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>心理语言学</category>
      </categories>
      <tags>
        <tag>psycholinguistics</tag>
      </tags>
  </entry>
</search>
