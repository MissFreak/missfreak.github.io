<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>目前共有44篇文章</title>
    <url>/all-posts/</url>
    <content><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="380" height="110" src="//music.163.com/outchain/player?type=0&amp;id=2112262923&amp;auto=1&amp;height=90">
</iframe>
<center>
『 好奇心造就诗人与科学家。』<br>Knowledge of languages is the doorway to wisdom. – Roger Bacon<br><br>
<details class="category_1">
<summary>
人生规划
</summary>
<details open>
<summary>
毕业论文
</summary>
<br> <a href="graduation-thesis">毕业论文准备</a><br>
</details>
</details>
<details class="category_1">
<summary>
代码
</summary>
<details open>
<summary>
Git
</summary>
<br> <a href="git">Git和Github常用操作大全</a><br>
</details>
<details open>
<summary>
文本处理
</summary>
<br> <a href="json">用python处理json数据</a><br> <a href="pdf">PDF文件处理大全</a><br> <a href="txt">纯文本文件的处理</a><br>
</details>
<details open>
<summary>
爬虫
</summary>
<br> <a href="scraping">Python爬虫代码-全</a><br>
</details>
<details open>
<summary>
编辑器
</summary>
<br> <a href="atom">从小白到起飞，一站解决Atom编辑器各种骚操作</a><br> <a href="programming-methodology">编程方法论</a><br>
</details>
</details>
<details class="category_1">
<summary>
未分类
</summary>
<br><a href="all-posts">目前共有44篇文章</a><br> <a href="cognitive-linguistics">认知语言学大纲</a><br> <a href="content">我的电脑文件目录管理</a><br> <a href="hello-world">Hello World</a><br> <a href="hexo-next">Hexo博客Next主题搭建全过程</a><br> <a href="information-theory-1">信息论知识与资源汇总</a><br> <a href="knowledge-graph-1">knowledge-graph-1</a><br> <a href="resources">免费好用网站软件资源集锦：从学习、办公到娱乐</a><br> <a href="web-dev-all">网站开发需要的工具：前端和后端</a><br> <a href="wiki-api">好用的API集锦</a><br>
</details>
</details>
<details class="category_1">
<summary>
网站开发
</summary>
<details open>
<summary>
django
</summary>
<br> <a href="django-1">Django网站开发全过程实录-1</a><br>
</details>
<details open>
<summary>
flask
</summary>
<br> <a href="flask-2">一文读懂Flask Web开发实战！</a><br> <a href="flask-api-1">基础：用flask搭建RESTful API</a><br> <a href="flask-api-2">部署Flask开发的API到Heroku</a><br> <a href="flask-api-3">如何在curl和python中使用API</a><br> <a href="flask-web">一个人开发信息检索与抽取网站的全过程</a><br> <a href="flask">flask干货总结</a><br>
</details>
</details>
<details class="category_1">
<summary>
自然语言处理
</summary>
<details open>
<summary>
依存分析
</summary>
<br> <a href="dependency-parsing-1">《自然语言处理综论》第14章-依存分析（上）</a><br> <a href="dependency-parsing-2">《自然语言处理综论》第14章-依存分析（中）</a><br> <a href="dependency-parsing">英文依存句法分析</a><br> <a href="pos-tagging">英文词性标记（POS Tagging）</a><br>
</details>
<details open>
<summary>
信息抽取
</summary>
<br> <a href="information-retrieval-1">《自然语言处理综论》第17章-信息抽取（上）</a><br> <a href="information-retrieval-2">《自然语言处理综论》第17章-信息抽取（中）</a><br> <a href="information-retrieval-3">《自然语言处理综论》第17章-信息抽取（下）</a><br> <a href="information-retrieval-4">信息抽取</a><br> <a href="information-retrieval">信息抽取技术综述</a><br> <a href="named-entity-recognition">英文文献的命名实体识别（上）</a><br>
</details>
<details open>
<summary>
语料库
</summary>
<br> <a href="corpus">语料库资源大全</a><br>
</details>
<details open>
<summary>
预处理
</summary>
<br> <a href="encoding">文本编码格式大全</a><br> <a href="spacy-1">spaCy超强指南之文本预处理和语言特征表</a><br>
</details>
</details>
<details class="category_1">
<summary>
计算机
</summary>
<details open>
<summary>
正则表达式
</summary>
<br> <a href="regex-1">正则表达式进阶</a><br>
</details>
<details open>
<summary>
算法
</summary>
<br> <a href="finite-state-machines">有限状态机</a><br>
</details>
</details>
<details class="category_1">
<summary>
语言学
</summary>
<details open>
<summary>
心理语言学
</summary>
<br> <a href="psycho-linguistics">心理语言学：资源和知识整理</a><br> <a href="relation-extraction-0">心理语言学：资源和知识整理</a><br>
</details>
<details open>
<summary>
语言测试
</summary>
<br> <a href="language-assessment">语言测试Syllabus</a><br> <a href="language-assessment2">语言测试glossary</a><br>
</details>
</details>
<br><br>关注公众号：鸽婆打字机！风里雨里，鸽鸽陪你~
</center>
<p><img align="center" width="120" height="120" alt="微信公众号：鸽婆打字机" src="https://i.loli.net/2021/03/04/dXVUZiRfW2o7wCy.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>spaCy超强指南之文本预处理和语言特征表</title>
    <url>/spacy-1/</url>
    <content><![CDATA[<h1 id="语言特征表token属性">语言特征表：token属性</h1>
<p>https://spacy.io/api/token#attributes</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&quot;I took a walk happily yesterday.&quot;</span></span><br><span class="line">doc = nlp(sentence)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> tok <span class="keyword">in</span> doc:</span><br><span class="line">    row = <span class="string">&quot;\t&quot;</span>.join([<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> [tok.text, tok.pos_, tok.dep_, tok.head, tok.left_edge, tok.right_edge, tok.lemma_, tok.morph, tok.suffix_, tok.is_stop, tok.tag_]])</span><br><span class="line">    print(row)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<table>
<colgroup>
<col style="width: 8%" />
<col style="width: 7%" />
<col style="width: 3%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 39%" />
<col style="width: 2%" />
<col style="width: 4%" />
<col style="width: 2%" />
<col style="width: 4%" />
</colgroup>
<thead>
<tr class="header">
<th>text</th>
<th>pos_</th>
<th>dep_</th>
<th>head</th>
<th>left_edge</th>
<th>right_edge</th>
<th>lemma_</th>
<th>morph</th>
<th>suffix</th>
<th>is_stop</th>
<th>tag_</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>I</td>
<td>PRON</td>
<td>nsubj</td>
<td>took</td>
<td>I</td>
<td>I</td>
<td>I</td>
<td>Case=Nom|Number=Sing|Person=1|PronType=Prs</td>
<td>I</td>
<td>True</td>
<td>PRP</td>
</tr>
<tr class="even">
<td>took</td>
<td>VERB</td>
<td>ROOT</td>
<td>took</td>
<td>I</td>
<td>.</td>
<td>take</td>
<td>Tense=Past|VerbForm=Fin</td>
<td>ook</td>
<td>False</td>
<td>VBD</td>
</tr>
<tr class="odd">
<td>a</td>
<td>DET</td>
<td>det</td>
<td>walk</td>
<td>a</td>
<td>a</td>
<td>a</td>
<td>Definite=Ind|PronType=Art</td>
<td>a</td>
<td>True</td>
<td>DT</td>
</tr>
<tr class="even">
<td>walk</td>
<td>NOUN</td>
<td>dobj</td>
<td>took</td>
<td>a</td>
<td>walk</td>
<td>walk</td>
<td>Number=Sing</td>
<td>alk</td>
<td>False</td>
<td>NN</td>
</tr>
<tr class="odd">
<td>happily</td>
<td>ADV</td>
<td>advmod</td>
<td>took</td>
<td>happily</td>
<td>happily</td>
<td>happily</td>
<td></td>
<td>ily</td>
<td>False</td>
<td>RB</td>
</tr>
<tr class="even">
<td>yesterday</td>
<td>NOUN</td>
<td>npadvmod</td>
<td>took</td>
<td>yesterday</td>
<td>yesterday</td>
<td>yesterday</td>
<td>Number=Sing</td>
<td>day</td>
<td>False</td>
<td>NN</td>
</tr>
<tr class="odd">
<td>.</td>
<td>PUNCT</td>
<td>punct</td>
<td>took</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>PunctType=Peri</td>
<td>.</td>
<td>False</td>
<td>.</td>
</tr>
</tbody>
</table>
<h1 id="文本预处理">文本预处理</h1>
<p>spaCy的功能：把原始文本处理和标记并返回一个<a href="https://spacy.io/api/doc"><code>Doc</code></a>对象。以下是处理的常见操作，其中lemmatization和形态学是spacy使用的：</p>
<table>
<colgroup>
<col style="width: 38%" />
<col style="width: 61%" />
</colgroup>
<thead>
<tr class="header">
<th>特征</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>规范化（normalization）</td>
<td>transforming a text into a canonical (standard) form, e.g. :) to smile, and goooood to good.</td>
</tr>
<tr class="even">
<td>词形还原（lemmatization）</td>
<td>Base form of the token, with no inflectional suffixes. Lemmatization is typically seen as much more informative than simple stemming, which is why Spacy has opted to only have Lemmatization available instead of Stemming.</td>
</tr>
<tr class="odd">
<td>词干提取（stemming）</td>
<td>reducing inflection in words (e.g. thanks, troubling) to their root form (e.g. thank, troubl).</td>
</tr>
<tr class="even">
<td>形态学（morphology）</td>
<td>"you": Case=Nom|Person=2|PronType=Prs</td>
</tr>
</tbody>
</table>
<h1 id="其他文本预处理的资料">其他文本预处理的资料</h1>
<p>https://medium.com/<span class="citation" data-cites="datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908">@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908</span></p>
<p>https://towardsdatascience.com/stemming-vs-lemmatization-2daddabcb221</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>spaCy</tag>
      </tags>
  </entry>
  <entry>
    <title>有限状态机</title>
    <url>/finite-state-machines/</url>
    <content><![CDATA[<p>有限状态机（Finite-state machine, FSM），是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。主要用来是描述对象在它的生命周期内所经历的状态序列，以及如何响应来自外界的各种事件。 <span id="more"></span></p>
<p>参考：</p>
<p>https://www.huaweicloud.com/articles/ff04e6e5d7386b32d58f9d5015104ce9.html</p>
<p>https://www.cnblogs.com/21207-iHome/p/6085334.html</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title>文本编码格式大全</title>
    <url>/encoding/</url>
    <content><![CDATA[<p><strong>字符——（翻译过程）——二进制数字</strong></p>
<p>这个过程实际就是一个字符如何对应一个特定数字的标准，这个标准称之为字符编码。</p>
<h2 id="字符编码发展史与编码类型">字符编码发展史与编码类型</h2>
<h3 id="ascii-码美国标准信息交换码">ASCII 码（美国标准信息交换码）</h3>
<p>ASCII 码一共规定了128个字符的编码，比如空格SPACE是32（二进制00100000），大写的字母A是65（二进制01000001）。这128个符号（包括32个不能打印出来的控制符号），只占用了一个字节的后面7位，最前面的一位统一规定为0。</p>
<span id="more"></span>
<h3 id="非-ascii-编码">非 ASCII 编码</h3>
<p>英语用128个符号编码就够了，但是用来表示其他语言，128个符号是不够的。比如，在法语中，字母上方有注音符号，它就无法用 ASCII 码表示。于是，一些欧洲国家就决定，利用字节中闲置的最高位编入新的符号。比如，法语中的é的编码为130（二进制10000010）。这样一来，这些欧洲国家使用的编码体系，可以表示最多256个符号。</p>
<h3 id="gb2312-码">GB2312 码</h3>
<p>中文字符的每个字节最高位规定为 1，这便是 GB2312 编码。</p>
<h3 id="unicode">Unicode</h3>
<p>可以想象，如果有一种编码，将世界上所有的符号都纳入其中。每一个符号都给予一个独一无二的编码，那么乱码问题就会消失。<u>这就是 Unicode，就像它的名字都表示的，这是一种所有符号的编码。</u></p>
<p>Unicode 当然是一个很大的集合，现在的规模可以容纳100多万个符号。每个符号的编码都不一样，比如，<code>U+0639</code>表示阿拉伯字母<code>Ain</code>，<code>U+0041</code>表示英语的大写字母<code>A</code>，<code>U+4E25</code>表示汉字<code>严</code>。具体的符号对应表，可以查询<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.unicode.org%2F">unicode.org</a>，或者专门的<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.chi2ko.com%2Ftool%2FCJK.htm">汉字对应表</a>。</p>
<h3 id="unicode-的问题">Unicode 的问题</h3>
<p>这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。</p>
<h3 id="utf-8">UTF-8</h3>
<p><strong>重复一遍，这里的关系是，UTF-8 是 Unicode 的实现方式之一。</strong></p>
<p>UTF-8 最大的一个特点，就是它是一种<strong>变长</strong>的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。</p>
<p>UTF-8 的编码规则很简单，只有二条：</p>
<p>1）对于单字节的符号，字节的第一位设为<code>0</code>，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。</p>
<p>2）对于<code>n</code>字节的符号（<code>n &gt; 1</code>），第一个字节的前<code>n</code>位都设为<code>1</code>，第<code>n + 1</code>位设为<code>0</code>，后面字节的前两位一律设为<code>10</code>。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。</p>
<p>参考：</p>
<p><a href="https://www.jianshu.com/p/51b40820848a">Python编码避坑指南——编码基础知识</a></p>
<p><a href="http://www.cnblogs.com/ysocean/p/6850811.html">Java 字符编码与解码</a></p>
<p><a href="https://blog.csdn.net/sinat_36972314/article/details/79745438">Python常见字符编码及其之间的转换</a></p>
<p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896">字符串和编码python</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>编程方法论</title>
    <url>/programming-methodology/</url>
    <content><![CDATA[<h1>编程方法总结</h1>
<p>首先，本着大道至简的原理，使用sublime text和terminal来查看和运行最终脚本。</p>
<p>其次，由于缺乏自动提示等插件，可以使用jupyter notebook作为打草稿的工具。</p>
<span id="more"></span>]]></content>
      <categories>
        <category>代码</category>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>methodology</tag>
      </tags>
  </entry>
  <entry>
    <title>信息论知识与资源汇总</title>
    <url>/information-theory-1/</url>
    <content><![CDATA[<h1 id="信息">信息</h1>
<p>信息是事物运动状态或存在方式不确定性的描述。不确定性大小 <span class="math inline">\(f(p(x))\)</span> 应该满足以下3个条件：</p>
<ol type="1">
<li><p><span class="math inline">\(f(1)=0\)</span></p></li>
<li><p><span class="math inline">\(f(p(x))\)</span> 是单调减函数</p></li>
<li><p>独立可加性，独立事件应具有增量的信息：<span class="math inline">\(f(p(x) p(y))=f(p(x))+f(p(y))\)</span></p></li>
</ol>
<p>因此，我们定义一个事件 X = x 的 <strong>自信息(self-information)</strong> 为： <span class="math display">\[
I(x) = -logP(x)
\]</span></p>
<p>使用底数为 2 的对数，单位是 <strong>比特(bit)</strong> 或者 <strong>香农(shannons)</strong>。</p>
<p>我们主要使用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。我们可以用 <strong>香农熵(Shannon entropy)</strong> 来对整个概率分布中的不确定性总量进行量化:</p>
<p><span class="math display">\[
H(x) = E_{x \sim P}[I(x)] = -E_{x \sim P}[logP(x)]
\]</span></p>
<p>二值随机变量的熵由 $(p − 1) log(1 − p) − p log p $给出。</p>
<span id="more"></span>
<h2 id="课程">课程</h2>
<p>信息论与编码理论：https://www.icourse163.org/course/XIDIAN-1002199004</p>
<h2 id="书目">书目</h2>
<p>《信息简史》</p>
<p>《信息论与编码理论》</p>
<h2 id="参考">参考</h2>
<p>https://blog.csdn.net/lyxleft/article/details/84867306</p>
<p>https://www.icourse163.org/learn/XDU-1002199004?tid=1463141462#/learn/content?type=detail&amp;id=1240328941&amp;sm=1</p>
]]></content>
      <categories>
        <category>数学</category>
        <category>信息论</category>
      </categories>
      <tags>
        <tag>information theory</tag>
      </tags>
  </entry>
  <entry>
    <title>心理语言学：资源和知识整理</title>
    <url>/relation-extraction-0/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>语言学</category>
        <category>心理语言学</category>
      </categories>
      <tags>
        <tag>psycholinguistics</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫代码-全</title>
    <url>/scraping/</url>
    <content><![CDATA[<h1 id="爬取网站音视频文件并整合">爬取网站音视频文件并整合</h1>
<p>https://www.naturalreaders.com/online/</p>
]]></content>
      <categories>
        <category>代码</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>web scraping</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（下）</title>
    <url>/information-retrieval-3/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i>
<br>
<i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr>
<h1>17.3 抽取时间</h1>
<p>时间和日期是一种特别重要的命名实体，它在自动问答、日历和私人助理应用中起着重要的作用。为了对时间和日期进行推理，在我们提取了这些时间表达式后，必须对它们进行归一化处理——将其转换为标准格式，这样我们才能对它们进行推理。在本节中，我们将同时考虑时间表达式的提取和归一化。</p>
<span id="more"></span>
<h2 id="17-3-1-时间表达式的提取">17.3.1 时间表达式的提取</h2>
<p>时间表达式是指绝对时间点、相对时间、持续时间以及这些的集合。<strong>绝对</strong>（absolute）时间表达式是指那些可以直接相对映射到日历日期、一天中的时间或两者都有的表达式。<strong>相对</strong>（Relative）时间表达式通过其他一些参考点映射到特定的时间（如从上周二开始一周的持续时间）。最后，<strong>持续时间</strong>（durations）表示不同粒度的时间跨度（秒、分、天、周、世纪等）。图17.11列出了这些类别中的一些时间表达式样本。</p>
<table>
<thead>
<tr>
<th>Absolute</th>
<th>Relative</th>
<th>Durations</th>
</tr>
</thead>
<tbody>
<tr>
<td>April 24, 1916</td>
<td>yesterday</td>
<td>four hours</td>
</tr>
<tr>
<td>The summer of ’77</td>
<td>next semester</td>
<td>three weeks</td>
</tr>
<tr>
<td>10:15 AM</td>
<td>two weeks from yesterday</td>
<td>six days</td>
</tr>
<tr>
<td>The 3rd quarter of 2006</td>
<td>last quarter</td>
<td>the last three quarters</td>
</tr>
</tbody>
</table>
<p>: Figure 17.11 Examples of absolute, relational and durational temporal expressions.</p>
<p>时间表达式是以时间词汇触发器<u>lexical triggers</u>为中心的语法结构。词汇触发器可以是名词、专有名词、形容词和副词；完整的时间表达式由它们的短语投射组成：名词短语、形容词短语和副词短语。图17.12提供了示例。</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Noun</td>
<td>morning, noon, night, winter, dusk, dawn</td>
</tr>
<tr>
<td>Proper Noun</td>
<td>January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet</td>
</tr>
<tr>
<td>Adjective</td>
<td>recent, past, annual, former</td>
</tr>
<tr>
<td>Adverb</td>
<td>hourly, daily, monthly, yearly</td>
</tr>
</tbody>
</table>
<p>: Figure 17.12 Examples of temporal lexical triggers.</p>
<p>让我们看看TimeML标注方案，其中时间表达式用XML标签、TIMEX3和该标签的各种属性进行标注（Pustejovsky et al.2005，Ferro et al.2005）。下面的示例说明了此方案的基本用法（我们将对属性的讨论推迟到第17.3.2节）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A fare increase initiated &lt;TIMEX3&gt;last week&lt;&#x2F;TIMEX3&gt; by UAL Corp’s United Airlines was matched by competitors over &lt;TIMEX3&gt;the weekend&lt;&#x2F;TIMEX3&gt;, marking the second successful fare increase in &lt;TIMEX3&gt;two weeks&lt;&#x2F;TIMEX3&gt;.</span><br></pre></td></tr></table></figure>
<p>时间表达式识别任务包括查找与这些时间表达式对应的所有文本跨度的开始和结束。基于规则的时间表达式识别方法使用级联自动机来识别复杂度不断提高的模式。首先标记词性，然后根据包含触发词（如二月）或类（如月份）的模式，从前一阶段的结果中识别出越来越大的语块。图17.13给出了一个基于规则的系统的片段。</p>
<figure class="highlight perl"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yesterday/today/tomorrow</span></span><br><span class="line">$string =˜ s/((($OT+the$CT+\s+)?$OT+day$CT+\s+$OT+(before|after)$CT+\s+)?$OT+$TERelDayExpr$CT+</span><br><span class="line">(\s+$OT+(morning|afternoon|evening|night)$CT+)?)/&lt;TIMEX$tever TYPE=\<span class="string">&quot;DATE\&quot;&gt;$1</span></span><br><span class="line"><span class="string">&lt;\/TIMEX$tever&gt;/gio;</span></span><br><span class="line"><span class="string">$string =˜ s/($OT+\w+$CT+\s+)&lt;TIMEX$tever TYPE=\&quot;DATE\&quot;[ˆ&gt;]*&gt;($OT+(Today|Tonight)$CT+)</span></span><br><span class="line"><span class="string">&lt;\/TIMEX$tever&gt;/$1$4/gso;</span></span><br><span class="line"><span class="string"># this (morning/afternoon/evening)</span></span><br><span class="line"><span class="string">$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+this$CT+\s*$OT+(morning|afternoon|evening)$CT+)/</span></span><br><span class="line"><span class="string">&lt;TIMEX$tever TYPE=\&quot;DATE\&quot;&gt;$1&lt;\/TIMEX$tever&gt;/gosi;</span></span><br><span class="line"><span class="string">$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+last$CT+\s*$OT+night$CT+)/&lt;TIMEX$tever</span></span><br><span class="line"><span class="string">TYPE=\&quot;DATE\&quot;&gt;$1&lt;\/TIMEX$tever&gt;/gsio;</span></span><br></pre></td></tr></table></figure>
<p>: Figure 17.13 Perl fragment from the GUTime temporal tagging system in Tarsqi (Verhagen et al., 2005).</p>
<p>序列标记方法遵循与命名实体标记相同的IOB方案，使用I、O和B标签标记TIMEX3分隔的时间表达式的内部、外部或开头的单词，如下所示：</p>
<p>A O fare O increase O initiated O last B week I by O UAL O Corp’s… O</p>
<p>从标记（token）及其上下文中提取特征，并训练统计序列标注器（可以使用任何序列模型）。图17.14列出了时间标注中使用的标准特征。时间表达式识别器使用通常的recall、precision和F-measure进行评估。所有这些非常词汇化的方法的一个主要困难是避免触发假阳性的表达式：</p>
<blockquote>
<p>(17.15) 1984 tells the story of Winston Smith…</p>
<p>(17.16) …U2’s classic Sunday Bloody Sunday</p>
</blockquote>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Token</td>
<td>The target token to be labeled</td>
</tr>
<tr>
<td>Tokens in window</td>
<td>Bag of tokens in the window around a target</td>
</tr>
<tr>
<td>Shape</td>
<td>Character shape features</td>
</tr>
<tr>
<td>POS</td>
<td>Parts of speech of target and window words</td>
</tr>
<tr>
<td>Chunk tags</td>
<td>Base phrase chunk tag for target and words in a window</td>
</tr>
<tr>
<td>Lexical triggers</td>
<td>Presence in a list of temporal terms</td>
</tr>
</tbody>
</table>
<hr>
<p>: Figure 17.14 Typical features used to train IOB-style temporal expression taggers.</p>
<h2 id="17-3-2-时间归一化">17.3.2 时间归一化</h2>
<p>时间归一化是将时间表达式映射到特定时间点或持续时间的时间规范化的过程。时间点与日历日期、一天中的时间或两者都对应。持续时间主要由时间长度组成，但也可能包括关于起点和终点的信息。标准化时间用ISO 8601标准中的值属性表示，该标准用于编码时间值（ISO86012004）。图17.15再现了我们前面的示例，其中添加了值属性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;TIMEX3 i d &#x3D; ’ ’ t 1 ’ ’ t y p e &#x3D;”DATE” v a l u e &#x3D;” 2007 −07 −02 ” f u n cti o n I n D o c u m e nt &#x3D;”CREATION TIME”</span><br><span class="line">&gt; J u l y 2 , 2007 &lt;&#x2F;TIMEX3&gt; A f a r e i n c r e a s e i n i t i a t e d &lt;TIMEX3 i d &#x3D;” t 2 ” t y p e &#x3D;”DATE”</span><br><span class="line">v a l u e &#x3D;” 2007 −W26” a nc h o rTime ID &#x3D;” t 1 ”&gt;l a s t week&lt;&#x2F;TIMEX3&gt; by U nit e d A i r l i n e s was</span><br><span class="line">matc he d by c o m p e t i t o r s o v e r &lt;TIMEX3 i d &#x3D;” t 3 ” t y p e &#x3D;”DURATION” v a l u e &#x3D;”P1WE”</span><br><span class="line">a nc h o rTime ID &#x3D;” t 1 ”&gt; t h e weekend &lt;&#x2F;TIMEX3&gt; , ma r ki n g t h e s e c o n d s u c c e s s f u l f a r e</span><br><span class="line">i n c r e a s e i n &lt;TIMEX3 i d &#x3D;” t 4 ” t y p e &#x3D;”DURATION” v a l u e &#x3D;”P2W” a nc h o rTime ID &#x3D;” t 1 ”&gt; two</span><br><span class="line">weeks &lt;&#x2F;TIMEX3&gt;.</span><br></pre></td></tr></table></figure>
<p>: Figure 17.15 TimeML markup including normalized values for temporal expressions.</p>
<p>此文本的日期行或文档日期为2007年7月2日。这种表达式的ISO表示为YYYY-MM-DD，在本例中为2007-07-02。我们示例文本中的时间表达式的编码都从这个日期开始，这里显示为VALUE属性的值。</p>
<p>正文中的第一个时间表达式是指一年中的某个特定星期。在ISO标准中，周数从01到53，一年中的第一周是一年中第一个星期四。这些周用模板yyyywnn表示。我们的文件日期的ISO周是第27周；因此上周的值表示为 “2007-W26”。</p>
<p>下一个时间表达式是周末。ISO周从周一开始；因此，周末发生在一周的末尾，并且完全包含在一周内。周末被视为持续时间，因此value属性的值必须是一个长度。持续时间根据模式Pnx表示，其中n是表示长度的整数，x表示单位，如P3Y表示三年，P2D表示两天。在本例中，一个周末被捕获为P1WE。在这种情况下，也有足够的信息锚定这个特定的周末作为一个特定的一周的一部分。这些信息编码在ANCHORTIMEID属性中。最后，短语two weeks还表示作为P2W捕获的持续时间。图17.16描述了表示其他时间和持续时间的一些基本方法。更多细节请参考ISO8601（2004）、Ferro等人（2005）和Pustejovsky等人（2005）。当前大多数时间标准化方法都是基于规则的（Chang和Manning 2012，Strotgen和Gertz 2013）。匹配时态表达式的模式与语义分析过程相关联。就像构图一样</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>认知语言学大纲</title>
    <url>/cognitive-linguistics/</url>
    <content><![CDATA[<p><strong><em>*Syllabus (Spring*</em></strong> <strong><em>*S*</em><em>*emester, 2021)*</em></strong></p>
<p><strong><em>*COURSE TITLE:*</em></strong> <strong><em>*Introduction to C*</em><em>*ognitive*</em></strong> <strong><em>*Linguistics*</em></strong> <strong><em>*认知语言学*</em></strong></p>
<figure>
<img src="file:///C:\Users\13607\AppData\Local\Temp\ksohtml15088\wps1.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><em>*INSTRUCTOR: LAN CHUN EMAIL:*</em></strong> beiwailanchun@126.com</p>
<p><strong><em>*STUDENTS: SEIS postgraduate*</em></strong> grade 2018</p>
<p><strong><em>*TIME: 10:10-12:00,Thursday*</em></strong> <strong><em>*PLACE*</em><em>*：S*</em><em>*EIS Building 417*</em></strong></p>
<p><strong><em>*OFFICE:*</em></strong> Room 202, SEIS Building <strong><em>*OFFICE HOURS:*</em></strong> 10:10-12.00, Friday</p>
<figure>
<img src="file:///C:\Users\13607\AppData\Local\Temp\ksohtml15088\wps2.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><em>*I. OBJECTIVES*</em></strong></p>
<p>This course aims at introducing to students the basic concepts, theories, approaches, research methods and hot issues of cognitive linguistics. It is expected that at the end of the course, students can have a general idea of the intricate relationships between language and cognition and of the various cognitive perspectives of doing linguistic research. Students are also expected to be able to apply the basic theoretical frameworks and methods of cognitive linguistics to the analysis of authentic data from different languages and cultures.</p>
<p><strong><em>*II. CLASS SCHEDULE*</em></strong></p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th><strong><em>*Week*</em></strong></th>
<th><strong><em>*Date*</em></strong></th>
<th><strong><em>*Content*</em></strong></th>
<th><strong><em>*Assignment*</em></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Mar. 4</td>
<td>An overview of cognitive science</td>
<td>Assigned reading 1: <strong>Metaphors We Live By</strong></td>
</tr>
<tr class="even">
<td>2</td>
<td>Mar. 11</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Mar. 18</td>
<td>The theory of prototype and basic-level categories</td>
<td>Assigned reading 2:<strong>Women, Fire and Dangerous Things: What Categories Reveal about the Mind</strong></td>
</tr>
<tr class="even">
<td>4</td>
<td>Mar. 25</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>April 1</td>
<td>Objectivism, subjectivism and embodied philosophy</td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>April 8</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>April 15</td>
<td>The prominence view and the attentional view of cognitive linguistics</td>
<td>Assigned reading 3:<strong>Cognitive Poetics: An Introduction</strong></td>
</tr>
<tr class="even">
<td>8</td>
<td>April 22</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>9</td>
<td>April 29</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>10</td>
<td>May 6</td>
<td>Conceptual metaphor and conceptual metonymy</td>
<td></td>
</tr>
<tr class="odd">
<td>11</td>
<td>May 13</td>
<td>Mini-research projects on assigned topics</td>
<td></td>
</tr>
<tr class="even">
<td>12</td>
<td>May 20</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>13</td>
<td>May 27</td>
<td>Workshop</td>
<td></td>
</tr>
<tr class="even">
<td>14</td>
<td>June 3</td>
<td>Cognitive grammar and cognitive semantics</td>
<td>Preparing for final exam/term paper</td>
</tr>
<tr class="odd">
<td>15</td>
<td>June 10</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>16</td>
<td>June 17</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>17</td>
<td>Exam Week</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>18</td>
<td>Exam Week</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong><em>*III. COURSE REQUIREMENTS*</em></strong></p>
<p>Students are expected to attend all the lectures, finish all the assigned reading and written work properly, contribute actively to both in-class and outside-class group discussions and team work, and take the final exam / submit a term paper at the end of the course.</p>
<p><strong><em>*IV. TEACHING APPROACH*</em></strong></p>
<p>This is a lecture-based course. The instructor will prepare detailed handouts and will use PPT to assist her teaching. Group discussions will be organized in class. Students are also expected to carry out mini research projects in small groups on assigned topics.</p>
<p><strong><em>*V. COURSE MATERIALS*</em></strong></p>
<p><strong><em>*1. Required Materials (or Readings)*</em></strong></p>
<p>蓝纯 2005 《认知语言学与隐喻研究》 北京：外语教学与研究出版社。</p>
<p>Evans, V. &amp; M. Green. 2006. <strong>Cognitive Linguistics: An Introduction</strong>. Berlin: Lawrence Erlbaum Associates Publishers.</p>
<p>Ungerer, F., &amp; Schmid, H.-J. 2006. <strong>An Introduction to Cognitive Linguistics</strong>. Beijing: FLTRP.</p>
<p><strong><em>*2. Recommended Materials (or Readings)*</em></strong></p>
<p>Croft, William and Alan Cruse. 2004. <strong>Cognitive Linguistics</strong>. Cambridge: Cambridge University Press.</p>
<p>Evans, Vyvyan, Benjamin K. Bergen and Jörg Zinken (eds.). 2007. <strong>The Cognitive Linguistic Reader</strong>. London: Equinox Publishing Company.</p>
<p>Lakoff, George, &amp; Johnson, Mark, 1980. <strong>Metaphors We Live By</strong>. Chicago: University of Chicago Press.</p>
<p>Johnson, Mark. 1987. <strong>The Body in the Mind</strong>. Chicago: University of Chicago Press.</p>
<p>Lakoff, George. 1987. <strong>Women, Fire, and Dangerous Things</strong>. Chicago: University of Chicago Press.</p>
<p>Langacker, Ronald W. 1987. <strong>Foundations of Cognitive Grammar: Theoretical Prerequisites</strong>. Stanford: Stanford University Press.</p>
<p>Langacker, Ronald W. 1991. <strong>Foundations of Cognitive Grammar: Descriptive Application</strong>. Stanford: Stanford University Press.</p>
<p>Lefrancois, G. R. 2004. <strong>Theories of Human Learning</strong>. Beijing: FLTRP.</p>
<p>Stockwell, Peter. 2002. <strong>Cognitive Poetics: An Introduction</strong>. London/New York: Routledge.</p>
<p>Talmy, Leonard. 2000. <strong>Toward a Cognitive Semantics</strong>. New York: MIT Press.</p>
<p><strong><em>*VI. ASSESSMENT*</em></strong></p>
<p>Students’ final score will consist of the following parts:</p>
<p><strong>1.</strong> <strong><em>*Post-class Work/Quiz/*</em></strong> <strong><em>*Assignments/*</em><em>*…*</em><em>*: 30%*</em></strong></p>
<p><strong>2.</strong> <strong><em>*C*</em><em>*lass*</em></strong> <strong><em>*Participation*</em></strong> <strong><em>*and Attendance: 20%*</em></strong></p>
<p>(<strong><em>*Absence of one third of class time*</em></strong>, excused or unexcused, will disqualify you from earning <strong><em>*credits for the course*</em></strong>. Showing up late for class or leaving before class is dismissed will also be penalized. <strong><em>*Two such records*</em></strong> will take one point off from your final marks.)</p>
<p><strong>3.</strong> <strong><em>*Final Exam and/or Term Paper: 50%*</em></strong></p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>认知语言学</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（中）</title>
    <url>/information-retrieval-2/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<h1 id="关系抽取算法">17.2 关系抽取算法</h1>
<p>关系抽取的算法主要有五类：<strong>手写模式</strong>、<strong>监督机器学习</strong>、<strong>半监督</strong>（通过<strong>bootstrapping</strong>和通过<strong>远程监督</strong>）以及<strong>无监督</strong>。我们将在接下来的章节中分别介绍这些算法。</p>
<h2 id="使用模式抽取关系">17.2.1 使用模式抽取关系</h2>
<p>最早并且现在依然常用的关系抽取算法是词法-句法模式（ <strong>lexico-syntactic pattern</strong>），由Hearst（1992a）第一个开发，因此通常被称为<u>Hearst patterns</u>。例如以下句子：</p>
<blockquote>
<p>Agar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use.</p>
<p>琼脂是一种由红藻（如Gelidium）混合制备的物质，用于实验室或工业用途。</p>
</blockquote>
<p>赫斯特指出，大多数人类读者不会知道Gelidium是什么，但他们可以很容易推断出它是一种红藻（的<u>下义词</u><em>hyponym</em>）。她认为以下词法-句法模式：</p>
<p><span class="math display">\[
N P_{0} \text { such as } N P_{1}\left\{, N P_{2} \ldots,(\text { and } \mid \text { or }) N P_{i}\right\}, i \geq 1
\]</span></p>
<p>意味着以下语义 <span class="math display">\[
\forall N P_{i}, i \geq 1, \text { hyponym }\left(N P_{i}, N P_{0}\right)
\]</span> 让我们可以推断出 <span class="math display">\[
hyponym(Gelidium,red algae)
\]</span></p>
<table>
<caption>Figure 17.5 Hand-built lexico-syntactic patterns for finding hypernyms, using {} to mark optionality (Hearst 1992a, Hearst 1998).</caption>
<colgroup>
<col style="width: 40%" />
<col style="width: 59%" />
</colgroup>
<thead>
<tr class="header">
<th>NP {, NP}* {,} (and|or) other NP<sub>H</sub></th>
<th>temples, treasuries, and other important <strong>civic buildings</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NP<sub>H</sub> such as {NP,}* {(or|and)} NP</td>
<td><strong>red algae</strong> such as Gelidium</td>
</tr>
<tr class="even">
<td>such NPH as {NP,}* {(or|and)} NP</td>
<td>such <strong>authors</strong> as Herrick, Goldsmith, and Shakespeare</td>
</tr>
<tr class="odd">
<td>NP<sub>H</sub> {,} including {NP,}* {(or|and)} NP</td>
<td><strong>common-law countries</strong>, including Canada and England</td>
</tr>
<tr class="even">
<td>NP<sub>H</sub> {,} especially {NP}* {(or|and)} NP</td>
<td><strong>European countries</strong>, especially France, England, and Spain</td>
</tr>
</tbody>
</table>
<span id="more"></span>
<p>图17.5显示了Hearst(1992a, 1998)提出的五种模式，用于推断上下位关系；我们用NP<sub>H</sub>表示parent/hyponym。现代版本的基于模式的方法通过增加命名实体约束来扩展它。例如，如果我们的目标是回答关于“谁在哪个组织中担任什么职务”的问题，我们可以使用如下模式。</p>
<p><img src="https://i.loli.net/2021/03/18/iyZINETdVrXG3cn.png" width="500"/></p>
<p>手工构建的模式具有高精度的优势，并且可以针对特定领域进行定制。但另一方面，它们通常是低召回率的，而且如果要创建所有可能的模式，工作量很大。</p>
<h2 id="通过监督学习进行关系抽取">17.2.2 通过监督学习进行关系抽取</h2>
<p>监督机器学习的关系抽取方法遵循的是一种大家现在应该很熟悉的方案。选择一组固定的关系和实体，用这些关系和实体手动标注训练语料，然后用标注后的文本来训练分类器标注一个未出现过的测试集。</p>
<p>最直接的方法，如图17.6所示：(1) 找到命名的实体对(通常在同一个句子中)；(2) 为每对实体进行关系分类。分类器可以使用任何有监督的技术（逻辑回归、RNN、Transformer、随机森林等）。</p>
<p>一个可选的中间过滤分类器可以用来加快处理速度，通过二元决策判定给定的一对实体是否相关（通过任何关系）。它的训练对象是直接从标注语料中的所有关系中抽取的正例，以及从未标注关系的句内实体对中生成的负例。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">FINDRELATIONS</span><span class="params">(words)</span> returns relations</span></span><br><span class="line">	relations←nil</span><br><span class="line">	entities←FINDENTITIES(words)</span><br><span class="line">	forall entity pairs &lt;e1, e2&gt; in entities <span class="keyword">do</span></span><br><span class="line">	<span class="keyword">if</span> RELATED?(e1, e2)</span><br><span class="line">		relations←relations+CLASSIFYRELATION(e1, e2)</span><br></pre></td></tr></table></figure>
<p>基于特征的监督关系分类器。让我们考虑基于特征的分类器（如逻辑回归或随机森林）的样本特征，对这个句子中的<em>美国航空公司</em>（Mention 1，或M1）和<em>Tim Wagner</em>（Mention 2，M2）之间的关系进行分类。</p>
<blockquote>
<p>(17.5) <strong>American Airlines</strong>, a unit of AMR, immediately matched the move, spokesman <strong>Tim Wagner</strong> said</p>
</blockquote>
<p>这些包括词的特征（如词嵌入、one-hot、无论是否抽取词干<em>stemming</em>）：</p>
<blockquote>
<ul>
<li><p>M1和M2的核心词（headwords）及其连接 （concatenation）</p>
<p><strong>Airlines Wagner Airlines-Wagner</strong></p></li>
<li><p>M1和M2的词袋和ngram</p>
<p><strong>American, Airlines, Tim, Wagner, American Airlines, Tim Wagner</strong></p></li>
<li><p>特定位置的词和ngram</p>
<p>M2: <strong>-1 spokesman</strong></p>
<p>M2: <strong>+1 said</strong></p></li>
<li><p>M1和M2之间的词袋或ngram</p>
<p><strong>a, AMR, of, immediately, matched, move, spokesman, the, unit</strong></p></li>
</ul>
</blockquote>
<p>以及<strong>命名实体</strong>特征：</p>
<blockquote>
<ul>
<li><p>命名实体类型及其连接（concatenation）</p>
<p>M1: <strong>ORG</strong>, M2: <strong>PER</strong>, M1M2: <strong>ORG-PER</strong></p></li>
<li><p>M1和M2的实体级别（从集合NAME、NOMINAL、PRONOUN中选择）。</p>
<p>M1: <strong>NAME</strong> [it or he would be <strong>PRONOUN</strong>]</p>
<p>M2: <strong>NAME</strong> [the company would be <strong>NOMINAL</strong>]</p></li>
<li><p>参数之间的实体数量（在本例中，对于AMR）。</p></li>
</ul>
</blockquote>
<p><strong>句法结构</strong>是一个有用的信号，通常表示为依存关系或穿越实体之间的树的成分<strong>句法路径</strong>。</p>
<blockquote>
<ul>
<li><p>M1和M2之间的成分路径</p>
<p><strong>NP ↑ NP ↑ S ↑ S ↓ NP</strong></p></li>
<li><p>依存树路径</p>
<p><strong>Airlines ←<sub>subj</sub> matched ←<sub>comp</sub>said →<sub>subj</sub> Wagner</strong></p></li>
</ul>
</blockquote>
<p><strong>神经监督关系分类器</strong> 关系抽取的神经模型同样将这个任务视为监督分类。让我们考虑一个应用于TACRED关系抽取数据集和任务的典型系统 (Zhang et al., 2017) 。在TACRED中，提供一个句子和其中的两个spans：主语，即人或组织；宾语，即任何其他实体。任务是从42个TAC关系中分配一个关系（或者没有关系）。</p>
<p>一个典型的Transformer-encoder算法，如图17.7所示，简单的说就是把一个像BERT这样的预训练编码器，在句子表示的基础上增加一个线性层（例如BERT [CLS] token），这个线性层被微调为1-of-N分类器，以分配43个标注中的一个。BERT编码器的输入是部分去词汇化的；主语和宾语实体在输入中被其NER标注所取代。这有助于保持系统不会对单个词项过度拟合 (Zhang et al., 2017) 。当使用BERT类型的Transformers进行关系抽取时，使用类似RoBERTa (Liu et al., 2019) 或SPANbert (Joshi et al., 2020) 这样的BERT版本会很有帮助，这些版本没有用[SEP]标记分隔的两个序列，而是将单个长序列的句子形成输入。</p>
<p><img src="https://i.loli.net/2021/03/18/mpbfZs9QJoPnv7a.png" width="700"/></p>
<p>一般来说，如果测试集与训练集足够相似，并且有足够多的手工标注数据，监督关系抽取系统可以获得很高的准确率。但对一个庞大的训练集进行标注是非常昂贵的，而且监督模型也很脆弱：它们不能很好地泛化到不同的文本类型。由于这个原因，关系抽取的很多研究都集中在我们接下来要讲的半监督和无监督方法上。</p>
<h2 id="通过-bootstrapping-进行半监督关系抽取">17.2.3 通过 Bootstrapping 进行半监督关系抽取</h2>
<p>监督机器学习假设我们有大量的标注数据。不幸的是，这很昂贵。但假设我们只有一些高精度的<u>种子模式</u>（seed patterns），就像第17.2.1节中的那些，或者可能有一些<u>种子元组</u>（seed tuples）。这已经足够boostrap一个分类器了！<u>Bootstrapping</u>的过程是获取种子对中的实体，然后找到包含这两个实体的句子（通过网络或者我们使用的任何数据集）。从所有这些句子中，我们抽取并归纳实体周围的上下文，以学习新的模式。图17.8给出了一个基本算法。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">BOOTSTRAP</span><span class="params">(Relation R)</span> returns <span class="keyword">new</span> relation tuples</span></span><br><span class="line">	tuples←Gather a set of seed tuples that have relation R</span><br><span class="line">	iterate</span><br><span class="line">		sentences←find sentences that contain entities in tuples</span><br><span class="line">		patterns←generalize the context between <span class="keyword">and</span> around entities in sentences</span><br><span class="line">		newpairs←use patterns to grep <span class="keyword">for</span> more tuples</span><br><span class="line">		newpairs←newpairs with high confidence</span><br><span class="line">		tuples←tuples + newpairs</span><br><span class="line">	<span class="keyword">return</span> tuples</span><br></pre></td></tr></table></figure>
<p>假设我们需要创建一个航空公司/枢纽对的列表，我们只知道Ryanair在Charleroi有一个枢纽。我们可以使用这个种子事实来发现新的模式，在我们的语料库中找到这个关系的其他提及。我们搜索Ryanair、Charleroi和hub在某种程度上接近的术语。也许我们会发现以下一组句子。</p>
<blockquote>
<p>(17.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all weekend flights out of the airport.</p>
<p>以沙勒罗瓦为枢纽的低价航空公司Ryanair取消了所有周末从机场起飞的航班。</p>
<p>(17.7) All flights in and out of Ryanair’s hub at Charleroi airport were grounded on Friday...</p>
<p>所有进出沙勒罗伊机场枢纽的航班都在周五停飞。</p>
<p>(17.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000 passengers had already been affected.</p>
<p>瑞安航空的核心枢纽查勒罗伊的一位发言人估计，已经有8000名乘客受到影响。</p>
</blockquote>
<p>从这些结果中，我们可以利用实体提及之间的上下文单词、提及一之前的词、提及二之后的词，以及两个提及的命名实体类型，还有其他可能的特征，来抽取如下的通用模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F; [ORG], which uses [LOC] as a hub &#x2F; </span><br><span class="line">&#x2F; [ORG]’s hub at [LOC] &#x2F; </span><br><span class="line">&#x2F; [LOC], a main hub for [ORG] &#x2F;</span><br></pre></td></tr></table></figure>
<p>这些新的模式可以继而用来搜索更多的元组。Bootstrapping系统还为新的元组分配了<u>置信度值</u>（confidence values），以避免<u>语义漂移</u>（semantic drift）。在语义漂移中，一个错误的模式会导致错误元组的引入，而这些元组又会导致不正确的模式的建立，使得抽取关系的意义发生漂移。</p>
<p>比如下面的例子。</p>
<blockquote>
<p>(17.9) Sydney has a ferry hub at Circular Quay.</p>
<p>(17.9) 悉尼在环形码头有一个轮渡枢纽。</p>
</blockquote>
<p>如果被接受为正例，这个表达式可能导致元组<span class="math inline">\(&lt;Sydney,CircularQuay&gt;\)</span>的错误引入。基于这个元组的模式可能会将更多的错误传播到数据库中。</p>
<p>模式的置信度值基于两个因素的平衡：模式相对于当前元组集的表现，以及模式就在文档集中产生的匹配数量而言的生产率。更正式的表达是，给定一个文档集 <span class="math inline">\(D\)</span>，一个当前的元组集 <span class="math inline">\(T\)</span>，以及一个引入的模式 <span class="math inline">\(p\)</span>，我们需要跟踪两个因素：</p>
<ul>
<li><p><span class="math inline">\(hits(p)\)</span>：在<span class="math inline">\(D\)</span>中查找时，<span class="math inline">\(p\)</span>所匹配的<span class="math inline">\(T\)</span>中的元组集</p></li>
<li><p><span class="math inline">\(finds(p)\)</span>：<span class="math inline">\(p\)</span>在<span class="math inline">\(D\)</span>中找到的元组的总集</p></li>
</ul>
<p>下面的公式平衡了这些考虑因素 (Riloff and Jones, 1999)： <span class="math display">\[
\operatorname{Conf}_{R \log F}(p)=\frac{|\operatorname{hits}(p)|}{|\operatorname{finds}(p)|} \log (|\operatorname{finds}(p)|)
\]</span> 这个度量标准一般经过标准化处理，产生一个概率。我们可以通过结合<span class="math inline">\(D\)</span>中与该元组相匹配的所有模式<span class="math inline">\(P&#39;\)</span>中支持该元组的证据来评估所提出的新元组的置信度 (Agichtein and Granoisy-or vano, 2000)。结合这种证据的一种方法是<u>noisy-or</u>技术。假设一个给定的元组是由<span class="math inline">\(P\)</span>中的一个模式子集支持的，每个模式都有自己的置信度，正如上面的度量所示。在noisy-or模型中，我们做了两个基本假设。首先，如果一个提出的元组是假的，它的所有支持模式一定是错误的；其次，它们各自失败的来源都是独立的。如果我们不严格地将我们的置信度视为概率，那么任何单个模式<span class="math inline">\(p\)</span>失败的概率为<span class="math inline">\(1-Conf(p)\)</span>；一个元组的所有支持模式出错的概率是它们单个失败概率的乘积，因此我们对一个新元组的置信度有以下公式： <span class="math display">\[
\operatorname{Conf}(t)=1-\prod_{p \in P^{\prime}}(1-\operatorname{Conf}(p))
\]</span> 在bootstrapping过程中，为接受新的模式和元组设置保守的置信度阈值，有助于防止系统偏离目标关系。</p>
<h2 id="远程监督式关系抽取">17.2.4 远程监督式关系抽取</h2>
<p>虽然手工标注文本的关系标注成本很高，但是有一些方法可以找到间接的训练数据来源。<u>远程监督</u>方法 <em>distant supervision</em> (Mintz et al., 2009) 结合了bootstrapping和监督学习的优点。远程监督不是只用少量的种子，而是使用一个大型数据库来获取大量的种子实例，从所有这些例子中创建大量的噪声模式特征（noisy pattern features），然后将它们结合在一个监督分类器中。例如，假设我们要学习人与出生城市之间的出生地关系。在基于种子的方法中，我们一开始可能只有5个例子。但基于维基百科的数据库，如<a href="https://en.wikipedia.org/wiki/DBpedia">DBPedia</a>或<a href="https://en.wikipedia.org/wiki/Freebase_(database)">Freebase</a>，有数万个许多关系的例子；包括超过10万个出生地的例子（<span class="math inline">\(&lt;Edwin Hubble，Marshfield&gt;\)</span>，<span class="math inline">\(&lt;Albert Einstein，Ulm&gt;\)</span>等）。下一步是在大量的文本上运行命名实体标注器-——Mintz et al. (2009) 使用了维基百科上的80万篇文章，并抽取出所有具有与元组相匹配的两个命名实体的句子，比如：</p>
<blockquote>
<p>...Hubble was born in Marshfield...</p>
<p>...Einstein, born (1879), Ulm...</p>
<p>...Hubble’s birthplace in Marshfield...</p>
</blockquote>
<p>现在可以从这些数据中抽取训练实例，每个元组&lt;关系，实体1，实体2&gt;都有一个相同的训练实例。因此，每个如下的元组都会有一个训练实例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;born-in, Edwin Hubble, Marshfield&gt;</span><br><span class="line">&lt;born-in, Albert Einstein, Ulm&gt;</span><br><span class="line">&lt;born-year, Albert Einstein, 1879&gt;</span><br></pre></td></tr></table></figure>
<p>然后我们可以应用基于特征或神经网络的分类。对于基于特征的分类，标准的监督关系抽取特征，比如两个提及的命名实体标注，提及之间的词和依存路径，以及相邻的词。每个元组都会有从许多训练实例中收集到的特征；像<span class="math inline">\(&lt;born-in,Albert Einstein,Ulm&gt;\)</span>这样的单个训练实例的特征向量将含有来自许多提到Einstein和Ulm的不同句子的词法和句法特征）。</p>
<p>因为远程监督有非常大的训练集，所以它也能够使用非常丰富的特征，这些特征是这些单个特征的联合体。所以我们会抽取出成千上万的模式，这些模式会将实体类型与中间的单词或依存路径结合在一起，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PER was born in LOC </span><br><span class="line">PER, born (XXXX), LOC </span><br><span class="line">PER’s birthplace in LOC</span><br></pre></td></tr></table></figure>
<p>回到我们的运行示例，对于这个句子：</p>
<blockquote>
<p>(17.12) American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said</p>
</blockquote>
<p>我们将学习丰富的连词（conjunction）特征，比如：</p>
<blockquote>
<p>M1 = ORG &amp; M2 = PER &amp; nextword=“said”&amp; path= <strong>NP ↑ NP ↑ S ↑ S ↓ NP</strong></p>
</blockquote>
<p>结果是一个监督分类器，它有大量丰富的特征集用于检测关系。由于并不是每个测试句子都会有一个训练关系，所以分类器还需要能够将一个例子标记为无关系。这个标注是通过随机选择在任何Freebase关系中都未出现的实体对，抽取它们的特征，并为每个这样的元组建立一个特征向量来训练的。最终的算法在如图17.9所示。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function DISTANT <span class="title">SUPERVISION</span><span class="params">(Database D, Text T)</span> returns relation classifier C</span></span><br><span class="line"><span class="function">	foreach relation R</span></span><br><span class="line"><span class="function">		foreach <span class="title">tuple</span> <span class="params">(e1,e2)</span> of entities with relation R in D</span></span><br><span class="line">			sentences←Sentences in T that contain e1 and e2</span><br><span class="line">			f←Frequent features in sentences</span><br><span class="line">			observations←observations + <span class="keyword">new</span> training tuple (e1, e2, f, R)</span><br><span class="line">		C←Train supervised classifier on observations</span><br><span class="line">	<span class="keyword">return</span> C</span><br></pre></td></tr></table></figure>
<p>远程监督享有我们所探讨的每一种方法的优势。像监督分类一样，远程监督使用一个具有大量特征的分类器，并通过详细的手工创建的知识进行监督。和基于模式的分类器一样，它可以利用高精度的证据来证明实体之间的关系。事实上，远程监督系统学习到的模式就像早期关系抽取器手工构建的模式。例如Snow et al. (2005)的is-a或hypernym抽取系统使用来自WordNet的hypernym/hyponym NP对作为远程监督，然后从大量的文本中学习新的模式。他们的系统精确地产生了Hearst（1992a）最初的5个模板模式，但也导致了包括这4个模式在内的7万个额外的模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NPH like NP Many hormones like leptin... </span><br><span class="line">NPH called NP ...using a markup language called XHTML </span><br><span class="line">NP is a NPH Ruby is a programming language... </span><br><span class="line">NP, a NPH IBM, a company with a long...</span><br></pre></td></tr></table></figure>
<p>这种同时使用大量特征的能力意味着，与基于种子的系统中模式的迭代扩展不同，没有语义漂移的发生。与无监督分类一样，它不使用有标注的训练语料库，而是依赖于非常大量的无标注数据，因此对训练语料库中的体裁（genre）问题不敏感。远程监督的另一个优势是，它可以创建训练元组，以便与神经网络分类器一起使用，而神经网络分类器不需要特征。远程监督的主要问题是，它往往会产生低精度的结果，因此目前的研究重点是如何提高精度。此外，远程监督只能帮助抽取已经存在的足够大的数据库的关系。如果要在没有数据集的情况下抽取新的关系，或者新领域的关系，就必须使用纯无监督的方法。</p>
<h2 id="关系抽取的评估">17.2.6 关系抽取的评估</h2>
<p><strong>有监督</strong>的关系抽取系统是通过使用带有人类标注的gold-standard关系的测试集并计算精度、召回率和 F-measure 来评估的。标注精度和召回率要求系统正确分类关系，而未标注的方法只是衡量系统检测相关实体的能力。</p>
<p><strong>半监督</strong>和<strong>无监督</strong>方法更难评估，因为它们是从网络或大型文本中抽取全新的关系。由于这些方法使用了非常多的文本，所以通常不可能只在一个小的标注测试集上运行它们，因此不可能预先标注出一个正确的关系实例的gold set。</p>
<p>对于这些方法，可以通过从输出中随机抽取一个关系样本来（仅仅是）近似精确性，并让人检查这些关系的准确性。通常，这种方法侧重于从文本中抽取的<strong>元组</strong>，而不是关系的<strong>提及</strong>；系统不需要检测每一个关系的提及来正确评分。相反，评估是基于系统完成任务时数据库中包含的元组集。也就是说，我们想知道系统是否能发现 Ryanair 在 Charleroi 有一个枢纽；我们并不关心它发现了多少次。那么，估计的精度<span class="math inline">\(\hat{P}\)</span> 就是： <span class="math display">\[
\hat{P}=\frac{\# \text { of correctly extracted relation tuples in the sample }}{\text { total } \# \text { of extracted relation tuples in the sample. }}
\]</span> 译：P = 样本中正确提取的关系元组 / 样本中提取的关系元组总数</p>
<p>另一种能给我们提供一点召回信息的方法是计算不同召回级别的精度。假设我们的系统能够对它所产生的关系进行排序（按照概率或置信度），我们可以分别计算前1000个新关系、前10000个新关系、前100000个新关系的精度等等。在每一种情况下，我们都会从该集合中随机抽取一个样本。这将向我们展示当我们抽取越来越多的元组时，精度曲线的表现。但是没有办法直接评估召回率。</p>
<hr />
<p><strong>本章剩余内容见：《自然语言处理综论》第17章-信息抽取（下）</strong></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>Git和Github常用操作大全</title>
    <url>/git/</url>
    <content><![CDATA[<blockquote>
<p>Git是目前世界上最先进的分布式版本控制系统（没有之一）。Git的功能有版本控制（版本管理、远程仓库、分支协作）。GitHub是一个非常流行的全球代码托管平台.</p>
</blockquote>
<p>参考：<a href="https://blog.csdn.net/Python_Ai_Road/article/details/109476021">30分钟吃掉Git和GitHub常用操作</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">git init</span><br><span class="line">git status</span><br><span class="line"></span><br><span class="line">git add -A </span><br><span class="line">git add . </span><br><span class="line">git add <span class="built_in">next</span>/_config.yml</span><br><span class="line"></span><br><span class="line">git commit -m <span class="string">&quot;comment&quot;</span> </span><br><span class="line"></span><br><span class="line">git remote add origin https://github.com/XX/XX</span><br><span class="line">git push -u origin master</span><br><span class="line"></span><br><span class="line">git clone https://github.com/XX/XX  ../XX</span><br><span class="line">    </span><br><span class="line">git remote -v</span><br><span class="line">git remote rm origin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除git</span></span><br><span class="line">rm -rf .git</span><br><span class="line"></span><br><span class="line">git reset HEAD^ <span class="comment">#可以回退到上一个版本。</span></span><br><span class="line">git reset HEAD^^ <span class="comment">#可以回退到上上个版本。</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
]]></content>
      <categories>
        <category>代码</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>从小白到起飞，一站解决Atom编辑器各种骚操作</title>
    <url>/atom/</url>
    <content><![CDATA[<blockquote>
<p>一站到底解决IDE搭建问题！从基础设置、插件安装、snippets填充、到github版本控制、代码调试……本文将不断更新以求完善！</p>
</blockquote>
<h2 id="安装文本编辑器atom">安装文本编辑器Atom</h2>
<p>入坑Sublime、VSCode无数次后，还是回头选择了Atom IDE，因为它颜值惊艳、操作便捷、界面简单，除了许多编辑器都有的<strong>代码折叠</strong>和<strong>自动补全</strong>，还自带原生Markdown支持！！！这漂亮的实时预览和代码高亮真让人春心荡漾！并且，插件非常丰富！ <span id="more"></span> 于是，果断选择Atom作为本博主的<strong>创作神器</strong>，为Python开发之旅保驾护航！</p>
<p>官网一键安装：<a href="https://atom.io/">AtomSetup-x64</a></p>
<p>确认操作系统无误，点击download，打开下载好的AtomSetup-x64.exe，极速体验！</p>
<figure>
<img src="https://pic2.zhimg.com/v2-e2e305deea69303a19d71da452138945_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>缺点是启动速度不如sublime</em></p>
<h2 id="安装插件失足卡顿的惨痛经历">安装插件：失足卡顿的惨痛经历</h2>
<p>此时你一定急不可耐地冲向Install a Package，风风火火地下载了一堆插件：minimap用来预览全貌，atom-beautify用来格式化（想到令人头痛的html），file-icons小图标好可爱呀，material主题貌似很热门吼，markdown-preview-enhanced吊打我的Typora呢（Typora是我常用的md编辑器），script可以运行代码嗷，autocomplete-python自动补全呢，python-autopep8调格式也不错哟，linter-flake8检查语法错误呢，Hydrogen简直是jupyter的孪生姐妹……你在界面乐此不疲地倒腾……</p>
<p>A few hours later...</p>
<p>突然，你意识到此时的atom一片混乱卡顿缓慢，再也不是当初清纯活泼的模样！你蹙起眉，两行清泪润湿了乌黑的下眼眶！</p>
<p>一怒之下，你卸载了atom，并剿杀了一切软件残留：<a href="https://cn.compbs.com/how-uninstall-atom-windows">如何彻底删除atom</a>，<a href="https://www.coder.work/article/552966">如何更彻底地删除</a>！</p>
<h2 id="正确的打开方式是什么">正确的打开方式是什么？</h2>
<p>正文从这里开始！</p>
<p>那么，配置环境和安装插件的正确方式是什么呢？</p>
<ol type="1">
<li>打开Editor Settings，<strong>勾选Scroll Past End和Show Indent Guide，设置Tab Length为4，Font Size为20</strong>，护眼第一啦！</li>
<li>主题我喜欢atom自带的<strong>One Light，</strong>打开themes-&gt;One Light UI-&gt;Settings-&gt;<strong>Font Size设置为15</strong>，语法主题我选择<strong>Monokai</strong>。还是为了护眼！码字时还可以随时ctrl+和ctrl-调整字体大小。</li>
<li>下载插件：<strong>minimap、file-icons、atom-beautify、markdown-preview-enhanced、python-autopep8、markdown-writer</strong>。大道至简，这是我目前选择的基础组件，可能以后会更新！进入python-autopep8-&gt;Settings，勾选Format On Save。</li>
</ol>
<figure>
<img src="https://pic3.zhimg.com/v2-62a1e257adc2ca9978dee539798d3342_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>atom-material-syntax-dark语法主题也好看，只是Python语法高亮略丑，就不放了</em></p>
<p>个人认为，<strong>编辑器只需提供高效、舒适的代码体验即可</strong>，各种花里胡哨的功能比较鸡肋，反而会加重的负担。</p>
<p><a href="https://atom.io/themes/list?direction=desc&amp;sort=downloads">atom热门主题排行榜</a>：material、monokai、seti。</p>
<p><a href="https://atom.io/packages/list?direction=desc&amp;sort=stars">atom热门插件排行榜</a>：minimap、file-icons、atom-beautify、linter、script。</p>
<p><strong>下面重点来了，下载插件的方式！！！</strong></p>
<p>打开cmd，执行以下命令（apm是Atom Package Manager的缩写）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install autopep8</span><br><span class="line">apm install monokai, minimap, file-icons, atom-beautify, markdown-preview-enhanced, python-autopep8</span><br></pre></td></tr></table></figure>
<p>如果一起下载速度太慢，也可以apm install <package_name>分开下载。</p>
<p>晃悠了一杯茶的时间，已经全部done啦！Voila！</p>
<p>重启Atom，Ctrl+Shift+P打开Settings-&gt;Packages，是不是整齐陈列着我们要的插件呢？（不知为什么ctrl+逗号不能打开settings）</p>
<p>码上行动叭！！！</p>
<figure>
<img src="https://pic3.zhimg.com/v2-873f85f52366e6ab44b3eda548930102_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>这是最终的windows界面</em></p>
<p>另外，有童鞋选择github上面的源码git clone，然后cd进文件夹、npm install来安装，也不错呢~</p>
<p>对了，卸载插件的话，apm uninstall <package_name> 就好啦！</p>
<p>----------2021-03-04更新---------</p>
<h2 id="代码填充功能snippets">代码填充功能Snippets</h2>
<p>当我们需要重复使用一套模板时，不如试试<a href="https://www.jianshu.com/p/2ee34d8da142">Atom自带的Snippets代码块功能</a>，这种快捷填充，省去了重复码字的时间。打开终端输入下面指令，用atom打开snippets.cson：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">atom C:\Users\用户名\.atom\snippets.cson</span><br></pre></td></tr></table></figure>
<p>可以看到目前空空如也，在这个文件里面打“snip”，然后敲下tab键，会跳出来用于创建snippets的snippets（像套娃一样耶）。换上你想存储的代码块吧，示例如下：</p>
<figure>
<img src="https://pic1.zhimg.com/v2-d5d6ec618868382045aa1044eae6e988_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以后每次想插入代码块，直接输我们预设的“暗号”，然后按tab键即可，迅如闪电呀！</p>
<p><strong>注意</strong>：snippets只在相应语言中有效！并且，除了常见的Python、Java、JS等，<strong>其他有些语言是不支持snippets功能的</strong>，此时有两种方法：</p>
<ol type="1">
<li>通过apm install language-语言，安装相应语言的支持包；</li>
<li>将.source.语言 改成* 。</li>
</ol>
<p>我比较建议第二种方法。</p>
<p>另外，mardown中输入table、img、L等按下tab键，可以快捷插入表格、图片、链接等。</p>
<figure>
<img src="https://pic2.zhimg.com/v2-4d43fef791abc2195246b9621b37f4b9_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>成功啦，美滋滋~</p>
<h2 id="代码调试">代码调试</h2>
<p>暂时用不到，先占个坑，下次完善！</p>
<h2 id="使用github远程版本控制">使用Github远程版本控制</h2>
<p>又解锁atom连接github和git啦！赶紧更新日志！</p>
<p>话说，atom本来就是github开发的编辑器好嘛！我们来测试下好用吗！</p>
<p>Github注册无须多言吧，我创建了一个新的repository，命名blog，用来云端存储我的写作素材与稿件。这时我们看到页面提供了一系列指令：</p>
<figure>
<img src="https://pic1.zhimg.com/v2-6daf82cf178c578fd4cf4c19da68cb88_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>这里其实是blog仓库，但我为了演示又重新建了名为test的仓库</em></p>
<p>我们只需要打开windows系统的git bash，一句一句输入。我输入的是以下指令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd blog</span><br><span class="line">echo &quot;#blog&quot; &gt;&gt; README.md</span><br><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;first commit&quot;</span><br><span class="line">git branch -M main</span><br><span class="line">git remote add origin https:&#x2F;&#x2F;github.com&#x2F;MissFreak&#x2F;blog.git</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>
<p>成功后，在Atom打开blog这个文件夹（也就是你想要进行版本控制的项目），我们看到右侧显示github登录界面，提示我们打开github.atom.io，复制GitHub token到Atom的登录表单。Success！</p>
<figure>
<img src="https://pic1.zhimg.com/v2-6e1aa37602728c2eb6bdc8701b013270_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>接下来就随心所欲地增改文件，然后stage all-&gt;commit to main-&gt;pull 吧！</p>
<p>终于不用在终端敲指令，也可以和Github Desktop说拜拜啦！</p>
<figure>
<img src="https://pic4.zhimg.com/v2-713ea328565e6babf7e9559c20bc2e8b_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>参考资料：</strong><a href="https://flight-manual.atom.io/using-atom/sections/github-package/">Atom Documentation-GitHub package</a></p>
<h2 id="你可能不知道的快捷键">你可能不知道的快捷键</h2>
<table>
<thead>
<tr class="header">
<th>功能</th>
<th>快捷键</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>在所有项目文件中查找字符串</td>
<td>Ctrl + Shift + f</td>
</tr>
<tr class="even">
<td>打开导航栏的File、Edit、View等</td>
<td>Alt+导航栏首字母</td>
</tr>
<tr class="odd">
<td>向上/下移动该行</td>
<td>Ctrl + up/down</td>
</tr>
</tbody>
</table>
<p><a href="https://yanyinhong.github.io/2017/07/23/Atom-keyboard-shortcuts/">Windows环境下的Atom快捷键</a></p>
<p><a href="https://www.itread01.com/content/1549978931.html">mac下Atom编辑器快捷键大全</a></p>
<figure>
<img src="https://pic4.zhimg.com/v2-e0ed2bebdccf67e44a7c2bb61ff7b26b_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>在项目所有文件中搜索“正则”</em></p>
<p>----------2021-03-04更新---------</p>
<h2 id="html神器">HTML神器</h2>
<p>又屁颠屁颠跑来更新啦，虽然只有我一个人自娱自乐。</p>
<p>Python开发网站离不开的插件，亲测好用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apm install atom-html-preview, pigments, highlight-selected, autoclose-html-plus, color-picker, color-tabs</span><br></pre></td></tr></table></figure>
<p>效果如下：</p>
<figure>
<img src="https://pic3.zhimg.com/v2-692783c713218dc64a68c8faca88c096_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="markdown神器">Markdown神器</h2>
<p>还有，大家赶紧把这款神器mardown-preview-enhanced用起来吧，功能齐全到不可想象！<strong>可以折叠一级、二级、三级标题，专注于当前标题下的内容！（但是我刚用了貌似会卡）</strong></p>
<p>可以运行代码，并渲染运行结果：</p>
<figure>
<img src="https://pic3.zhimg.com/v2-12f773fb3a40b67afa0a432aa3dbc9b2_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>各种流程图、结构图：</p>
<figure>
<img src="https://pic2.zhimg.com/v2-9c4fd8ff9abd02d7cbb3f6566bf0549d_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>甚至制作幻灯片：</p>
<figure>
<img src="https://pic4.zhimg.com/v2-cdcf68324e69b094ea314250405b9e9b_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>还有太多亮瞎眼的操作不一一列举，详情请戳：https://shd101wyy.github.io/markdown-preview-enhanced/#/zh-cn/</p>
<p>VScode也可以用~真的很想把sublime扔进垃圾桶了！虽然sublime打开速度确实快！</p>
<h2 id="terminal-神器">Terminal 神器</h2>
<p>我又发现了一款终端程序包，terminal-plus！可以更改主题、查看终端当前运行的命令进程！用颜色标记状态图标和排序！从文本编辑器插入并运行文本！还有太多功能大家自己查阅！再次挖到宝贝啦！</p>
<p><a href="https://github.com/jeremyramin/terminal-plus">jeremyramin/terminal-plus</a></p>
<figure>
<img src="https://pic1.zhimg.com/v2-a579e162c502f89da0d9036ac21abfb8_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>总结：我目前安装的插件和主题如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── atom-beautify@0.33.4</span><br><span class="line">├── atom-html-preview@0.2.6</span><br><span class="line">├── autoclose-html-plus@0.27.2</span><br><span class="line">├── color-picker@2.3.0</span><br><span class="line">├── color-tabs@0.1.8</span><br><span class="line">├── file-icons@2.1.46</span><br><span class="line">├── highlight-selected@0.17.0</span><br><span class="line">├── markdown-preview-enhanced@0.18.8</span><br><span class="line">├── markdown-writer@2.11.11</span><br><span class="line">├── minimap@4.39.9</span><br><span class="line">├── monokai@0.27.0</span><br><span class="line">├── pigments@0.40.6</span><br><span class="line">├── python-autopep8@0.1.3</span><br><span class="line">└── python-tools@0.6.9</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>atom</tag>
        <tag>editor</tag>
      </tags>
  </entry>
  <entry>
    <title>语料库资源大全</title>
    <url>/corpus/</url>
    <content><![CDATA[<h1 id="现存语料库">现存语料库</h1>
<h2 id="词语">词语</h2>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">条目</th>
<th style="text-align: left;">长度</th>
<th style="text-align: left;">数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">汉字</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">16142</td>
</tr>
<tr class="even">
<td style="text-align: left;">词语</td>
<td style="text-align: left;">2~3</td>
<td style="text-align: left;">264434</td>
</tr>
<tr class="odd">
<td style="text-align: left;">成语</td>
<td style="text-align: left;">4~5</td>
<td style="text-align: left;">31648</td>
</tr>
<tr class="even">
<td style="text-align: left;">歇后语</td>
<td style="text-align: left;">6+</td>
<td style="text-align: left;">14032</td>
</tr>
</tbody>
</table>
<p>新华网成语、歇后语和词语：https://github.com/pwxcoo/chinese-xinhua</p>
<p>查询接口：https://github.com/netnr/zidian</p>
<p>近反义词：https://github.com/guotong1988/chinese_dictionary</p>
<span id="more"></span>
<h2 id="句子">句子</h2>
<p>微博短句、句子迷</p>
<h3 id="idea">idea</h3>
<p>除了欣赏，也可以收集美句美文，并表达不同心情和志趣。</p>
<h2 id="文章">文章</h2>
<p>新闻：https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset</p>
<p>微信公众号语料库：https://github.com/nonamestreet/weixin_public_corpus</p>
<p>每行文章，是JSON格式，名称是微信公众号名字，帐户是微信公众号ID，标题是译文，内容是正文。</p>
<h3 id="idea-1">idea</h3>
<p>文本分类！主题建模！可以按照风格分类！</p>
<h2 id="名字">名字</h2>
<p>豆瓣影视和书籍的名字</p>
<p>微信、知乎文章标题</p>
<p>书名号</p>
<h3 id="idea-2">idea</h3>
<p>抓取所有名字，动态展现，且按照不同风格分类！就叫标题网！</p>
<p>功能：帮助大家取名、让标题更吸引人、更有文采！并研究怎样的文章更吸引读者。</p>
<h2 id="诗词">诗词</h2>
<p>中华古诗词数据库：</p>
<p>https://github.com/chinese-poetry/chinese-poetry</p>
<h2 id="维基百科">维基百科</h2>
<p>信息检索？</p>
<h2 id="学术语料">学术语料</h2>
<p>信息检索？</p>
<h1 id="抓取原则">抓取原则</h1>
<h2 id="频率tf-idf">频率tf-idf</h2>
<h2 id="打分">打分</h2>
<ul>
<li>通过文章和书籍的阅读量和点赞</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语料库</category>
      </categories>
      <tags>
        <tag>corpus</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第14章-依存分析（上）</title>
    <url>/dependency-parsing-1/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/14.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<p>前两章的重点是<strong>上下文无关语法</strong>（context-free grammars）<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>，及其用于自动生成基于成分的表征。 我们在这里介绍另一种称为<strong>依存语法</strong>（dependency grammars）的语法形式主义派系，它在当代语音和语言处理系统中极其重要。 在这些形式主义中，短语成分（phrasal constituents）和短语结构规则（phrase-structure rules）并不直接发挥作用。相反，一个句子的句法结构，仅根据句中单词（或词元lemmas）以及单词间存在的一组关联的有向二元语法关系来描述。</p>
下图展示了标准的依存分析风格的句法图示。
<center>
<img src="https://i.loli.net/2021/03/17/3XaE6umlQIgpZrs.png"  alt="" width="700" />
</center>
<p>在句子上方，用从<strong>头部</strong>（heads）到<strong>依存项</strong>（dependents）的有向的标记弧来表示单词之间的关系。我们称之为<strong>类型依存结构</strong>（typed dependency structure），因为标签是从固定的语法关系清单中提取的。它还包括一个根（root）节点，显式地标记句法树的根，即整个结构的中心。</p>
<span id="more"></span>
<p>图14.1显示了与第12章中给出的相应短语结构分析相同的依存分析及树形结构。注意依存分析中没有对应短语成分或词汇类别的节点；<strong>其内部结构仅由句子中词汇项之间的定向关系组成。</strong>这些关系<strong>直接编码重要信息</strong>，这些信息往往隐藏在更复杂的短语结构分析中。例如，动词prefer的<strong>论元</strong>（arguments）在依存结构中直接链接到它，而在短语结构树中它们与主动词的连接不那么紧密。类似地，flight的修饰语morning和Denver在依存结构中直接与之链接。</p>
<center>
<img src="https://i.loli.net/2021/03/17/c1yRPAQCefvESZW.png"  alt="" width="700" />
</center>
<p><strong>依存语法的一个主要优势是能够处理形态丰富、词序相对自由（free word order）的语言。</strong>例如，捷克语的词序可能比英语灵活得多；宾语可能出现在位置状语之前或之后。短语结构语法会需要为解析树中每个可能出现这样一个状语短语的位置单独制定一条规则。基于依存关系的方法只用一种连接类型来表示这种特殊的状语关系。因此，依存分析的方法抽象出了词序信息，只表示解析所需的信息。</p>
<p>使用依存分析的另一个实际性的动机是，<strong>头部-依存（head-dependent）关系</strong>提供了一种近似于谓词及其论元之间的语义关系，这使得它们对指代消歧、自动问答和信息提取之类的许多应用都能产生直接的帮助。基于成分（constituent-based）的语法解析也提供了类似的信息，但通常必须通过诸如第12章讨论的中心语规则等技术从树中提炼出来。</p>
<p>在下面的章节，我们将更详细地讨论依存分析中使用的关系清单，以及这些依存结构的形式基础。然后我们将继续讨论用于自动生成这些结构的主流算法派系。最后，我们将讨论如何评估依存分析器，并指出它们在语言处理中应用的一些方式。</p>
<h1 id="依存关系">14.1 依存关系</h1>
<p>传统语言学的语法关系概念为构成这些依存结构的二元语法关系提供了基础。这些头关系（head relations）的参数由一个<strong>头部</strong>（heads）和一个<strong>依存项</strong>（dependents）组成。在第12章和附录C中，我们已经在成分结构的语境下中讨论过头部的依存项这个概念。在那里，一个成分的头部是一个更大成分的中心组织词（例如名词短语中的关键名词，或动词短语中的动词）。成分中其余的词都是其头部的直接或间接的依存项。在基于依存关系的方法中，通过直接将头部与紧靠头部的词连接起来，绕过成分结构，使头部-依存关系变得明确。</p>
<p>除了指定头部-依存对，依存语法还允许我们根据依存项相对于头部的作用，进一步划分语法关系种类或<strong>语法功能</strong>（grammatical function）。我们熟悉的主语、直接宾语和间接宾语等概念都是可能会想到的关系种类。在英语中，这些概念虽然与一个词在句中的位置和成分类型密切相关，但不起决定性作用，因此与短语结构树中提供的信息重复累赘。然而，在更灵活的语言中，直接编码这些语法关系中的信息是至关重要的，因为基于短语的成分句法提供的帮助很小。</p>
<p>毫不奇怪，语言学家们已经发明了远远超出我们熟悉的主语和宾语概念的关系分类学。虽然不同的理论之间大相径庭，但有足够的共性使其发展出一个在计算上有用的标准。<strong>通用依存关系</strong>（Universal Dependencies）项目（Nivre et al.，2016）提供了一个语言驱动的、利于计算的、跨语言适用的依存关系清单。</p>
<table>
<caption>图14.2 通用依存关系集中的部分依存关系 (de Marneffe et al., 2014)</caption>
<thead>
<tr class="header">
<th>Clausal Argument Relations</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NSUBJ</td>
<td>Nominal subject</td>
</tr>
<tr class="even">
<td>DOBJ</td>
<td>Direct object</td>
</tr>
<tr class="odd">
<td>IOBJ</td>
<td>Indirect object</td>
</tr>
<tr class="even">
<td>CCOMP</td>
<td>Clausal complement</td>
</tr>
<tr class="odd">
<td>XCOMP</td>
<td>Open clausal complement</td>
</tr>
<tr class="even">
<td><strong>Nominal Modifier Relations</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="odd">
<td>NMOD</td>
<td>Nominal modifier</td>
</tr>
<tr class="even">
<td>AMOD</td>
<td>Adjectival modifier</td>
</tr>
<tr class="odd">
<td>NUMMOD</td>
<td>Numeric modifier</td>
</tr>
<tr class="even">
<td>APPOS</td>
<td>Appositional modifier</td>
</tr>
<tr class="odd">
<td>DET</td>
<td>Determiner</td>
</tr>
<tr class="even">
<td>CASE</td>
<td>Prepositions, postpositions and other case markers</td>
</tr>
<tr class="odd">
<td><strong>Other Notable Relations</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td>CONJ</td>
<td>Conjunct</td>
</tr>
<tr class="odd">
<td>CC</td>
<td>Coordinating conjunction</td>
</tr>
</tbody>
</table>
<p>图14.2显示了这项工作中的关系子集。图 14.3 提供了一些例句来说明选定的关系。通用依存方案中所有关系的来由超出了本章的范围，但常用关系的核心集可以分成两组：描述与谓语（通常是动词）有关的句法角色的子句关系（clausal relations），以及对头部修饰词进行分类的修饰关系（modifier relations）。</p>
<table>
<caption>Figure 14.3 Examples of core Universal Dependency relations.</caption>
<thead>
<tr class="header">
<th>Relation</th>
<th>Examples with <em>head</em> and dependent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NSUBJ</td>
<td><strong>United</strong> <em>canceled</em> the flight.</td>
</tr>
<tr class="even">
<td>DOBJ</td>
<td>United <em>diverted</em> the <strong>flight</strong> to Reno.</td>
</tr>
<tr class="odd">
<td></td>
<td>We <em>booked</em> her the first <strong>flight</strong> to Miami.</td>
</tr>
<tr class="even">
<td>IOBJ</td>
<td>We <em>booked</em> <strong>her</strong> the flight to Miami.</td>
</tr>
<tr class="odd">
<td>NMOD</td>
<td>We took the <strong>morning</strong> <em>flight</em>.</td>
</tr>
<tr class="even">
<td>AMOD</td>
<td>Book the <strong>cheapest</strong> <em>flight</em>.</td>
</tr>
<tr class="odd">
<td>NUMMOD</td>
<td>Before the storm JetBlue canceled <strong>1000</strong> <em>flights</em>.</td>
</tr>
<tr class="even">
<td>APPOS</td>
<td><em>United</em>, a <strong>unit</strong> of UAL, matched the fares.</td>
</tr>
<tr class="odd">
<td>DET</td>
<td><strong>The</strong> <em>flight</em> was canceled.</td>
</tr>
<tr class="even">
<td></td>
<td><strong>Which</strong> <em>flight</em> was delayed?</td>
</tr>
<tr class="odd">
<td>CONJ</td>
<td>We <em>flew</em> to Denver and <strong>drove</strong> to Steamboat.</td>
</tr>
<tr class="even">
<td>CC</td>
<td>We flew to Denver <strong>and</strong> <em>drove</em> to Steamboat.</td>
</tr>
<tr class="odd">
<td>CASE</td>
<td>Book the flight <strong>through</strong> <em>Houston</em>.</td>
</tr>
</tbody>
</table>
<p>参考以下例句，子句关系NSUBJ和DOBJ分别表示主语和谓语cancel的直接宾语，而NMOD、DET和CASE关系表示名词flights和Houston的修饰语。</p>
<p><img src="https://i.loli.net/2021/03/17/XgwzFkLVSdaIfDj.png" /></p>
<h1 id="依存形式主义">14.2 依存形式主义</h1>
<p>在最普通的形式中，我们讨论的依存关系结构仅仅是有向图，即由一组顶点<span class="math inline">\(V\)</span>和一组有序的顶点对<span class="math inline">\(A\)</span>组成的结构<span class="math inline">\(G=(V, A)\)</span>，我们称之为弧（arcs）。</p>
<p>大多数情况下，我们假设顶点集<span class="math inline">\(V\)</span>完全对应于给定句子中单词的集合。然而，它们也可能对应于标点符号，或者当处理形态复杂的语言时，顶点集可能由词干和词缀组成。弧线集<span class="math inline">\(A\)</span>捕获了<span class="math inline">\(V\)</span>中元素之间的头部-依存关系和语法功能关系。</p>
<p>对这些依存结构的进一步限制是针对底层语法理论或形式主义的。其中比较常见的限制是，这些结构必须是连接的、有一个指定的根节点，并且是无环或平面的。与本章讨论的解析方法最相关的是对有根树的常见的、以计算为目的的限制。也就是说，<strong>依存树</strong>（dependency tree）是一个满足以下约束的有向图。</p>
<ol type="1">
<li>有一个指定的根结点，它没有传入弧。</li>
<li>除根节点外，每个顶点恰好有一个传出弧。</li>
<li>从根节点到<span class="math inline">\(V\)</span>中的每个顶点有一条唯一的路径。</li>
</ol>
<p>综上所述，这些约束条件保证了每个词都有一个头，依存结构是连接的，并且有唯一的根节点，从这个根节点可以沿着唯一的定向路径到句子中的每个词。</p>
<h2 id="投射性">14.2.1 投射性</h2>
<p><strong>投射性</strong>（projectivity）的概念施加了一个额外的约束条件，这个约束条件来自于输入（input）中词的顺序。如果在句子中<strong>存在一条从头部到位于头部和依存项之间的每个词的路径</strong>，那么就说这条从头部到依存项的弧线具有投射性。如果组成依存树的所有弧线都有投射性，那么就可以说它是投射的。到目前为止，我们看到的所有依存树都是投射的。然而，有许多完全合乎规则的结构会生成非投射树，特别是在词序相对灵活的语言中。</p>
<p>请看下面的例子。</p>
<p><img src="https://i.loli.net/2021/03/17/2Jicfx1AThkBFId.png" /></p>
<p>在这个例子中，从flight到它的修饰词was的弧线是非投射的，因为从flight到中间的单词this和morning没有路径。正如我们从这张图中看到的，投射性（和非投射性）可以通过画树的方式来检测。<strong>如果能画出没有交叉边的依存树，那么它就是投射性的。</strong>在这里，如果不跨越连接morning和它的头部的弧线，就无法将flight和它的依存项was联系起来。</p>
<p>我们对投射性的关注来自于两个相关的问题。首先，最广泛使用的英语依存关系树库是通过使用中心语查找规则从短语结构树库中自动导出的（第12章）。以这种方式生成的树是保证投射性的，因为它们是由上下文无关语法生成的。第二，最广泛使用的一系列解析算法存在计算上的限制。第14.4节中讨论的基于转换的方法只能生成投射树，因此任何具有非投射结构的句子都必然会出错。这个限制是第14.5节中描述的更灵活的基于图的解析方法的动机之一。</p>
<h1 id="依存树库">14.3 依存树库</h1>
<p>与基于成分的方法一样，树库（treebanks）在依存分析器（dependency parsers）的开发和评估中起着至关重要的作用。<strong>依存树库</strong>（dependency treebanks）的创建方法与第12章中讨论的方法类似——让人类标注者直接为给定的语料库生成依存结构，或者使用自动解析器（automatic parsers ）提供初始解析，然后让标注者手动修正这些解析器。我们也可以用一个确定性过程<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>（deterministic process）通过标注中心语规则将现有的基于成分的树库翻译成依存树。</p>
<p>大多形态丰富的语言（如捷克语、印地语和芬兰语）都已经建立了直接标注的依存树库用于依存分析，其中捷克语的Prague依存树库（Bejcek et al., 2013）是最著名的工作。主流英语依存树库主要是从现有资源中提取出来的，比如Penn树库的华尔街日报部分（Marcus等人，1993）。最近的OntoNotes项目（Hovy et al. 2006, Weischedel et al. 2011）扩展了这种方法，超越了传统的新闻文本，涵盖了英语、汉语和阿拉伯语的电话对话、网络日志、usenet新闻组、广播和脱口秀。</p>
<p>从成分结构到依存结构的翻译过程有两个子任务：识别结构中所有的头部-依存关系，以及正确识别这些关系的种类。第一个任务主要依赖于第12章中讨论的中心语规则（head rules）的使用，这些规则最早是为词汇化概率解析器（ lexicalized probabilistic parsers）而开发的(Magerman 1994, Collins 1999, Collins 2003)。下面是Xia和Palmer（2001）提出的一个简单有效的算法。</p>
<ol type="1">
<li>使用适当的中心语规则，标记短语结构中每个节点的头部子节点。</li>
<li>在依存结构中，让每个非头部子节点的头部依存于头部子节点的头部。</li>
</ol>
<p>当一个短语结构解析包含了额外的语法关系和函数标签形式的信息时，如在Penn Treebank的情况下，这些标签可以用来标记生成的树的边。当应用于图14.4中的解析树时，这种算法将产生例14.4中的依存结构。这些提取方法的主要缺点是它们受到原始结构树中存在的信息的限制。其中最重要的问题是未能将形态学信息与短语结构树整合在一起，不能轻易地表示非宾语结构，以及大多数名词短语缺乏内部结构，这反映在大多数树库语法通常所使用的平面规则中。由于这些原因，除了英语之外，大多数依存树库都是直接靠人类标注者开发的。</p>
<hr />
<p><strong>本章剩余内容见：<a href="http://nlpcourse.cn/dependency-parsing-2/">《自然语言处理综论》第14章-依存分析（中）</a></strong></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>也称为短语结构语法 (phrase-structure grammar)<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>译者注：对应<a href="https://baike.baidu.com/item/随机过程">随机过程（<em>Stochastic Process</em>）</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>dependency</tag>
      </tags>
  </entry>
  <entry>
    <title>英文依存句法分析</title>
    <url>/dependency-parsing/</url>
    <content><![CDATA[<blockquote>
<p>依存分析是根据句子中单词之间的依存关系来分析句子语法结构的过程。</p>
</blockquote>
<p>在依存分析中，各种标签代表了一个句子中两两词语之间的关系。例如，在'rainy weather'这个短语中，rainy是修饰名词weather的形容词。weather -&gt; rainy 形成了依存关系，其中weather是head（中心词），而rainy则是dependent（依赖）。该依存关系用amod标签表示，即形容词修饰语。我们可以用依存关系箭头标注语法关系： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     root</span><br><span class="line">      |</span><br><span class="line">      | +-------dobj---------+</span><br><span class="line">      | |                    |</span><br><span class="line">nsubj | |   +------det-----+ | +-----nmod------+</span><br><span class="line">+--+  | |   |              | | |               |</span><br><span class="line">|  |  | |   |      +-nmod-+| | |      +-case-+ |</span><br><span class="line">+  |  + |   +      +      || + |      +      | |</span><br><span class="line">I  prefer  the  morning   flight  through  Denver</span><br></pre></td></tr></table></figure></p>
<p>也可以表示为依存句法树（Dependency Tree Graph）：</p>
<p><img src="https://i.loli.net/2021/03/16/JtlAKsfoXhZwLjP.png" /></p>
<p>图片来源：<a href="https://zhuanlan.zhihu.com/p/66268929">CS224N笔记(五):Dependency Parsing</a></p>
<h1 id="通用依存关系">通用依存关系</h1>
<p>截至目前，UD项目的通用依存关系共有37个，这些关系的完整<a href="https://universaldependencies.org/u/dep/">列表</a>可以在这里查看并深入研究。</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Nominals</strong></th>
<th><strong>Clauses</strong></th>
<th><strong>Modifier words</strong></th>
<th><strong>Function Words</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Core arguments</strong></td>
<td><a href="https://universaldependencies.org/u/dep/nsubj.html">nsubj</a> <a href="https://universaldependencies.org/u/dep/obj.html">obj</a> <a href="https://universaldependencies.org/u/dep/iobj.html">iobj</a></td>
<td><a href="https://universaldependencies.org/u/dep/csubj.html">csubj</a> <a href="https://universaldependencies.org/u/dep/ccomp.html">ccomp</a> <a href="https://universaldependencies.org/u/dep/xcomp.html">xcomp</a></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Non-core dependents</strong></td>
<td><a href="https://universaldependencies.org/u/dep/obl.html">obl</a> <a href="https://universaldependencies.org/u/dep/vocative.html">vocative</a> <a href="https://universaldependencies.org/u/dep/expl.html">expl</a> <a href="https://universaldependencies.org/u/dep/dislocated.html">dislocated</a></td>
<td><a href="https://universaldependencies.org/u/dep/advcl.html">advcl</a></td>
<td><a href="https://universaldependencies.org/u/dep/advmod.html">advmod</a>* <a href="https://universaldependencies.org/u/dep/discourse.html">discourse</a></td>
<td><a href="https://universaldependencies.org/u/dep/aux_.html">aux</a> <a href="https://universaldependencies.org/u/dep/cop.html">cop</a> <a href="https://universaldependencies.org/u/dep/mark.html">mark</a></td>
</tr>
<tr class="odd">
<td><strong>Nominal dependents</strong></td>
<td><a href="https://universaldependencies.org/u/dep/nmod.html">nmod</a> <a href="https://universaldependencies.org/u/dep/appos.html">appos</a> <a href="https://universaldependencies.org/u/dep/nummod.html">nummod</a></td>
<td><a href="https://universaldependencies.org/u/dep/acl.html">acl</a></td>
<td><a href="https://universaldependencies.org/u/dep/amod.html">amod</a></td>
<td><a href="https://universaldependencies.org/u/dep/det.html">det</a> <a href="https://universaldependencies.org/u/dep/clf.html">clf</a> <a href="https://universaldependencies.org/u/dep/case.html">case</a></td>
</tr>
<tr class="even">
<td><strong>Coordination</strong></td>
<td><strong>MWE</strong></td>
<td><strong>Loose</strong></td>
<td><strong>Special</strong></td>
<td><strong>Other</strong></td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/dep/conj.html">conj</a> <a href="https://universaldependencies.org/u/dep/cc.html">cc</a></td>
<td><a href="https://universaldependencies.org/u/dep/fixed.html">fixed</a> <a href="https://universaldependencies.org/u/dep/flat.html">flat</a> <a href="https://universaldependencies.org/u/dep/compound.html">compound</a></td>
<td><a href="https://universaldependencies.org/u/dep/list.html">list</a> <a href="https://universaldependencies.org/u/dep/parataxis.html">parataxis</a></td>
<td><a href="https://universaldependencies.org/u/dep/orphan.html">orphan</a> <a href="https://universaldependencies.org/u/dep/goeswith.html">goeswith</a> <a href="https://universaldependencies.org/u/dep/reparandum.html">reparandum</a></td>
<td><a href="https://universaldependencies.org/u/dep/punct.html">punct</a> <a href="https://universaldependencies.org/u/dep/root.html">root</a> <a href="https://universaldependencies.org/u/dep/dep.html">dep</a></td>
</tr>
</tbody>
</table>
<p>此外还有一些基于特定语言的依存关系。斯坦福依存分析定义了接近50个依存关系，具体的定义可参考：<a href="https://nlp.stanford.edu/software/dependencies_manual.pdf">Stanford typed dependencies manual</a>。</p>
<p>关于依存句法分析，也可以参考Daniel Jurafsky的经典NLP书籍Speech and Language Processing相关<a href="https://web.stanford.edu/~jurafsky/slp3/14.pdf">章节</a>。</p>
<span id="more"></span>
<h1 id="工具推荐">工具推荐</h1>
<h2 id="stanford-parser句法分析">Stanford Parser句法分析</h2>
<p>比较有名的工具是Stanford Parser，我们可以在<a href="http://nlp.stanford.edu:8080/corenlp/">这里</a>在线使用并进行可视化。</p>
<p>或者安装斯坦福的Python NLP软件包<a href="https://stanfordnlp.github.io/stanza/installation_usage.html">Stanza</a>，里面集成了<a href="https://stanfordnlp.github.io/stanza/depparse.html">依存关系分析</a>工具。</p>
<p>如何解决download()下载异常的问题可以参考<a href="https://blog.csdn.net/superyangtze/article/details/105252193">这里</a>。</p>
<p>我们可以进行词性标注：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanza</span><br><span class="line"><span class="comment"># stanza.download(&#x27;en&#x27;)</span></span><br><span class="line">nlp = stanza.Pipeline(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">doc = nlp(<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> doc.sentences:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence.words:</span><br><span class="line">        print(word.text, word.lemma, word.pos)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">It it PRON</span></span><br><span class="line"><span class="string">took take VERB</span></span><br><span class="line"><span class="string">me I PRON</span></span><br><span class="line"><span class="string">more more ADJ</span></span><br><span class="line"><span class="string">than than ADP</span></span><br><span class="line"><span class="string">two two NUM</span></span><br><span class="line"><span class="string">hours hour NOUN</span></span><br><span class="line"><span class="string">to to PART</span></span><br><span class="line"><span class="string">translate translate VERB</span></span><br><span class="line"><span class="string">a a DET</span></span><br><span class="line"><span class="string">few few ADJ</span></span><br><span class="line"><span class="string">pages page NOUN</span></span><br><span class="line"><span class="string">of of ADP</span></span><br><span class="line"><span class="string">English English PROPN</span></span><br><span class="line"><span class="string">. . PUNCT</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>以及依存句法分析： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanza</span><br><span class="line"><span class="comment"># stanza.download(&#x27;en&#x27;)</span></span><br><span class="line">nlp = stanza.Pipeline(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">doc = nlp(<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span>)</span><br><span class="line">print(*[<span class="string">f&#x27;id: <span class="subst">&#123;word.<span class="built_in">id</span>&#125;</span>\tword: <span class="subst">&#123;word.text&#125;</span>\thead id: <span class="subst">&#123;word.head&#125;</span>\thead: <span class="subst">&#123;sent.words[word.head-<span class="number">1</span>].text <span class="keyword">if</span> word.head &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&quot;root&quot;</span>&#125;</span>\tdeprel: <span class="subst">&#123;word.deprel&#125;</span>&#x27;</span> <span class="keyword">for</span> sent <span class="keyword">in</span> doc.sentences <span class="keyword">for</span> word <span class="keyword">in</span> sent.words], sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="built_in">id</span>: <span class="number">1</span>	word: It	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: expl</span><br><span class="line"><span class="built_in">id</span>: <span class="number">2</span>	word: took	head <span class="built_in">id</span>: <span class="number">0</span>	head: root	deprel: root</span><br><span class="line"><span class="built_in">id</span>: <span class="number">3</span>	word: me	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: iobj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">4</span>	word: more	head <span class="built_in">id</span>: <span class="number">6</span>	head: two	deprel: advmod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">5</span>	word: than	head <span class="built_in">id</span>: <span class="number">4</span>	head: more	deprel: fixed</span><br><span class="line"><span class="built_in">id</span>: <span class="number">6</span>	word: two	head <span class="built_in">id</span>: <span class="number">7</span>	head: hours	deprel: nummod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">7</span>	word: hours	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: obj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">8</span>	word: to	head <span class="built_in">id</span>: <span class="number">9</span>	head: translate	deprel: mark</span><br><span class="line"><span class="built_in">id</span>: <span class="number">9</span>	word: translate	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: csubj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">10</span>	word: a	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: det</span><br><span class="line"><span class="built_in">id</span>: <span class="number">11</span>	word: few	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: amod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">12</span>	word: pages	head <span class="built_in">id</span>: <span class="number">9</span>	head: translate	deprel: obj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">13</span>	word: of	head <span class="built_in">id</span>: <span class="number">14</span>	head: English	deprel: case</span><br><span class="line"><span class="built_in">id</span>: <span class="number">14</span>	word: English	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: nmod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">15</span>	word: .	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: punct</span><br></pre></td></tr></table></figure> ## Spacy依存句法分析</p>
<p>我们也可以使用Spacy进行依存句法分析并画出句法树。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $python -m spacy download en_core_web_sm</span></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp=spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line">text=<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(text):</span><br><span class="line">	print(token.text,<span class="string">&#x27;=&gt;&#x27;</span>,token.dep_,<span class="string">&#x27;=&gt;&#x27;</span>,token.head.text)</span><br></pre></td></tr></table></figure>
<p>输出结果如下，对比stanza的结果，还是有明显差异，例如此处it被标记为<code>nsubj</code>，但其实这种情况下it是虚词expletive（我们都学过形式主语），不担任谓词的语义角色，因此应该标记为expl。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">It &#x3D;&gt; nsubj &#x3D;&gt; took</span><br><span class="line">took &#x3D;&gt; ROOT &#x3D;&gt; took</span><br><span class="line">me &#x3D;&gt; dative &#x3D;&gt; took</span><br><span class="line">more &#x3D;&gt; amod &#x3D;&gt; two</span><br><span class="line">than &#x3D;&gt; quantmod &#x3D;&gt; two</span><br><span class="line">two &#x3D;&gt; nummod &#x3D;&gt; hours</span><br><span class="line">hours &#x3D;&gt; dobj &#x3D;&gt; took</span><br><span class="line">to &#x3D;&gt; aux &#x3D;&gt; translate</span><br><span class="line">translate &#x3D;&gt; xcomp &#x3D;&gt; took</span><br><span class="line">a &#x3D;&gt; det &#x3D;&gt; pages</span><br><span class="line">few &#x3D;&gt; amod &#x3D;&gt; pages</span><br><span class="line">pages &#x3D;&gt; dobj &#x3D;&gt; translate</span><br><span class="line">of &#x3D;&gt; prep &#x3D;&gt; pages</span><br><span class="line">English &#x3D;&gt; pobj &#x3D;&gt; of</span><br><span class="line">. &#x3D;&gt; punct &#x3D;&gt; took</span><br></pre></td></tr></table></figure>
<p>画图的方法参考<a href="https://spacy.io/usage/visualizers">spacy可视化工具</a>，如果是在sublime text之类的编辑器内运行代码，我们需要打开浏览器地址http://localhost:5000/查看画图的结果。如果是jupyter notebook，则可以直接显示图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">doc = nlp(<span class="string">&quot;It took me more than two hours to translate a few pages of English.&quot;</span>)</span><br><span class="line">displacy.serve(doc, style=<span class="string">&quot;dep&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2021/03/16/dInSpW2aBw1Dg4u.png" /></p>
<p>spacy中的依存关系有45个，完整的标签如下，具体的定义可以参考：<a href="https://nlp.stanford.edu/software/dependencies_manual.pdf">Stanford typed dependencies manual</a>。</p>
<table>
<thead>
<tr class="header">
<th>标签</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ROOT</code></td>
<td>root</td>
</tr>
<tr class="even">
<td><code>acl</code></td>
<td>clausal modifier of noun (adjectival clause)</td>
</tr>
<tr class="odd">
<td><code>acomp</code></td>
<td>adjectival complement</td>
</tr>
<tr class="even">
<td><code>advcl</code></td>
<td>adverbial clause modifier</td>
</tr>
<tr class="odd">
<td><code>advmod</code></td>
<td>adverbial modifier</td>
</tr>
<tr class="even">
<td><code>agent</code></td>
<td>agent</td>
</tr>
<tr class="odd">
<td><code>amod</code></td>
<td>adjectival modifier</td>
</tr>
<tr class="even">
<td><code>appos</code></td>
<td>appositional modifier</td>
</tr>
<tr class="odd">
<td><code>attr</code></td>
<td>attribute</td>
</tr>
<tr class="even">
<td><code>aux</code></td>
<td>auxiliary</td>
</tr>
<tr class="odd">
<td><code>auxpass</code></td>
<td>auxiliary (passive)</td>
</tr>
<tr class="even">
<td><code>case</code></td>
<td>case marking</td>
</tr>
<tr class="odd">
<td><code>cc</code></td>
<td>coordinating conjunction</td>
</tr>
<tr class="even">
<td><code>ccomp</code></td>
<td>clausal complement</td>
</tr>
<tr class="odd">
<td><code>compound</code></td>
<td>compound</td>
</tr>
<tr class="even">
<td><code>conj</code></td>
<td>conjunct</td>
</tr>
<tr class="odd">
<td><code>csubj</code></td>
<td>clausal subject</td>
</tr>
<tr class="even">
<td><code>csubjpass</code></td>
<td>clausal subject (passive)</td>
</tr>
<tr class="odd">
<td><code>dative</code></td>
<td>dative</td>
</tr>
<tr class="even">
<td><code>dep</code></td>
<td>unclassified dependent</td>
</tr>
<tr class="odd">
<td><code>det</code></td>
<td>determiner</td>
</tr>
<tr class="even">
<td><code>dobj</code></td>
<td>direct object</td>
</tr>
<tr class="odd">
<td><code>expl</code></td>
<td>expletive</td>
</tr>
<tr class="even">
<td><code>intj</code></td>
<td>interjection</td>
</tr>
<tr class="odd">
<td><code>mark</code></td>
<td>marker</td>
</tr>
<tr class="even">
<td><code>meta</code></td>
<td>meta modifier</td>
</tr>
<tr class="odd">
<td><code>neg</code></td>
<td>negation modifier</td>
</tr>
<tr class="even">
<td><code>nmod</code></td>
<td>modifier of nominal</td>
</tr>
<tr class="odd">
<td><code>npadvmod</code></td>
<td>noun phrase as adverbial modifier</td>
</tr>
<tr class="even">
<td><code>nsubj</code></td>
<td>nominal subject</td>
</tr>
<tr class="odd">
<td><code>nsubjpass</code></td>
<td>nominal subject (passive)</td>
</tr>
<tr class="even">
<td><code>nummod</code></td>
<td>numeric modifier</td>
</tr>
<tr class="odd">
<td><code>oprd</code></td>
<td>object predicate</td>
</tr>
<tr class="even">
<td><code>parataxis</code></td>
<td>parataxis</td>
</tr>
<tr class="odd">
<td><code>pcomp</code></td>
<td>complement of preposition</td>
</tr>
<tr class="even">
<td><code>pobj</code></td>
<td>object of preposition</td>
</tr>
<tr class="odd">
<td><code>poss</code></td>
<td>possession modifier</td>
</tr>
<tr class="even">
<td><code>preconj</code></td>
<td>pre-correlative conjunction</td>
</tr>
<tr class="odd">
<td><code>predet</code></td>
<td>predeterminer</td>
</tr>
<tr class="even">
<td><code>prep</code></td>
<td>prepositional modifier</td>
</tr>
<tr class="odd">
<td><code>prt</code></td>
<td>particle</td>
</tr>
<tr class="even">
<td><code>punct</code></td>
<td>punctuation</td>
</tr>
<tr class="odd">
<td><code>quantmod</code></td>
<td>modifier of quantifier</td>
</tr>
<tr class="even">
<td><code>relcl</code></td>
<td>relative clause modifier</td>
</tr>
<tr class="odd">
<td><code>xcomp</code></td>
<td>open clausal complement</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>dependency</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式进阶</title>
    <url>/regex-1/</url>
    <content><![CDATA[<p>至此，我们已经建立了正则表达式的思维框架（参见「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">正则表达式入门</a>」），以及如何用python中的re模块编译正则表达式来匹配文本（参见「<a href="https://mp.weixin.qq.com/s/ibNb0rOSnBr4YC0PzCyIBA">Python实操篇</a>」）。尽管此时我们已经可以流畅地编写和使用它，但我们需要一些进阶知识，让我们的正则表达式更准确和精练。本文使用的语言依然是python。 <span id="more"></span></p>
<p><strong>全文概览：</strong></p>
<p>1.分组与捕获：<code>MatchObject.group()</code>的奥秘</p>
<p>2.四种类型的环视：匹配位置，而非匹配文本</p>
<p>3.贪婪与非贪婪：匹配优先 VS 忽略优先</p>
<h1 id="分组与捕获">分组与捕获</h1>
<p>在「<a href="https://mp.weixin.qq.com/s/ibNb0rOSnBr4YC0PzCyIBA">Python实操篇</a>」我们讲到<code>MatchObject</code>的方法<code>group()</code>可以返回匹配的整个字符串，但这并不全面。本节，我们将谈谈括号的一个关键功能：捕获（capturing）。</p>
<p><strong>捕获</strong>就是用括号提取文本以便后续访问，我们可以把这个过程想象成“闰土捕鸟”，把每个括号里的鸟儿都抓起来放进“笼子”。</p>
<p>而捕获进所有“笼子”里的内容，正是通过<code>MatchObject.groups()</code>访问。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.search(<span class="string">r&quot;(\w+) (\w+)&quot;</span>, <span class="string">&quot;John Smith&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：(&#x27;John&#x27;, &#x27;Smith&#x27;)</span></span><br></pre></td></tr></table></figure>
<p>我们可以通过索引来访问每个“笼子”，<code>group(0)</code>返回整个正则表达式匹配的文本，相当于编号为0的隐式捕获，而<code>group(1)</code>、<code>group(2)</code>则按顺序返回显式捕获分组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = re.search(<span class="string">r&quot;(\w+) (\w+)&quot;</span>, <span class="string">&quot;John Smith&quot;</span>)</span><br><span class="line">m.group(<span class="number">0</span>) <span class="comment"># 输出：&#x27;John Smith&#x27;</span></span><br><span class="line">m.group(<span class="number">1</span>)  <span class="comment"># 输出：&#x27;John&#x27;</span></span><br><span class="line">m.group(<span class="number">2</span>)  <span class="comment"># 输出：&#x27;Smith&#x27;</span></span><br></pre></td></tr></table></figure>
<p>你可能会想到「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">入门篇</a>」讲过的<strong>反向引用</strong>，在python中我们依然可以使用序号<code>\1</code>、<code>\2</code>来引用捕获的组别。下面这个例子将“数字-字母”组成的产品ID进行替换，变成了“字母-数字”的形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\d+)-(\w+)&quot;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&quot;\2-\1&quot;</span>, <span class="string">&quot;1-a\n20-baer\n34-afcr&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;a-1\nbaer-20\nafcr-34&#x27;</span></span><br></pre></td></tr></table></figure>
<p>我们也可以给每个“笼子”取上名字，即<strong>命名捕获</strong>，在python正则表达式中表示为<code>(?P&lt;name&gt;pattern)</code>，依然是通过<code>group()</code>访问单个“笼子”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(?P&lt;first&gt;\w+) (?P&lt;last&gt;\w+)&quot;</span>)</span><br><span class="line">match = pattern.search(<span class="string">&quot;John Smith&quot;</span>)</span><br><span class="line">match.group(<span class="string">&quot;first&quot;</span>) <span class="comment"># 输出：&#x27;John&#x27;</span></span><br><span class="line">match.group(<span class="string">&quot;last&quot;</span>) <span class="comment"># 输出：&#x27;Smith&#x27;</span></span><br></pre></td></tr></table></figure>
<p>需要注意，在<code>sub()</code>替换操作中，如果用名称来引用组别，我们需要写成<code>\g&lt;name&gt;</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(?P&lt;country&gt;\d+)-(?P&lt;id&gt;\w+)&quot;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&quot;\g&lt;id&gt;-\g&lt;country&gt;&quot;</span>, <span class="string">&quot;1-a\n20-baer\n34-afcr&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;a-1\nbaer-20\nafcr-34&#x27;</span></span><br></pre></td></tr></table></figure>
<p>当然，很多时候我们使用括号不是为了捕获，而仅仅是为了<strong>分组</strong>，即用于构建子表达式、多选结构或者量词作用的对象。此时，我们可以使用<strong>非捕获型括号</strong><code>(?:)</code>，告诉正则引擎，不需要提取括号内的任何信息。非捕获型括号能够提高效率、节约内存，不浪费咱们的“笼子”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.search(<span class="string">r&quot;gr(a|e)y&quot;</span>, <span class="string">&quot;gray&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：(&#x27;a&#x27;,)</span></span><br><span class="line">re.search(<span class="string">r&quot;gr(?:a|e)y&quot;</span>, <span class="string">&quot;gray&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：()</span></span><br></pre></td></tr></table></figure>
<h1 id="四种类型的环视">四种类型的环视</h1>
<p><strong>环视</strong>（Look Around）是正则表达式最强大的技术之一。我们可以把环视想象成前视镜和后视镜，<strong>顺序环视</strong>就是向前看（从左到右），<strong>逆序环视</strong>就是向后看（从右到左）。通过左右环视，我们进行“闰土捕鸟”的位置会更精确，确保匹配内容的上下文满足特定要求。</p>
<p>以下是四种类型的环视：</p>
<table>
<thead>
<tr class="header">
<th>类型</th>
<th>正则表达式</th>
<th>匹配成功的条件</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>肯定顺序环视</td>
<td>(?=……)</td>
<td>子表达式能够匹配右侧文本</td>
</tr>
<tr class="even">
<td>否定顺序环视</td>
<td>(?!……)</td>
<td>子表达式不能匹配右侧文本</td>
</tr>
<tr class="odd">
<td>肯定逆序环视</td>
<td>(?&lt;=……)</td>
<td>子表达式能够匹配左侧文本</td>
</tr>
<tr class="even">
<td>否定逆序环视</td>
<td>(?&lt;!……)</td>
<td>子表达式不能匹配左侧文本</td>
</tr>
</tbody>
</table>
<p>我们要注意，环视相当于作用于匹配位置的附加条件、不占用任何字符。因此它和分界符类似，是一种<strong>零宽度断言</strong>（zero-width assertions）。我们拿肯定顺序环视举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?=fox)&#x27;</span>)</span><br><span class="line">result = pattern.search(<span class="string">&quot;The quick brown fox jumps over the lazy dog&quot;</span>)</span><br><span class="line">result.span()</span><br><span class="line"><span class="comment">#输出 (16, 16)</span></span><br></pre></td></tr></table></figure>
<p>我们发现，表达式<code>(?=fox)</code>只匹配fox之前的位置，也就是索引16。</p>
<p><img src="C:\Users\13607\AppData\Roaming\Typora\typora-user-images\image-20210218222948086.png" alt="image-20210218222948086" style="zoom: 33%;" /></p>
<p>环视的作用不可小觑，我们可以精准定位，并剔除多余字符，保留更加干净、准确的匹配文本。下面的例子定位<code>&lt;p&gt;</code>标签的同时，只匹配标签里的内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;=&lt;p&gt;)[^&lt;]+(?=&lt;p&gt;)&#x27;</span>)</span><br><span class="line">pattern.search(<span class="string">&quot;&lt;p&gt;test&lt;p&gt;&quot;</span>).group(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;test&#x27;</span></span><br></pre></td></tr></table></figure>
<p>环视的另一典型应用就是将文本变成逗号分隔的货币形式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;\d&#123;1,3&#125;(?=(\d&#123;3&#125;)+(?!\d))&#x27;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&#x27;\g&lt;0&gt;,&#x27;</span>, <span class="string">&quot;123456789&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;123,456,789&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="贪婪与非贪婪">贪婪与非贪婪</h1>
<p>在「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">入门篇</a>」中，我们接触了量词，但并未讲到贪婪与非贪婪的区别。</p>
<p>在python的re模块中，量词默认为<strong>贪婪模式</strong>：尽可能多地匹配更长的字符串。这就是为什么，<code>.*</code>通常会匹配到一行文本的末尾。如果要采用<strong>非贪婪模式</strong>，我们可以在量词后添加一个额外的问号，例如<code>??</code>、<code>*?</code>或<code>+?</code>，使得匹配的长度最小。这两种模式又称为<strong>匹配优先</strong>和<strong>忽略优先</strong>。</p>
<ul>
<li><p>贪婪量词（Greedy quantifiers）：<code>?</code>, <code>*</code>, <code>+</code>, <code>&#123;num,num&#125;</code></p></li>
<li><p>非贪婪/懒惰量词（Lazy quantifiers）：<code>??</code>, <code>*?</code>, <code>+?</code> , <code>&#123;num,num&#125;?</code></p></li>
</ul>
<p>比如，如果要匹配引号内的内容，会得到下面的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = <span class="string">&#x27;The name &quot;McDonald\&#x27;s&quot; ! is said &quot;makudonarudo&quot;in Japanese.&#x27;</span></span><br><span class="line">re.search(<span class="string">r&#x27;&quot;.*&quot;&#x27;</span>, s).group()</span><br><span class="line"><span class="comment"># 输出：&#x27;&quot;McDonald\&#x27;s&quot; is said &quot;makudonarudo&quot;&#x27;</span></span><br><span class="line">re.search(<span class="string">r&#x27;&quot;.*?&quot;&#x27;</span>, s).group()</span><br><span class="line"><span class="comment"># 输出：&#x27;&quot;McDonald\&#x27;s&quot;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>事实上，还有第三种量词，但目前python并不支持：</p>
<ul>
<li>占有量词（Possessive quantifiers）：<code>?+</code>, <code>*+</code>, <code>++</code>, <code>&#123;num,num&#125;+</code></li>
</ul>
<p>占有量词与贪婪量词类似，只是它们从不会交还已经匹配的字符。当然，我们也可以用固化分组<code>(?&gt;...)</code>来代替，比如<code>!.++</code>其实等同于<code>(?&gt;!.+)</code>。然而，固化分组在python中同样不被支持，所以此处不过多讨论。</p>
<p>以上です！</p>
<p>整理完python正则表达式的进阶知识，相信你已经能得心应手解决很多问题。但是，要真正打造高效、规范、美妙的正则表达式，我们仍需要了解正则引擎的原理，以及一些平衡法则、以及测试和优化的技巧，我们下篇再谈！</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title>纯文本文件的处理</title>
    <url>/txt/</url>
    <content><![CDATA[<h1 id="正则表达式">正则表达式</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">file_path = <span class="string">r&#x27;D:\books\Psychology_of_Language.txt&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    txt_string = f.read()</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;第一 初见.*第二 再见&#x27;</span>, re.DOTALL)</span><br><span class="line">cha1 = re.search(pattern, txt_string).group()</span><br><span class="line">print(re.findall(<span class="string">r&#x27;蓝色&#x27;</span>, cha1))</span><br><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;第一 初见[^蓝色]*(蓝色)*.*第二 再见&#x27;</span>, re.DOTALL)</span><br><span class="line">match = re.search(pattern, txt_string)</span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">    print(match.groups())</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="获取文章目录">获取文章目录</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">file_path = <span class="string">r&#x27;D:\project\hexo-final\source\_posts&#x27;</span></span><br><span class="line"><span class="comment"># 这里要修改</span></span><br><span class="line"><span class="comment"># weblink = &#x27;https://github.com/MissFreak/writings/blob/main/&#x27;</span></span><br><span class="line">weblink = <span class="string">&#x27;http://nlpcourse.cn/&#x27;</span></span><br><span class="line">lst = os.listdir(file_path)</span><br><span class="line">post_list = []</span><br><span class="line"><span class="comment"># 这里要修改</span></span><br><span class="line">md_name = <span class="string">&#x27;all-posts.md&#x27;</span></span><br><span class="line"><span class="comment"># md_name = &#x27;README.md&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain the titles and categories of all posts</span></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> lst:</span><br><span class="line">	<span class="comment"># make sure it is a markdown file</span></span><br><span class="line">	<span class="keyword">if</span> filename[-<span class="number">3</span>:] == <span class="string">&#x27;.md&#x27;</span>:</span><br><span class="line">		<span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(file_path, filename), encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">			s = f.read()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># create a dict that stores attributes: title and category_1 and category_2</span></span><br><span class="line">		post_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment"># get the title</span></span><br><span class="line">		match_title = re.search(<span class="string">r&#x27;(?&lt;=title: ).*(?=\n)&#x27;</span>, s)</span><br><span class="line">		<span class="keyword">if</span> match_title:</span><br><span class="line">			title = match_title.group().strip(<span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			title = filename</span><br><span class="line"></span><br><span class="line">		<span class="comment"># convert into linked title</span></span><br><span class="line">        <span class="comment"># 这里要修改</span></span><br><span class="line">		<span class="comment"># linked_title = &#x27;[&#123;&#125;](&#123;&#125;&#123;&#125;)&#x27;.format(title, weblink, filename)</span></span><br><span class="line">		linked_title = <span class="string">&#x27;[&#123;&#125;](&#123;&#125;&#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(title, weblink, filename[:-<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">		<span class="comment"># get the categories</span></span><br><span class="line">		match_categories = re.search(<span class="string">r&#x27;categories:\n- (.*)\n- (.*)\n&#x27;</span>, s)</span><br><span class="line">		<span class="keyword">try</span>:</span><br><span class="line">			category_1 = match_categories.group(<span class="number">1</span>)</span><br><span class="line">			category_2 = match_categories.group(<span class="number">2</span>)</span><br><span class="line">		<span class="keyword">except</span>:</span><br><span class="line">			category_1 = <span class="string">&#x27;未分类&#x27;</span></span><br><span class="line">			category_2 = <span class="string">&#x27;未分类&#x27;</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># save into a list</span></span><br><span class="line">		post_dict[<span class="string">&#x27;title&#x27;</span>] = linked_title</span><br><span class="line">		post_dict[<span class="string">&#x27;category_1&#x27;</span>] = category_1</span><br><span class="line">		post_dict[<span class="string">&#x27;category_2&#x27;</span>] = category_2</span><br><span class="line">		post_list.append(post_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sort and group by the first category</span></span><br><span class="line">get_first_category = <span class="keyword">lambda</span> dct: dct.get(<span class="string">&#x27;category_1&#x27;</span>)</span><br><span class="line">group_1 = itertools.groupby(<span class="built_in">sorted</span>(post_list, key=get_first_category), get_first_category)</span><br><span class="line">num_post = <span class="built_in">len</span>(post_list)</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;---\ntitle: 所有文章目录\n---\n&lt;center&gt;目前共有&#123;&#125;篇文章：&lt;/center&gt;\n&lt;!-- more --&gt;\n\n&#x27;</span>.<span class="built_in">format</span>(num_post)</span><br><span class="line"><span class="keyword">for</span> k1,v1 <span class="keyword">in</span> group_1:</span><br><span class="line">	<span class="comment"># add the first category into content</span></span><br><span class="line">	content += (<span class="string">&#x27;# &#x27;</span>+k1+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">	<span class="comment"># sort and group by the first category</span></span><br><span class="line">	get_second_category = <span class="keyword">lambda</span> dct: dct.get(<span class="string">&#x27;category_2&#x27;</span>)</span><br><span class="line">	group_2 = itertools.groupby(<span class="built_in">sorted</span>(v1, key=get_second_category), get_second_category)</span><br><span class="line">	<span class="keyword">for</span> k2,v2 <span class="keyword">in</span> group_2:</span><br><span class="line">		<span class="comment"># add the second category into content</span></span><br><span class="line">		<span class="keyword">if</span> k2 != <span class="string">&#x27;未分类&#x27;</span>:</span><br><span class="line">			content += (<span class="string">&#x27;- _&#x27;</span>+k2+<span class="string">&#x27;_\n&#x27;</span>)</span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> v2:</span><br><span class="line">				<span class="comment"># add the title into content</span></span><br><span class="line">				content += (<span class="string">&#x27;\t- &#x27;</span>+i[<span class="string">&#x27;title&#x27;</span>]+<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> v2:</span><br><span class="line">				<span class="comment"># add the title into content</span></span><br><span class="line">				content += (<span class="string">&#x27;- &#x27;</span>+i[<span class="string">&#x27;title&#x27;</span>]+<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">print(content)</span><br><span class="line"><span class="comment"># write the content into md file</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(file_path, md_name), <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	f.write(content)</span><br></pre></td></tr></table></figure>
<h1 id="分离章节">分离章节</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">file_path = <span class="string">r&#x27;D:\books\Psychology_of_Language.txt&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    txt_string = f.read()</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;\ng\n&#x27;</span>)</span><br><span class="line">chapters = pattern.split(txt_string)[<span class="number">6</span>:]</span><br><span class="line"></span><br><span class="line">cha8 = chapters[<span class="number">7</span>]</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;\nn\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>, cha8)</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;\n&#123;2,&#125;&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, cha8)</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;-&#x27;</span>, <span class="string">&#x27;&#x27;</span>, cha8)</span><br><span class="line"><span class="comment"># print(repr(cha8))</span></span><br><span class="line">print(cha8)</span><br></pre></td></tr></table></figure>
<h1 id="去除字符">去除字符</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，\n 请加入，谢谢\n\n\n&#x27;</span></span><br><span class="line">print(ss.strip(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，\n 请加入，谢谢\n\n\n&#x27;</span></span><br><span class="line">print(ss.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>))</span><br><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，请加入，谢谢啦啦嗯&#x27;</span></span><br><span class="line">print(ss.strip(<span class="string">&#x27;嗯啦&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h1 id="表格pytablewriter">表格：<a href="https://github.com/thombashi/pytablewriter/">pytablewriter</a></h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MarkdownTableWriter, ExcelXlsxTableWriter, UnicodeTableWriter, JavaScriptTableWriter, JsonTableWriter, HtmlTableWriter</span><br></pre></td></tr></table></figure>
<p>先获得matrix：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tempo.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	s = f.read()</span><br><span class="line">matrix = [i.split(<span class="string">&#x27;: &#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> s.split(<span class="string">&#x27;\n&#x27;</span>)] <span class="comment"># 行分隔符和列分隔符</span></span><br><span class="line">print(matrix)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tempo2.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	s = f.read()</span><br><span class="line">matrix = [i.split(<span class="string">&#x27; &#x27;</span>, <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> s.split(<span class="string">&#x27;\n&#x27;</span>)] <span class="comment"># split on first occurrence</span></span><br><span class="line">print(matrix)</span><br></pre></td></tr></table></figure>
<p>然后生成相应的表格：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line">writer = pytablewriter.MarkdownTableWriter()</span><br><span class="line">writer.value_matrix = matrix</span><br><span class="line">writer.write_table()</span><br></pre></td></tr></table></figure>
<h2 id="markdown表格">markdown表格</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># s = spacy.explain(&#x27;``&#x27;)</span></span><br><span class="line"><span class="comment"># print(repr(s))</span></span><br><span class="line">lst1 = <span class="string">&#x27;`$`, `&#x27;</span><span class="string">&#x27;`, `,`, `-LRB-`, `-RRB-`, `.`, `:`, `ADD`, `AFX`, `CC`, `CD`, `DT`, `EX`, `FW`, `HYPH`, `IN`, `JJ`, `JJR`, `JJS`, `LS`, `MD`, `NFP`, `NN`, `NNP`, `NNPS`, `NNS`, `PDT`, `POS`, `PRP`, `PRP$`, `RB`, `RBR`, `RBS`, `RP`, `SYM`, `TO`, `UH`, `VB`, `VBD`, `VBG`, `VBN`, `VBP`, `VBZ`, `WDT`, `WP`, `WP$`, `WRB`, `XX`&#x27;</span>.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">lst2 = [spacy.explain(i.strip(<span class="string">&#x27; `&#x27;</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> lst1]</span><br><span class="line"><span class="comment"># print(lst2)</span></span><br><span class="line"></span><br><span class="line">matrix = np.column_stack((lst1, lst2)).tolist()</span><br><span class="line"><span class="comment"># print(type(matrix))</span></span><br><span class="line"><span class="comment"># print(matrix)</span></span><br><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line">writer = pytablewriter.MarkdownTableWriter()</span><br><span class="line">writer.value_matrix = matrix</span><br><span class="line">writer.write_table()</span><br></pre></td></tr></table></figure>
<h2 id="csv">csv</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    writer = pytablewriter.CsvTableWriter()</span><br><span class="line">    writer.headers = [<span class="string">&quot;int&quot;</span>, <span class="string">&quot;float&quot;</span>, <span class="string">&quot;str&quot;</span>, <span class="string">&quot;bool&quot;</span>, <span class="string">&quot;mix&quot;</span>, <span class="string">&quot;time&quot;</span>]</span><br><span class="line">    writer.value_matrix = [</span><br><span class="line">        [<span class="number">0</span>,   <span class="number">0.1</span>,      <span class="string">&quot;hoge&quot;</span>, <span class="literal">True</span>,   <span class="number">0</span>,      <span class="string">&quot;2017-01-01 03:04:05+0900&quot;</span>],</span><br><span class="line">        [<span class="number">2</span>,   <span class="string">&quot;-2.23&quot;</span>,  <span class="string">&quot;foo&quot;</span>,  <span class="literal">False</span>,  <span class="literal">None</span>,   <span class="string">&quot;2017-12-23 45:01:23+0900&quot;</span>],</span><br><span class="line">        [<span class="number">3</span>,   <span class="number">0</span>,        <span class="string">&quot;bar&quot;</span>,  <span class="string">&quot;true&quot;</span>,  <span class="string">&quot;inf&quot;</span>, <span class="string">&quot;2017-03-03 33:44:55+0900&quot;</span>],</span><br><span class="line">        [-<span class="number">10</span>, -<span class="number">9.9</span>,     <span class="string">&quot;&quot;</span>,     <span class="string">&quot;FALSE&quot;</span>, <span class="string">&quot;nan&quot;</span>, <span class="string">&quot;2017-01-01 00:00:00+0900&quot;</span>],</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    writer.write_table()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="其他格式">其他格式</h2>
<p><a href="https://pytablewriter.readthedocs.io/en/latest/pages/examples/table_format/text/json.html">JSON</a></p>
<p><a href="https://pytablewriter.readthedocs.io/en/latest/pages/examples/table_format/text/html.html">HTML</a></p>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>txt</tag>
      </tags>
  </entry>
  <entry>
    <title>好用的API集锦</title>
    <url>/wiki-api/</url>
    <content><![CDATA[<h1 id="wikipedia-python库">Wikipedia Python库</h1>
<p><strong>Wikipedia</strong>是一个Python库，可轻松访问和解析Wikipedia中的数据，搜索Wikipedia、获取文章摘要、从页面获取链接和图像等数据等等。Wikipedia封装了<a href="https://www.mediawiki.org/wiki/API">MediaWiki API，</a>因此您可以专注于使用Wikipedia数据，而不是获取数据。 <span id="more"></span></p>
]]></content>
      <tags>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>如何在curl和python中使用API</title>
    <url>/flask-api-3/</url>
    <content><![CDATA[<p>除了自己构建API，我们也可能需要使用其他网站提供的API。 <span id="more"></span></p>
<h1 id="在curl中使用api">在curl中使用API</h1>
<p>我们以学术检索网站<a href="https://api.semanticscholar.org/">semantic scholar</a>为例，查找某论文的信息，并保存为JSON文件<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;api.semanticscholar.org&#x2F;v1&#x2F;paper&#x2F;10.1038&#x2F;nrn3241 &gt; paper1.json</span><br></pre></td></tr></table></figure>
<p>但此时的json数据结构并不整洁，我们用python格式化json字符串并保存在paper2.json<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m json.tool paper1.json paper2.json</span><br></pre></td></tr></table></figure>
<p>或者一步到位<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;api.semanticscholar.org&#x2F;v1&#x2F;paper&#x2F;10.1038&#x2F;nrn3241 | python -mjson.tool &gt; paper3.json</span><br></pre></td></tr></table></figure>
<p>如果仅仅是想显示为Pretty Print打印出来，把后面的filename.json去掉即可。</p>
<h1 id="用python获取api数据">用python获取API数据</h1>
<p>依旧是这个例子，我们将返回的数据转成字典格式<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">&quot;https://api.semanticscholar.org/v1/paper/10.1038/nrn3241&quot;</span>)</span><br><span class="line">response_dic = response.json() <span class="comment"># if the result was written in JSON format, if not it raises an error</span></span><br></pre></td></tr></table></figure>
<p>更多requests相关参考<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>。</p>
<p><strong>参考文献：</strong></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://www.ruanyifeng.com/blog/2019/09/curl-reference.html">curl 的用法指南</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="https://docs.python.org/3/library/json.html">python json官方文档</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="https://skorks.com/2013/04/the-best-way-to-pretty-print-json-on-the-command-line/">在命令行上漂亮地打印JSON的最佳方法</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><a href="https://jzchangmark.wordpress.com/2016/06/12/%E9%80%8F%E9%81%8E-curl%E3%80%81python%E3%80%81postman-%E4%BE%86-request-api/">透过curl、Python、Postman 来Request API</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a href="https://www.w3schools.com/python/ref_requests_response.asp">python请求</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>一个人开发信息检索与抽取网站的全过程</title>
    <url>/flask-web/</url>
    <content><![CDATA[<h1>我的网站开发学习</h1>
<p>我不会使用css或js文件，因为我只专注于python后端开发，前端基本都省去，只用bootstrap，但是要精通bootstrap哟！</p>
<p>我的网站是动态呈现，所以一定要用flask或者django，我现在先不考虑部署到服务器，只是本地使用！！！</p>
<p>计划：</p>
<ol>
<li>完成检索功能，包括错误检测。检索成语、句子、词语等等。</li>
<li>其他小插件的完善</li>
<li>重点是毕业论文</li>
</ol>
<span id="more"></span>
<h1>搜索框</h1>
<p>首先我们把boostrap加入html页面，直接使用<a href="https://www.bootstrapcdn.com/">BootstrapCDN</a>跳过下载，将Bootstrap编译的CSS和JS的缓存版本交付给我们的项目。我们使用最简单的主题即可，也可以使用其他主题（比如<a href="https://bootswatch.com/minty/">minty</a>）。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">title</span>&gt;</span>Document<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">href</span>=<span class="string">&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css&quot;</span> <span class="attr">integrity</span>=<span class="string">&quot;sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u&quot;</span> <span class="attr">crossorigin</span>=<span class="string">&quot;anonymous&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">form</span> <span class="attr">class</span>=<span class="string">&quot;form my-2 my-lg-0&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">class</span>=<span class="string">&quot;form-control mr-sm-2&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text&quot;</span> <span class="attr">placeholder</span>=<span class="string">&quot;Search&quot;</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">button</span> <span class="attr">class</span>=<span class="string">&quot;btn btn-secondary my-2 my-sm-0&quot;</span> <span class="attr">type</span>=<span class="string">&quot;submit&quot;</span>&gt;</span>Search<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js&quot;</span> <span class="attr">integrity</span>=<span class="string">&quot;sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa&quot;</span> <span class="attr">crossorigin</span>=<span class="string">&quot;anonymous&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;idiom.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    idiom_list = json.load(read_file)</span><br><span class="line"></span><br><span class="line">idiom_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> idiom <span class="keyword">in</span> idiom_list:</span><br><span class="line">    idiom_dict[idiom[<span class="string">&#x27;word&#x27;</span>]] = idiom</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;idiom_dict.json&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">    json.dump(idiom_dict, write_file)</span><br><span class="line">print(<span class="string">&#x27;成功将原列表转换为key为成语的字典！&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>查看源代码，</p>
<p>参考：</p>
<p><a href="https://www.bootdey.com/snippets/view/Search-Results#html">https://www.bootdey.com/snippets/view/Search-Results#html</a></p>
<p><a href="https://blog.csdn.net/star_xing123/article/details/101271925">百度搜索框</a></p>
<p><a href="https://www.html.cn/qa/css3/12786.html">怎么在HTML中加入css样式？ - html中文网</a></p>
<p><a href="https://getbootstrap.com/docs/4.3/getting-started/introduction/">Bootstrap Introduction</a></p>
<p><a href="https://bootswatch.com/">Free themes for Bootstrap</a></p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo博客Next主题搭建全过程</title>
    <url>/hexo-next/</url>
    <content><![CDATA[<p>经过三次不明觉厉的error崩盘导致我重新初始化博客（不知道哪里出错所以只有推倒重来），2021年3月18日博客终于配置完毕。在此记录全过程。最后一次配置，我只修改了三个文件：_config.yml和next/_config.yml以及custom_file_path（当然还有css/images和source）。其中custom_file_path设置为styles.styl，用于<strong>修改文章内链接文本样式</strong>、<strong>文章内单行代码的样式设置</strong>等等，需要在custom_file_path中加入这个地址。 <span id="more"></span></p>
<h1 id="hexo博客">Hexo博客</h1>
<h2 id="博客的初始化">博客的初始化</h2>
<p>关于怎么部署参考：https://zhuanlan.zhihu.com/p/26625249</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd hexo-final</span><br><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo init</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></figure>
<p>完成以上步骤后，我们看到目前的插件如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-- hexo-generator-archive@1.0.0</span><br><span class="line">+-- hexo-generator-category@1.0.0</span><br><span class="line">+-- hexo-generator-index@2.0.0</span><br><span class="line">+-- hexo-generator-tag@1.0.0</span><br><span class="line">+-- hexo-renderer-ejs@1.0.0</span><br><span class="line">+-- hexo-renderer-marked@4.0.0</span><br><span class="line">+-- hexo-renderer-stylus@2.0.1</span><br><span class="line">+-- hexo-server@2.0.0</span><br><span class="line">+-- hexo-theme-landscape@0.0.3</span><br><span class="line">&#96;-- hexo@5.4.0</span><br></pre></td></tr></table></figure>
<p>我们需要安装其他插件，我使用pandoc渲染markdown，它也用于typora的渲染，可以很好地支持数学公式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-- hexo-browsersync@0.3.0</span><br><span class="line">+-- hexo-deployer-git@3.0</span><br><span class="line">+-- hexo-generator-index-pin-top@0.2.2</span><br><span class="line">+-- hexo-generator-searchdb@1.3.3</span><br><span class="line">+-- hexo-related-popular-posts@5.0.1</span><br><span class="line">+-- hexo-renderer-pandoc@0.3.0</span><br><span class="line">+-- hexo-symbols-count-time@0.7.1</span><br><span class="line">+-- hexo-hide-posts@0.1.1</span><br></pre></td></tr></table></figure>
<p>安装方式： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br><span class="line">npm install 插件 --save</span><br></pre></td></tr></table></figure> 安装完毕后，我们把_config.yml和next/_config.yml以及css/images和source中的文件（注意有_data/styles.styl，这是我的个性化设置）复制粘贴过来即可。</p>
<h2 id="使用hexo还是hugo">使用Hexo还是Hugo</h2>
<p>3月19日，我又尝试了hugo，的确渲染速度很快、有很多漂亮的主题，并且很多主题都有详细的文档，而且hugo是按照文件夹结构生成侧边栏的目录，非常适合做文档和电子书！但缺点是很多主题不能折叠侧边栏，提供沉浸式阅读体验，所以我还是放弃了hugo，可能以后遇到喜欢的主题还是会迁移到hugo吧。</p>
<h2 id="最终我的博客页面外观">最终我的博客页面外观</h2>
<p>我花了很多时间去研究怎样让博客更美观和简洁，其实有很多是无用功，所以3月19日我正式敲定了目前的版本，并不再继续进行博客主题的个性化配置。以下是博客文章的截图：</p>
<p><img src="https://i.loli.net/2021/03/19/MjEKeWoSuidzVxT.png"/></p>
<p>主色调为蓝绿色，辅以紫色，我把最终的配置文件备份在github地址：_config.yml，next/_config.yml，next/source/css/images和source（source中有styles.styl和我写的文章），并不再修改。</p>
<p>但是我经常用typora撰写笔记，因此<code>_post</code>文件夹下经常会更新，所以我写完文章有时会打开server，确认渲染无误，然后保存好写的文章。每隔一天或一周，我就hexo d一下，这样不至于操作太频繁。我可能也会使用<a href="https://novnan.github.io/Hexo/hexo-draft/">draft功能</a>。同时，我将_post文件夹同步在github上，经常push到这个repo。由此，我的博客配置和文章都得以云端保存了。</p>
<h2 id="百度和谷歌搜索">百度和谷歌搜索</h2>
<p>使用dns配置完成了百度认证，使用html完成了谷歌认证，但目前还不能搜到我的网站。不过这不重要，以后再publicize吧。</p>
<h1 id="我的写作工具">我的写作工具</h1>
<p>我用Typora写作，记录零碎的想法、翻译、笔记、代码等，但是<strong>上传网站后访问速度有点慢</strong>，所以一般我会用本地服务器查看笔记。手机的话，我会使用知乎或微信或csdn查看，所以<strong>重要的文章需要同步到各个网站</strong>。可能之后网站优化就好了~</p>
<p>各大同步地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">知乎、微信、csdn、hexo、github</span><br></pre></td></tr></table></figure>
<p>好啦，终于不用浪费时间在网站美工和找工具上啦！专心写文章吧！</p>
]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（上）</title>
    <url>/information-retrieval-1/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<blockquote>
<p><em>I am the very model of a modern Major-General,</em> <em>I’ve information vegetable, animal, and mineral,</em> <em>I know the kings of England, and I quote the fights historical</em> <em>From Marathon to Waterloo, in order categorical...</em></p>
<p>Gilbert and Sullivan, Pirates of Penzance</p>
</blockquote>
<p>假设你是一家跟踪航空公司股票的投资公司分析师。你的任务是确定航空公司机票价格上涨的公告与他们股票第二天的表现之间的关系（如果有的话）。关于股票价格的历史数据很容易得到，但是航空公司的公告呢？你至少需要知道航空公司的名称、提议票价上涨的性质、公告的日期，可能还需要知道其他航空公司的反应。幸运的是，这些都可以在新闻文章中找到，比如这一条。</p>
<span id="more"></span>
<p>美国联合航空公司（United Airlines）周五表示，以高燃油价格为理由，已将飞往一些城市的往返机票价格提高了6美元，而这些航班也由成本较低的航空公司提供服务。发言人蒂姆-瓦格纳（Tim Wagner）表示，AMR Corp.旗下的美国航空（American Airlines）立即配合这一举措。美国联合航空公司（UAL Corp.）旗下的联合航空公司（United）表示，涨价于周四生效，适用于该公司与折扣航空公司竞争的大部分航线，如芝加哥至达拉斯、丹佛至旧金山等。</p>
<p>本章介绍了从文本中提取有限种类的语义内容的技术。这种信息提取过程（IE）将嵌入文本中的非结构化信息提取信息转换为结构化数据，例如用于填充关系数据库以实现进一步处理。关系提取与填充关系数据库有着密切的联系。事实上，知识图谱，即结构化关系数据库的数据集，是搜索引擎向用户展示信息的一种常见方式。</p>
<p>接下来，我们讨论与事件相关的三个任务。事件提取是找到这些实体参与的事件，比如在我们的样本文本中，<em>United</em>（”美联航“）和<em>American</em>（”美航“）的票价上涨以及报道事件<em>said</em>（”说“）和<em>cite</em>（”引用“）。需要指代消歧（第22章）来弄清文本中哪些事件提及是指同一个事件；在我们的运行示例中，<em>increase</em>的两次出现和短语<em>the move</em>都是指同一个事件。</p>
<p>为了弄清文本中事件发生的时间，我们提取时间表达式，比如一周中的几天（周五和周四）；相对表达式，比如从现在起或明年起；以及时刻，比如下午3点半。这些表达式必须归一化到特定的日历日期或时间上，以便定位事件的时间。在我们的示例任务中，这将使我们能够将周五与美联航宣布的时间联系起来，将周四与前一天的票价上涨联系起来，并生成一条时间线，其中美联航的宣布紧随票价上涨，而美航的宣布紧随这两个事件。</p>
<p>最后，许多文本描述了反复出现的模式化事件或情境。模板填充的任务就是在文档中找到这样的情况，并填充到模板槽中。这些槽位填充物可能由直接从文本中提取的文本片段组成，也可能是通过额外处理从文本元素中推断出来的概念，如时间、金额或本体实体。我们的航空公司文本就是这种模式化情境的一个例子，因为航空公司经常会提高票价，然后等着看竞争对手是否跟进。在这种情况下，我们可以将美联航确定为最初提高票价的牵头航空公司，6美元为金额，周四为涨价日期，而美航则为跟风的航空公司，从而得到如下的填充模板。</p>
<figure>
<img src="C:\Users\13607\AppData\Roaming\Typora\typora-user-images\image-20210321212530121.png" alt="image-20210321212530121" /><figcaption aria-hidden="true">image-20210321212530121</figcaption>
</figure>
<h1 id="关系抽取">17.1 关系抽取</h1>
<p>假设我们已经检测到了样本文本中的命名实体（也许使用了第8章的技术），并想识别出检测到的实体之间的关系：</p>
<blockquote>
<p>以高油价为由，[ORG联合航空公司]表示，[时间周五]它已将飞往一些同样由低成本航空公司服务的城市的航班的票价每往返提高了[MONEY 6美元]。发言人[per蒂姆-瓦格纳]表示，[ORG美国航空]是[ORG AMR Corp.]的一个单位，立即配合这一举措。ORG美联航]，[ORG UAL Corp.]的一个单位，说增加生效[时间周四]，适用于它与折扣航空公司竞争的大多数航线，如[LOC芝加哥]到[LOC达拉斯]和[LOC丹佛]到[LOC旧金山]。<em>(机翻结果)</em></p>
</blockquote>
<p><img src="https://i.loli.net/2021/03/18/senloyDcQAGipId.png" width="700"/></p>
<p>例如，这段文本告诉我们，Tim Wagner是American Airlines的发言人，United是UAL Corp.的一个单位，American是AMR的一个单位。这些二元关系是更通用关系的实例，比如part-of或employes，它们在新闻风格的文本中出现得相当频繁。图17.1列出了ACE关系提取评估中使用的17种关系，图17.2显示了一些关系的样例。我们还可以提取更多的特定领域的关系，比如航空路线的概念。例如从这个文本中我们可以得出美联航有到芝加哥、达拉斯、丹佛和旧金山的航线。</p>
<center>
<img src="https://i.loli.net/2021/03/18/B5QEbCoyKd6ZsFi.png"  alt="" width="700" />
</center>
<table>
<caption>Figure 17.2 Semantic relations with examples and the named entity types they involve.</caption>
<thead>
<tr class="header">
<th>Relations</th>
<th>Types Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Physical-Located</td>
<td>PER-GPE He was in Tennessee</td>
</tr>
<tr class="even">
<td>Part-Whole-Subsidiary</td>
<td>ORG-ORG XYZ, the parent company of ABC</td>
</tr>
<tr class="odd">
<td>Person-Social-Family</td>
<td>PER-PER Yoko鈥檚 husband John</td>
</tr>
<tr class="even">
<td>Org-AFF-Founder</td>
<td>PER-ORG Steve Jobs, co-founder of Apple...</td>
</tr>
</tbody>
</table>
<p>这些关系很好地对应了我们在第15章中引入的模型理论概念，为逻辑形式的含义提供了基础。也就是说，一个关系由一系列有序的元组组成，基于一个领域的元素。在大多数标准的信息提取应用中，领域元素对应于文本中出现的命名实体，对应于指代消歧产生的基础实体，或者对应于从领域本体中选择的实体。图 17.3 显示了一个基于模型的视图，可以从我们的运行示例中提取实体和关系的集合。</p>
<table>
<colgroup>
<col style="width: 60%" />
<col style="width: 39%" />
</colgroup>
<thead>
<tr class="header">
<th>Domain领域</th>
<th><span class="math inline">\(D = {a,b, c,d, e, f,g,h,i}\)</span>元素</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>United, UAL, American Airlines, AMR</td>
<td><span class="math inline">\(a,b, c,d\)</span></td>
</tr>
<tr class="even">
<td>Tim Wagner</td>
<td><span class="math inline">\(e\)</span></td>
</tr>
<tr class="odd">
<td>Chicago, Dallas, Denver, and San Francisco</td>
<td><span class="math inline">\(f,g,h,i\)</span></td>
</tr>
<tr class="even">
<td><strong>Classes类别</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>United, UAL, American, and AMR are organizations</td>
<td><span class="math inline">\(Org = {a,b, c,d}\)</span></td>
</tr>
<tr class="even">
<td>Tim Wagner is a person</td>
<td><span class="math inline">\(Pers = {e}\)</span></td>
</tr>
<tr class="odd">
<td>Chicago, Dallas, Denver, and San Francisco are places</td>
<td><span class="math inline">\(Loc = { f,g,h,i}\)</span></td>
</tr>
<tr class="even">
<td><strong>Relations关系</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>United is a unit of UAL</td>
<td><span class="math inline">\(PartOf = {&lt;a,b&gt;,&lt;c,d&gt;}\)</span></td>
</tr>
<tr class="even">
<td>American is a unit of AMR</td>
<td></td>
</tr>
<tr class="odd">
<td>Tim Wagner works for American Airlines</td>
<td><span class="math inline">\(OrgAff = {&lt;c, e&gt;}\)</span></td>
</tr>
<tr class="even">
<td>United serves Chicago, Dallas, Denver, and San Francisco</td>
<td><span class="math inline">\(Serves = {&lt;a, f&gt;,&lt;a,g&gt;,&lt;a,h&gt;,&lt;a,i&gt;}\)</span></td>
</tr>
</tbody>
</table>
<p>请注意这种模型理论的角度是如何将NER任务也包含在内的；命名实体识别对应于一类一元关系的识别。</p>
<p>许多其他领域都定义了关系集。例如美国国家医学图书馆的统一医学语言系统（UMLS）有一个网络，定义了134个大的主题类别、实体类型和54个实体之间的关系，示例如下。</p>
<table>
<thead>
<tr class="header">
<th>Entity</th>
<th>Relation</th>
<th>Entity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Injury</td>
<td>disrupts</td>
<td>Physiological Function</td>
</tr>
<tr class="even">
<td>Bodily Location</td>
<td>location-of</td>
<td>Biologic Function</td>
</tr>
<tr class="odd">
<td>Anatomical Structure</td>
<td>part-of</td>
<td>Organism</td>
</tr>
<tr class="even">
<td>Pharmacologic Substance</td>
<td>causes</td>
<td>Pathological Function</td>
</tr>
<tr class="odd">
<td>Pharmacologic Substance</td>
<td>treats</td>
<td>Pathologic Function</td>
</tr>
</tbody>
</table>
<p>给出这样一个医学句子：</p>
<blockquote>
<p>(17.1) Doppler echocardiography can be used to diagnose left anterior descending artery stenosis in patients with type 2 diabetes</p>
<p>多普勒超声心动图可以用来诊断2型糖尿病患者的左前降支动脉狭窄</p>
</blockquote>
<p>我们可以提取UMLS关系：</p>
<blockquote>
<p>Echocardiography, Doppler Diagnoses Acquired stenosis</p>
<p>超声心动图，多普勒诊断获得性狭窄</p>
</blockquote>
<p>维基百科也提供了大量的关系，这些关系来自于<u>信息框</u>(infoboxes)，即与某些维基百科文章相关的结构化表格。例如，维基百科上斯坦福（Stanford）的信息框包括诸如state = "California"或president = "Marc Tessier-Lavigne"这样的结构化事实。这些事实可以转化为president-of或located-in，或转化为一种称为<u>RDF</u>（Resource Description Framework，资源描述框架）的元语言中的关系。一个RDF triple是实体-关系-实体构成的元组，被称为主-谓-宾（subject-predicate-object）表达式。下面是一个RDF三元组的实例：</p>
<table>
<thead>
<tr class="header">
<th>subject</th>
<th>predicate</th>
<th>object</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Golden Gate Park</td>
<td>location</td>
<td>San Francisco</td>
</tr>
</tbody>
</table>
<p>例如众包的DBpedia (Bizer et al., 2009) 是一个从维基百科衍生出来的本体，包含超过20亿个RDF三元组。另一个来自维基百科信息框的数据集Freebase (Bollacker et al., 2008)，现在是Wikidata的一部分 (Vrandeciˇ c and Kr ´ otzsch, 2014)，涵盖了人员和他们的国籍或位置以及他们出现在的其他位置之间的关系。WordNet 或其他本体提供了有用的本体关系，描述了词或概念之间的层级关系。例如WordNet在类之间有is-a或hypernym的关系：</p>
<blockquote>
<p>Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...</p>
<p>长颈鹿是反刍动物是蹄类动物是哺乳动物是脊椎动物</p>
</blockquote>
<p>WordNet也有个体和类之间的Instance-of关系，例如San Francisco就和city处于Instance-of关系。提取这些关系是扩展或构建本体的重要步骤。</p>
<p>最后，有一些大型的数据集，其中包含了手动标注的句子和它们的关系，用于训练和测试关系提取器。TACRED数据集 (Zhang et al., 2017) 包含106,264个关于特定人或组织的关系三元组的例子，在来自年度TAC知识库人口（TAC KBP）挑战赛的新闻和网络文本的句子中标记得来。TACRED包含41种关系类型（如per:出生城市、org:子公司、org:成员、per:配偶），以及一个无关系标签；如图17.4所示。大约80%的例子都被标注为无关系；拥有足够的负数据对于训练监督分类器非常重要。</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="header">
<th>Example</th>
<th>Entity Types Label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Carey will succeed <strong>Cathleen P. Black</strong>, who held the position for 15 years and will take on a new role as <u>chairwoman</u> of Hearst Magazines, the company said.</td>
<td><strong>PERSON</strong>/<u>TITLE</u><br/>Relation: <em>per:title</em></td>
</tr>
<tr class="even">
<td><strong>Irene Morgan Kirkaldy</strong>, who was born and reared in <u>Baltimore</u>, lived on Long Island and ran a child-care center in Queens with her second husband, Stanley Kirkaldy.</td>
<td><strong>PERSON</strong>/<u>CITY</u><br/>Relation: <em>per:city of birth</em></td>
</tr>
<tr class="odd">
<td><strong>Baldwin</strong> declined further comment, and said JetBlue chief <u>executive</u> Dave Barger was unavailable.</td>
<td>Types: <strong>PERSON</strong>/<u>TITLE</u><br/>Relation: <em>no relation</em></td>
</tr>
</tbody>
</table>
<p>SemEval 2010任务8也推出了一个标准数据集，检测命名词（nominal）之间的关系 (Hendrickx et al., 2009)。该数据集有10,717个例子，每个例子都有一个命名词对（未分类），手工标注为9个定向关系之一，比如产品-生产者<em>product-producer</em> (一家工厂生产西装 <em>a factory manufactures suits</em>) ，或者成分-整体<em>component-whole</em> (我的公寓有一个大厨房 <em>my apartment has a large kitchen</em>)。</p>
<hr />
<p><strong>本章剩余内容见：<a href="http://nlpcourse.cn/information-retrieval-2/">《自然语言处理综论》第17章-信息抽取（中）</a></strong></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>信息抽取技术综述</title>
    <url>/information-retrieval/</url>
    <content><![CDATA[<p>信息抽取是指从非结构化或半结构化文本中寻找结构化信息的任务。它是文本挖掘的一项重要任务，在自然语言处理、信息检索和网络挖掘等各个领域得到了广泛的研究。 <span id="more"></span> <strong>信息抽取(Information Extraction, IE)的两个基本任务：</strong></p>
<ul>
<li><p><strong>命名实体识别</strong>：识别实体的名称，如人、组织和地点。</p></li>
<li><p><strong>关系抽取</strong>：提取实体之间的语义关系，如FounderOf和HeadquarteredIn。</p></li>
</ul>
<p>本文，我们将对过去几十年命名实体识别和关系抽取方面的主要工作进行综述。</p>
<h1 id="信息抽取系统">信息抽取系统</h1>
<p>命名实体识别、指代消歧和关系抽取等任务，是成熟的、特定领域的信息提取系统的基本支持组件。</p>
<p>例如，给定下面的英文句子，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In 1998, Larry Page and Sergey Brin founded Google Inc.</span><br><span class="line">1998年，Larry Page和Sergey Brin创立了Google公司。</span><br></pre></td></tr></table></figure>
<p>我们可以提取以下信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">FounderOf(Larry Page, Google Inc.),</span><br><span class="line">FounderOf(Sergey Brin, Google Inc.),</span><br><span class="line">FoundedIn(Google Inc., <span class="number">1998</span>).</span><br></pre></td></tr></table></figure>
<p>提取的信息可以被其他计算机系统（如搜索引擎和数据库管理系统）利用，为最终用户提供更好的服务。具体提取的信息类型和结构取决于特定应用的需要。下面给出一些信息提取的应用实例：</p>
<blockquote>
<ul>
<li>生物医学研究人员经常需要从大量的科学出版物中筛选出与特定基因、蛋白质或其他生物医学实体相关的发现。为了协助这项工作，基于关键词匹配的简单搜索可能并不足够，因为生物医学实体通常具有同义词和模糊的名称，因此很难准确地检索相关文档。因此，生物医学文献挖掘的一项关键任务是从文本中自动识别生物医学实体的提及，并将其与现有知识库（如FlyBase）中的相应条目链接起来。</li>
<li>金融专业人员经常需要从新闻文章中寻找特定的信息，以帮助他们进行日常决策。例如，一家金融公司可能需要知道在某个时间跨度内发生的所有公司收购，以及每次收购的细节。从文本中自动查找此类信息需要标准的信息提取技术，如命名实体识别和关系提取。</li>
<li>情报分析员审查大量文本，以搜索参与恐怖主义事件的人员、使用的武器和攻击目标等信息。虽然信息检索技术可以用来快速查找描述恐怖主义事件的文件，但需要信息提取技术来进一步确定这些文件中的具体信息单元。</li>
<li>随着网络的快速发展，搜索引擎已经成为人们日常生活中不可缺少的一部分，现在用户的搜索行为也更加了解。基于文档的词袋表示的搜索已经不能提供满意的结果。更高级的搜索问题，如实体搜索、结构化搜索和问题解答等，可以为用户提供更好的搜索体验。为了方便这些搜索功能，通常需要将信息提取作为一个预处理步骤，以丰富文档表示或填充基础数据库。</li>
</ul>
</blockquote>
<p>早期的信息提取系统，如参与MUCs的系统，通常是基于规则的系统（如[32，42]）。它们使用人类开发的语言提取模式来匹配文本和定位信息单元。它们可以在特定的目标域上取得很好的性能，但是设计好的提取规则需要耗费大量的人力，而且开发的规则对领域的依赖性很强。意识到这些人工开发系统的局限性，研究人员转而采用统计机器学习的方法。而随着信息提取系统被分解为命名实体识别等组件，许多信息提取子任务可以转化为分类问题，这些问题可以通过标准的监督学习算法，如支持向量机和最大熵模型来解决。由于信息提取涉及到识别扮演不同角色的文本片段，序列标签方法，如隐藏马尔科夫模型和条件随机场也得到了广泛的应用。</p>
<p>在本章中，我们将重点关注信息提取中最基本的两个任务，即命名实体识别和关系提取。这两个任务的最先进的解决方案都依赖于统计机器学习方法。我们还讨论了传统上没有引起太多关注的无监督信息提取。本章的其余部分组织如下。第2节讨论了当前命名实体识别的方法，包括基于规则的方法和统计学习方法。第3节讨论了完全监督环境和弱监督环境下的关系提取。然后，我们在第4节讨论了无监督的关系发现和开放的信息提取。在第5节中，我们讨论了信息提取系统的评估。最后我们在第6节中总结。</p>
<h2 id="模板填充"><strong>模板填充</strong></h2>
<p>如图的恐怖主义模板中，左边是槽位子集，右边是槽位填充值。</p>
<p>其中一些槽位填充值，如"Enrique Ormazabal Ormazabal"和 "商人"是直接从文本中提取的，而其他的槽位填充值，如抢劫、完成、枪支等则是根据文档从对应槽位的预定义值集中选择的。</p>
<p><img width=500 src="https://i.loli.net/2021/03/09/An4ryIzRKLDSTsg.png"/></p>
<h1 id="有监督的方法">有监督的方法</h1>
<p>传统的信息提取任务对提取信息的结构有明确的定义，例如命名实体的类型、关系的类型、或者模板槽。在某些场景下，我们事先并不知道我们想要提取的信息结构，而希望从大型语料库中挖掘这样的结构。例如，从一组地震新闻文章中，我们可能希望自动发现地震的日期、时间、震中、震级和伤亡是新闻文章中报道的最重要的信息。</p>
<p>最近已经有一些关于这类无监督信息提取问题的研究，但总体上沿着这个方向的工作仍然有限。另一个新的方向是开放信息提取，系统要从Web这样一个庞大的、多样化的语料库中提取所有有用的实体关系。这种系统的输出不仅包括关系中涉及的论据，还包括从文本中提取的关系的描述。最近在这个方向上取得的进展包括TextRunner[6]、Woe[66]和ReVerb[29]等系统。</p>
<h1 id="无监督的方法">无监督的方法</h1>
<h2 id="现存系统">现存系统</h2>
<p>TextRunner</p>
<p>Woe</p>
<p>ReVerb</p>
<h2 id="术语解释">术语解释</h2>
<p>国防高级研究计划局, Defense Advanced Research Projects Agency, DARPA</p>
<p>消息理解会议, Message Understanding Conferences, MUC (DARPA发起并资助)</p>
<p>DeJong的FRUMP计划</p>
<h2 id="监督学习算法">监督学习算法</h2>
<h2 id="序列标注方法">序列标注方法</h2>
<h1 id="命名实体识别">命名实体识别</h1>
<p><strong>命名实体</strong>：指代某种现实世界实体的词语序列，例如“California,” “Steve Jobs” and “Apple Inc.”。</p>
<p><strong>命名实体识别（NER）</strong>：从自由形式的文本中识别出命名实体，并将其分类为一组预定义的类型，如人名、组织和地点。</p>
<p>命名实体识别是信息提取中最基本的任务。关系和事件提取等更复杂的任务，需要准确的命名实体识别作为基础。</p>
<p>通常情况下，这项任务不能简单地通过与预先编译的地名录进行字符串匹配来完成，因为给定实体类型的命名实体通常不会形成一个封闭的集合，任何地名录都是不完整的。另一个原因是，命名实体的类型可能取决于上下文。例如，"JFK "可能指的是 "John F. Kennedy"这个人或者"JFK International Airport "这个地点，或任何其他具有相同缩写的实体。为了确定在特定文档中出现的 "JFK "的实体类型，必须考虑其上下文。</p>
<p>NER已有多个评估项目，包括自动内容提取(ACE)项目、2002年和2003年自然语言学习会议(CoNLL)的共享任务[63]、BioCreAtIvE(Critical Assessment of Information Extraction Systems in Biology)挑战评估[2]。</p>
<p>最常研究的命名实体类型是人名、组织和地点，这是由MUC-6首次定义的。这些类型足够通用，对许多应用领域都有用。日期、时间、货币值和百分比等表达式的提取，也是由MUC-6引入的，通常也是在NER下研究的，尽管严格来说这些表达式不是命名实体。除了这些一般的实体类型外，其他类型的实体通常是针对特定领域和应用而定义的。例如，GENIA语料库使用细粒度的本体对生物实体进行分类[52]。在在线搜索和广告中，产品名称的提取是一项有用的任务。</p>
<p>参考文献：</p>
<p>https://zhuanlan.zhihu.com/p/266056681</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>用python处理json数据</title>
    <url>/json/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://baike.baidu.com/item/JSON">JSON</a>(<a href="https://baike.baidu.com/item/JavaScript">JavaScript</a> Object Notation, JS 对象简谱) 是一种轻量级的数据交换格式。它基于 <a href="https://baike.baidu.com/item/ECMAScript">ECMAScript</a> (欧洲计算机协会制定的js规范)的一个子集，采用完全独立于编程语言的文本格式来存储和表示数据。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。 易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。——百度百科</p>
</blockquote>
<p>编码和解码JSON数据的过程相当于把同一样东西翻译成中文和日语。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_str = &#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;example&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">18</span>&#125;</span><br><span class="line"><span class="comment"># 序列化json</span></span><br><span class="line">json_str = json.dumps(params, sort_keys=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 反序列化json</span></span><br><span class="line">dict_json = json.loads(json_str)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="序列化json">序列化JSON</h1>
<h2 id="将python对象转换为json">将Python对象转换为JSON</h2>
<table>
<thead>
<tr class="header">
<th>Python</th>
<th>JSON格式</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>dict</code></td>
<td><code>object</code></td>
</tr>
<tr class="even">
<td><code>list</code>， <code>tuple</code></td>
<td><code>array</code></td>
</tr>
<tr class="odd">
<td><code>str</code></td>
<td><code>string</code></td>
</tr>
<tr class="even">
<td><code>int</code>，<code>long</code>，<code>float</code></td>
<td><code>number</code></td>
</tr>
<tr class="odd">
<td><code>True</code></td>
<td><code>true</code></td>
</tr>
<tr class="even">
<td><code>False</code></td>
<td><code>false</code></td>
</tr>
<tr class="odd">
<td><code>None</code></td>
<td><code>null</code></td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;president&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Zaphod Beeblebrox&quot;</span>,</span><br><span class="line">        <span class="string">&quot;species&quot;</span>: <span class="string">&quot;Betelgeusian&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">type</span>(data)</span><br><span class="line"><span class="comment">#&lt;class &#x27;dict&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存为json文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data_file.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">    json.dump(data, write_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者将序列化的JSON数据写入python字符串对象</span></span><br><span class="line">json_string = json.dumps(data)</span><br><span class="line"><span class="comment"># &lt;class &#x27;str&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="一些有用的关键字参数">一些有用的关键字参数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json.dumps(data, indent=<span class="number">4</span>, sort_keys = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>indent定义缩进的级别；如果<em>sort_keys</em>为true，则字典的输出将按key排序。</p>
<h1 id="反序列化json读取json数据">反序列化JSON：读取JSON数据</h1>
<h2 id="将json编码的数据转换为python对象">将JSON编码的数据转换为Python对象</h2>
<table>
<thead>
<tr class="header">
<th>JSON</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>object</code></td>
<td><code>dict</code></td>
</tr>
<tr class="even">
<td><code>array</code></td>
<td><code>list</code></td>
</tr>
<tr class="odd">
<td><code>string</code></td>
<td><code>str</code></td>
</tr>
<tr class="even">
<td><code>number</code> (int)</td>
<td><code>int</code></td>
</tr>
<tr class="odd">
<td><code>number</code> (real)</td>
<td><code>float</code></td>
</tr>
<tr class="even">
<td><code>true</code></td>
<td><code>True</code></td>
</tr>
<tr class="odd">
<td><code>false</code></td>
<td><code>False</code></td>
</tr>
<tr class="even">
<td><code>null</code></td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<h2 id="从文件中读取">从文件中读取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data_file.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    data = json.load(read_file)</span><br><span class="line"><span class="built_in">type</span>(data)</span><br><span class="line"><span class="comment"># &lt;class &#x27;list&#x27;&gt;</span></span><br><span class="line"><span class="built_in">type</span>(data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="从字符串创建">从字符串创建</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_string = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    &quot;researcher&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;name&quot;: &quot;Ford Prefect&quot;,</span></span><br><span class="line"><span class="string">        &quot;species&quot;: &quot;Betelgeusian&quot;,</span></span><br><span class="line"><span class="string">        &quot;relatives&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                &quot;name&quot;: &quot;Zaphod Beeblebrox&quot;,</span></span><br><span class="line"><span class="string">                &quot;species&quot;: &quot;Betelgeusian&quot;</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">data = json.loads(json_string)</span><br></pre></td></tr></table></figure>
<h2 id="从api获取">从API获取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&quot;https://jsonplaceholder.typicode.com/todos&quot;</span>)</span><br><span class="line">todos = json.loads(response.text)</span><br><span class="line">todos == response.json()</span><br></pre></td></tr></table></figure>
<p>这里的伪json数据适合用来练习。</p>
<h1 id="遍历json字符串">遍历JSON字符串</h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">items &#x3D; data.items()</span><br><span class="line">for key, value in items:</span><br><span class="line">    print(str(key) + &#39;&#x3D;&#39; + str(value))</span><br></pre></td></tr></table></figure>
<p><strong>参考文献：</strong></p>
<p>https://realpython.com/python-json/</p>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title>英文文献的命名实体识别（上）</title>
    <url>/named-entity-recognition/</url>
    <content><![CDATA[<p>命名实体识别（NER）是指自动识别文本中的命名实体并将其分类为预定义的类别。实体可以是人员、组织、位置、时间、数量、货币价值、百分比等的名称。作为信息抽取的基础步骤，NER从非结构化文本中提取关键元素，因此它是一项至关重要的技术。</p>
<p>我们可以创建自己的实体类别以适应不同的任务。现在有许多出色的开源库，包括<a href="https://www.nltk.org/">NLTK</a>，<a href="https://spacy.io/">SpaCy</a>和<a href="https://nlp.stanford.edu/software/CRF-NER.shtml">Stanford NER</a>。如何用这些工具实现，参考<a href="https://www.kdnuggets.com/2018/08/named-entity-recognition-practitioners-guide-nlp-4.html">命名实体识别：NLP从业人员指南</a>和<a href="https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da">NLTK和SpaCy命名实体识别</a>。也可以参考<a href="https://monkeylearn.com/blog/named-entity-recognition/">这篇</a>文章。</p>
<span id="more"></span>
<h2 id="学术文献">学术文献</h2>
<p><a href="https://www.frontiersin.org/articles/10.3389/fcell.2020.00673/full">用于生物医学信息提取的命名实体识别和关系检测</a></p>
<p><a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3321-4">使用具有上下文信息的深度神经网络进行生物医学命名实体识别</a></p>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0885230815300504">用于命名实体识别工具的多领域评估框架</a></p>
<p><a href="https://devopedia.org/named-entity-recognition#Wang-et-al.-2019">命名实体识别</a></p>
<p><a href="https://paperswithcode.com/task/named-entity-recognition-ner">带代码的文献</a></p>
<h1 id="如何训练ner分类器">如何训练NER分类器</h1>
<p>通常使用BIO表示法，该表示法区分实体的开始（B）和内部（I），O标记非实体。NER往往需要特定领域的训练，尤其是更细粒度的NER。</p>
<h2 id="评估">评估</h2>
<p><em>F-Score</em>是用于评估NER的一种常用度量，它是Precision和Recall的组合。Precision, recall, and F-Score are defined as follows (<a href="https://www.frontiersin.org/articles/10.3389/fcell.2020.00673/full#B21">Campos et al., 2012</a>):</p>
<p><span class="math display">\[
\begin{array}{c}
\text { Precision }=\frac{\text { Relevant Names Recognized }}{\text { Total Names Recognized }} \\
=\frac{\text { True Positives }}{\text { True Positives+False Positives }} \\
\text { Recall }=\frac{\text { Relevant Names Recognized }}{\text { Relevant Names in Corpus }} \\
=\frac{\text { True Positives }}{\text { True Positives+False Negatives }} \\
\text { F-score }=2 \times \frac{\text { Precision } \times \text { Recall }}{\text { Precision+Recáll }}
\end{array}
\]</span></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>PDF文件处理大全</title>
    <url>/pdf/</url>
    <content><![CDATA[<h1 id="pdfplumberpdf文件预处理">Pdfplumber：PDF文件预处理</h1>
<h2 id="用pdfminer把pdf转为txt">用pdfminer把pdf转为txt</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pdfminer.high_level <span class="keyword">import</span> extract_text</span><br><span class="line">file_path = <span class="string">r&#x27;D:\pdf-file\Psychology_of_Language.pdf&#x27;</span></span><br><span class="line">text = extract_text(file_path, page_numbers=<span class="built_in">range</span>(<span class="number">33</span>,<span class="number">35</span>))</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p><code>pdfminer.high_level.``extract_text</code>（<em>pdf_file</em>，<em>password =''</em>，<em>page_numbers = None</em>，<em>maxpages = 0</em>，<em>caching = True</em>，<em>codec ='utf-8'</em>，<em>laparams = None</em> ）</p>
<p>参考：https://pdfminersix.readthedocs.io/en/latest/reference/highlevel.html#api-extract-text</p>
<h2 id="更精确的转换除去换行符">更精确的转换（除去换行符）</h2>
<h3 id="少量文件转换">少量文件转换</h3>
<h4 id="先把pdf转换为word">先把pdf转换为word</h4>
<p>转换工具：https://pdf2doc.com/zh/</p>
<h4 id="然后把word转为txt">然后把word转为txt</h4>
<p>文件多的话用多线程PDF转Word：https://github.com/python-fan/pdf2word</p>
<h3 id="批量转换">批量转换</h3>
<p>还不知道，参考：https://www.zhihu.com/question/357994254</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdfplumber</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">file_path = <span class="string">&#x27;book.pdf&#x27;</span></span><br><span class="line">pdf = pdfplumber.<span class="built_in">open</span>(file_path)</span><br><span class="line">start_page = <span class="number">34</span></span><br><span class="line">end_page = <span class="number">35</span></span><br><span class="line">text_string = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> page_num <span class="keyword">in</span> <span class="built_in">range</span>(start_page-<span class="number">1</span>, end_page):</span><br><span class="line">    text_string += pdf.pages[page_num].extract_text()+<span class="string">&#x27; &#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;book.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text_string)</span><br></pre></td></tr></table></figure>
<h2 id="把表格保存为csv">把表格保存为csv</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdfplumber</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">file_path = <span class="string">&#x27;book.pdf&#x27;</span></span><br><span class="line">pdf = pdfplumber.<span class="built_in">open</span>(file_path)</span><br><span class="line"></span><br><span class="line">chars = []</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> pdf.pages[<span class="number">7</span>:<span class="number">166</span>]:</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> page.chars:</span><br><span class="line">        chars.append(char)</span><br><span class="line">width_unique = <span class="built_in">set</span>([char[<span class="string">&#x27;height&#x27;</span>] <span class="keyword">for</span> char <span class="keyword">in</span> chars])</span><br><span class="line">print(<span class="string">&#x27;The unique heights: &#x27;</span>+<span class="built_in">str</span>(width_unique))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title>英文词性标记（POS Tagging）</title>
    <url>/pos-tagging/</url>
    <content><![CDATA[<h1 id="词性标记pos-tagging">词性标记（POS Tagging）</h1>
<p>POS标签大致分为两种：通用POS标签和细粒度POS标签。</p>
<h2 id="跨语言的通用pos标签">跨语言的通用POS标签</h2>
<p>通用依存关系（<a href="https://universaldependencies.org/">UD</a>）是一个致力于开发跨语言树库标注的项目，为不同语言标注一致的语法<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。 <span id="more"></span> 该标注方案基于（通用的）斯坦福依存关系（de Marneffe et al., 2006, 2008, 2014）、Google通用词性标签（Petrov et al., 2012）和Interset interlingua的形态语义标签集（Zeman, 2008）的发展进化。</p>
<p>该<a href="https://universaldependencies.org/u/pos/index.html">网站</a>列举了各个通用POS标签的详细解释。</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 14%" />
<col style="width: 34%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th>Tag</th>
<th>Category</th>
<th>Explanation</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/ADJ.html">ADJ</a></td>
<td>adjective</td>
<td>形容词，修饰名词或充当谓语</td>
<td>big, African, first</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/ADP.html">ADP</a></td>
<td>adposition</td>
<td>介词，包括前置词和后置词（prepositions and postpositions）</td>
<td>in, during</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/ADV.html">ADV</a></td>
<td>adverb</td>
<td>副词，修饰动词、形容词和副词本身，表示时间、地点、方向或方式</td>
<td>very, up, tomorrow, where, never</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/AUX_.html">AUX</a></td>
<td>auxiliary</td>
<td>助词，包括Tense auxiliaries, Passive auxiliaries, Modal auxiliaries, Verbal copulas</td>
<td>has, was, should</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/CCONJ.html">CCONJ</a></td>
<td>coordinating conjunction</td>
<td>并列连词</td>
<td>and, or, but</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/DET.html">DET</a></td>
<td>determiner</td>
<td>限定词，修饰名词</td>
<td>the, a, an</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/INTJ.html">INTJ</a></td>
<td>interjection</td>
<td>感叹词</td>
<td>ouch, yes</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/NOUN.html">NOUN</a></td>
<td>noun</td>
<td>普通名词</td>
<td>girl, tree, air</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/NUM.html">NUM</a></td>
<td>numeral</td>
<td>数词</td>
<td>2014, one, II</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/PART.html">PART</a></td>
<td>particle</td>
<td>小品词，与其他词语结合</td>
<td>'s, not</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/PRON.html">PRON</a></td>
<td>pronoun</td>
<td>代词，代替名词或名词短语</td>
<td>you, who, somebody, it</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/PROPN.html">PROPN</a></td>
<td>proper noun</td>
<td>专有名词，特定任务、地点或物体的名称</td>
<td>Mary, London, NATO</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/PUNCT.html">PUNCT</a></td>
<td>punctuation</td>
<td>标点符号是非字母字符和字符组</td>
<td>. , ()</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/SCONJ.html">SCONJ</a></td>
<td>subordinating conjunction</td>
<td>从属连词，引入从句的连词</td>
<td>since, that, who, if, while</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/SYM.html">SYM</a></td>
<td>symbol</td>
<td>符号，可以用普通单词代替（比如$换作美元）</td>
<td>$, %, ♥‿♥, 😝</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/VERB.html">VERB</a></td>
<td>verb</td>
<td>动词，表示事件和动作</td>
<td>run, runs, running</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/X.html">X</a></td>
<td>other</td>
<td>其他</td>
<td>xfgh</td>
</tr>
</tbody>
</table>
<p>该项目的<a href="https://github.com/UniversalDependencies/docs">Github地址</a>涵盖了不同语言。</p>
<h2 id="特定语言的细粒度pos标签">特定语言的细粒度POS标签</h2>
<p>我们可以将通用POS标签细化，例如将英语中的NOUN（普通名词）进一步划分为复数普通名词（NNS），NN（单数普通名词），但这些标签是基于特定语言的。</p>
<p>我们可以打开<a href="https://spacy.io/usage/linguistic-features">spacy</a>，线上运行以下代码。其中pos表示<a href="https://universaldependencies.org/docs/u/pos/">通用POS标签集</a>的粗粒度<a href="https://universaldependencies.org/docs/u/pos/">词性</a>，而tag表示细粒度的词性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">doc = nlp(<span class="string">&quot;It took me more than two hours to translate a few pages of English.&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">	print(token.text, <span class="string">&#x27;=&gt;&#x27;</span>,token.pos_,<span class="string">&#x27;=&gt;&#x27;</span>,token.tag_)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">It &#x3D;&gt; PRON &#x3D;&gt; PRP</span><br><span class="line">took &#x3D;&gt; VERB &#x3D;&gt; VBD</span><br><span class="line">me &#x3D;&gt; PRON &#x3D;&gt; PRP</span><br><span class="line">more &#x3D;&gt; ADJ &#x3D;&gt; JJR</span><br><span class="line">than &#x3D;&gt; SCONJ &#x3D;&gt; IN</span><br><span class="line">two &#x3D;&gt; NUM &#x3D;&gt; CD</span><br><span class="line">hours &#x3D;&gt; NOUN &#x3D;&gt; NNS</span><br><span class="line">to &#x3D;&gt; PART &#x3D;&gt; TO</span><br><span class="line">translate &#x3D;&gt; VERB &#x3D;&gt; VB</span><br><span class="line">a &#x3D;&gt; DET &#x3D;&gt; DT</span><br><span class="line">few &#x3D;&gt; ADJ &#x3D;&gt; JJ</span><br><span class="line">pages &#x3D;&gt; NOUN &#x3D;&gt; NNS</span><br><span class="line">of &#x3D;&gt; ADP &#x3D;&gt; IN</span><br><span class="line">English &#x3D;&gt; PROPN &#x3D;&gt; NNP</span><br><span class="line">. &#x3D;&gt; PUNCT &#x3D;&gt; .</span><br></pre></td></tr></table></figure>
<p>有关spaCy模型在不同语言中分配的细粒度和粗粒度词性标签的列表，请参阅<a href="https://spacy.io/models">models目录中</a>记录的标签方案。</p>
<p>比如英语的所有细粒度<a href="https://spacy.io/models/en">标签</a>如下：</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="header">
<th>A</th>
<th>B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>$</code></td>
<td>symbol, currency</td>
</tr>
<tr class="even">
<td><code>''</code></td>
<td>closing quotation mark</td>
</tr>
<tr class="odd">
<td><code>,</code></td>
<td>punctuation mark, comma</td>
</tr>
<tr class="even">
<td>```<code>| opening quotation mark                    | |</code>-LRB-<code>| left round bracket                        | |</code>-RRB-<code>| right round bracket                       | |</code>.<code>| punctuation mark, sentence closer         | |</code>:<code>| punctuation mark, colon or ellipsis       | |</code>ADD<code>| email                                     | |</code>AFX<code>| affix                                     | |</code>CC<code>| conjunction, coordinating                 | |</code>CD<code>| cardinal number                           | |</code>DT<code>| determiner                                | |</code>EX<code>| existential there                         | |</code>FW<code>| foreign word                              | |</code>HYPH<code>| punctuation mark, hyphen                  | |</code>IN<code>| conjunction, subordinating or preposition | |</code>JJ<code>| adjective                                 | |</code>JJR<code>| adjective, comparative                    | |</code>JJS<code>| adjective, superlative                    | |</code>LS<code>| list item marker                          | |</code>MD<code>| verb, modal auxiliary                     | |</code>NFP<code>| superfluous punctuation                   | |</code>NN<code>| noun, singular or mass                    | |</code>NNP<code>| noun, proper singular                     | |</code>NNPS<code>| noun, proper plural                       | |</code>NNS<code>| noun, plural                              | |</code>PDT<code>| predeterminer                             | |</code>POS<code>| possessive ending                         | |</code>PRP<code>| pronoun, personal                         | |</code>PRP<span class="math inline">\(` | pronoun, possessive | | `RB` | adverb | | `RBR` | adverb, comparative | | `RBS` | adverb, superlative | | `RP` | adverb, particle | | `SYM` | symbol | | `TO` | infinitival &quot;to | | `UH` | interjection | | `VB` | verb, base form | | `VBD` | verb, past tense | | `VBG` | verb, gerund or present participle | | `VBN` | verb, past participle | | `VBP` | verb, non-3rd person singular present | | `VBZ` | verb, 3rd person singular present | | `WDT` | wh-determiner | | `WP` | wh-pronoun, personal | | `WP\)</span><code>| wh-pronoun, possessive                    | |</code>WRB<code>| wh-adverb                                 | |</code>XX`</td>
<td>unknown</td>
</tr>
</tbody>
</table>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/#:~:text=Dependency%20parsing%20is%20the%20process,tags%20are%20the%20dependency%20tags.">词性标记和依存关系分析</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>pos</tag>
      </tags>
  </entry>
  <entry>
    <title>心理语言学：资源和知识整理</title>
    <url>/psycho-linguistics/</url>
    <content><![CDATA[<h1 id="心理语言学知识大纲">心理语言学知识大纲</h1>
<p><strong><em>*COURSE TITLE: Psycholinguistics (2020*</em><em>*-2021 spring*</em></strong> <strong><em>*semester)*</em></strong></p>
<p>Instructor: Zhaohong Wu Email: wuzhaohong@bfsu.edu.cn</p>
<p>Students: 2nd year Classroom hours: 2 hours / week</p>
<p>Class time and place: 10:10-12:00 Mon., 310 SEIS</p>
<p>Office hours: 15:10-17:00 Thur. (by appointment), 202 SEIS</p>
<p><strong><em>*A. OBJECTIVES*</em></strong></p>
<p>Introduce students to the psycholinguistic research and research methods of language comprehension, language production and language acquisition.</p>
<p><strong><em>*B. ORGANIZATION*</em></strong></p>
<p>The first meeting will provide an introduction to sentence/language processing.</p>
<p>The next three weeks will be general introductions to the three areas of psycholinguistics: language comprehension, language production, and L1 language acquisition.</p>
<p>Two weeks will be devoted to psycholinguistic methods and experimental design.</p>
<p>The next two weeks will be on statistical analysis using SPSS and R.</p>
<p>After that, for the remaining weeks, each course period will be organized around a question that has driven study of language comprehension, production, or acquisition. Required readings for the class will address the day’s topic from multiple perspectives. Class periods will begin with an introduction about the topic and theories by Zhaohong, and the remainder will be class discussion of the readings, facilitated by students.</p>
<p>Three students will collaboratively facilitate each class discussion per week. This will involve helping to choose the readings for that week and leading discussion based on your ideas and the discussion questions contributed by others.</p>
<p>Possible topics:</p>
<ol type="1">
<li><p>What role does prediction play in comprehension?</p></li>
<li><p>What role does experience with language play in comprehension?</p></li>
<li><p>What role does priming play in normal language comprehension/production?</p></li>
<li><p>What is the role of syntax/ linguistics formalisms in language processing?</p></li>
<li><p>How does prosody affect the way we understand language? Is there such a thing as implicit prosody and does it affect comprehension during reading?</p></li>
<li><p>Do speakers/writers maintain a model of what their interlocutor knows? How much do speakers/ writers take their listeners/readers into account when they’re producing language?</p></li>
<li><p>Is the processing of languages with very different syntactic properties (head-final languages, etc.) done very differently? How do people process head-final languages?</p></li>
<li><p>How is figurative language processed?</p></li>
<li><p>How do (and which) individual differences affect language processing (and why)? (e.g. working memory, executive functioning, social orientation)</p></li>
<li><p>What role do disfluencies play in language comprehension/production?</p></li>
<li><p>What information do learners use to determine the meanings of novel words?</p></li>
<li><p>What are the individual differences in first and second language acquisition?</p></li>
<li><p>Are there cognitive consequences of bilingualism?</p></li>
<li><p>Does a bilingual speaker have one or two lexicons? How are words in the two languages linked to each other and to the conceptual system?</p></li>
<li><p>What are the first language effects in learning a second language?</p></li>
</ol>
<p><strong><em>*C. SCHEDULE*</em></strong></p>
<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 93%" />
</colgroup>
<thead>
<tr class="header">
<th>Week</th>
<th>Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Psycholinguistics, a general introduction (Carroll, Chapter 1)</td>
</tr>
<tr class="even">
<td>2</td>
<td>Language Comprehension (Carrol, Chapter 4 &amp; 6)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Language Production (Carrol, Chapter 8)</td>
</tr>
<tr class="even">
<td>4</td>
<td>Language Acquisition (Carrol, Chapter 10 &amp; 12) UG parameter, statistical learning(frequency, use statistical information to segment speech into words), rule learning (generalize a rule from the inputs), topic+comment, overgeneralization, undergeneralizaiton, reduction; reinforecement or imitation; active construction of a grammar theory; social interaction theory; UG-based approach: phrase structure rules; comlementizer ; transforamtions; minimal link condition for wh-movement; neuroplacity</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Psycholinguistic Methods and Experimental Design</td>
</tr>
<tr class="even">
<td>6</td>
<td>Psycholinguistic Methods and Experimental Design</td>
</tr>
<tr class="odd">
<td>7</td>
<td>Statistical analyses</td>
</tr>
<tr class="even">
<td>8</td>
<td>Statistical analyses</td>
</tr>
<tr class="odd">
<td>9</td>
<td>Leading discussion: Topic 1</td>
</tr>
<tr class="even">
<td>10</td>
<td>Leading discussion: Topic 2</td>
</tr>
<tr class="odd">
<td>11</td>
<td>Leading discussion: Topic 3</td>
</tr>
<tr class="even">
<td>12</td>
<td>Leading discussion: Topic 4</td>
</tr>
<tr class="odd">
<td>13</td>
<td>Leading discussion: Topic 5</td>
</tr>
<tr class="even">
<td>14</td>
<td>Leading discussion: Topic 6</td>
</tr>
<tr class="odd">
<td>15</td>
<td>Individual project presentation</td>
</tr>
<tr class="even">
<td>16</td>
<td>Individual project discussion</td>
</tr>
</tbody>
</table>
<p><strong><em>*D.*</em></strong> <strong><em>*REQUIRED READINGS*</em></strong> <strong><em>*(*</em><em>*RELEVANT CHAPTERS)*</em></strong></p>
<p>Carroll, David W. (2008) <strong>Psychology of Language.</strong> 5th edition. Thomson Wadsworth</p>
<p><strong><em>*E*</em><em>*. ASSESSMENT:*</em></strong></p>
<p>l Final term paper: 50%</p>
<p>l Participation in class: 15%</p>
<p>l Leading discussion: 30%</p>
<p>l Final project presentation: 5%</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>心理语言学</category>
      </categories>
      <tags>
        <tag>psycholinguistics</tag>
      </tags>
  </entry>
  <entry>
    <title>信息抽取</title>
    <url>/information-retrieval-4/</url>
    <content><![CDATA[<h3 id="11-1-1-知识库-knowledge-base">11.1.1 知识库  (knowledge base)</h3>
<p>本章旨在教会你的机器人理解它所阅读的内容，并将知识存储在一个灵活的数据结构中，即将信息记录在一个知识库中，用于后期查询。除了识别文本中的数字和日期等简单任务之外，机器人还要提取更多关于世界的一般信息。</p>
<p>例如，它能够从自然语言文档中（比如维基百科）学习这句话：</p>
<blockquote>
<p>In 1983, Stanislav Petrov, a lieutenant colonel of the Soviet Air Defense Forces,  saved the world from nuclear war.</p>
<p>1983年，斯坦尼斯拉夫·彼得罗夫，苏联防空部队的一名中校从核战争中拯救了世界。</p>
</blockquote>
<p>如果你在历史课上读完或听完这样的内容后做笔记，你可能会对事情进行解读，并在大脑中建立概念或词语之间的联系。你可能会把它还原成一个知识，那个你“从中得到的东西”。你希望你的机器人也能做同样的事情， “记下”它所学到的任何东西，比如斯坦尼斯洛夫·彼得罗夫是一名中校的事实或知识。</p>
<p>它可以存储在一个类似这样的数据结构中：</p>
<blockquote>
<p>(‘Stanislav Petrov’, ‘is-a’, ‘lieutenant colonel’)</p>
</blockquote>
<p>这是知识图谱或知识库中两个<u>命名实体节点</u>(‘Stanislav Petrov’和’lieutenant colonel’)和它们之间的<u>关系</u>或连接(‘is a’)的例子。当这样的关系以符合知识图谱的<u>RDF标准</u>（关系描述格式）的形式存储时，它被称为<u>RDF三元模型(triplet)</u>。历史上，这些RDF triplet存储在XML文件中，但也可以存储在任何文件格式或数据库中，这些文件格式或数据库以 <u>(subject, relation, object)</u> 的形式保存，这些三元组的集合就是一个知识图谱。这有时也被语言学家称为<u>本体</u>（ontology），因为它存储的是关于词的结构化信息。但当图谱的目的是为了表示关于世界的事实而不仅仅是单词时，它就被称为<u>知识图谱</u>或<u>知识库</u>。</p>
<p>图11.1是你想从这样的句子中提取的知识图谱的图形表示。 图11.1顶部的“is-a”关系代表了一个不能直接从斯坦尼斯拉夫的陈述中提取的事实。但从一个军事组织成员的头衔是军衔这一事实可以推断出，“中校”是一个军衔。这种从知识图谱中推导出事实的逻辑操作叫做<u>知识推理</u>。也可以称为查询知识库，类似于查询关系型数据库。</p>
<img src="https://i.loli.net/2021/04/13/NoW1JAQhvMOaCTX.png"/>
<p>对于斯坦尼斯洛夫的军衔这一特殊推断或查询，你的知识图谱必须已经包含了关于军队和军衔的事实。也许你现在可以看到知识库是如何帮助机器理解一个语句的。如果没有这个知识基础，像这样简单的语句中的许多事实都会让你的聊天机器人“摸不着头脑”。你甚至可以说，关于职业等级的问题对于一个只知道如何根据随机分配的主题对文件进行分类的机器人来说，是“超乎寻常的&quot;。如果你曾经与一个不懂“哪条路是向上的“的聊天机器人进行过互动，你就会明白。在人工智能研究中，最令人生畏的挑战之一是如何编译和高效查询常识性知识的知识图谱。</p>
<p>**机器很难找到常识性知识的语料库来阅读和学习。**没有常识性知识的维基百科文章存在，你的机器就无法对其进行信息抽取。而有些知识是本能，是硬编码在我们的DNA中的，事物和人之间存在各种事实关系，比如“kindof&quot;、“is-used-for”、“has-a”、“is-famous-for”、“was-born“和“has-profession”。</p>
<p>卡耐基梅隆大学永无止境的语言学习机器人NELL，几乎完全专注于提取“kind-of”关系信息的任务。大多数知识库都会对定义这些关系的字符串进行归一化处理，这样“kind of”和“type of”就会被分配一个归一化的字符串或ID来表示这个特定的关系。而有些知识库也会对知识库中代表obejct的名词进行解析。所以“Stanislav Petrov”这个bigram可能会被分配一个特定的ID。“Stanislav Petrov”的同义词，比如“S. Petrov”和“Lt Col Petrov”，也会被分配给同一个ID，如果NLP管道怀疑它们指的是同一个人。 知识库可以用来构建一种实用型的聊天机器人，称为问答系统（QA系统）。客服聊天机器人，包括大学TA机器人，几乎完全依靠知识库来生成他们的回答。问答系统对于帮助人类发现事实信息非常有用，它可以让人类的大脑自由地做自己擅长的事情，比如试图从这些事实中归纳总结。<strong>人类不善于准确地记住事实，但善于发现这些事实之间的联系和模式，这是机器尚未掌握的。</strong></p>
<h3 id="11-1-2-信息抽取">11.1.2 信息抽取</h3>
<p>所以你已经了解到“信息抽取“是将非结构化文本转换为存储在知识库或知识图谱中的结构化信息。信息抽取是被称为自然语言理解（NLU）的研究领域的一部分，尽管这个术语经常与自然语言处理同义使用。  信息抽取和NLU是一种不同于你在研究数据科学时可能想到的学习。它不仅仅是无监督的学习，甚至连“模型“本身，即关于世界如何运作的逻辑，都可以在没有人类干预的情况下组成。与其说是给机器钓鱼（事实），不如说是教它如何钓鱼（提取信息）。尽管如此，机器学习技术还是经常被用来训练信息抽取器。</p>
<h3 id="11-2-2-信息抽取作为ML特征提取">11.2.2 信息抽取作为ML特征提取</h3>
<p>模式匹配（和正则表达式）仍然是信息抽取的最先进的方法。即使使用机器学习方法来处理自然语言，你也需要进行特征工程。你需要创建词袋或词嵌入，以尝试将自然语言文本中近乎无限的意义可能性减少到机器可以轻松处理的向量中。信息抽取只是机器学习从非结构化自然语言数据中提取特征的另一种形式，比如创建一个词袋，或者在这个词袋上做PCA。而这些模式和特征即使在最先进的自然语言机器学习管道中仍然被采用，比如谷歌的Assistant、Siri、亚马逊Alexa和其他最先进的机器人。</p>
<p>信息抽取可以事先完成，以填充事实的知识库。或者，当聊天机器人被问到一个问题或搜索引擎被查询时，可以按需找到所需的语句和信息。当提前建立知识库时，可以优化数据结构，以便在更大的知识领域内进行更快的查询。预先构建的知识库可以让聊天机器人快速响应关于更广泛信息的问题。如果在聊天机器人被查询时实时检索信息，这通常被称为“搜索&quot;。</p>
<p>Google和其他搜索引擎结合了这两种技术，查询知识库（knowledge base），如果没有找到必要的事实，就回落到文本搜索。你在学校里学到的许多自然语言语法规则可以被编码在正式的语法中，设计成对代表语言部分的单词或符号进行操作。而英语语言可以被认为是构成语言的单词和语法规则，或者你也可以把它看成是你可以说的所有可能的事情的集合，这些事情会被英语语言使用者认可为有效的语句。 而这就引出了<u>形式化语法</u>和<u>有限状态机</u>的另一个特点，它将在NLP中派上用场。</p>
<p>任何形式化语法都可以被机器以两种方式使用：</p>
<ul>
<li>
<p>识别与该语法相匹配的内容</p>
</li>
<li>
<p>生成新的符号序列</p>
</li>
</ul>
<p>你不仅可以使用模式（正则表达式）从自然语言中提取信息，还可以在聊天机器人中使用它们，让它“说“出与该模式相匹配的东西！我们将向你展示如何用有限状态机来实现这一点。我们在这里向你展示如何使用一个名为rstr4的包来实现这个功能，用于你的一些信息抽取模式。 这种形式化语法和有限状态机的模式匹配方法还有其他一些很棒的特性。</p>
<p>一个真正的有限状态机可以保证总是在有限时间内运行（停止）。它将始终告诉你是否在你的字符串中找到了匹配。它永远不会陷入永久循环…只要你不使用正则表达式引擎的一些高级功能，这些功能允许你“欺骗“并将循环纳入你的有限状态机。 所以，你要坚持使用不需要这些“回看“或“前瞻“作弊的正则表达式。你将确保你的正则表达式匹配器会处理每个字符，并且只有当它匹配时才会前进到下一个字符–有点像一个严格的列车员在座位上走动检查车票。如果你没有，列车员就会停下来，并宣布有问题，不匹配，他拒绝继续前进，或查看你的前面或后面，直到他解决这个问题。火车上的乘客没有“走回头路“或“重来&quot;，也没有严格的正则表达式。</p>
<h2 id="11-3-值得提取的信息">11.3 值得提取的信息</h2>
<p>一些关键的定量信息值得“手工制作”正则表达式的努力：</p>
<blockquote>
<p>GPS位置｜日期｜价格｜数字</p>
</blockquote>
<p>其他重要的自然语言信息需要比正则表达式更复杂的模式：</p>
<blockquote>
<p>问题触发词｜问题目标词｜命名实体</p>
</blockquote>
<h3 id="11-3-1-提取GPS位置">11.3.1 提取GPS位置</h3>
<p>GPS位置是典型的数字数据，你需要使用正则表达式从文本中提取。GPS位置是以一对经纬度的数字值来表示的。它们有时还包括第三个数字，即海拔高度，或海平面以上的高度，但你现在将忽略它。我们只提取十进制的经纬度对，用度数表示。这将适用于许多谷歌地图的URL。虽然URL在技术上不是自然语言，但它们通常是非结构化文本数据的一部分，你想提取这一点信息，这样你的聊天机器人就可以知道地方以及事物。 让我们使用之前例子中的十进制数字模式，但让我们更具限制性，确保该值在纬度（+/- 90度）和经度（+/- 180度）的有效范围内。你不能比北极（+90度）更北，也不能比南极（-90度）更南。而如果你从英国格林威治向东航行180度（+180度经度），你就会到达日期线，在那里你也是从格林威治向西180度（-180度）。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lat = <span class="string">r&#x27;([-]?[0-9]?[0-9][.][0-9]&#123;2,10&#125;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lon = <span class="string">r&#x27;([-]?1?[0-9]?[0-9][.][0-9]&#123;2,10&#125;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sep = <span class="string">r&#x27;[,/ ]&#123;1,3&#125;&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps = re.<span class="built_in">compile</span>(lat + sep + lon)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&#x27;http://...maps/@34.0551066,-118.2496763...&#x27;</span>)</span><br><span class="line">[(<span class="number">34.0551066</span>, -<span class="number">118.2496763</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&quot;https://www.openstreetmap.org/#map=10/5.9666/116.0566&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;5.9666&#x27;</span>, <span class="string">&#x27;116.0566&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&quot;Zig Zag Cafe is at 45.344, -121.9431 on my GPS.&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;45.3440&#x27;</span>, <span class="string">&#x27;-121.9431&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>数字数据很容易提取，特别是如果数字是机器可读字符串的一部分。URL和其他机器可读的字符串将经纬度等数字以可预测的顺序、格式和单位排列，以方便我们。这种模式仍然会接受一些不属于这个世界的经纬度值，但它可以满足你从OpenStreetMap等地图网络应用中复制的大多数URL的要求。 但是日期呢？正则表达式对日期有用吗？如果你想让你的日期提取器能在欧洲和美国工作，那该怎么办，因为那里的日/月顺序经常是相反的。</p>
<h3 id="11-3-2-提取日期">11.3.2 提取日期</h3>
<p>日期比 GPS 坐标更难提取。日期是一种比较自然的语言，类似的事情有不同的方言来表达。在美国，2017年的圣诞节是“12/25/17&quot;。在欧洲，2017年的圣诞节是“25/12/17&quot;。你可以检查你的用户的所在地，并假设他们和他们所在地区的其他人一样写日期。但这个假设可能是错误的。 因此，大多数日期和时间提取器都会尝试使用这两种日/月顺序，并检查以确保这是一个有效的日期。当我们读到这样的日期时，人脑就是这样工作的。即使你是一个美式英语的人，你在圣诞节前后在布鲁塞尔，你也可能会认出“25/12/17“是一个节日，因为一年只有12个月。</p>
<p>**这种在计算机编程中行之有效的“鸭子打字“ (duck-typing) 法<a href="https://www.pythonf.cn/read/129220">^1</a>也可以用于自然语言。如果它看起来像一只鸭子，行为又像鸭子，那它可能就是一只鸭子。**如果它看起来像个日期，行为又像个日期，那可能就是个日期。你会把这种“先试后买“的方法也用在其他自然语言处理任务上。你会尝试一堆选项，然后接受一个有效的选项。你会尝试你的提取器或生成器，然后你会在上面运行一个验证器，看看它是否合理。 对于聊天机器人来说，这是一个特别强大的方法，允许你结合多个自然语言生成器的优点。在第10章中，你使用LSTMs生成了一些聊天机器人的回复。为了改善用户体验，你可以生成很多回复，然后选择拼写、语法和情感最好的那个。我们将在第12章中详细讨论这个问题。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>us = <span class="string">r&#x27;((([01]?\d)[-/]([0123]?\d))([-/]([0123]\d)\d\d)?)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mdy = re.findall(us, <span class="string">&#x27;Santa came 12/25/2017. An elf appeared 12/12.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mdy</span><br><span class="line">[(<span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;25&#x27;</span>, <span class="string">&#x27;/2017&#x27;</span>, <span class="string">&#x27;20&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>列表理解可以用来为提取的数据提供一点结构，方法是将月份、日期和年份转换为整数，并用有意义的名称标记这些数字信息，如下面的列表所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates = [&#123;<span class="string">&#x27;mdy&#x27;</span>: x[<span class="number">0</span>], <span class="string">&#x27;my&#x27;</span>: x[<span class="number">1</span>], <span class="string">&#x27;m&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">2</span>]), <span class="string">&#x27;d&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">3</span>]),</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;y&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">4</span>].lstrip(<span class="string">&#x27;/&#x27;</span>) <span class="keyword">or</span> <span class="number">0</span>), <span class="string">&#x27;c&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">5</span>] <span class="keyword">or</span> <span class="number">0</span>)&#125; <span class="keyword">for</span> x <span class="keyword">in</span> mdy]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates</span><br><span class="line">[&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">25</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">0</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>即使对于这些简单的日期，也不可能设计出一个能够解决第二个日期“12/12”中所有歧义的regex。在日期的语言中，有一些含糊不清的地方，只有人类能够利用圣诞节等知识和文本作者的意图来猜测解决。</p>
<p>比如“12/12“可能意味着：</p>
<blockquote>
<p>December 12th, 2017—month/day in the estimated year based on anaphora resolution</p>
<p>December 12th, 2018—month/day in the current year at time of publishing</p>
<p>December 2012—month/year in the year 2012</p>
</blockquote>
<p>因为在美国日期和我们的regex中，month/day在年份之前，&quot;12/12“被假定为未知年份的12月12日。你可以使用内存中的结构化数据中的上下文来填充任何缺失的数字字段，如下面的列表所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(dates):</span><br><span class="line"><span class="meta">... </span><span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line"><span class="meta">... </span><span class="keyword">if</span> <span class="keyword">not</span> v:</span><br><span class="line"><span class="meta">... </span>d[k] = dates[<span class="built_in">max</span>(i - <span class="number">1</span>, <span class="number">0</span>)][k]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates</span><br><span class="line">[&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">25</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> date</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>datetimes = [date(d[<span class="string">&#x27;y&#x27;</span>], d[<span class="string">&#x27;m&#x27;</span>], d[<span class="string">&#x27;d&#x27;</span>]) <span class="keyword">for</span> d <span class="keyword">in</span> dates]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>datetimes</span><br><span class="line">[datetime.date(<span class="number">2017</span>, <span class="number">12</span>, <span class="number">25</span>), datetime.date(<span class="number">2017</span>, <span class="number">12</span>, <span class="number">12</span>)]</span><br></pre></td></tr></table></figure>
<p>这是一种从自然语言文本中提取日期信息的基本但相当健壮的方法。要把它变成一个生产型的日期提取器，剩下的主要任务就是添加一些适合你的应用的异常捕获和上下文维护。如果你通过拉取请求将其添加到nlpia包(<a href="http://github.com/">http://github.com/</a> totalgood/nlpia)中，我相信你的读者朋友们会很感激。而如果你为时间添加了一些提取器，好吧，那你就是相当的英雄了。 一些手工制作的逻辑有机会处理几个月甚至几天的边缘情况和自然语言名称。但是再复杂也无法解决“12/11“这个日期的歧义。这可能是12月11日，无论你在哪一年读到或听到它11月12日，如果你在伦敦或塔斯马尼亚州朗塞斯顿（一个联邦领土）听到它2011年12月，如果你在美国报纸上读到它2012年11月，如果你在欧盟报纸上读到它有些自然语言的歧义是无法解决的，即使是人脑。但是，让我们确保你的日期提取器可以通过在你的regex中颠倒月份和日期来处理欧洲的日/月顺序。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>eu = <span class="string">r&#x27;((([0123]?\d)[-/]([01]?\d))([-/]([0123]\d)?\d\d)?)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy = re.findall(eu, <span class="string">&#x27;Alan Mathison Turing OBE FRS (23/6/1912-7/6/1954) \</span></span><br><span class="line"><span class="string">... was an English computer scientist.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy</span><br><span class="line">[(<span class="string">&#x27;23/6/1912&#x27;</span>, <span class="string">&#x27;23/6&#x27;</span>, <span class="string">&#x27;23&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/1912&#x27;</span>, <span class="string">&#x27;19&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;7/6/1954&#x27;</span>, <span class="string">&#x27;7/6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/1954&#x27;</span>, <span class="string">&#x27;19&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy = re.findall(eu, <span class="string">&#x27;Alan Mathison Turing OBE FRS (23/6/12-7/6/54) \</span></span><br><span class="line"><span class="string">... was an English computer scientist.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy</span><br><span class="line">[(<span class="string">&#x27;23/6/12&#x27;</span>, <span class="string">&#x27;23/6&#x27;</span>, <span class="string">&#x27;23&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/12&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;7/6/54&#x27;</span>, <span class="string">&#x27;7/6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/54&#x27;</span>, <span class="string">&#x27;&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>这个正则表达式正确地从维基百科的摘录中提取了图灵的生卒日期。但我作弊了，我把“6月”这个月份转换成了数字6，然后才在维基百科的那句话上测试正则表达式。所以这不是一个现实的例子。而且如果不指定世纪的话，你对年份的解析还是会有一些歧义的。54年是指1954年还是指2054年？你希望你的聊天机器人能够从未经修改的维基百科文章中提取日期，这样它就可以阅读名人的资料，学习导入日期。为了让您的正则表达式能够在更多的自然语言日期上发挥作用，例如维基百科文章中的日期，您需要在您的日期提取正则表达式中添加诸如“June&quot;（及其所有缩写）这样的单词。 你不需要任何特殊符号来表示单词（按顺序排列的字符）。你可以在正则表达式中完全按照你希望它们在输入中的拼写来输入，包括大写。你所要做的就是在它们之间的正则表达式中加上一个OR符号(|)。你需要确保它可以处理美国的月/日顺序以及欧洲的顺序。你将把这两种可供选择的日期“拼法“添加到你的正则表达式中，在它们之间加上一个“大“的 OR (|)，作为正则表达式中决策树的分叉。 让我们使用一些命名组来帮助你识别年份，比如&quot;‘84’&quot;是1984，&quot;08“是2008。而且让我们尝试更精确地匹配4位数的年份，只匹配未来到2399年的年份和过去到0.6年的年份，请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_19xx = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr_19xx&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>, <span class="number">100</span>)) +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_20xx = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr_20xx&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;:02d&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)) + <span class="string">&#x27;|&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>, <span class="number">30</span>)) +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_cent = <span class="string">r&#x27;\b(?P&lt;yr_cent&gt;&#x27;</span> + <span class="string">&#x27;|&#x27;</span>.join(</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">40</span>)) + <span class="string">r&#x27;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_ccxx = <span class="string">r&#x27;(?P&lt;yr_ccxx&gt;&#x27;</span> + <span class="string">&#x27;|&#x27;</span>.join(</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;&#123;:02d&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>)) + <span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_xxxx = <span class="string">r&#x27;\b(?P&lt;yr_xxxx&gt;(&#x27;</span> + yr_cent + <span class="string">&#x27;)(&#x27;</span> + yr_ccxx + <span class="string">r&#x27;))\b&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span>yr_19xx + <span class="string">&#x27;|&#x27;</span> + yr_20xx + <span class="string">&#x27;|&#x27;</span> + yr_xxxx +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>groups = <span class="built_in">list</span>(re.finditer(</span><br><span class="line"><span class="meta">... </span>yr,“<span class="number">0</span>, <span class="number">2000</span>, 01, <span class="string">&#x27;08, 99, 1984, 2030/1970 85 47 `66&quot;))</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; full_years = [g[&#x27;</span>y<span class="string">r&#x27;] for g in groups]</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; full_years</span></span><br><span class="line"><span class="string">[&#x27;</span><span class="number">2000</span><span class="string">&#x27;, &#x27;</span>01<span class="string">&#x27;, &#x27;</span>08<span class="string">&#x27;, &#x27;</span><span class="number">99</span><span class="string">&#x27;, &#x27;</span><span class="number">1984</span><span class="string">&#x27;, &#x27;</span><span class="number">2030</span><span class="string">&#x27;, &#x27;</span><span class="number">1970</span><span class="string">&#x27;, &#x27;</span><span class="number">85</span><span class="string">&#x27;, &#x27;</span><span class="number">47</span><span class="string">&#x27;, &#x27;</span><span class="number">66</span><span class="string">&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>哇！这可真够费劲的。这是一个很大的工作，只是为了在regex中而不是在Python中处理一些简单的年份规则。别担心，有一些包可以用来识别常见的日期格式。它们更精确 (更少的错误匹配) 和更通用 (更少的失误)。所以，你不需要自己能够编写复杂的正则表达式，比如这样。这个例子只是给你提供了一个模式，以备将来你需要使用正则表达式提取某种特定的数字。货币值和 IP 地址是一些例子，在这些例子中，一个更复杂的正则表达式（带有命名组）可能会派上用场。 让我们完成你的提取日期的正则表达式，为维基百科上的日期添加月份名称的模式，比如图灵生日中的“June“或“Jun&quot;，如下面的列表所示。你能看到如何将这些正则表达式组合成一个更大的可以处理欧盟和美国日期格式的正则表达式吗？一个复杂的问题是，你不能为一个组重复使用相同的名称（正则表达式的括号部分）。所以你不能在月份和年份的命名正则表达式的美国和欧盟排序之间放一个OR。而且你需要在日、月、年之间包含一些可选的分隔符的模式。最后，你需要验证这些日期，看看它们是否可以变成有效的Python日期时间对象，如下面的列表所示。想想像Python-dateutil和datefinder这样的包是如何解决歧义和处理更多“自然“语言的日期，比如“今天“和“下周一&quot;。如果你认为你能比这些包做得更好，就给他们发个拉请求吧!  如果你只是想要一个最先进的日期提取器，统计（机器学习）方法会让你更快地达到目的。Stanford Core NLP SUTime库(https:// <a href="http://nlp.stanford.edu/software/sutime.html">nlp.stanford.edu/software/sutime.html</a>)和Google的dateutil.parser.parse是最先进的。</p>
<h2 id="11-4-提取关系（relation）">11.4 提取关系（relation）</h2>
<p>到目前为止，你只研究了提取棘手的名词实例，如日期和GPS经纬度值。而且你主要是处理数字模式。现在是时候解决从自然语言中提取知识这个更难的问题了。</p>
<p>你想让你的机器人从阅读知识百科全书（如维基百科）中了解关于世界的事实。你希望它能够将这些日期和GPS坐标与它所阅读的实体联系起来。你的大脑可以从维基百科的这句话中提取什么知识呢？</p>
<blockquote>
<p>On March 15, 1554, Desoto wrote in his journal that the Pascagoula people ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5.</p>
<p>1554年3月15日，德索托在他的日记中写道，帕斯卡古拉人的范围是位于30.4，-88.5的最北边的利夫河和奇卡索河的交汇处。</p>
</blockquote>
<p>提取日期和GPS坐标也许能让你把这个日期和地点、德索托、帕斯卡古拉人以及这两条你念不出名字的河流联系起来。你希望你的机器（和你的大脑）能够将这些事实与更大的事实联系起来——例如，德索托是一个西班牙征服者，而帕斯卡古拉人是一个和平的美国土著部落。而且你希望日期和地点能与正确的“东西”联系起来：分别是德索托和两条河流的交汇处。</p>
<p>这就是大多数人听到自然语言理解这个词时想到的。要理解一个语句，你需要能够提取关键的信息，并将其与相关知识关联起来。对于机器来说，你将这些知识存储在一个图谱中，也称为知识库。你的知识图谱的边就是事物之间的关系。而你的知识图谱的节点就是在你的语料库中找到的名词或对象。 你要用来提取这些关系（或关系）的模式是一种subject - verb - object这样的模式。为了识别这些模式，你需要你的NLP管道知道句子中每个词的词性。</p>
<h3 id="11-4-1-词性（POS）标记">11.4.1 词性（POS）标记</h3>
<p>POS标记可以通过语言模型来完成，这些语言模型具有包含所有可能词性的单词字典。然后，他们可以对正确标记的句子进行训练，以识别新句子和字典中的其他单词中的词性。NLTK和spaCy都实现了POS标记功能。你在这里会使用spaCy，因为它更快、更准确。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> spacy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>en_model = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = (<span class="string">&quot;In 1541 Desoto wrote in his journal that the Pascagoula people&quot;</span> + <span class="string">&quot;ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent = en_model(sentence)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent.ents</span><br><span class="line">(<span class="number">1541</span>, Desoto, Pascagoula, Leaf, Chickasawhay, <span class="number">30.4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27; &#x27;</span>.join([<span class="string">&#x27;&#123;&#125;_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tok, tok.tag_) <span class="keyword">for</span> tok <span class="keyword">in</span> parsed_sent])</span><br><span class="line"><span class="string">&#x27;In_IN 1541_CD Desoto_NNP wrote_VBD in_IN his_PRP$ journal_NN that_IN the_DT Pascagoula_NNP people_NNS ranged_VBD as_RB far_RB north_RB as_IN the_DT confluence_NN of_IN the_DT Lea f_NNP and_CC Chickasawhay_NNP rivers_VBZ at_IN 30.4_CD ,_, -88.5_NFP ._.&#x27;</span></span><br></pre></td></tr></table></figure>
<p>因此，为了建立你的知识图谱，你需要弄清楚哪些对象（名词短语）应该配对。你想把日期“1554年3月15日“与命名实体Desoto配对。然后你可以将这两个字符串（名词短语）解析为指向你知识库中的对象。1554年3月15日可以转换为具有归一化表示的datetime.date对象。</p>
<p>spaCy-parsed句子还包含嵌套字典中的依存树。而spacy.displacy可以生成一个可扩展的矢量图形SVG字符串（或一个完整的HTML页面），它可以在浏览器中作为图像查看。这种可视化可以帮助您找到使用该树创建标签模式以进行关系提取的方法。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> spacy.displacy <span class="keyword">import</span> render</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = <span class="string">&quot;In 1541 Desoto wrote in his journal about the Pascagoula.&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent = en_model(sentence)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;pascagoula.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="meta">... </span>f.write(render(docs=parsed_sent, page=<span class="literal">True</span>, options=<span class="built_in">dict</span>(compact=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>
<p>这个短句的依存树显示名词短语“The Pascagoula”是主语“Desoto”关系“met”的宾语（见图11.2）。两个名词都被标记为专有名词。</p>
<img src="https://i.loli.net/2021/04/13/oF3AcQ9CBVrps1X.png"/>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">token_dict</span>(<span class="params">token</span>):</span></span><br><span class="line"><span class="meta">... </span>	<span class="keyword">return</span> OrderedDict(ORTH=token.orth_, LEMMA=token.lemma_, POS=token.pos_, TAG=token.tag_, DEP=token.dep_)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">doc_dataframe</span>(<span class="params">doc</span>):</span></span><br><span class="line"><span class="meta">... </span>	<span class="keyword">return</span> pd.DataFrame([token_dict(tok) <span class="keyword">for</span> tok <span class="keyword">in</span> doc])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc_dataframe(en_model(<span class="string">&quot;In 1541 Desoto met the Pascagoula.&quot;</span>))</span><br><span class="line">         ORTH       LEMMA    POS  TAG    DEP</span><br><span class="line"><span class="number">0</span>          In          <span class="keyword">in</span>    ADP   IN   prep</span><br><span class="line"><span class="number">1</span>        <span class="number">1541</span>        <span class="number">1541</span>    NUM   CD   pobj</span><br><span class="line"><span class="number">2</span>      Desoto      desoto   NOUN   NN  nsubj</span><br><span class="line"><span class="number">3</span>         met        meet   VERB  VBD   ROOT</span><br><span class="line"><span class="number">4</span>         the         the    DET   DT    det</span><br><span class="line"><span class="number">5</span>  Pascagoula  Pascagoula  PROPN  NNP   dobj</span><br><span class="line"><span class="number">6</span>           .           .  PUNCT    .  punct</span><br></pre></td></tr></table></figure>
<p>现在您可以看到POS或TAG特性的序列，它们将构成一个良好的模式。如果您正在查找人员和组织之间的“has meet”关系，您可能希望允许使用诸如“PROPN met PROPN”、“PROPN met the PROPN”、“PROPN met the PROPN”、“PROPN met with the PROPN”和“PROPN frequency meeting with PROPN”之类的模式。您可以分别指定这些模式中的每一个，或者尝试使用一些*或者？专有名词之间的“任意词”模式的运算符：</p>
<blockquote>
<p>‘PROPN ANYWORD? met ANYWORD? ANYWORD? PROPN’</p>
</blockquote>
<p>spaCy中的模式比前面的伪代码更强大、更灵活，因此您必须更详细地解释您想要匹配的word特性。在spaCy模式规范中，您可以使用字典来捕获要为每个标记或单词匹配的所有标记，如下面的清单所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = [[&#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, [&#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;],</span><br><span class="line"><span class="meta">... </span>	[&#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;meet&#x27;</span>&#125;],</span><br><span class="line"><span class="meta">... </span>	[&#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;, &#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;]]</span><br></pre></td></tr></table></figure>
<p>然后可以从解析的句子中提取所需的标记标记，如下面的清单所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;In 1541 Desoto met the Pascagoula.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher = Matcher(en_model.vocab)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher.add(<span class="string">&#x27;met&#x27;</span>, [pattern])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">[(<span class="number">12280034159272152371</span>, <span class="number">2</span>, <span class="number">6</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[<span class="number">0</span>][<span class="number">1</span>]:m[<span class="number">0</span>][<span class="number">2</span>]]</span><br><span class="line">Desoto met the Pascagoula</span><br></pre></td></tr></table></figure>
<p>所以你从创建pattern的原始句子中提取了一个匹配项，但是维基百科中类似的句子呢？请参见下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;October 24: Lewis and Clark met their first Mandan Chief, Big White.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">(<span class="number">12280034159272152371</span>, <span class="number">3</span>, <span class="number">11</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[<span class="number">1</span>]:m[<span class="number">2</span>]]</span><br><span class="line">Lewis <span class="keyword">and</span> Clark met their first Mandan Chief</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;On 11 October 1986, Gorbachev and Reagan met at a house&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher(doc)</span><br><span class="line">[]</span><br></pre></td></tr></table></figure>
<p>您需要添加第二个模式，以允许动词出现在主语和宾语名词之后，如下面的清单所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;On 11 October 1986, Gorbachev and Reagan met at a house&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pattern = [&#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, &#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;and&#x27;</span>&#125;, &#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, &#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;, &#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;meet&#x27;</span>&#125;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher.add(<span class="string">&#x27;met&#x27;</span>, <span class="literal">None</span>, pattern)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">[(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">9</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">11</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">7</span>, <span class="number">11</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">12</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[-<span class="number">1</span>][<span class="number">1</span>]:m[-<span class="number">1</span>][<span class="number">2</span>]]</span><br><span class="line">Gorbachev <span class="keyword">and</span> Reagan met at a house</span><br></pre></td></tr></table></figure>
<p>所以现在你有了你的实体和关系。您甚至可以构建一个模式，该模式对中间的动词（“met”）限制较少，对两边的人和组的名称限制更多。这样做可能会让你识别出暗示一个人或一组人遇到了另一个人或另一组人的其他动词，比如动词“知道”，甚至是被动短语，比如“有过一次谈话”或“变得熟悉”。然后你可以使用这些新动词来为两边的新专有名词添加关系。但你可以看到你是如何偏离你的种子关系模式的最初含义的。这就是所谓的语义漂移。</p>
<p>幸运的是，spaCy不仅用词性和依存关系树信息来标记解析文档中的单词，而且还用词性模式匹配器（如清单11.18所示）将多个模式结合起来，以获得更健壮的模式匹配器该模式不匹配Wikipedia中句子的任何子字符串。添加附加图案而不删除以前的图案。这里的“met”是一个任意键。随便你怎么命名你的图案。“+”运算符增加了重叠替代匹配的数量。最长的匹配是匹配列表中的最后一个。357提取关系（relations）还提供Word2vec单词向量。您可以使用此向量来防止连接动词和两边的专有名词偏离种子模式的原始含义太远。</p>
<h3 id="11-4-2实体名称规范化">11.4.2实体名称规范化</h3>
<p>实体的规范化表示通常是字符串，即使对于日期等数字信息也是如此。此日期的标准化ISO格式为“1541-01-01”。实体的标准化表示使您的知识库能够将世界上在同一日期发生的所有不同事件连接到图形中的同一节点（实体）。对其他命名实体也会这样做。</p>
<p>你应该纠正单词的拼写，并尝试解决物体、动物、人、地方等名称的歧义。规范化命名实体和解决歧义通常被称为共指消解或回指消解，特别是对于依存上下文的代词或其他“名称”。这类似于我们在第2章讨论的lemmatization。命名实体的规范化可确保拼写和命名变体不会因混淆、冗余的名称而污染实体名称的词汇表。例如，“Desoto”在一个特定的文档中至少可以用五种不同的方式来表达：</p>
<ul>
<li>“de Soto”</li>
<li>“Hernando de Soto”</li>
<li>“Hernando de Soto (c. 1496/1497–1542), Spanish conquistador”</li>
<li><a href="https://en.wikipedia.org/wiki/Hernando_de_Soto">https://en.wikipedia.org/wiki/Hernando_de_Soto</a> (a URI)</li>
<li>A numerical ID for a database of famous and historical people</li>
</ul>
<p>类似地，您的规范化算法可以选择这些形式中的任何一种。知识图应该以相同的方式规范化每种实体，以防止同一类型的多个不同实体共享相同的名称。您不希望多个人名引用同一个自然人。更重要的是，无论是在向知识库中写入新事实时，还是在阅读或查询知识库时，规范化都应该始终如一地应用。如果在填充数据库之后决定更改规范化方法，则应该“迁移”或更改知识中现有实体的数据，以遵循新的规范化方案。无模式数据库（键值存储）与用于存储知识图或知识库的数据库一样，不能免除关系数据库的迁移责任。毕竟，无模式数据库是关系数据库的接口包装器。您的规范化实体还需要“is-a”关系来将它们连接到定义实体类型或类别的实体类别。这些“is-a”关系可以看作是标记，因为每个实体可以有多个“is-a”关系。就像7，这是积极研究的主题：<a href="https://nlp.stanford.edu/pubs/structuredVS.pdf%E3%80%82%E5%A6%82%E6%9E%9C%E6%82%A8%E6%83%B3%E5%B0%86%E4%BA%BA%E5%90%8D%E6%88%96POS%E6%A0%87%E7%AD%BE%E3%80%81%E6%97%A5%E6%9C%9F%E5%92%8C%E5%85%B6%E4%BB%96%E7%A6%BB%E6%95%A3%E7%9A%84%E6%95%B0%E5%AD%97%E5%AF%B9%E8%B1%A1%E5%90%88%E5%B9%B6%E5%88%B0%E6%82%A8%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E4%B8%AD%EF%BC%8C%E5%88%99%E9%9C%80%E8%A6%81%E5%AF%B9%E5%AE%83%E4%BB%AC%E8%BF%9B%E8%A1%8C%E8%A7%84%E8%8C%83%E5%8C%96%E3%80%82%E5%AE%9E%E4%BD%93%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%E9%9C%80%E8%A6%81%E4%BB%A5%E6%AD%A3%E5%B8%B8%E7%9A%84%E6%96%B9%E5%BC%8F%E5%AD%98%E5%82%A8%E5%90%97%EF%BC%9F">https://nlp.stanford.edu/pubs/structuredVS.pdf。如果您想将人名或POS标签、日期和其他离散的数字对象合并到您的知识库中，则需要对它们进行规范化。实体之间的关系需要以正常的方式存储吗？</a></p>
<h3 id="11-4-3关系规范化和提取">11.4.3关系规范化和提取</h3>
<p>现在需要一种方法来规范关系，以识别实体之间的关系类型。这样做可以让你找到日期和人之间的所有生日关系，或者历史事件发生的日期，例如“Hernando de Soto”和“Pascagola people”之间的遭遇。您需要编写一个算法来为您的关系选择正确的标签。这些关系可以有一个层次名称，例如“发生在/大约”和“发生在/确切地”以允许您找到特定的关系或关系类别。您还可以使用该关系的“置信度”、“概率、权重或标准化频率”（术语/词的ANLO  GOU到TF-IDF）的数值属性标记这些关系。每次从新文本中提取的事实证实或与数据库中存在的事实相矛盾时，都可以调整这些置信值。现在，您需要一种方法来匹配可以找到这些关系的模式。</p>
<h3 id="11-4-4字型字模式">11.4.4字型字模式</h3>
<p>与正则表达式一样，而非字符。您没有字符类，而是有单词类。例如，您可能会有一个单词模式决定来匹配所有的单数名词（“NN”POS标记），8这通常是通过机器学习完成的。有些种子句子被标记了一些正确的关系（事实）从这些句子中提取。POS模式可以用来找到类似的句子，在这些句子中主语和宾语，甚至关系都可能发生变化。无论您希望匹配多少模式，您都可以使用spaCy包两种不同的方法来匹配O（1）（恒定时间）中的这些模式：”“。任何单词/标记序列模式的phrasether 9™POS标记序列模式匹配器10，以确保新句子中找到的新关系与原始种子（例如）真正类似（示例）关系，你经常需要约束主语，关系，和宾语的意思，以类似于种子句。最好的方法是用一些向量表示单词的意思。这个响吗？第4章讨论的词向量是目前应用最广泛的词义表示方法之一。它们有助于最小化语义漂移。</p>
<p>参考：</p>
<p><a href="https://livebook.manning.com/book/natural-language-processing-in-action/chapter-11/1">Chapter 11. Information extraction (named entity extraction and question answering) - Natural Language Processing in Action: Understanding, analyzing, and generating text with Python (manning.com)</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>一文读懂Flask Web开发实战！</title>
    <url>/flask-2/</url>
    <content><![CDATA[
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title>基础：用flask搭建RESTful API</title>
    <url>/flask-api-1/</url>
    <content><![CDATA[<p><a href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a>是目前发展最迅速的 Python 框架之一，它是一个微型的 Python 开发的 Web 框架，基于<a href="https://www.oschina.net/p/werkzeug">Werkzeug</a> WSGI工具箱和<a href="https://www.oschina.net/p/jinja">Jinja2</a> 模板引擎。以下是用flask来构建 RESTful API 的全过程实录，力求完整、准确、无误地记录每个操作步骤与细节。我用的是windows系统，但我也会提到其他系统的操作。 <span id="more"></span> # 环境搭建与工具安装</p>
<p>首先依然是创建虚拟环境并安装 flask 和 <a href="https://flask-restful.readthedocs.io/en/latest/">flask-restful</a> package。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkvirtualenv flask-api</span><br><span class="line">pip install flask</span><br><span class="line">pip install flask-restful</span><br></pre></td></tr></table></figure>
<h1 id="使用flask搭建api">使用flask搭建API</h1>
<h2 id="最简单的flask应用">最简单的flask应用</h2>
<p>新建一个python脚本，linux用touch，windows操作如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">workon flask-api <span class="comment"># 激活虚拟环境</span></span><br><span class="line">D:</span><br><span class="line"><span class="built_in">type</span> nul&gt;hello.py <span class="comment"># 新建文件</span></span><br><span class="line">atom hello.py <span class="comment"># 用atom编辑器打开</span></span><br></pre></td></tr></table></figure>
<p>粘贴以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)  </span><span class="comment"># 把Flask对象中的route()函数作为一个装饰器，增强该函数的功能</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello, World!&#x27;</span></span><br></pre></td></tr></table></figure>
<p>再打开 Command Prompt 输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set FLASK_APP&#x3D;hello.py</span><br><span class="line">set FLASK_ENV&#x3D;development</span><br><span class="line">flask run</span><br></pre></td></tr></table></figure>
<p>按下快捷键Alt+Shift+=左右分屏，输入：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl http://127.0.0.1:5000/</span><br><span class="line">curl -v http://127.0.0.1:5000/</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">&lt; Content-Type: text&#x2F;html; charset&#x3D;utf-8</span><br><span class="line">&lt; Content-Length: 13</span><br><span class="line">...</span><br><span class="line">&lt;</span><br><span class="line">Hello, World!* Closing connection 0</span><br></pre></td></tr></table></figure>
<p>我们发现返回的是html类型，而不是通常从 RESTful API 获得的 json 文件，于是修改hello.py的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, jsonify</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;about&#x27;</span>: <span class="string">&#x27;Hello, World!&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者：</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(about=<span class="string">&#x27;Hello, World!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>调试模式下，服务器会监测你的代码更新并自动加载，所以无须重启才看到效果。这时curl得到json file：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">&lt; Content-Type: application&#x2F;json</span><br><span class="line">&lt; Content-Length: 31</span><br><span class="line">...</span><br><span class="line">&lt;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;hello&quot;: &quot;Hello, World!&quot;</span><br><span class="line">&#125;</span><br><span class="line">* Closing connection 0</span><br></pre></td></tr></table></figure>
<h2 id="通过url传递参数">通过URL传递参数</h2>
<p>我们可以通过url传递参数给flask api，例如<code>/&lt;int:num&gt;/</code></p>
<p>将hello.py修改为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, jsonify, request</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/multi/&lt;int:num&gt;&#x27;</span>, methods=[<span class="string">&#x27;GET&#x27;</span>]</span>) </span><span class="comment"># methods一般默认为GET</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_multiply10</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;result&#x27;</span>: num*<span class="number">10</span>&#125;) <span class="comment"># 返回数字乘以10后的结果</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run(debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后在cmd输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;multi&#x2F;10</span><br></pre></td></tr></table></figure>
<p>我们得到10乘以10后的结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;result&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然，我们也可以在浏览器内访问以下地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;multi&#x2F;10</span><br></pre></td></tr></table></figure>
<h2 id="使用post方法">使用POST方法</h2>
<p>这一小节我们将使用POST method。</p>
<p>在hello.py中加入以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span>, methods=[<span class="string">&quot;GET&quot;</span>, <span class="string">&quot;POST&quot;</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span>():</span></span><br><span class="line">    <span class="keyword">if</span> (request.method == <span class="string">&quot;POST&quot;</span>):</span><br><span class="line">        some_json = request.get_json()</span><br><span class="line">        <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;You sent&#x27;</span>: some_json&#125;), <span class="number">201</span> <span class="comment"># 如果是post则返回post内容</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> jsonify(&#123;<span class="string">&quot;about&quot;</span>: <span class="string">&quot;Hello World!&quot;</span>&#125;) <span class="comment"># 如果是get则返回hello world</span></span><br></pre></td></tr></table></figure>
<p>现在，我们可以发送如下curl请求并返回POST的内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application&#x2F;json&quot; -X POST -d &#39;&#123;&quot;name&quot;:&quot;Example&quot;,&quot;email&quot;:&quot;example@example.com&quot;&#125;&#39; http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;</span><br></pre></td></tr></table></figure>
<p>注意：Windows的命令行不支持单引号、且需要将双引号转义，所以需要改成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application&#x2F;json&quot; -X POST -d &quot;&#123;\&quot;name\&quot;:\&quot;Example\&quot;,\&quot;email\&quot;:\&quot;example@example.com\&quot;&#125;&quot; http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;</span><br></pre></td></tr></table></figure>
<p>因此，Windows用户建议在git bash上完成以上操作。</p>
<h1 id="使用flask-restful搭建api">使用flask-restful搭建API</h1>
<p>我们把hello.py修改为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"><span class="keyword">from</span> flask_restful <span class="keyword">import</span> Resource, Api</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">api = Api(app)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span>(<span class="params">Resource</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self</span>):</span></span><br><span class="line">        some_json = request.get_json()</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;You sent&#x27;</span>: some_json&#125;, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Multi</span>(<span class="params">Resource</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self, num</span>):</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;result&#x27;</span>: num*<span class="number">10</span>&#125;</span><br><span class="line"></span><br><span class="line">api.add_resource(HelloWorld, <span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">api.add_resource(Multi, <span class="string">&#x27;/multi/&lt;int:num&gt;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>以上代码实现完全相同的功能，不过看起来更简洁和舒服。</p>
<p>参考教程：</p>
<p>https://www.youtube.com/watch?v=s_ht4AKnWZg</p>
<p>参考文献：</p>
<p>https://blog.csdn.net/weixin_41010198/article/details/85230424#API_3</p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>部署Flask开发的API到Heroku</title>
    <url>/flask-api-2/</url>
    <content><![CDATA[<p>上篇文章我们介绍了如何用flask开发简单的web api，下面我们把它部署到heroku上，方便更多人使用。 <span id="more"></span></p>
<p><strong>步骤总结：</strong></p>
<ul>
<li><p>注册<a href="https://signup.heroku.com/">Heroku帐号</a></p></li>
<li><p>下载客户端</p></li>
<li><p>在本地命令行登录</p>
<ul>
<li>如果出现IP Address Mismatch，复制并粘贴<code>heroku login -i</code>到终端，用邮箱密码登录</li>
</ul></li>
<li><p>创建应用</p>
<ul>
<li>``` heroku apps:create flask-microblog <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line">- 初始化</span><br><span class="line">  </span><br><span class="line">  - &#96;&#96;&#96;</span><br><span class="line">    mkdir flask-api</span><br><span class="line">    cd flask-api</span><br><span class="line">    git init</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>编写应用代码</p>
<ul>
<li>run.py</li>
<li>requirements.txt</li>
<li><code>echo web: gunicorn run:app &gt; Procfile</code>
<ul>
<li><code>web: gunicorn &lt;filename&gt;:&lt;main method name&gt;</code></li>
</ul></li>
<li>用<code>tree/F</code>检验上述文件是否齐全</li>
</ul></li>
<li><p>部署应用</p>
<ul>
<li><p>关联github，自动部署</p></li>
<li><pre><code>  git add .
  git commit -m &quot;Initialize repo&quot;
  git push -u origin master</code></pre></li>
</ul></li>
<li><p>访问应用地址：https://nlp-ch.herokuapp.com/</p></li>
</ul>
<p><strong>参考文献：</strong></p>
<p>官方教程：</p>
<p>https://devcenter.heroku.com/articles/getting-started-with-python</p>
<p>以及这些博客：</p>
<p>https://noviachen.github.io/posts/b4cb2e1c.html</p>
<p>https://wizardforcel.gitbooks.io/the-flask-mega-tutorial-2017-zh/content/docs/18.html</p>
<p>http://www.bjhee.com/flask-heroku.html</p>
<p>windows上部署参考（建议使用git bash）：https://caijialinxx.github.io/2018/07/25/deploy-on-heroku/</p>
<p>git bash创建并编辑文件参考：https://blog.csdn.net/qq_34289537/article/details/53994070</p>
<p>windows cmd常用命令参考：https://www.jianshu.com/p/80c3ac7bea8f</p>
<p>上传文件到github: https://www.jianshu.com/p/5227f837070b</p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>flask干货总结</title>
    <url>/flask/</url>
    <content><![CDATA[<h1 id="flask">Flask</h1>
<blockquote>
<p>在一个Web应用里，客户端和服务器上的Flask程序的交互可以简单概括为以下几步：</p>
<p>1）用户在浏览器输入URL访问某个资源。</p>
<p>2）Flask接收用户请求并分析请求的URL。</p>
<p>3）为这个URL找到对应的处理函数。</p>
<p>4）执行函数并生成响应，返回给浏览器。</p>
<p>5）浏览器接收并解析响应，将信息显示在页面中。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span id="more"></span></p>
</blockquote>
<h1 id="术语解释">术语解释</h1>
<ul>
<li>路由：作为动词时，含义是“按某路线发送”，即调用与请求URL对应的视图函数。</li>
<li>视图函数（view function）：处理请求并生成响应的函数。当用户访问URL时会触发视图函数，该函数可执行任意操作，比如从数据库中获取信息，获取请求信息，对用户输入的数据进行计算和处理等。最后，视图函数返回的值将作为响应的主体，一般来说就是HTML页面。</li>
<li>模板：包含程序页面的HTML文件。</li>
<li>静态文件：需要在HTML文件中加载的CSS和Java Script文件，以及图片、字体文件等资源文件。</li>
<li>模板文件存放在项目根目录中的templates文件夹中，静态文件存放在static文件夹下，这两个文件夹需要和包含程序实例的模块处于同一个目录下。</li>
<li>HTTP（Hypertext TransferProtocol，超文本传输协议）定义了服务器和客户端之间信息交流的格式和传递方式，它是万维网（World Wide Web）中数据交换的基础。</li>
<li>WSGI：将HTTP格式的请求数据转换成Flask程序能够使用的Python数据，并把python程序的响应经过WSGI转换生成HTTP响应。</li>
<li>URL中的查询字符串用来向指定的资源传递参数。查询字符串从问号?开始，以键值对的形式写出，多个键值对之间使用&amp;分隔。</li>
<li>这种浏览器与服务器之间交互的数据被称为报文（message），请求时浏览器发送的数据被称为请求报文（request message），而服务器返回的数据被称为响应报文（responsemessage）。</li>
<li>请求报文由请求的方法、URL、协议版本、首部字段（header）以及内容实体组成。</li>
<li>报文由报文首部和报文主体组成，两者由空行分隔，请求报文的主体一般为空。如果URL中包含查询字符串，或是提交了表单，那么报文主体将会是查询字符串和表单数据。</li>
<li>用来映射到数据库表的Python类通常被称为数据库模型（model），一个数据库模型类对应数据库中的一个表。所有的模型类都需要继承Flask-SQLAlchemy提供的db.Model基类。</li>
</ul>
<h1 id="架构">架构</h1>
<p>在MVC架构中，程序被分为三个组件：数据处理（Model）、用户界面（View）、交互逻辑（Controller）。</p>
<p>如果想要使用Flask来编写一个MVC架构的程序，那么视图函数可以作为控制器（Controller），视图（View）则是使用Jinja2渲染的HTML模板，而模型（Model）可以使用其他库来实现。</p>
<p>参考文献：</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://book.douban.com/subject/30310340/">《Flask Web开发实战》</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title>Django网站开发全过程实录-1</title>
    <url>/django-1/</url>
    <content><![CDATA[<p>网站一般需要实现三种基本功能：<strong>连接数据库、处理用户请求、页面设计的删改</strong>。Django的优势在于将这些功能设计成独立的模块，形成一套web框架。利用Django框架开发网站，能让我们专注于编写应用程序而无需重新造轮子。 <span id="more"></span> Django 采用了 <strong>MVT 的软件设计模式</strong>，即<strong>模型</strong>（Model）、<strong>视图</strong>（View）和<strong>模板</strong>（Template）。这种设计模式的优势在于<strong>各个组件松散结合</strong>，每个APP应用都有明确的目的，并且可独立更改而不影响其它部分。如此，使得页面设计与业务逻辑互不影响。同时，Django是一套出色的<strong>动态内容管理系统</strong>，擅长动态提供数据库驱动的信息。</p>
<p>以下是我使用Django 3.1.7搭建网站过程的实录，力求完整、准确、无误地记录每个操作步骤与细节。</p>
<h2 id="环境搭建与工具安装"><strong>环境搭建与工具安装</strong></h2>
<blockquote>
<p><em>参考：</em><a href="https://stormsha.com/article/2026/"><em>https://stormsha.com/article/2026/</em></a></p>
</blockquote>
<p>我们需要在合适的目录内创建一个<strong>虚拟环境</strong>（用virtualenv, virtualenvwrapper皆可，参考<a href="https://blog.csdn.net/a200822146085/article/details/89048172">virtualenvwrapper的使用</a>），我给它取名为webdev。</p>
<p>在该虚拟环境，安装<strong>django</strong>和<strong>psycopg2</strong>工具包（用于管理PostgreSQL数据库）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install virtualenv, virtualenvwrapper-win</span><br><span class="line">mkvirtualenv webdev</span><br><span class="line">workon webdev</span><br><span class="line">pip install django</span><br><span class="line">pip install psycopg2</span><br><span class="line">pip list</span><br><span class="line">pip freeze</span><br><span class="line"></span><br><span class="line"># 如果需要退出或删除虚拟环境</span><br><span class="line">deactivate</span><br><span class="line">rmvirtualenv webdev</span><br></pre></td></tr></table></figure>
<p><strong>安装数据库：</strong></p>
<p>Django支持四种数据库：PostgreSQL、SQLite 3、MySQL、Oracle。</p>
<p>我选择PostgreSQL，它比MySQL更适合Django，Django的创建者如是说：</p>
<blockquote>
<p>如果您不受任何遗留系统的束缚，并且可以自由选择数据库后端，那么我们建议您使用PostgreSQL，它可以在成本、功能、速度和稳定性之间取得很好的平衡。（《 Django权威指南》第15页）</p>
</blockquote>
<p>PostgreSQL的安装步骤参考：https://www.runoob.com/postgresql/windows-install-postgresql.html</p>
<p>打开后设置语言为中文，然后关闭。</p>
<h2 id="创建项目project"><strong>创建项目（project）</strong></h2>
<p>下面在刚刚的虚拟环境webdev内创建一个项目mysite（你可以选择任意其他名字），项目是我们所建立的网站上所有应用程序的集合，并共用一套数据库配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd webdev</span><br><span class="line">django-admin startproject mysite</span><br></pre></td></tr></table></figure>
<p>我们看到新建了一个文件夹mysite及下面的子文件夹mysite/mysite：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysite&#x2F;</span><br><span class="line">    manage.py</span><br><span class="line">    mysite&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">        settings.py</span><br><span class="line">        urls.py</span><br><span class="line">        asgi.py</span><br><span class="line">        wsgi.py</span><br></pre></td></tr></table></figure>
<p>我们可以把子文件夹mysite/mysite视为整个项目的配置，其中的settings.py和urls.py这两个文件是我们以后需要经常修改的。</p>
<h2 id="启动服务器server"><strong>启动服务器（server）</strong></h2>
<p>启动服务器，服务器会监测你的代码更新并自动加载，所以无须重启才看到效果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd mysite</span><br><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<p>在settings.py内将语言改成中文，时区改为上海：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LANGUAGE_CODE &#x3D; &#39;zh-hans&#39;</span><br><span class="line"></span><br><span class="line">TIME_ZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39;</span><br></pre></td></tr></table></figure>
<p>刷新浏览器看到中文页面。</p>
<p>我们可以指定服务器的端口和IP地址。比如，把地址设为自己的IP地址（例如192.168.1.110）或0.0.0.0，让联网的其他计算机可见：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py runserver 0.0.0.0:8000</span><br></pre></td></tr></table></figure>
<p>使用Windows的用户用ipconfig命令获取本地网络中的IP 地址，然后复制到setting.py中，比如我是这个：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALLOWED_HOSTS &#x3D; [&#39;192.168.1.110&#39;]</span><br></pre></td></tr></table></figure>
<p>于是，在其他电脑或手机浏览器打开 http://192.168.1.110:8000/ 就可以访问啦！完美！不过网站还在开发中，就不要随便开放共享啦~</p>
<h2 id="创建应用程序app"><strong>创建应用程序（APP）</strong></h2>
<p><strong>项目和应用的区分：</strong></p>
<ul>
<li><strong>应用</strong>是用于执行某项具体操作的程序，<strong>项目</strong>是特定网站的配置和应用程序的集合。</li>
<li><strong>多对多的关系</strong>：一个项目可以包含多个应用程序，一个应用程序可以用在多个项目中。</li>
</ul>
<p>应用放在任意路径都可以，但我们一般放在<strong>manage.py文件相同的目录</strong>中，与mysite子文件夹平行。在这里我创建一个成语检索的app，名为idiom：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py startapp idiom</span><br></pre></td></tr></table></figure>
<p>看看这个应用程序下有哪些文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">idiom&#x2F;</span><br><span class="line">    __init__.py</span><br><span class="line">    admin.py</span><br><span class="line">    apps.py</span><br><span class="line">    migrations&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">    models.py</span><br><span class="line">    tests.py</span><br><span class="line">    views.py</span><br></pre></td></tr></table></figure>
<h2 id="编写视图views"><strong>编写视图（views）</strong></h2>
<p>下面为这个应用程序idiom添砖加瓦，分为三个步骤：</p>
<ol type="1">
<li>创建视图函数</li>
<li>将视图函数映射到APP的urls</li>
<li>将APP中的urls连入网站的根urls</li>
</ol>
<p>这样看逻辑可能更清晰：视图函数 --&gt; APP的urls --&gt; 网站的urls</p>
<h2 id="创建视图函数"><strong>创建视图函数</strong></h2>
<p>打开文件idiom/views.py ，加入以下Python代码，<strong>创建index视图函数</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    return HttpResponse(&quot;Hello, world. You&#39;re at the idiom index.&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="映射到app的urls"><strong>映射到APP的urls</strong></h2>
<p>要调用该视图，我们要将其映射到URL，为此，我们需要添加一个URL配置（URLconf）。<strong>URLconf</strong>相当于网站的目录，也就是<strong>URL模式与视图函数之间的映射表</strong>。</p>
<p>我们在idiom应用的目录下创建一个名为urls.py的文件，windows操作如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type nul&gt;urls.py</span><br></pre></td></tr></table></figure>
<p>看看现在的应用目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">idiom&#x2F;</span><br><span class="line">    __init__.py</span><br><span class="line">    admin.py</span><br><span class="line">    apps.py</span><br><span class="line">    migrations&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">    models.py</span><br><span class="line">    tests.py</span><br><span class="line">    urls.py</span><br><span class="line">    views.py</span><br></pre></td></tr></table></figure>
<p>然后在idiom/urls.py这个空文件中加入以下代码，将index视图映射到APP的url模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.urls import path</span><br><span class="line"></span><br><span class="line">from . import views</span><br><span class="line"></span><br><span class="line">urlpatterns &#x3D; [</span><br><span class="line">    path(&#39;&#39;, views.index, name&#x3D;&#39;index&#39;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="连入网站的urls"><strong>连入网站的urls</strong></h2>
<p>下一步是将根URLconf（mysite/urls.py）指向idiom.urls模块，使得网站域名连接到app的url。我们打开mysite/urls.py，修改为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.contrib import admin</span><br><span class="line">from django.urls import include, path</span><br><span class="line"></span><br><span class="line">urlpatterns &#x3D; [</span><br><span class="line">    path(&#39;idiom&#x2F;&#39;, include(&#39;idiom.urls&#39;)),</span><br><span class="line">    path(&#39;admin&#x2F;&#39;, admin.site.urls),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>该include()功能允许引用其他URLconf，这样我们就将刚刚创建的index视图连接到了网站的URLconf。</p>
<p>也就是说，目前我们可以打开两个网址：</p>
<p><a href="http://127.0.0.1:8000/idiom/">http://example.com/idiom/</a></p>
<p><a href="http://127.0.0.1:8000/idiom/">http://example.com/admin/</a></p>
<p>注意idiom和admin在引用URL模式时的区别：除了admin.site.urls（用于管理后台），我们引用其他URL模式时，都应使用include()。</p>
<p>最后，验证下是否正常运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<p>打开http://127.0.0.1:8000/idiom/ ，可以看到 Hello, world. You're at the idiom index. 这行文字。</p>
<h2 id="urlconf的工作原理"><strong>URLconf的工作原理</strong></h2>
<p>Django允许我们根据需要设计每个应用程序的URL，通过创建<strong>URLconf</strong>（URL配置）。</p>
<p>在 idiom和mysite文件夹下的urls.py中，我们都使用了<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.path">path()</a>函数，这个函数有两个必需的参数 route和view。</p>
<p>path（<em>route</em>，<em>view</em>，<em>kwargs = None</em>，<em>name = None</em>）</p>
<ul>
<li>route是包含URL模式的字符串，比如目前我们有idiom/和admin/。在处理请求时，Django从第一个模式开始，沿列表的顺序，将请求的URL（域名后的部分）与每个模式进行比较，直到找到匹配的URL。这个字符串支持用尖括号匹配和捕获URL的一部分并将其作为关键字参数发送到视图，</li>
<li>view就是指定的视图函数，也可以是一个<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.include">django.urls.include()</a>。kwargs参数允许我们将其他参数传递给视图函数。</li>
<li>name不是必须的，但是命名URL的好处是便于在Django中的其他地方（尤其是在模板内部）明确地引用它。</li>
</ul>
<p>参考：<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.path">django.urls functions for use in URLconfs</a></p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第14章-依存分析（中）</title>
    <url>/dependency-parsing-2/</url>
    <content><![CDATA[<h1 id="基于转换的依存分析">14.4 基于转换的依存分析</h1>
<p>我们的第一个依存分析方法是由一种基于堆栈的方法启发的，这种方法被称为shift-reduce parsing，最初是为分析程序语言而开发的(Aho and Ullman, 1972)。这个经典的方法简单而优雅，采用了一个上下文无关语法、一个堆栈和一个待解析的标记列表。输入的标记被连续地移动到堆栈上，堆栈的前两个元素与语法中的右侧规则进行匹配；当发现匹配时，匹配的元素在堆栈上被匹配的规则左侧的非终端替换（还原）。在将这种方法改编为依存性解析时，我们放弃了对语法的明确使用，并改变了reduce操作，使其不是在解析树上添加一个非终端，而是引入了一个词与其头部之间的依存关系。更具体地说，reduce操作被两种可能的操作所取代：在堆栈顶部的词和它下面的词之间断言一个词头依存关系，或者反之。图14.5说明了这种解析器的基本操作。 配置 在基于过渡的解析中，一个关键的元素是配置的概念，它由一个堆栈、一个词或标记的输入缓冲区和一组代表依存树的关系组成。在这个框架下，解析过程由一个通过可能配置空间的过渡序列组成。这个过程的目标是找到一个最终的配置，在这个配置中，所有的词都已经被计算在内，并且已经合成了一个合适的依存树。 为了实现这样的搜索，我们将定义一组过渡运算符，当它们应用于一个配置时，会产生新的配置。考虑到这个设置，我们可以将解析器的操作看作是在配置空间中搜索从起始状态到目标状态的过渡序列。在这个过程的开始，我们创建一个初始配置，其中堆栈包含ROOT节点，t</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>dependency</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
</search>
