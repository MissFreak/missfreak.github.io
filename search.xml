<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>目前共有71篇文章</title>
    <url>/all-posts/</url>
    <content><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="380" height="110" src="//music.163.com/outchain/player?type=0&amp;id=2112262923&amp;auto=1&amp;height=90">
</iframe>
<center>
『 好奇心造就诗人与科学家。』<br>Knowledge of languages is the doorway to wisdom. – Roger Bacon<br><br>
<details class="category_1">
<summary>
代码
</summary>
<details open>
<summary>
Git
</summary>
<br> <a href="git">Git和Github常用操作大全</a><br>
</details>
<details open>
<summary>
pandas
</summary>
<br> <a href="mind-graph">我的知识库</a><br> <a href="pandas">Pandas的一些常用操作</a><br>
</details>
<details open>
<summary>
文本处理
</summary>
<br> <a href="json">用python处理json数据</a><br> <a href="pdf-explained">PDF相关知识大全</a><br> <a href="pdf">PDF文件处理大全</a><br> <a href="txt">纯文本文件的处理</a><br>
</details>
<details open>
<summary>
爬虫
</summary>
<br> <a href="scraping">Python爬虫代码-全</a><br>
</details>
<details open>
<summary>
编辑器
</summary>
<br> <a href="atom">从小白到起飞，一站解决Atom编辑器各种骚操作</a><br> <a href="programming-methodology">编程方法论</a><br>
</details>
</details>
<details class="category_1">
<summary>
数学
</summary>
<details open>
<summary>
微积分
</summary>
<br> <a href="calculus-I-0">第0章：为什么要学习微积分？</a><br>
</details>
<details open>
<summary>
方法
</summary>
<br> <a href="math-0">数学中的英语名词汇</a><br> <a href="math-1">如何学习数学</a><br>
</details>
</details>
<details class="category_1">
<summary>
未分类
</summary>
<br><a href="all-posts">目前共有60篇文章</a><br> <a href="cognitive-linguistics">认知语言学大纲</a><br> <a href="content">我的电脑文件目录管理</a><br> <a href="hello-world">Hello World</a><br> <a href="hexo-next">Hexo博客Next主题搭建全过程</a><br> <a href="information-theory-1">信息论知识与资源汇总</a><br> <a href="knowledge-graph-1">knowledge-graph-1</a><br> <a href="knowledge-graph-2">用Python搭建学术知识图谱</a><br> <a href="middle-test">中期考核的复习计划</a><br> <a href="prime-citation">priming</a><br> <a href="priming-content">syntactic priming-2</a><br> <a href="priming-pre">syntactic priming</a><br> <a href="resources">免费好用网站软件资源集锦：从学习、办公到娱乐</a><br> <a href="web-dev-all">网站开发需要的工具：前端和后端</a><br> <a href="wiki-api">好用的API集锦</a><br>
</details>
</details>
<details class="category_1">
<summary>
网站开发
</summary>
<details open>
<summary>
django
</summary>
<br> <a href="django-1">Django网站开发全过程实录-1</a><br>
</details>
<details open>
<summary>
flask
</summary>
<br> <a href="flask-2">一文读懂Flask Web开发实战！</a><br> <a href="flask-api-1">基础：用flask搭建RESTful API</a><br> <a href="flask-api-2">部署Flask开发的API到Heroku</a><br> <a href="flask-api-3">如何在curl和python中使用API</a><br> <a href="flask-web">一个人开发信息检索与抽取网站的全过程</a><br> <a href="flask">flask干货总结</a><br>
</details>
</details>
<details class="category_1">
<summary>
自然语言处理
</summary>
<details open>
<summary>
依存分析
</summary>
<br> <a href="dependency-parsing-1">《自然语言处理综论》第14章-依存分析（上）</a><br> <a href="dependency-parsing-2">《自然语言处理综论》第14章-依存分析（中）</a><br> <a href="dependency-parsing">英文依存句法分析</a><br> <a href="pos-tagging">英文词性标记（POS Tagging）</a><br>
</details>
<details open>
<summary>
信息抽取
</summary>
<br> <a href="information-retrieval-1">《自然语言处理综论》第17章-信息抽取（上）</a><br> <a href="information-retrieval-2">《自然语言处理综论》第17章-信息抽取（中）</a><br> <a href="information-retrieval-3">《自然语言处理综论》第17章-信息抽取（下）</a><br> <a href="information-retrieval-4">信息抽取</a><br> <a href="information-retrieval">信息抽取技术综述</a><br> <a href="named-entity-recognition">英文文献的命名实体识别（上）</a><br>
</details>
<details open>
<summary>
句子切分
</summary>
<br> <a href="sentence-segmentation">英文句子切分的要点和工具</a><br>
</details>
<details open>
<summary>
语料库
</summary>
<br> <a href="corpus">语料库资源大全</a><br>
</details>
<details open>
<summary>
预处理
</summary>
<br> <a href="encoding">文本编码格式大全</a><br> <a href="spacy-1">spaCy超强指南之文本预处理和语言特征表</a><br>
</details>
</details>
<details class="category_1">
<summary>
计算机
</summary>
<details open>
<summary>
正则表达式
</summary>
<br> <a href="regex-1">正则表达式进阶</a><br>
</details>
<details open>
<summary>
算法
</summary>
<br> <a href="finite-state-machines">有限状态机</a><br>
</details>
</details>
<details class="category_1">
<summary>
语言学
</summary>
<details open>
<summary>
二语习得
</summary>
<br> <a href="sla-3">二语习得课本</a><br> <a href="sla-4">二语习得最终</a><br> <a href="sla">二语习得</a><br>
</details>
<details open>
<summary>
句法
</summary>
<br> <a href="syntax-final">句法学最终</a><br> <a href="syntax-main">句法学主要内容</a><br> <a href="syntax-terms">句法学框架及术语大全</a><br> <a href="syntax-trees">句法树大全</a><br> <a href="syntax">句法学超人大总结</a><br>
</details>
<details open>
<summary>
心理语言学
</summary>
<br> <a href="psycho-linguistics-content">心理语言学：资源和知识整理</a><br> <a href="psycho-linguistics">心理语言学：资源和知识整理</a><br>
</details>
<details open>
<summary>
语义
</summary>
<br> <a href="semantics-concepts">语义学框架</a><br> <a href="semantics-final">语义学最终</a><br> <a href="semantics-terms">语义学术语大全</a><br> <a href="semantics">语义学术语</a><br>
</details>
<details open>
<summary>
语言测试
</summary>
<br> <a href="language-assessment">语言测试Syllabus</a><br> <a href="language-assessment2">语言测试glossary</a><br>
</details>
</details>
<details class="category_1">
<summary>
项目
</summary>
<details open>
<summary>
信息抽取
</summary>
<br> <a href="ir-project-0">信息抽取-Pilot Study</a><br> <a href="ir-project-1">爬虫构建语料库的方法</a><br> <a href="ir-project-2">根据POS生成候选词</a><br> <a href="ir-project">信息抽取的毕业论文准备</a><br>
</details>
</details>
<br><br>关注公众号：鸽婆打字机！风里雨里，鸽鸽陪你~
</center>
<p><img align="center" width="120" height="120" alt="微信公众号：鸽婆打字机" src="https://i.loli.net/2021/03/04/dXVUZiRfW2o7wCy.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>根据POS生成候选词</title>
    <url>/ir-project-2/</url>
    <content><![CDATA[<h1 id="候选词满足的要求">候选词满足的要求</h1>
<p>我们先查看keywords的特点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml-dataset.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">keyword_list = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df[<span class="string">&#x27;keywords&#x27;</span>]:</span><br><span class="line">    keyword_list.extend(<span class="built_in">str</span>(row).split(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line">sorted_keyword_list = Counter(keyword_list).most_common()</span><br><span class="line"></span><br><span class="line">gram_count = [<span class="built_in">len</span>(w.split()) <span class="keyword">for</span> w <span class="keyword">in</span> keyword_list]</span><br><span class="line">sorted_gram_count = Counter(gram_count).most_common()</span><br><span class="line"></span><br><span class="line">pos_tags = [<span class="string">&#x27; &#x27;</span>.join(tok.tag_ <span class="keyword">for</span> tok <span class="keyword">in</span> nlp(keyword[<span class="number">0</span>])) <span class="keyword">for</span> keyword <span class="keyword">in</span> sorted_keyword_list]</span><br><span class="line">sorted_pos_tags = Counter(pos_tags)</span><br><span class="line">print(sorted_pos_tags)</span><br></pre></td></tr></table></figure>
<p>然后把数据集分开保存为txt文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml-dataset.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    os.mkdir(<span class="string">r&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> FileExistsError <span class="keyword">as</span> error:</span><br><span class="line">    print(error)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> abstract <span class="keyword">in</span> df[<span class="string">&#x27;abstract&#x27;</span>]:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml/&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(abstract.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    i += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>从中抽取出pos满足规则的词组。要注意的是，这里的词组不影响后续的ranking 算法所以先把这一步做出来就行。要注意抽取的是meaningful的词组。相关的是text chunking (syntactic constituents), collocation extraction. 事实上这一步要做的就是分块，我们抽取出所有的名词词组！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&#x27;D:/毕业论文/academic-information-retrieval/corpus/jml/0.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line">chunks = <span class="built_in">list</span>(doc.noun_chunks)</span><br><span class="line">print(chunks)</span><br><span class="line"><span class="comment"># [a focused element, a focus particle, recall, alternatives, Recall benefit, both inclusive (even) and exclusive (only) particles, Particles, the relevance, alternatives, discourse interpretation, Higher discourse relevance, improved memory encoding, the alternatives, Focus sensitive particles, the relevance, contextual alternatives, the interpretation, a sentence, Two experiments, better encoding, focus alternatives, Participants, auditory stimuli, a set, elements, three different versions, the critical sentences, the inclusive particle, no particle, (control condition, blocks, ten trials, participants, the elements, the context sentence, The results, both particles, memory performance, the alternatives, the focused element, the control condition, The results, the assumption, information-structural alternatives, memory, the presence, a focus sensitive particle]</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>爬虫构建语料库的方法</title>
    <url>/ir-project-1/</url>
    <content><![CDATA[<blockquote>
<p>Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。</p>
<p>Scrapy 常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
<p>通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。</p>
</blockquote>
<p>由于<code>scrapy</code>在结构化数据提取中的重要作用，我将用它实现一个功能全面的学术文章信息采集的工具，并搭建为语料库。事实上，整套程序完全可以利用requests、beautifulsoup等实现（我会提供相关代码），但是利用scrapy框架会更高效、简洁，不用重复造轮子。而且，对于下载失败的URL，Scrapy也会重新下载。</p>
<p>预计2~3天完成这个项目，除了用于毕业论文的语料库，还将帮助我更好地巩固Python基础。2021年暑假，我将花大量时间精力用于开展毕业论文相关的项目，并研究算法，预计暑假结束前完成数学模型的搭建。这是我的第一个项目，所有项目都在<code>pycharm</code>进行（<a href="https://zhuanlan.zhihu.com/p/36147819">快捷键</a>）。</p>
<p>关于scrapy的详细介绍和架构图见<a href="https://www.runoob.com/w3cnote/scrapy-detail.html">菜鸟教程</a>。我也会按照这个教程描述的步骤完成整个项目。</p>
<h1 id="一新建项目">一、新建项目</h1>
<p>我们在pycharm的terminal中<code>pip install scrapy</code>并新建项目：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy startproject paperscraper</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/weixin_44322399/article/details/104439555">Bash指令：windows下使用tree指令快速查看文件目录树_10 DAY'S-CSDN博客</a></p>
<p>使用上述方式得到项目的文件结构，其中settings.py和spiders比较重要：</p>
<ul>
<li><strong>settings.py –</strong>该文件包含项目设置。</li>
<li><strong>Spiders/ –</strong>此文件夹将存储自定义spider，每次scrapy 运行一个spider，它都会在这个文件夹中寻找它。</li>
</ul>
<h2 id="如何防止反爬虫">1.1 如何防止反爬虫？</h2>
<p>由于可能出现403错误（无权限访问），如何避免爬虫被网站识别出来导致被禁呢？我们需要伪装浏览器，设置request headers，在settings.py中进行如下修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;User-Agent&#x27;</span>: ua.random</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同时要保证：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="创建paperbot-spider">1.2 创建paperbot spider</h2>
<p>我们使用基本模板在<strong>spiders/</strong>文件夹中创建一个新的spider“paperbot.py”：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy genspider paperbot www.sciencedirect.com&#x2F;journal&#x2F;journal-of-memory-and-language</span><br></pre></td></tr></table></figure>
<p>生成内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaperbotSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;paperbot&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.sciencedirect.com/journal/journal-of-memory-and-language&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.sciencedirect.com/journal/journal-of-memory-and-language/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li>allowed_domains是是过滤爬取的域名，限制爬虫爬取当前域名下的网页。</li>
<li>start_urls是起始爬取页面。</li>
</ul>
<h2 id="背后的逻辑">1.3 背后的逻辑</h2>
<p>这段代码其实是简化版的，我们可以看下面的代码实例来弄清楚背后的逻辑：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span>(<span class="params">self</span>):</span></span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">&#x27;http://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;http://quotes.toscrape.com/page/2/&#x27;</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        page = response.url.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">2</span>]</span><br><span class="line">        filename = <span class="string">f&#x27;quotes-<span class="subst">&#123;page&#125;</span>.html&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">f&#x27;Saved file <span class="subst">&#123;filename&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>start_requests()：必须返回一个请求的迭代器 (iterable) （您可以返回一个请求列表或编写一个生成器函数），爬行器将从中开始爬行。随后的请求将从这些初始请求依次生成。</li>
<li>parse()：将调用该方法来处理为每个请求下载的响应。response参数是TextResponse的一个实例，它保存页面内容，并有更多有用的方法来处理它。并查找要跟踪的新url并从中创建新请求（Request）。</li>
</ul>
<p>可以简化成：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;http://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://quotes.toscrape.com/page/2/&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        page = response.url.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">2</span>]</span><br><span class="line">        filename = <span class="string">f&#x27;quotes-<span class="subst">&#123;page&#125;</span>.html&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br></pre></td></tr></table></figure>
<h1 id="二明确目标">二、明确目标</h1>
<p>我们打算抓取https://www.sciencedirect.com/journal/journal-of-memory-and-language/issues期刊2010-2021年间的所有文章的题目、年份、doi、作者、Highlights、摘要和关键词（可能的话还有全文）。</p>
<ol type="1">
<li>打开 mySpider 目录下的 items.py。</li>
<li>创建一个 ItcastItem 类，定义类型为 scrapy.Field 的类属性</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaperscraperItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    year = scrapy.Field()</span><br><span class="line">    doi = scrapy.Field()</span><br><span class="line">    author = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>这个我们稍后修改，</p>
<h1 id="三搭建spider">三、搭建spider</h1>
<p>下面是项目的关键部分——spider的搭建，负责提取信息。</p>
<blockquote>
<p>Spider（爬虫）：它负责处理所有Responses，从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)。</p>
</blockquote>
<h2 id="使用scrapy-shell调试选择器">3.1 使用scrapy shell调试选择器</h2>
<p>Scrapy提取数据有自己的一套机制，被称做选择器（Selector类），它能够自由“选择”由XPath或CSS表达式指定的HTML文档的某些部分。</p>
<p>为了获得每期的文章ID，我们打开scrapy shell调试和验证选择器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scrapy shell</span><br><span class="line">fetch(<span class="string">&quot;https://www.sciencedirect.com/journal/journal-of-memory-and-language/vol/120/suppl/C&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在调试过程中，我们发现可以在该页面确定发表时间，并找到所有文章标题，由于标题可能存在内部标签，所以用正则表达式提取；以及作者（editorial board没有作者）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response.css(<span class="string">&#x27;.js-issue-status.text-s::text&#x27;</span>).get()</span><br><span class="line">response.css(<span class="string">&#x27;span.js-article-title&#x27;</span>).re(<span class="string">r&#x27;&lt;span[^&gt;]*&gt;([\s\S]*?)&lt;/span&gt;&#x27;</span>)</span><br><span class="line"><span class="comment"># &#x27;Revealing pragmatic processes through a one-word answer: When the French reply &lt;em&gt;Si&lt;/em&gt;&#x27;，</span></span><br><span class="line"><span class="comment"># 得到12条类似的标题，其中第一个Editorial Board要去掉</span></span><br><span class="line">response.css(<span class="string">&#x27;div.u-clr-grey8.text-s::text&#x27;</span>).getall()</span><br></pre></td></tr></table></figure>
<p>但要模仿浏览器点击才能获取摘要，因此我们换种方式，首先获取每篇文章的doi，然后去每篇文章的页面爬取信息。 ## 3.2 爬虫逻辑</p>
<p>如果用doi来构建每篇文章的url，还是会出现版本不同时url规则不同的情况：</p>
<p><img src="https://i.loli.net/2021/07/02/l8Vqe4Uv2ZBMkAO.png"/></p>
<p>因此，我们发现最好的方式是模拟浏览器进行操作，不断爬取链接并下载内容。在这里我们重新创建一个<a href="https://www.jianshu.com/p/a6a08b4f7c04">crawlspider</a>，来抓取链接，我们取名为paperbot2。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy genspider -t crawl paperbot2 www.sciencedirect.com</span><br></pre></td></tr></table></figure>
<p>生成新的文件paperbot2.py，我们继续调试代码，这时可能出现403错误，<a href="https://blog.csdn.net/u011781521/article/details/70211474">我们使用这篇文章的第二个办法</a>，把scrapy package里面的user-agent换掉。 ## 3.3 代码</p>
<p>在paperbot2.py写入如下内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> paperscraper.items <span class="keyword">import</span> AbstractItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Paperbot2Spider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;paperbot2&#x27;</span></span><br><span class="line">    issue_urls = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">70</span>, <span class="number">121</span>):</span><br><span class="line">        issue_urls.append(<span class="string">&#x27;https://www.sciencedirect.com/journal/journal-of-memory-and-language/vol/&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;/suppl/C&#x27;</span>)</span><br><span class="line">    start_urls = issue_urls</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(restrict_css=<span class="string">&#x27;.article-content-title&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        self.logger.info(<span class="string">&#x27;Hi, this is an item page! %s&#x27;</span>, response.url)</span><br><span class="line">        item = AbstractItem()</span><br><span class="line">        item[<span class="string">&#x27;title&#x27;</span>] = response.css(<span class="string">&#x27;.title-text&#x27;</span>).re(<span class="string">r&#x27;&lt;span[^&gt;]*&gt;([\s\S]*?)&lt;/span&gt;&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">&#x27;year&#x27;</span>] = response.css(<span class="string">&#x27;.publication-volume .text-xs&#x27;</span>).re(<span class="string">r&#x27;[^\d](\d&#123;4&#125;)[^\d]&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">&#x27;link&#x27;</span>] = response.url</span><br><span class="line">        item[<span class="string">&#x27;highlights&#x27;</span>] = <span class="string">&#x27;\n&#x27;</span>.join(response.css(<span class="string">&#x27;.author-highlights .list p::text&#x27;</span>).getall()) <span class="comment"># 这里用\n分割每一句话</span></span><br><span class="line">        item[<span class="string">&#x27;abstract&#x27;</span>] = <span class="string">&#x27;\n&#x27;</span>.join(response.css(<span class="string">&#x27;#abstracts p::text&#x27;</span>).getall())</span><br><span class="line">        item[<span class="string">&#x27;keywords&#x27;</span>] = <span class="string">&#x27;\n&#x27;</span>.join(response.css(<span class="string">&#x27;.keyword span::text&#x27;</span>).getall()</span><br><span class="line">)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>在settings.py中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BOT_NAME = <span class="string">&#x27;paperscraper&#x27;</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;paperscraper.spiders&#x27;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;paperscraper.spiders&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;User-Agent&#x27;</span>: ua.random</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在items.py：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AbstractItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    year = scrapy.Field()</span><br><span class="line">    link = scrapy.Field()</span><br><span class="line">    highlights = scrapy.Field()</span><br><span class="line">    abstract = scrapy.Field()</span><br><span class="line">    keywords = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>由于我们定义了两个items，需要定制写入不同CSV文件的<a href="https://blog.csdn.net/sc_lilei/article/details/79590696">Pipeline</a>，并且在setting中激活该pipeline。</p>
<p>参照<a href="https://zhuanlan.zhihu.com/p/58944212">这篇文章</a>，我们也可以直接在输出的时候写入csv：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy crawl paperbot2 -o jml.csv </span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p>https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/</p>
<p>https://www.bookstack.cn/read/piaosanlang-spiders/fbe165cd1d0be224.md</p>
<p>https://docs.scrapy.org/en/latest/intro/tutorial.html</p>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>句法学最终</title>
    <url>/syntax-final/</url>
    <content><![CDATA[<h3 id="the-logical-problem-of-language-acquisition">the logical problem of language acquisition</h3>
<p>The linguistic data to which children are exposed appear to be insufficient to determine, by themselves, the linguistic knowledge that children eventually attain. The gap between available experience and attained competence forms what has been called the logical problem of language acquisition.</p>
<h3 id="binding-principles">Binding Principles</h3>
<p>A: An anaphor must be bound in its binding domain. Principle B: A pronoun must be free in its binding domain. Principle C: An R-expression must be free.</p>
<h3 id="the-theta-criterion">The theta criterion</h3>
<ol type="a">
<li>Each argument is assigned one and only one theta role. b. Each theta role is assigned to one and only one argument.</li>
</ol>
<p>The theta criterion is a constraint or filter that rules out otherwise well-formed sentences.</p>
<p>The theta criterion requires that there is a strict one-to-one matching between the number and kind of theta roles and the number and kind of arguments (Slide 2, 5).</p>
<h3 id="i-language-vs.-e-language">I-Language vs. E-language</h3>
<p>An I-language is a computational system that is encoded in, or internalized within, an individual brain. It is acquired early and employed by users through life.</p>
<p>The linguistic forms that we see and hear are expressions of externalized language that are used (in conversations, discourse, etc.) in real contexts.</p>
<h3 id="head-specifier-and-complement">head, specifier and complement</h3>
<p>Specifier: Sister to X', daughter of XP.</p>
<p>Adjunct: Sister to X', daughter of X'.</p>
<p>Complement: Sister to X, daughter of X'.</p>
<p>Head: The word that gives its category to the phrase.</p>
<h3 id="dp-movement">DP-movement</h3>
<p>Non-finite T can’t assign NOM case. NOM case is assigned by finite T.</p>
<p>Subject-to-object Raising (also called Exceptional Case Marking or ECM): A kind of DP movement where the subject of an embedded non-finite clause moves to the specifier of AgrO in the main clause to get accusative Case. Expect is an ECM verb. It exceptionally marks ACC case to the Spec of TP (a defective TP which does not have a CP layer).</p>
<p>Extended projection principle (EPP): All clauses must have subjects.</p>
<h2 id="types-of-verbs">types of verbs</h2>
<p><strong>Unaccusative</strong></p>
<p>In modern linguistics, an unaccusative verb is an intransitive verb whose grammatical subject is not a semantic agent. In other words, it does not actively initiate, or is not actively responsible for, the action of the verb.</p>
<p>Like middles, unaccusative verbs project only the internal argument into syntax. Unlike middles, no agentivity is involved in an unaccusative sentence. Unaccusatives are eventive. They depict internally caused events or results. An unaccusative verb is unable to assign a structural case to its internal argument.</p>
<p>Most unaccusative verbs are derivationally (morphologically) related to corresponding transitive verbs.</p>
<p><strong>Middle</strong></p>
<p>Middle constructions depict a state, which does not involve a particular agent and the state does not change with time. A middle sentence is generic; the generic agent is not syntactically present in a middle construction; a middle verb does not project an external argument into syntax.</p>
<p>Middle verbs are derived morphologically from corresponding transitive verbs, e.g. read, sell, bribe, translate, etc..</p>
<p><strong>Passive</strong></p>
<p>A passive sentence depicts an event, involving a particular agent. The agent may or may not appear as an adjunct, and the adjunct can be modified by time, location, manner, intention, etc.</p>
<h3 id="lexemes">Lexemes</h3>
<p>In providing a semantic description of a language, we do not need to treat all the variant morphological forms of a single word separately. Instead, we describe the meanings of a language’s lexemes, or the abstract units which unite all the morphological variants of a single word.</p>
<h3 id="deictic">Deictic</h3>
<p>Deictic expressions (otherwise known as deictics or indexicals) are defined as those which make reference to some aspect of the context of utterance as an essential part of their meaning. Examples of deictics in English include the words I, you and here.</p>
<h3 id="x-bar-theory-recapitulated">X-bar theory recapitulated</h3>
<p>• Motivation of X-bar theory • Head, projection, complement, specifier, adjunct • Lexical head projection, functional head projection • Root/matrix/main clause vs. embedded/subordinate clause • Tensed/finite clause vs. tenseless/non-finite clause • Subcategory, selectional restrictions • Theta roles, theta criterion • The projection principle, EPP, expletive insertion • Two parts of the human language faculty: a. computational component b. lexicon</p>
<p>Aspect is defined by making reference to some other point, typically other than the speech time, then looking at when the event happens relative to that reference point. • perfect aspect: The perfect aspect indicates that the time of an event occurs before the reference time. HAVE + PARTICIPLE (-en)</p>
<p>• progressive aspect: The progressive aspect indicates an event that is ongoing in relation to the reference time. BE + GERUND (-ing)</p>
<p>Combination of tense and aspect: a. I had taken an umbrella. (past perfect) b. I was taking an umbrella. (past progressive) c. I have taken an umbrella. (present perfect) d. I am taking an umbrella. (present progressive) e. I will have taken an umbrella. (future perfect) f. I will be taking an umbrella. (future progressive)</p>
<p>Voice refers to a phenomenon that changes the number of arguments and position of arguments that a verb uses. • Active voice: The agent of the action occupies the subject position, whereas the theme occupies the object position. e.g. John did the work. • Passive voice: The theme appears in subject position, whereas the agent is either introduced by a “by-phrase” or omitted. BE + PARTICIPLE (-en) e.g. The work was done.</p>
<p>Mood refers to the speaker’s perspective on the event— whether the event described is a possibility, a probability, a necessity, or an obligation. •Modals of English: can, could, may, might, would, shall, should, must, ought a. John must have done his work. a’. * John has must done his work. b. John must not have done his work. b’. * John not must have done his work. c. * John musts have done his work.</p>
<p>Predicates in syntax defines the relation between the individuals being talked about and the real world—as well as with each other. The participants participating in the relation are called arguments.</p>
<p>Selectional restrictions A verb subcategorizes (takes/selects) for zero, one or more complements There are semantic restrictions on what can appear in a particular syntactic position or what can be selected by predicates</p>
<p>Information relating to arguments of predicates is called the argument structure (of predicates).</p>
<p>Argument structure indicates what kinds of XP arguments the verbs require in order to complete their meaning. They assign theta-roles to these required XPs, which indicate the semantic role that the argument plays in syntax. • Theta-roles encode the semantic roles possessed by the various elements which are required by the predicate.</p>
<p>Theta roles (Ɵ) : bundles of thematic relations that cluster on one argument.</p>
<p>Thematic relations Agent: the initiator or doer of an action a. John hit him on the nose. b. A cat is chasing a dog. Theme: an entity that is moved or affected by an action; an entity that is experienced or perceived a. John hit him on the nose. b. A cat is chasing a dog. c. John watched the football match at home. Patient: an entity undergoing an action a. The enemies destroyed the city. 8 Thematic relations Experiencer: an entity that feels or perceives events a. Mary really enjoyed the movie. b. The tigers frightened the children. c. Mary saw Bill in the library. Goal: an entity towards which a motion takes place a. John will go to Shanghai tomorrow. b. An evil thought struck Dave. Recipient: an entity that receives. It only occurs with verbs that denote the change of possession. a. John gave a book to Mary. b. John gave Mary a book. c. Mary received John’s book. 9 Thematic relations Source: an entity from which a motion takes place a. John gave Mary a book. b. John came from Beijing. Location: a place where an action occurs a. We have our syntax class in 304. b. The accident occurred at the crossroads. Instrument: an object with which an action is performed a. This key opens Rm. 307. b. He opened the door with a key. Beneficiary: an entity for whose benefit an event takes place a. John baked Mary a cake. b. John bought Mary a bunch of flowers.</p>
<p>The fact that lexical information affects the form of the sentence is formalized in Projection Principle. • The Projection Principle: Lexical information is syntactically represented at all levels.</p>
<p>Expletive (or Pleonastic) Pronoun: A pronoun (usually it or there) without a theta role. Usually found in subject position.</p>
<p>Extended projection principle (EPP): All clauses must have subjects. • The EPP is an extension of an older principle the Projection Principle, which said that all information in the argument structure of a lexical item had to be present in syntax. • Expletive insertion rule: Insert an expletive pronoun into the specifier of TP.</p>
<p>Unpronounced DP: PRO a. She i was wondering whether PRO i to trust Bill. b. [ PRO To buy a bicycle now] is very important.</p>
<p>Theta criterion is observed. EPP is satisfied.</p>
<p>The VP-internal Subject Hypothesis: Subjects are generated in the specifier of the voice-headed VP.</p>
<p>VP-internal subject hypothesis The arguments of a predicate are within the immediate dominance domain of the verb: DP1 _ DP2. The external argument originates from the specifier position of the VP. The surface presence of the grammatical subject is a result of moving (A-movement/DP-movement, to be discussed), i.e., it moves/raises to the specifier position of TP (Spec, TP). ◼ The Locality Constraint on Theta Role Assignment Theta roles are assigned within the projection of the head that assigns them (i.e., the VP or other predicate).</p>
<p>DP movement/A-movement moves an argument to a non-argument position. It is a movement motivated/triggered by the requirement of Structural Case. Some of the universal principles regarding such a movement include Theta Criterion, Case Theory and the Extended Projection Principle (EPP)</p>
<p>Related theories • Theta Criterion: Each argument is assigned one and only one theta role; each theta role is assigned to one and only one argument. • The Locality Constraint on Theta Role Assignment: Theta roles are assigned within the projection of the head that assigns them. • VP Internal-subject Hypothesis: Subjects are generated in the specifier position of the VP. • EPP: All clauses must have subjects. • It-insertion: Insert an expletive pronoun it or there into the specifier of TP. • Case Theory: Case theory deals with the assignment of particular morphological case (overt or covert) to DPs in the sentences. Case assignment has been theorized as the Case Filter. • Case Filter: *DP if DP has phonetic content and has no Case. In other words: Every phonetically realized DP must be marked with Case. If a DP doesn’t get Case, the derivation will crash.</p>
<p>DP movement/A-movement: Move a DP to a specifier position. Trigger of movement: EPP and Case Filter Non-finite T can’t assign NOM case. It-insertion for EPP and NOM case is assigned by finite T. ø</p>
<p>Wh-movement: Move a wh-phrase to the specifier of CP to check the [+WH] feature in C.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>句法</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title>二语习得最终</title>
    <url>/sla-4/</url>
    <content><![CDATA[<h1 id="theories">Theories</h1>
<h3 id="critical-period-hypothesis">Critical Period Hypothesis</h3>
<p>There is a specific and limited period for language acquisition. The language acquisition device, like other biological functions, works successfully only when it is stimulated at the right time called “critical period”.</p>
<h3 id="behaviorist-perspective">Behaviorist perspective</h3>
<p>Language learning is the result of imitation, practice, feedback on success, and habit formation. They view imitation and practice as primary processes in language development.</p>
<h3 id="what-is-the-logical-problem-of-language-acquisition">What is the “logical problem” of language acquisition?</h3>
<p>The question of how children achieve the final state of L1 development with ease and success when the linguistic system is very complex and their cognitive ability is not fully developed. Children come to know more about the structure of their language than they could reasonably be expected to learn on the basis of the samples of language they hear.</p>
<h3 id="what-is-contrastive-analysis-what-are-its-limitations">What is contrastive analysis? What are its limitations?</h3>
<p>Contrastive Analysis (CA) is an approach to the study of SLA which involves predicting and explaining learner problems based on a comparison of L1 and L2 to determine similarities and differences, influenced by Structuralism and Behaviorism.</p>
<h3 id="what-is-error-analysis-what-are-its-limitations">What is error analysis? What are its limitations?</h3>
<p>Error Analysis (EA) is the first approach to the study of SLA which includes an internal focus on learners’ creative ability to construct language. It is based on the description and analysis of actual learner errors in L2, rather than on idealized linguistic structures attributed to native speakers of L1 and L2 (as in CA).</p>
<h3 id="what-is-acquisition-learning-hypothesis-made-in-the-monitor-model">What is Acquisition-Learning Hypothesis made in the Monitor Model?</h3>
<p>Acquisition-Learning Hypothesis. There is a distinction to be made between acquisition and learning. Acquisition is subconscious, and involves the innate language acquisition device which accounts for children’s L1. Learning is conscious and is exemplified by the L2 learning which takes place in many classroom contexts.</p>
<h3 id="what-is-input-hypothesis">What is Input Hypothesis?</h3>
<p>Input Hypothesis. Language acquisition takes place because there is comprehensible input. If input is understood, and if there is enough of it, the necessary grammar is automatically provided.</p>
<h3 id="what-is-interlanguage">What is interlanguage?</h3>
<p>Interlanguage: the intermediate states (or interim grammars) of a learner’s language as it moves toward the target L2. </p>
<h3 id="define-the-following-terms-competence-performance-principles-parameters.">Define the following terms: competence, performance; principles, parameters.</h3>
<p>Linguists emphasize the characteristics of the differences and similarities in the languages that are being learned, and the linguistic competence (underlying knowledge) and linguistic performance (actual production) of learners at various stages of acquisition.</p>
<p>linguistic competence: The underlying knowledge that speakers/hearers have of a language. Chomsky distinguishes this from linguistic performance. linguistic performance: The use of language knowledge in actual production</p>
<p>Interlanguage: the intermediate states (or interim grammars) of a learner’s language as it moves toward the target L2. </p>
<h3 id="what-is-fossilization">What is fossilization?</h3>
<p>Fossilization: cessation of learning/IL development in some aspects before they reach target language norms, in spite of continuing L2 input and passage of time. Because age of learning, factors of social identity and communicative need.</p>
<h3 id="how-is-language-learning-viewed-in-the-connectionist-approaches-to-language-learning">How is language learning viewed in the Connectionist approaches to language learning?</h3>
<p>Connectionist approaches focus on the increasing strength of associations between stimuli and responses rather than on the inferred abstraction of “rules” or on restructuring. Indeed, from a connectionist perspective learning essentially is change in the strength of these connections.</p>
<h3 id="comprehension-of-written-or-spoken-language-involves-both-bottom-up-and-top-down-processing.">Comprehension of written or spoken language involves both bottom-up and top-down processing.</h3>
<p>Bottom-up processing requires prior knowledge of the language system (i.e. vocabulary, morphology, phonology, syntax, and discourse structure) and interpretation of physical (graphic and auditory) cues.</p>
<p>Top-down processing can compensate for linguistic limitations to some extent by allowing learners to guess the meaning of words they have not encountered before, and to make some sense out of larger chunks of written and oral text. For both L1 and L2 speakers, top-down processing utilizes prior knowledge of content, context, and culture, which were shown in 6.1 to be essential components of communicative competence.</p>
<h3 id="zpd-mediation-scaffolding">ZPD, mediation, scaffolding</h3>
<p>Zone of Proximal Development (ZPD). This is an area of potential development, where the learner can achieve that potential only with assistance. One way in which others help the learner in language development within the ZPD is through scaffolding. This includes the “vertical constructions” mentioned above as a type of modified interaction between NSs and NNSs, in which experts commonly provide learners with chunks of talk that the learners can then use to express concepts which are beyond their independent means. This According to S-C Theory, learning occurs when simple innate mental activities are transformed into “higher order,” more complex mental functions. This transformation typically involves symbolic mediation, which is a link between a person’s current mental state and higher order functions that is provided primarily by language. This is considered the usual route to learning, whether what is being learned is language itself or some other area of knowledge. The results of learning through mediation include learners’ having heightened awareness of their own mental abilities and more control over their thought processes.</p>
<h3 id="according-to-the-interaction-hypothesis-how-do-modifications-and-collaborative-efforts-facilitate-sla">According to the Interaction Hypothesis, how do modifications and collaborative  efforts facilitate SLA?</h3>
<p>According to claims made in the Interaction Hypothesis, the modifications and collaborative efforts that take place in social interaction facilitate SLA because they contribute to the accessibility of input for mental processing: “negotiation for meaning, and especially negotiation work that triggers interactional adjustments by the NS or more competent interlocutor, facilitates acquisition because it connects input, internal learner capacities, particularly selective attention, and output in productive ways”</p>
<h3 id="how-is-interaction-viewed-in-sociocultural-theory">How is interaction viewed in Sociocultural Theory?</h3>
<p>Sociocultural (S-C) Theory (Vygotsky 1962, 1978). interaction not only facilitates language learning but is a causative force in acquisition. considering interaction as an essential force rather than as merely a helpful condition for learning.</p>
<h3 id="output-hypothesis">output hypothesis</h3>
<p>The output hypothesis claims that the act of producing language (speaking or writing) constitutes, under certain circumstances, part of the process of second language learning. Three functions of output in second language learning:</p>
<p>Accessibility Hierarchy A continuum of relative clause types such that the presence of one type implies the presence of other types higher on the hierarchy.</p>
<p>Affective Filter Part of the Monitor Model. The claim is that affect is an important part of the learning process and that one has a “raised” or “lowered” affective filter. The latter leads to better learning.</p>
<p>attention The concentration of mental powers.</p>
<p>automaticity The degree of routinized control that one has over linguistic knowledge.</p>
<p>behaviorism A school of psychology that bases learning on a stimulus— response paradigm.</p>
<p>comprehensible input Originally formulated as part of the Monitor Model, this concept refers to the understandable input that learners need for learning. Input that is slightly more advanced than the learner’s current level of grammatical knowledge.</p>
<p>connectionism An approach that assumes that learning takes place based on the extraction of regularities from the input. (See also emergentism.)</p>
<p>contrastive analysis A way of comparing two languages to determine similarities and dissimilarities.</p>
<p>Contrastive Analysis Hypothesis The prediction that similarities between two languages do not require learning and that the differences are what need to be learned.</p>
<p>Creative Construction Hypothesis The proposal that child second language learners construct rules of the second language on the basis of innate mechanisms.</p>
<p>declarative knowledge Knowledge that learners have about something. This information is relatively accessible to conscious awareness. (See also procedural knowledge.)</p>
<p>error analysis A procedure for analyzing second language data that begins with the errors learners make and then attempts to explain them.</p>
<p>explicit knowledge Knowledge about language that involves awareness. (See also declarative knowledge, explicit learning, implicit knowledge, implicit learning, procedural knowledge.)</p>
<p>explicit learning Acquisition of language that involves deliberate hypothesis testing as learners search for structure. (See also declarative knowledge; explicit knowledge; implicit knowledge; implicit learning; procedural knowledge.)</p>
<p>feedback An intervention in which information is provided to a learner that a prior utterance is correct or incorrect. (See also corrective feedback.)</p>
<p>fossilization The cessation of learning. Permanent plateaus that learners reach resulting from no change in some or all of their interlanguage forms. (See also interlanguage; stabilization.)</p>
<p>implicit knowledge Knowledge about language that does not involve awareness of that knowledge. (See also declarative knowledge; explicit learning; explicit knowledge; implicit learning; procedural knowledge.)</p>
<p>implicit learning Acquisition of knowledge about the underlying structure of a complex stimulus environment without doing so consciously. (See also explicit knowledge; explicit learning; implicit knowledge.)</p>
<p>incidental vocabulary learning Learning that takes place with an explicit focus on meaning as opposed to having an explicit goal being the learning of new words.</p>
<p>input enhancement A technique that attempts to make parts of the input salient.</p>
<p>intake That part of the language input that is internalized by the learner.</p>
<p>interference The use of the first language (or other languages known) in a second language context when the resulting second language form is incorrect. (See also language transfer; negative transfer.)</p>
<p>interlanguage The language produced by a nonnative speaker of a 518 G LO S S A RY language (i.e., a learner’s output). Refers to the systematic knowledge underlying learners’ production. interlanguage transfer The influence of one L2 over another in instances where there are multiple languages acquired after the L1.</p>
<p>Language Acquisition Device (LAD) A language faculty that constrains and guides the acquisition process.</p>
<p>language transfer The use of the first language (or other languages known) in a second language context. (See also cross-linguistic influence; facilitation; interference; negative transfer; positive transfer.)</p>
<p>metalinguistic knowledge What one knows (or thinks one knows) about the language. It is to be differentiated from what one does in using language.</p>
<p>Monitor Model A model of second language acquisition based on the concept that learners have two systems (acquisition and learning) and that the learned system monitors the acquired system.</p>
<p>negative transfer The use of the first language (or other languages known) in a second language context resulting in a nontarget-like second language form. (See also interference; language transfer; positive transfer.)</p>
<p>negotiation of meaning The attempt made in conversation to clarify a lack of understanding.</p>
<p>Poverty of the Stimulus A proposal made within the confines of Universal Grammar that input alone is not sufficiently specific to allow a child to attain the complexities of the adult grammar. (See also Universal Grammar.)</p>
<p>procedural knowledge Knowledge that relates to cognitive skills that involve sequencing information. This information is relatively inaccessible. (See also declarative knowledge.) Processability Theory A theory that proposes that production and comprehension of second language forms only takes place to the extent that they can be handled by the linguistic processor. Understanding how the processor functions allows one to understand developmental paths.</p>
<p>recasts Reformulations of an incorrect utterance that maintain the original’s meaning. restructuring Changes or reorganization of one’s grammatical knowledge.</p>
<p>Sociocultural Theory A theory based on work by the Russian psychologist Vygotsky that considers knowledge/learning arises from a social context. Learning, being socially mediated, comes from faceto-face interaction. Knowledge is internalized from learners jointly constructing knowledge in dyadic interactions.</p>
<p>stabilization The plateaus that learners reach when there is little change in some or all of their interlanguage forms (See also fossilization; interlanguage.)</p>
<p>Universal Grammar A set of innate principles common to all languages.</p>
<p>U-shaped learning Learning whereby early forms appear to be correct, followed by a period of incorrect forms, with a final stage of correct forms.</p>
<p>working memory Memory that involves storage capacity and processing capacity.</p>
<h1 id="emergentism-and-usage-based-theories">Emergentism and usage-based theories</h1>
<p>Emergentism is a theory within cognitive psychology that attempts to account for human learning and knowledge. <strong>Central to the theory is that all human learning uses the same general architecture for knowledge and performance development be it language, tennis, or assembly-line work.</strong></p>
<p>In contrast to linguistic theory that posits a Universal Grammar that is unique to language and is responsible for the nature of language, emergentism would claim <strong>there is nothing special about language.</strong></p>
<p>What is more, the theory says that <strong>all learning and representation (knowledge) in the mind/brain is sensitive to frequency.</strong> So, those things that are more frequent in the environment or those things with which a person has more frequent experience are more likely to be learned before others.</p>
<p>At the same time, more frequent things are likely to have more robust representations in the mind/brain. Emergentism is aligned with what are called usage-based theories because such theories rely on people’s experience with their environment and how this is indicated in their performance. Performance is central to Emergentism in that observable behaviors are taken to be indicators of what exists in representation and how robustly it is represented. Applied to language, emergentism would suggest that something like sentence structure is a result of frequencies of occurrence in the input that people are exposed to. As the reader might surmise, <strong>a central idea within emergentism is that the human mind/brain is a statistical tabulator par excellence</strong>, and although other factors may come into play in how important a piece of linguistic data is, human knowledge and performance with that knowledge is a direct result of experience with the environment. There is nothing special in the mind/brain that constrains or regulates language.</p>
<p>Emergentism does have problems, however, in accounting for the poverty of the stimulus problem. Within the theory, there is no real way to explain how people come to know more than what they are exposed to. Because the theory relies heavily on frequency in the input and general learning architecture, it cannot explain why people know that certain kinds of sentence permutations are impossible.</p>
<p>The Declarative/Procedural Model</p>
<h2 id="complexity-theorydynamic-systems"><strong>Complexity Theory/Dynamic Systems</strong></h2>
<p>complexity (in any domain, not just language) arises as a result of the interaction of various individuals or components.</p>
<p><strong>Sociocultural Theory</strong></p>
<p><strong>The basics</strong></p>
<p>Sociocultural Theory, based on the work of the Russian psychologist Vygotsky, argues that the development of <strong>human cognitive functions derives from social interactions and that through participation in social activities individuals are drawn into the use of these functions.</strong> The theory focuses not only on how adults and peers influence individual learning, but also on how <strong>cultural beliefs and attitudes</strong> impact how instruction and learning take place. The central constructs of the theory are: mediation; the zone of proximal development (ZPD); and verbal thought. <strong>Language learning is a socially mediated process and language is a cultural artifact that mediates social and psychological activities.</strong> <strong>Mediation refers to the idea that humans possess certain cultural tools, such as language, literacy, numeracy, and others, that they purposefully use to control and interact with their environment.</strong></p>
<p>The ZPD is a difficult concept to articulate and is often subject to misinterpretation, but to summarize here, the ZPD refers to the distance between a learner’s current ability to independently solve problems and the level of potential development present when guided by more capable persons. According to Vygotsky, learning occurs in this zone and it is achieved through the cooperation of experts and novices working together. Applied to language, learning concerns the development of language function and mental function, along with the combination of language and thought. Generally speaking, acquisition includes the process in which the low level external or social speech develops into the highest level inner speech or verbal thought.</p>
<h1 id="the-monitor-model.">the Monitor Model.</h1>
<h1 id="mediation">Mediation</h1>
<p>Human activity (including cognitive activity) is mediated by what are known as symbolic artifacts (higher-level cultural tools) such as language and literacy and by material artifacts. These artifacts mediate the relationship between humans and the social and material world around us.</p>
<p>Within sociocultural theory, humans use symbols as tools to mediate psychological activity and to control our psychological processes. This control is voluntary and allows us to attend to certain things, to plan, and to think rationally. The primary tool that humans have available is language and it is a tool that allows us to connect to our environment (both physical and social). Language gives humans the power to go beyond the immediate environment and to think about and talk about events and objects that are far removed both physically and temporally.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>二语习得</category>
      </categories>
  </entry>
  <entry>
    <title>语义学最终</title>
    <url>/semantics-final/</url>
    <content><![CDATA[<h2 id="vendler-classes"><strong>Vendler classes</strong></h2>
<p>The philosopher Zeno Vendler described types of verbs based on their inher ent <strong>ASPECTUAL</strong> differences. The classes he distinguished can be broken down <strong>COMPONENTIALLY</strong> according to three dichotomies, as shown in the table below. Vendler’s classification remains one of the most popular in describing types of <strong>SITUATIONS</strong>.</p>
<p>While Vendler intended his categories as categories of verb meanings, it has since been noted that <strong>TELICITY</strong> in particular is sensitive to the <strong>ARGUMENTS</strong> of the verb. For example, in the table the predicate sing is classified as an activity, but when it is joined with a <strong>BOUNDED</strong> noun phrase like three songs, it is an achievement, since the singing of three songs has a conclusion – at the end of the third song.</p>
<h2 id="denotational-versus-representational"><strong>Denotational versus representational</strong></h2>
<p>Semantic approaches can be divided into two main types. <strong>Denotational</strong> approaches attempt descriptions of the relation between language and the world – that is, between words or other expressions and the things or situations that they refer to (see <strong>DENOTATION</strong>). <strong>Representational</strong> approaches try to model how meaning is represented in the human mind – in other words, they are <strong>mentalistic</strong> in nature.</p>
<h2 id="sense"><strong>Sense</strong></h2>
<p>The term <strong>sense</strong> is one of two aspects of meaning, the other being <strong>REFERENCE</strong>. While reference is what an expression points to in the world, sense is the semantic aspect of meaning – the definitional properties that determine which things are referred to when an expression is used. <strong>Sense</strong> can also refer to a <strong>DEFINITION</strong> in a dictionary.</p>
<h2 id="semiotic-triangle"><strong>Semiotic triangle</strong></h2>
<p>For the purposes of linguistics, we can isolate three particularly important factors relevant to the study of meaning: the psychology of speakers, which creates and interprets language, the referent of the language expression as projected by the language user’s psychology, and the linguistic expression itself: these three points constitute the semiotic triangle.</p>
<h2 id="sense-reference-denotation-and-connotation">Sense, reference, denotation and connotation</h2>
<p>referent on any one occasion of use,</p>
<p>denotation: the set of all its referents</p>
<p>sense: the abstract, general meaning which can be translated from one language to another, paraphrased, or defined in a dictionary.</p>
<p>Connotation names those aspects of meaning which do not affect a word’s sense, reference or denotation, but which have to do with secondary factors such as its emotional force, its level of formality, its character as a euphemism, etc.</p>
<h2 id="explanations-of-meaning-in-terms-of-meanings-are-circular-and-four-ways-of-breaking-the-circle">Explanations of meaning in terms of meanings are circular and Four ways of breaking the circle</h2>
<p>All definitions of meaning in language, therefore, are ultimately circular because they use one kind of meaning to explain another.</p>
<p>There are four important answers to the question ‘what is meaning?’: the referential/denotational theory of meaning, the conceptual theory of meaning, the brain states theory and the use theory. We do not have to categorically choose between these theories. Instead, recognizing that the notion of meaning in linguistics is a way of talking about the factors which explain language use, we can see referents, concepts, brain states and uses as all relevant to this task.</p>
<h2 id="truth">Truth</h2>
<p>From this perspective, truth that is known before or without experience has traditionally been called <strong>a priori</strong>. This <strong>a priori truth</strong> is contrasted with <strong>a posteriori truth</strong>: truth which can only be known on the basis of empirical testing. Another related concept is Leibniz’s distinction between <strong>necessary truths</strong>, which cannot be denied without forcing a contradiction, for example the arithmetical statement Two and two make four, and <strong>contingent truths</strong> which can be contradicted, depending on the facts, for example the sentence The dodo is extinct. If someone unexpectedly found a dodo in a forest on Mauritius, this latter sentence would become false. It is difficult, on the other hand, to imagine circumstances in which Two and two make four would unexpectedly become false. This is similar to our a priori/a posteriori distinction but comes at truth from another viewpoint: not in terms of what the speaker knows but in terms of what the world is like. We can say that it is hard to think how our sentence about two and two making four could not be true without changing our view of the present facts of the world. From this perspective a sentence like 4.40 is also necessarily true and a contradiction like 4.41 is necessarily false. In another, related terminology tautologies like 4.39 are <strong>analytic</strong> while a sentence like My father is a sailor is <strong>synthetic</strong>. <strong>Analytic</strong> statements are those where the truth follows from the meaning relations within the sentence, regardless of any relationship with the world, while a <strong>synthetically</strong> true statement is true because it accords with the facts of the world.</p>
<h2 id="analytic">Analytic</h2>
<p>Analytic PROPOSITIONS are those that require no external verification, which is to say that their truth or falsity can be established by examining only their linguistic matter and internal logic, rather than appealing to our senses in order to verify the claims they describe. Consider, for example, the following: No dead person is alive.</p>
<h2 id="truth-1">Truth</h2>
<p>From this perspective, truth that is known before or without experience has traditionally been called a priori. This a priori truth is contrasted with a posteriori truth: truth which, as in our examples 4.17 and 4.18 earlier, can only be known on the basis of empirical testing. Another related concept is Leibniz’s distinction between necessary truths, which cannot be denied without forcing a contradiction, for example the arithmetical statement Two and two make four, and contingent truths which can be contradicted, depending on the facts, for example the sentence The dodo is extinct.</p>
<h2 id="presuppose-presupposition">Presuppose, presupposition</h2>
<p>A presupposition is a proposition that must be supposed to be true in order for another proposition to be judged true or false. For example, The king of France is bald presupposes the proposition that ‘there is a king of France.’ Unlike ENTAILMENTS, the presupposition remains the same when the sentence is negated. So, The king of France is not bald still presupposes that ‘there is a king of France.’</p>
<h2 id="anaphor-anaphora-anaphoric">Anaphor, anaphora, anaphoric</h2>
<p>In anaphora, a linguistic expression (called an anaphor or anaphoric pronoun) is understood to have the same REFERENCE as another linguistic expression (its antecedent), which typically precedes it in the same sentence or in the earlier discourse.</p>
<h2 id="anomalous-anomaly">Anomalous, anomaly</h2>
<p>A semantically anomalous linguistic expression is one that has an abnormal meaning or fails to make sense, despite being grammatically well-formed.</p>
<p>Semantic anomalies are sometimes accounted for as violations of SELECTIONAL RESTRICTIONS that words place on other words that they occur with. Thus The telephone ate my gingerbread is anomalous because the verb eat is restricted to only occur with subjects that refer to animate beings – unless the expression is to be interpreted FIGURATIVELY.</p>
<h2 id="antecedent">Antecedent</h2>
<p>An antecedent is something that comes before something else. In reference to ANAPHORA, it refers to an earlier expression to which a pronoun CO-REFERS.</p>
<h2 id="argument">Argument</h2>
<p>In a PROPOSITION or SENTENCE, (semantic) arguments are the participants in the EVENT or STATE expressed by the PREDICATE.</p>
<p>Argument In a PROPOSITION or SENTENCE, (semantic) arguments are the participants in the EVENT or STATE expressed by the PREDICATE. Within a sentence/proposition, arguments play particular roles, called SEMANTIC ROLES. The number and type of arguments that a predicate requires and their syntactic realization is known as argument structure.</p>
<h2 id="aspect-aspectual">Aspect, aspectual</h2>
<p>The SITUATIONS described by PROPOSITIONS can take place in time in different ways. The internal organization of a situation with relation to time – that is, how the situation unfolds in time or the temporal perspective taken on the situation – is its aspect as opposed to its TENSE, which is the when of the situation.</p>
<h2 id="causative-alternation">causative alternation</h2>
<p>In linguistics, causative alternation is a phenomenon in which certain verbs that express a change of state (or a change of degree) can be used transitively or intransitively. A causatively alternating verb, such as "open", has both a transitive meaning ("I opened the door") and an intransitive meaning ("The door opened"). When causatively alternating verbs are used transitively they are called causatives since, in the transitive use of the verb, the subject is causing the action denoted by the intransitive version. When causatively alternating verbs are used intransitively, they are referred to as anticausatives or inchoatives because the intransitive variant describes a situation in which the theme participant (in this case "the door") undergoes a change of state, becoming, for example, "opened".</p>
<h2 id="deictic-deixis">Deictic, deixis</h2>
<p>Deixis refers to the phenomenon where the meaning of some linguistic item relies inherently on the extralinguistic CONTEXT. Thus understanding deictic expressions like that book, here, yesterday or I depends on knowing where the utterance is spoken, when it is spoken and who is speaking.</p>
<h2 id="demonstrative">Demonstrative</h2>
<p>Demonstratives are expressions such as this, that, those and these. A distinction can be made between demonstrative determiners and demonstrative pronouns – the former occur with a noun in a noun phrase (this book, those boxes), while the latter substitute for a whole noun phrase and therefore stand alone (This is nice.).</p>
<h2 id="denotational-versus-representational-1">Denotational versus representational</h2>
<p>Semantic approaches can be divided into two main types. Denotational approaches attempt descriptions of the relation between language and the world – that is, between words or other expressions and the things or situations that they refer to (see DENOTATION). Representational approaches try to model how meaning is represented in the human mind – in other words, they are mentalistic in nature. Denotational approaches tend to derive from philosophy and mathematical logic (and thus FORMAL SEMANTIC approaches are often denotational), but much of the modern linguistic tradition in semantics is more representational than denotational – for example, COGNITIVE LINGUISTICS, CONCEPTUAL SEMANTICS and NATURAL SEMANTIC METALANGUAGE approaches are all representational.</p>
<h2 id="encyclopaedic-meaning">Encyclopaedic meaning</h2>
<p>Encyclopaedic meaning is general world knowledge, known by virtue of our experience of the world.</p>
<h2 id="entailment">Entailment</h2>
<p>Entailment is the PROPOSITIONAL RELATION in which if one PROPOSITION is true, then it is always the case that the related proposition is true. This can be stated as the MATERIAL CONDITIONAL</p>
<h2 id="evidential-evidentiality">Evidential, evidentiality</h2>
<p>Evidentiality is a type of MODALITY that indicates the speaker’s source of the knowledge that is expressed in a PROPOSITION or SENTENCE – for instance, through personal observation, by hearsay or by inferring it from other knowledge. Languages like Turkish have morphological markers of evidentiality, but in English it can only be marked by lexical means (e.g. apparently) or through constructions such as be supposed to: Daisy’s stew is supposed to be delicious (i.e. ‘I have heard that it is, but I do not have experience of it myself’).</p>
<h2 id="extension">Extension</h2>
<p>The extension of an expression is the set of all potential REFERENTS of the expression with respect to some world or MODEL.</p>
<h2 id="inchoative">Inchoative</h2>
<p>Inchoative Verbs (or PREDICATES) whose meanings involve ‘beginning’ or ‘becoming’ are referred to as inchoative (also inceptive) from the Latin verb for ‘begin’. Inchoative verbs describe a change of state or a beginning of having a state. For example redden means ‘become red’ and open denotes the change from being closed to not being closed. CAUSATIVE verbs can also be considered to be inchoative.</p>
<h2 id="inclusion-inclusive">Inclusion, inclusive</h2>
<p>The terms inclusion and inclusive can apply to a number of semantic phenomena. In SEMANTIC (particularly LEXICAL) RELATIONS, a relation of inclusion is when the EXTENSION of one term is a proper subset of the extension of another. For example, the things denoted by kitten are also among the things denoted by cat (see HYPONYM). Proper inclusion is when a SUBORDINATE CATEGORY is wholly contained within another, SUPERORDINATE category. The set of blue pens is properly included within the set of pens because there is nothing that is in the set of blue pens that is not also in the set of pens.</p>
<h2 id="lexeme">Lexeme</h2>
<p>A lexeme (cf. LEXICAL ITEM) is a unit of language that is represented in the LEXICON. Like other linguistic -eme terms, lexeme refers to an abstraction from the actual spoken or written language – that is, it refers to the word as it is represented in the mind, rather than in the mouth or on the page.</p>
<h2 id="lexicon">Lexicon</h2>
<p>Traditionally, the lexicon is a collection of information about a language’s LEXEMES, that is, the expressions that are learnt by the language’s users, rather than derived anew each time they are used. The term lexicon can refer to the following: a) a dictionary, especially a dictionary of a classical language; or b) the vocabulary of a language (also known as lexis); or c) a particular language user’s knowledge of her/his own vocabulary, as stored in her/his mind – the mental lexicon.</p>
<h2 id="meaning">Meaning</h2>
<p>Linguistic meaning is, of course, the object of study in semantics. However, meaning is rarely used as a technical term in semantic study because of its POLYSEMY and generality. For example, it may be used to refer to an expression’s DEFINITION or SENSE, but it may instead be used to include non-denotational aspects of meaning, such as CONNOTATION, or to the particular INTERPRETATION of the expression’s REFERENCE in a particular CONTEXT. Where it is used, it is usually because a distinction between sense and reference is not needed in the particular discussion or it is used as a synonym for sense or interpretation.</p>
<h2 id="metalanguage">Metalanguage</h2>
<p>A metalanguage is a system used for describing a language without using that language itself. A metalanguage resolves the inevitable circularity that arises if one, for instance, uses English to describe the semantics of English. One could, in principle, use one natural language to describe another (e.g. Finnish to describe Polish). However, this has the problem that the meanings of the object language (the language being described) would not necessarily translate in an equivalent way into the other natural language used as a metalanguage. An ideal metalanguage should provide a complete and unambiguous description of the object language.</p>
<h2 id="modality-modal-verbs">Modality, modal verbs</h2>
<p>In linguistics, modality refers to the expression of a speaker’s attitude towards a PROPOSITION. This involves notions such as obligation, permission, possibility, necessity and ability. In English these notions are typically expressed via the modal verbs may, must, can, will, shall, might, could and should, or semiGRAMMATICALIZED expressions such as have to, need to or had better. Expressions of MOOD may also indicate a type of modality distinction – that between the reality and irreality of a proposition.</p>
<p>Modality can be divided into different types and, within those types, into different degrees. The most established distinction of modality types is that between deontic and epistemic modality. Deontic modality involves a duty, obligation, permission or (when negated) prohibition being imposed on someone or something. Different degrees of deontic modality are shown below: Tim must take the dog out for a walk. (obligation) Tim should take the dog out for a walk. (weaker obligation) Tim may/can take the dog out for a walk. (permission) Epistemic modality, on the other hand, relates to the speaker’s judgement of how probable the truth of the proposition is, based on some available evidence.</p>
<h2 id="montague-grammar">Montague grammar</h2>
<p>The grammar presents a COMPOSITIONAL means of representing linguistic meaning through the use of a formal system of representation (LAMBDA CALCULUS) combined with set and type theories from mathematics.</p>
<h2 id="ontological-category-ontology">Ontological category, ontology</h2>
<p>Ontology is the philosophical field that attempts to organize everything that exists into a limited number of general CATEGORIES.</p>
<h2 id="polarity-item">Polarity item</h2>
<p>A polarity item is a LEXEME that is sensitive to the POLARITY of the constituent to which it belongs. For instance, the determiner any is a negative polarity item that can only occur in negated clauses, while the positive polarity item some must occur in affirmative clauses</p>
<h2 id="predicate">Predicate</h2>
<p>In semantics, predicate refers to the part of a PROPOSITION that expresses the relation or property that is being ascribed to some entities, ARGUMENTS.</p>
<h2 id="presuppose-presupposition-1">Presuppose, presupposition</h2>
<p>A presupposition is a proposition that must be supposed to be true in order for another proposition to be judged true or false. For example, The king of France is bald presupposes the proposition that ‘there is a king of France.’ Unlike ENTAILMENTS, the presupposition remains the same when the sentence is negated. So, The king of France is not bald still presupposes that ‘there is a king of France.’</p>
<h2 id="primitive-semantic">Primitive, semantic</h2>
<p>A primitive or atomic unit is one that cannot be broken down or defined further and thus forms the most basic unit of analysis. In semantics, the notion of primitives is particularly important in COMPONENTIAL ANALYSIS, which assumes that the meanings of linguistic items are built out of smaller units of meaning, meaning components.</p>
<h2 id="pronouns">Pronouns</h2>
<p>Personal pronouns like me, she and they have DEFINITE reference, as do the DEMONSTRATIVE pronouns this, that, these and those. In English, indefinite pronouns are often similar in form to indefinite QUANTIFIERS – for example, one or some in I want some.</p>
<p>Pronouns are often used ANAPHORICALLY, which is to say that they refer to something that was mentioned previously in the discourse. But pronouns are also used for DEIXIS, which involves reference to something in the extralinguistic context. An example of this would be when a speaker says That was there while pointing first to an object and then a location.</p>
<h2 id="proper-name-proper-noun">Proper name, proper noun</h2>
<p>A proper name, or proper noun, is a nominal expression that denotes the same individual (or particular set of individuals) every time it is used. This is opposed to a COMMON NOUN, which indicates a type of thing, and therefore can be used to denote different individuals each time it is used.</p>
<h2 id="proposition">Proposition</h2>
<p>A proposition may be defined as the meaning of a SENTENCE that makes a statement about some state of affairs. As such, a proposition has a TRUTH VALUE; it can be either true or false. A proposition is independent of the linguistic structure used to express it, which is to say that the same proposition can be expressed by different sentences.</p>
<h2 id="referring-expression">Referring expression</h2>
<p>A referring expression is an expression that REFERS in a context to an individual or set of individuals. In natural language, this would usually be expressed as a NOUN phrase, but not all uses of noun phrases refer. For example, in Noam Chomsky is a linguist, Noam Chomsky refers to a particular individual, but a linguist does not refer to any particular linguist, but rather describes the PROPERTY of being a linguist.</p>
<h2 id="selectional-restriction">Selectional restriction</h2>
<p>Selectional (or selection) restrictions are constraints that determine which co-occurrences of words or meanings of words are semantically well-formed, rather than ANOMALOUS or abnormal. Selectional restrictions are generally considered to be separate from grammatical constraints such as that transitive verbs must take noun phrases as their objects.</p>
<p>One meaning of vagueness refers to the underspecification or generality of SENSE. For example, the word cousin can refer to either a male or a female, or an infant or a pensioner and it therefore has a sense that is vague or underspecified with respect to gender and age. This meaning of vagueness contrasts with AMBIGUITY, where a linguistic form has multiple distinct senses (e.g. light ‘not dark’; ‘not heavy’ or bug ‘an insect’; ‘a listening device’).</p>
<h2 id="valency">Valency</h2>
<p>Valency refers to the number of ARGUMENTS a VERB requires in order to express a complete PROPOSITION. Valency corresponds to the classification of PREDICATES into one-, two- and three-place predicates.</p>
<h2 id="middle-voice">Middle voice</h2>
<p>Some languages also have a further grammatical category of middle voice. Middle voice typically serves to emphasize that the argument that occurs in the subject position is affected by the event expressed by the verb. In English, middle voice occurs in a very limited class of examples such as the ones below: These oranges peel very easily. The tickets aren’t selling that well. Like passives, these examples involve the promotion of a non-AGENT participant into the subject position, but English does not have any special morphological marking of middle voice.</p>
<h2 id="zeugma">Zeugma</h2>
<p>In semantics, a zeugma (or syllepsis) is a linguistic construction where a single constituent is related to two different semantic interpretations, in a way that gives rise to a semantic ANOMALY.</p>
<p>For the zeugmatic, anomalous reading to arise, the two interpretations that are evoked simultaneously need to be semantically distinct. Therefore, the possibility of constructing a zeugmatic sentence provides a way of testing whether particular readings of a linguistic form are AMBIGUOUS (POLYSEMOUS or HOMONYMOUS) or just VAGUE.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>语义</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>semantics</tag>
      </tags>
  </entry>
  <entry>
    <title>二语习得课本</title>
    <url>/sla-3/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2021/04/30/FMh8DeCfcWELUd5.png"/></p>
<p>See <a href="https://share.mubu.com/doc/71bKL8XSabV">mubu</a> for more details.</p>
<ul>
<li><p>1 Introduction</p>
<ul>
<li><p>1.1 The study of second language acquisition</p></li>
<li><p>1.2 Definitions</p></li>
<li><p>1.3 The nature of language</p>
<ul>
<li><p>1.3.1 Sound systems</p></li>
<li><p>1.3.2 Syntax</p></li>
<li><p>1.3.3 Morphology and the lexicon</p></li>
<li><p>1.3.4 Semantics</p></li>
<li><p>1.3.5 Pragmatics</p></li>
</ul></li>
<li><p>1.4 The nature of nonnative speaker knowledge</p></li>
</ul></li>
<li><p>2 Related disciplines</p>
<ul>
<li><p>2.1 SLA and related disciplines</p></li>
<li><p>2.2 Third language acquisition/multilingualism</p></li>
<li><p>2.3 Heritage language acquisition</p></li>
<li><p>2.4 Bilingual acquisition</p></li>
<li><p>2.5 First language acquisition</p>
<ul>
<li><p>2.5.1 Babbling</p></li>
<li><p>2.5.2 Words</p></li>
<li><p>2.5.3 Sounds and pronunciation</p></li>
<li><p>2.5.4 Syntax</p></li>
<li><p>2.5.5 Morphology</p></li>
</ul></li>
</ul></li>
<li><p>3 Second and foreign language data</p>
<ul>
<li><p>3.1 Data analysis</p>
<ul>
<li><p>3.1.1 Data set I: plurals</p></li>
<li><p>3.1.2 Data set II: verb + -ing markers</p></li>
<li><p>3.1.3 Data set III: prepositions</p></li>
</ul></li>
<li><p>3.2 What data analysis does not reveal</p></li>
<li><p>3.3 Data collection</p>
<ul>
<li><p>3.3.1 Eliciting speech samples</p></li>
<li><p>3.3.2 Eliciting reactions to data</p></li>
<li><p>3.3.3 Verbal report data</p></li>
<li><p>3.3.4 Measuring non-linguistic information</p>
<ul>
<li>3.3.5 Measuring general proficiency:</li>
</ul></li>
<li><p>standardized language tests</p></li>
</ul></li>
<li><p>3.4 Replication</p></li>
<li><p>3.5 Issues in data analysis</p></li>
<li><p>3.6 What is acquisition?</p></li>
</ul></li>
<li><p>4 The role of the native language: an historical overview</p>
<ul>
<li><p>4.1 Introduction</p></li>
<li><p>4.2 Behaviorism</p>
<ul>
<li><p>4.2.1 Linguistic background</p></li>
<li><p>4.2.2 Psychological background</p></li>
</ul></li>
<li><p>4.3 Contrastive Analysis Hypothesis</p></li>
<li><p>4.4 Error analysis</p></li>
</ul></li>
<li><p>5 Recent perspectives on the role of previously known languages</p>
<ul>
<li><p>5.1 Theories of learning</p></li>
<li><p>5.2 Child second language acquisition</p></li>
<li><p>5.3 Child second language morpheme order studies</p></li>
<li><p>5.4 Adult second language morpheme order studies</p></li>
<li><p>5.5 Revised perspectives on the role of the native language</p>
<ul>
<li><p>5.5.1 Avoidance</p></li>
<li><p>5.5.2 Differential learning rates</p></li>
<li><p>5.5.3 Different paths</p></li>
<li><p>5.5.4 Overproduction</p></li>
<li><p>5.5.5 Predictability/selectivity</p></li>
<li><p>5.5.6 Second language processing</p></li>
</ul></li>
<li><p>5.6 Interlanguage transfer</p></li>
</ul></li>
<li><p>6 Formal approaches to SLA</p>
<ul>
<li><p>6.1 Introduction</p></li>
<li><p>6.2 Universal Grammar</p>
<ul>
<li><p>6.2.1 Initial state</p></li>
<li><p>6.2.2 UG principles</p></li>
<li><p>6.2.3 UG parameters</p></li>
<li><p>6.2.4 Falsification</p></li>
</ul></li>
<li><p>6.3 Transfer: the UG perspective</p>
<ul>
<li><p>6.3.1 Levels of representation</p></li>
<li><p>6.3.2 Clustering</p></li>
<li><p>6.3.3 Learnability</p></li>
</ul></li>
<li><p>6.4 Phonology</p>
<ul>
<li><p>6.4.1 Markedness Differential Hypothesis</p></li>
<li><p>6.4.2 Similarity/dissimilarity: Speech Learning Model</p></li>
<li><p>6.4.3 Optimality Theory</p></li>
<li><p>6.4.4 Ontogeny Phylogeny Model</p></li>
</ul></li>
</ul></li>
<li><p>7 Typological and functional approaches</p>
<ul>
<li><p>7.1 Introduction</p></li>
<li><p>7.2 Typological universals</p>
<ul>
<li><p>7.2.1 Test case I: the Accessibility Hierarchy</p></li>
<li><p>7.2.2 Test case II: the acquisition of questions</p></li>
<li><p>7.2.3 Test case III: voiced/voiceless consonants</p></li>
<li><p>7.2.4 Falsifiability</p></li>
<li><p>7.2.5 Typological universals: conclusions</p></li>
</ul></li>
<li><p>7.3 Functional approaches</p>
<ul>
<li><p>7.3.1 Tense and aspect: the Aspect Hypothesis</p></li>
<li><p>7.3.2 The Discourse Hypothesis</p></li>
<li><p>7.3.3 Concept-oriented approach</p></li>
</ul></li>
</ul></li>
<li><p>8 Looking at interlanguage processing</p>
<ul>
<li><p>8.1 Introduction</p></li>
<li><p>8.2 Connectionist/emergentist models</p></li>
<li><p>8.3 Processing approaches</p>
<ul>
<li><p>8.3.1 Processability Theory</p>
<ul>
<li>8.3.2 Information processing: automaticity,</li>
</ul></li>
<li><p>restructuring, and U-shaped learning</p></li>
<li><p>8.3.3 Input Processing</p></li>
</ul></li>
<li><p>8.4 Knowledge types</p>
<ul>
<li><p>8.4.1 Acquisition–Learning</p></li>
<li><p>8.4.2 Declarative/procedural</p></li>
<li><p>8.4.3 Implicit/explicit</p></li>
<li><p>8.4.4 Representation and control</p></li>
</ul></li>
<li><p>8.5 Interface of knowledge types</p>
<ul>
<li><p>8.5.1 No interface</p></li>
<li><p>8.5.2 Weak interface</p></li>
<li><p>8.5.3 Strong interface</p></li>
</ul></li>
<li><p>8.6 Psycholinguistic constructs</p>
<ul>
<li><p>8.6.1 Attention</p></li>
<li><p>8.6.2 Working memory</p></li>
<li><p>8.6.3 Monitoring</p></li>
</ul></li>
</ul></li>
<li><p>9 Interlanguage in context</p>
<ul>
<li><p>9.1 Introduction</p></li>
<li><p>9.2 Variation</p></li>
<li><p>9.3 Systematic variation</p>
<ul>
<li><p>9.3.1 Linguistic context</p></li>
<li><p>9.3.2 Social context relating to the native language</p>
<ul>
<li>9.3.3 Social context relating to interlocutor, task</li>
</ul></li>
<li><p>type, and conversational topic</p></li>
</ul></li>
<li><p>9.4 Social interactional approaches</p>
<ul>
<li><p>9.4.1 Conversation Analysis</p></li>
<li><p>9.4.2 Sociocultural theory</p></li>
</ul></li>
<li><p>9.5 Communication strategies</p></li>
<li><p>9.6 Interlanguage pragmatics</p></li>
</ul></li>
<li><p>10 Input, interaction, and output</p>
<ul>
<li><p>10.1 Introduction</p></li>
<li><p>10.2 Input</p></li>
<li><p>10.3 Comprehension</p></li>
<li><p>10.4 Interaction</p></li>
<li><p>10.5 Output</p>
<ul>
<li><p>10.5.1 Feedback</p></li>
<li><p>10.5.2 Hypothesis testing</p></li>
<li><p>10.5.3 Automaticity</p></li>
<li><p>10.5.4 Meaning-based to grammar-based processing</p></li>
</ul></li>
<li><p>10.6 The role of input and interaction in language learning</p>
<ul>
<li><p>10.6.1 Attention</p></li>
<li><p>10.6.2 Contrast theory</p></li>
<li><p>10.6.3 Metalinguistic awareness</p></li>
</ul></li>
<li><p>10.7 Limitations of input</p></li>
</ul></li>
<li><p>11 Instructed second language learning</p>
<ul>
<li><p>11.1 Introduction</p></li>
<li><p>11.2 Classroom language</p></li>
<li><p>11.3 Processing instruction</p></li>
<li><p>11.4 Teachability/learnability</p></li>
<li><p>11.5 Focus on form</p>
<ul>
<li><p>11.5.1 Timing</p></li>
<li><p>11.5.2 Forms to focus on</p></li>
<li><p>11.5.3 Input manipulation and input enhancement</p></li>
</ul></li>
<li><p>11.6 Uniqueness of instruction</p></li>
<li><p>11.7 Effectiveness of instruction</p></li>
</ul></li>
<li><p>12 Beyond the domain of language</p>
<ul>
<li><p>12.1 Introduction</p></li>
<li><p>12.2 Research traditions</p>
<ul>
<li><p>12.2.1 Linguistics</p></li>
<li><p>12.2.2 Psychology</p></li>
<li><p>12.2.3 Psycholinguistics</p></li>
</ul></li>
<li><p>12.3 Affect</p>
<ul>
<li><p>12.3.1 Language shock and culture shock</p></li>
<li><p>12.3.2 Anxiety</p></li>
<li><p>12.3.3 Affective Filter</p></li>
</ul></li>
<li><p>12.4 Social distance</p></li>
<li><p>12.5 Age differences</p></li>
<li><p>12.6 Aptitude</p></li>
<li><p>12.7 Motivation</p>
<ul>
<li><p>12.7.1 Motivations as a function of time and success</p></li>
<li><p>12.7.2 Changes over time</p>
<ul>
<li>12.7.3 Influence of success on motivation and</li>
</ul></li>
<li><p>demotivation</p></li>
</ul></li>
<li><p>12.8 Personality and learning style</p>
<ul>
<li><p>12.8.1 Extroversion and introversion</p></li>
<li><p>12.8.2 Risk taking</p></li>
<li><p>12.8.3 Field independence/dependence</p></li>
<li><p>12.8.4 Visual/auditory/kinesthetic</p></li>
<li><p>12.8.5 Obtaining learning style information</p></li>
</ul></li>
<li><p>12.9 Learning strategies</p></li>
</ul></li>
<li><p>13 The lexicon</p>
<ul>
<li><p>13.1 The significance of the lexicon</p></li>
<li><p>13.2 Categories of lexical knowledge: some dichotomies</p>
<ul>
<li><p>13.2.1 Production and reception</p></li>
<li><p>13.2.2 Knowledge and control</p></li>
<li><p>13.2.3 Breadth and depth</p></li>
</ul></li>
<li><p>13.3 Lexical knowledge, development, and influences</p>
<ul>
<li><p>13.3.1 Subcategorization</p></li>
<li><p>13.3.2 Word associations and networks</p></li>
<li><p>13.3.3 Word formation</p></li>
<li><p>13.3.4 Word combinations, collocations, and phraseology</p></li>
</ul></li>
<li><p>13.4 L1 influence</p>
<ul>
<li><p>13.4.1 Incidental vocabulary learning</p></li>
<li><p>13.4.2 Incremental vocabulary learning</p></li>
</ul></li>
<li><p>13.5. Using lexical skills</p>
<ul>
<li><p>13.5.1 Production</p></li>
<li><p>13.5.2 Perception</p></li>
</ul></li>
</ul></li>
<li><p>14 An integrated view of second language acquisition</p>
<ul>
<li><p>14.1 An integration of subareas</p>
<ul>
<li><p>14.1.1 Apperceived input</p></li>
<li><p>14.1.2 Comprehended input</p></li>
<li><p>14.1.3 Intake</p></li>
<li><p>14.1.4 Integration</p></li>
<li><p>14.1.5 Output</p></li>
</ul></li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>语言学</category>
        <category>二语习得</category>
      </categories>
  </entry>
  <entry>
    <title>二语习得</title>
    <url>/sla/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2021/04/30/Fb5mVBlC9XPAGpg.png"/></p>
<p>See <a href="https://share.mubu.com/doc/3f9VeztlAXV">mubu</a> for more details.</p>
<h2 id="what-is-second-language-acquisition">What is second language acquisition?</h2>
<h2 id="a-brief-history-of-sla">A brief history of SLA</h2>
<p>input</p>
<p>intake</p>
<p>interlanguage</p>
<p>transfer</p>
<p>fossilization</p>
<h3 id="the-1970s">The 1970s</h3>
<p>acquisition orders</p>
<p>morpheme studies</p>
<p>transitional stages</p>
<p>error analysis</p>
<h3 id="the-1980s">The 1980s</h3>
<p>Monitor</p>
<p>Theory</p>
<p>acquisition versus learning</p>
<p>Input Hypothesis</p>
<h3 id="the-1990s">The 1990s</h3>
<p>noticing</p>
<p>Output Hypothesis</p>
<p>Interaction Hypothesis</p>
<p>connectionism</p>
<p>Universal Grammar</p>
<p>processability</p>
<p>input processing</p>
<p>Sociocultural Theory</p>
<h3 id="the-2000s-and-beyond">The 2000s and beyond</h3>
<h2 id="second-language-acquisition-and">Second language acquisition and</h2>
<h2 id="second-language-teaching">second language teaching</h2>
<h2 id="about-this-book">About this book</h2>
<p>noticing</p>
<p>Output Hypothesis</p>
<p>Interaction Hypothesis</p>
<p>Can L2</p>
<p>learners become native-like?</p>
<h1 id="key-questions-in-second-language-acquisition">Key Questions in Second Language Acquisition</h1>
<h2 id="question-1-what-is-the-initial-state">Question 1: What is the initial state?</h2>
<h3 id="l1-initial-state">L1 = Initial state</h3>
<p>(Universal Grammar)</p>
<p>parameter</p>
<p>form-function</p>
<p>functional approaches</p>
<p>connectionism</p>
<p>processing</p>
<p>parsing</p>
<p>grammaticality judgment</p>
<h3 id="universals-initial-state">Universals = Initial state</h3>
<h3 id="limited-or-partial-l1-transfer">Limited or partial L1 transfer</h3>
<p>lexicon</p>
<p>processability</p>
<h3 id="assessment">Assessment</h3>
<h2 id="question-2-can-l2-learners">Question 2: Can L2 learners</h2>
<h2 id="become-native-like">become native-like?</h2>
<p>ultimate attainment</p>
<p>critical period</p>
<h3 id="l2-learners-cannot-become-native-like">L2 learners cannot become native-like</h3>
<p>subjacency</p>
<h3 id="l2-learners-can-become-native-like">L2 learners can become native-like</h3>
<p>syntax</p>
<p>morphology</p>
<h3 id="l2-learners-can-achieve-native-likeness">L2 learners can achieve native-likeness</h3>
<h3 id="in-some-domains">in some domains</h3>
<p>competence</p>
<p>mental representation of language</p>
<p>performance</p>
<p>(9a)</p>
<p>(9b)</p>
<p>(10b)</p>
<h3 id="assessment-1">Assessment</h3>
<p>fossilization</p>
<p>Critical Period Hypothesis</p>
<h2 id="question-3-is-there-a-critical-period">Question 3: Is there a critical period?</h2>
<p>Critical Period Hypothesis</p>
<h3 id="there-is-a-critical-period">There is a critical period</h3>
<p>What are the roles of</p>
<p>explicit and implicit learning in SLA?</p>
<p>Fundamental Difference</p>
<p>Hypothesis.</p>
<p>Universal Grammar</p>
<h3 id="there-is-no-critical-period-or-at-least">There is no critical period (or at least,</h3>
<h3 id="its-questionable">it’s questionable)</h3>
<p>Can L2 learners become</p>
<p>native-like?</p>
<p>Universal Grammar</p>
<h3 id="there-are-critical-periods-for-some-things">There are critical periods for some things</h3>
<p>syntax</p>
<p>morphology</p>
<p>phonology</p>
<p>principles</p>
<p>parameters</p>
<h3 id="assessment-2">Assessment</h3>
<h3 id="sla-includes-stage-like-development">SLA includes stage-like development</h3>
<p>developmental sequences</p>
<p>Does instruction</p>
<p>make a difference?</p>
<p>U-shaped acquisition</p>
<h3 id="ordered-development">Ordered development</h3>
<p>acquisition orders</p>
<p>morphemes</p>
<p>salience</p>
<p>form-function</p>
<h3 id="variation-and-variability">Variation and variability</h3>
<p>variation</p>
<h3 id="a-comment-about-first-language-influence">A comment about first language influence</h3>
<p>Universal Grammar</p>
<h3 id="assessment-3">Assessment</h3>
<p>processability</p>
<h2 id="question-5-what-are-the-roles-of-explicit-and-implicit-learning-in-sla">Question 5: What are the roles of explicit and implicit learning in SLA?</h2>
<h3 id="sla-is-largely-implicit">SLA is largely implicit</h3>
<p>acquisition</p>
<p>learning</p>
<p>competence</p>
<p>monitoring</p>
<p>Monitor</p>
<p>Theory</p>
<p>Universal</p>
<p>Grammar</p>
<p>connectionism</p>
<h3 id="sla-is-largely-explicit">SLA is largely explicit</h3>
<p>Adaptive Control of Thought model</p>
<p>skills</p>
<p>noticing</p>
<h3 id="assessment-4">Assessment</h3>
<p>poverty of the stimulus</p>
<p>Does instruction</p>
<p>make a difference?</p>
<h2 id="question-6-what-are-the-roles-of-input-and-output-in-sla">Question 6: What are the roles of input and output in SLA?</h2>
<p>input</p>
<p>output</p>
<h3 id="input">Input</h3>
<p>competence, mental representation of language</p>
<p>Universal Grammar</p>
<p>principles</p>
<p>connectionism</p>
<p>input enhancement</p>
<h3 id="output">Output</h3>
<p>Interaction</p>
<p>Hypothesis</p>
<p>feedback</p>
<p>negative evidence</p>
<h3 id="other-perspectives">Other perspectives</h3>
<p>Adaptive Control of Thought</p>
<p>model</p>
<p>skills</p>
<h3 id="assessment-5">Assessment</h3>
<p>syntax</p>
<h3 id="question-7-what-are-individual-differences-and-how-do-they-affect-acquisition">Question 7: What are individual differences and how do they affect acquisition?</h3>
<p>developmental sequences, acquisition orders, Universal</p>
<p>Grammar</p>
<h3 id="aptitude">Aptitude</h3>
<p>working memory</p>
<h3 id="motivation">Motivation</h3>
<h3 id="learning-styles">Learning styles</h3>
<h3 id="learning-strategies">Learning strategies</h3>
<h3 id="assessment-6">Assessment</h3>
<p>Can L2</p>
<p>learners become native-like?</p>
<h2 id="question-8-does-instruction">Question 8: Does instruction</h2>
<h2 id="make-a-difference">make a difference?</h2>
<p>formal instruction</p>
<h3 id="instruction-makes-no-difference">Instruction makes no difference</h3>
<p>acquisition/acquisition versus</p>
<p>learning, Monitory Theory</p>
<p>acquisition orders</p>
<p>developmental sequences</p>
<p>morphemes</p>
<p>Universal</p>
<p>Grammar</p>
<p>principles</p>
<p>parameters</p>
<h3 id="instruction-is-constrained">Instruction is constrained</h3>
<p>processability</p>
<h3 id="instruction-is-beneficial">Instruction is beneficial</h3>
<p>Noticing Hypothesis</p>
<h3 id="instruction-is-necessary">Instruction is necessary</h3>
<p>fossilization</p>
<p>Can L2 learners become native-like?</p>
<h3 id="assessment-7">Assessment</h3>
<p>input enhancement</p>
<p>focus on form</p>
<h2 id="question-9-what-constraints-are">Question 9: What constraints are</h2>
<h2 id="there-on-acquisition">there on acquisition?</h2>
<h3 id="linguistic-constraints">Linguistic constraints</h3>
<p>Universal Grammar</p>
<p>typological universals</p>
<p>markedness</p>
<h3 id="processing-constraints">Processing constraints</h3>
<p>processability</p>
<h3 id="other-constraints">Other constraints</h3>
<p>developmental sequences</p>
<p>ultimate attainment</p>
<h1 id="key-theories-and-frameworks-in-second-language-acquisition">Key Theories and Frameworks in Second Language Acquisition</h1>
<h2 id="universal-grammar-and-linguistic-theory">Universal Grammar and linguistic theory</h2>
<h3 id="the-basics">The basics</h3>
<p>mental representation</p>
<p>mental representation</p>
<p>poverty of the stimulus</p>
<h3 id="the-claims">The claims</h3>
<h3 id="conclusion">Conclusion</h3>
<h2 id="emergentism-and-usage-based-theories">Emergentism and usage-based theories</h2>
<h3 id="the-basics-1">The basics</h3>
<p>Universal Grammar</p>
<p>input</p>
<h3 id="the-claims-1">The claims</h3>
<h3 id="conclusion-1">Conclusion</h3>
<p>behaviorism</p>
<p>poverty of the stimulus</p>
<h2 id="the-declarativeprocedural-model">The Declarative/Procedural Model</h2>
<h3 id="the-basics-2">The basics</h3>
<p>grammaticality judgments</p>
<p>truth-value tasks</p>
<p>event-related potentials/ERPs, fMRIs, PET or positive</p>
<p>emission topography</p>
<h3 id="the-claims-2">The claims</h3>
<p>morphology</p>
<h3 id="conclusion-2">Conclusion</h3>
<h2 id="complexity-theorydynamic-systems">Complexity Theory/Dynamic Systems</h2>
<h3 id="the-basics-3">The basics</h3>
<h3 id="the-claims-3">The claims</h3>
<h3 id="conclusion-3">Conclusion</h3>
<h2 id="input-processing">Input processing</h2>
<h3 id="the-basics-4">The basics</h3>
<p>form-meaning connections</p>
<p>mental representation</p>
<p>input processing</p>
<p>noticing</p>
<h3 id="the-claims-4">The claims</h3>
<p>developing system</p>
<h3 id="conclusion-4">Conclusion</h3>
<p>processing instruction</p>
<h2 id="the-interaction-hypothesis">The Interaction Hypothesis</h2>
<h3 id="the-basics-5">The basics</h3>
<p>Interaction</p>
<p>feedback</p>
<p>feedback</p>
<h3 id="the-claims-5">The claims</h3>
<h3 id="conclusion-5">Conclusion</h3>
<h2 id="processability-theory">Processability Theory</h2>
<h3 id="the-basics-6">The basics</h3>
<h3 id="the-claims-6">The claims</h3>
<h3 id="conclusion-6">Conclusion</h3>
<p>Key Question “Does Instruction Make a</p>
<p>Difference?”</p>
<h2 id="sociocultural-theory">Sociocultural Theory</h2>
<h3 id="the-basics-7">The basics</h3>
<h3 id="the-claims-7">The claims</h3>
<h3 id="conclusion-7">Conclusion</h3>
<h2 id="skill-acquisition-theory">Skill Acquisition Theory</h2>
<h3 id="the-basics-8">The basics</h3>
<p>declarative knowledge</p>
<p>procedural knowledge</p>
<p>automatization</p>
<h3 id="the-claims-8">The claims</h3>
<h3 id="conclusion-8">Conclusion</h3>
]]></content>
      <categories>
        <category>语言学</category>
        <category>二语习得</category>
      </categories>
  </entry>
  <entry>
    <title>句法学主要内容</title>
    <url>/syntax-main/</url>
    <content><![CDATA[<h1 id="chapter-1">Chapter 1</h1>
<h2 id="learning-objectives">Learning Objectives</h2>
<p>After reading chapter 1 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Explain why Language is a psychological property of humans.</p></li>
<li><p>Distinguish between prescriptive and descriptive rules.</p></li>
<li><p>Explain the scientific method as it applies to syntax.</p></li>
<li><p>Explain the differences between the kinds of data gathering, including corpora and linguistic judgments.</p></li>
<li><p>Explain the difference between competence and performance.</p></li>
<li><p>Provide at least three arguments for Universal Grammar.</p></li>
<li><p>Explain the logical problem of language acquisition.</p></li>
<li><p>Distinguish between learning and acquisition.</p></li>
<li><p>Distinguish among observational, descriptive and explanatory adequacy.</p></li>
</ul>
<h2 id="conclusions">Conclusions</h2>
<p>In this chapter, we’ve done very little syntax but talked a lot about the assumptions underlying the approach we’re going to take to the study of sentence structure. The basic approach to syntax that we’ll be using here is generative grammar; we’ve seen that this approach is scientific in that it uses the scientific method. It is descriptive and rule-based. Further, it assumes that a certain amount of grammar is built in and the rest is acquired.</p>
<h1 id="chapter-2">Chapter 2</h1>
<h2 id="learning-objectives-1">Learning Objectives</h2>
<p>After reading chapter 2 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Distinguish between distributional and semantic definitions of parts of speech.</p></li>
<li><p>Identify a part of speech by its distribution.</p></li>
<li><p>Identify cases of complementary distribution.</p></li>
<li><p>Know the difference between an open-class and a closed-class part of speech.</p></li>
<li><p>Explain the difference between lexical and functional categories.</p></li>
<li><p>Identify different subcategories using feature notations.</p></li>
<li><p>Identify plural nouns, mass nouns and count nouns and distinguish them with features.</p></li>
<li><p>Explain the difference between predicates and arguments.</p></li>
<li><p>Categorize verbs according to their argument structure (intransitive, transitive, ditransitive) and represent this using features.</p></li>
</ul>
<h2 id="conclusions-1">Conclusions</h2>
<p>In this chapter, we’ve surveyed the parts of speech categories that we will use in this book. We have the lexical parts of speech N, V, Adj, and Adv, and the functional categories D, P, C, Conj, Neg, and T. Determining part of speech is done not by traditional semantic criteria, but by using morphological and syntactic distribution tests. We also looked at distributional evidence for various subcategories of nouns and verbs, and represented these distinctions as feature notations on the major categories.</p>
<h1 id="chapter-3">Chapter 3</h1>
<h2 id="learning-objectives-2">Learning Objectives</h2>
<p>After reading chapter 3 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Be able to explain what a constituent is.</p></li>
<li><p>Show whether a string of words is a constituent or not.</p></li>
<li><p>Using phrase structure rules, draw the trees for English sentences.</p></li>
<li><p>Explain and apply the Principle of Modification.</p></li>
<li><p>Produce paraphrases for ambiguous sentences and draw trees for each meaning.</p></li>
<li><p>Using data, be able to extract a set of phrase structure rules for another language.</p></li>
<li><p>Define recursion and give an example.</p></li>
</ul>
<h2 id="conclusions-2">Conclusions</h2>
<p>We’ve done a lot in this chapter. We looked at the idea that sentences are hierarchically organized into constituent structures. We represented these constituent structures in trees and bracketed diagrams. We also developed a set of rules to generate those structures. We looked at constituency tests that can be used to test the structures. And finally we looked at the way constituent structure can vary across languages.</p>
<h1 id="chapter-4">Chapter 4</h1>
<h2 id="learning-objectives-3">Learning Objectives</h2>
<p>After reading chapter 4 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify dominance in a tree.</p></li>
<li><p>Distinguish dominance from immediate dominance.</p></li>
<li><p>Understand the relationship between exhaustive domination and constituency.</p></li>
<li><p>Identify precedence in a tree.</p></li>
<li><p>Understand the constraint against crossing lines.</p></li>
<li><p>Identify c-command in a tree.</p></li>
<li><p>Distinguish symmetric from asymmetric c-command.</p></li>
<li><p>Identify different government relations.</p></li>
<li><p>Define structurally subject, object, oblique, object of a preposition and indirect object.</p></li>
</ul>
<h2 id="conclusions-3">Conclusions</h2>
<p>This chapter has been a bit different from the rest of this book. It hasn’t been about Language per se, but rather about the mathematical properties of the system we use to describe language. We looked at the various parts of a syntactic tree and then at the three relations that can hold between these parts: domination, precedence, and c-command. In all the subsequent chapters of this book, you’ll find much utility for the terms and the relations described here.</p>
<h1 id="chapter-5">Chapter 5</h1>
<h2 id="learning-objectives-4">Learning Objectives</h2>
<p>After reading chapter 5 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify and distinguish R-expressions, pronouns and anaphors.</p></li>
<li><p>Understand antecedent and anaphor.</p></li>
<li><p>Distinguish coindexing from binding.</p></li>
<li><p>Define and apply binding to a tree.</p></li>
<li><p>Apply principles A, B, C to a tree.</p></li>
<li><p>Identify binding domains.</p></li>
</ul>
<h2 id="conclusions-4">Conclusions</h2>
<p>In this chapter, we looked at a very complex set of data concerning the distribution of different kinds of NPs. We saw that these different kinds of NPs can appear in different syntactic positions. A simple set of binding</p>
<p>principles (A, B, and C) governs the distribution of NPs. This set of binding principles is built upon the structural relations developed in the last chapter.</p>
<p>　　In the next chapter, we are going to look at how we can develop a similarly simple set of revisions to the phrase structure rules. The constraints developed in this chapter have the shape of locality constraints (in that they require local, or nearness, relations between certain syntactic objects). In later chapters, we’ll see a trend towards using locality constraints in other parts of the grammar.</p>
<p>　　The constraints developed in this chapter account for a wide range of data, but there are many cases that don’t work. In particular there is a problem with our definition of binding domain. You can see some of these problems by trying some of the Challenge Problem Sets at the end of this chapter. We return to a more sophisticated version of the binding theory in chapter 17 in the last part of this book.</p>
<h1 id="chapter-6">Chapter 6</h1>
<h2 id="learning-objectives-5">Learning Objectives</h2>
<p>After reading chapter 6, you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Explain the motivation for simplifying the PSRs into X-bar theory.</p></li>
<li><p>Apply the notation of X-bar theory using variables.</p></li>
<li><p>Be able to draw a tree in X-bar theory.</p></li>
<li><p>Apply tests to distinguish complements from adjuncts.</p></li>
<li><p>Draw trees correctly placing modifiers as complements, adjuncts, and specifiers.</p></li>
<li><p>Describe the notion of a parameter.</p></li>
<li><p>Be able to correctly set the complement, adjunct, and specifier parameters for any foreign language data.</p></li>
</ul>
<h2 id="conclusions-5">Conclusions</h2>
<h1 id="chapter-7">Chapter 7</h1>
<h2 id="learning-objectives-6">Learning Objectives</h2>
<p>After reading chapter 7 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify and distinguish subjects from predicate phrases.</p></li>
<li><p>Identify various kinds of T and C nodes.</p></li>
<li><p>Distinguish finite from non-finite clauses, using tests.</p></li>
<li><p>Identify embedded and root clauses, and distinguish specifier, adjunct or complement clauses.</p></li>
<li><p>Correctly use X-bar format for DPs, TPs, and CPs in tree drawing.</p></li>
<li><p>Explain the arguments for DPs, TPs, and CPs.</p></li>
<li><p>Identify subjects in all types of clauses and correctly place them in the specifier position of TP.</p></li>
</ul>
<h2 id="conclusions-6">Conclusions</h2>
<h1 id="chapter-8">Chapter 8</h1>
<h2 id="learning-objectives-7">Learning Objectives</h2>
<p>After reading chapter 8 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Distinguish between thematic relation and theta role.</p></li>
<li><p>Identify the thematic relations agent, theme, goal, source, experiencer, location, instrument, recipient, benefactor.</p></li>
<li><p>Explain how X-bar theory over-generates.</p></li>
<li><p>Explain the structure of the lexicon.</p></li>
<li><p>Draw the theta grids for a predicate.</p></li>
<li><p>Apply the theta criterion to a sentence as a filter to X-bar theory.</p></li>
<li><p>Distinguish sentences with expletive subjects from ones with theta-role-bearing subjects.</p></li>
<li><p>Explain the Extended Projection Principle and how it accounts for expletives.</p></li>
<li><p>Explain the ordering of the EPP with the theta criterion in the context of the model we are developing.</p></li>
</ul>
<h2 id="conclusions-7">Conclusions</h2>
<p>We started this chapter off with the observation that while X-bar rules capture important facts about constituency and cross-categorial generalizations, they over-generate (that is, they generate ungrammatical sentences). One way of constraining X-bar theory is by invoking lexical restrictions on sentences, such that particular predicates have specific argument structures, in the form of theta grids. The theta criterion rules out</p>
<p>any sentence where the number and type of arguments don’t match up one to one with the number and type of theta roles in the theta grid.</p>
<p>　　We also looked at one apparent exception to the theta criterion: theta-role-less expletive pronouns. These pronouns only show up when there is no other subject, and are forced by the EPP. They escape the theta criterion by being inserted after the theta criterion has filtered out the output of X-bar rules.</p>
<p>　　By using lexical information (like theta roles) we’re able to stop the X-bar rules from generating sentences that are ungrammatical. Unfortunately, as we’ll see in the next chapter, there are also many sentences that the X-bar rules cannot generate. In order to account for these, we’ll introduce a further theoretical tool: the movement rule.</p>
<h1 id="chapter-9">Chapter 9</h1>
<h2 id="learning-objectives-8">Learning Objectives</h2>
<p>After reading chapter 9 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Using theta grids, explain the restrictions that various kinds of C, T, and D nodes impose on their complements.</p></li>
<li><p>Learn to distinguish the various tense, aspect, voice, and mood properties of English verbal constructions.</p></li>
<li><p>Learn to identify the modals and various auxiliaries.</p></li>
<li><p>Identify participles, gerunds, bare forms, preterites, and present tense forms of verbs.</p></li>
<li><p>Demonstrate the similarities and differences between main verbs, auxiliaries, and modals.</p></li>
<li><p>Draw trees showing stacked VPs.</p></li>
<li><p>Discuss the properties of do-support.</p></li>
</ul>
<h2 id="conclusions-8">Conclusions</h2>
<p>In this chapter, we’ve extended the use of theta grids to explain other kinds of restrictions on X-bar-theoretic trees. We saw that VPs not only select for specific theta roles, they also select for various features of CPs (such as finiteness). Complementizers themselves select for specific kinds of TPs. For example, that can’t have a TP headed by to as its complement. We also looked at several kinds of determiners, all of which place restrictions on the kinds of nouns they can take as complements. Finally we examined the scary world of English modals and auxiliaries. After breaking down all the parts of</p>
<p>a complex verb string in English into its component modal, tense, aspectual, and voice parts, we saw how various auxiliaries select for both the range of possible auxiliary and verbal complements (perfect, progressive, passive, main verbs) and for the form that complement takes (bare, preterite, present, participle, or gerund). This gives us an explanation for the ordering and realization of each of these auxiliaries in sentences like “the grand slam” given in section 4.7 above.</p>
<p>　　There are a number of issues that remain unaddressed here. The bulk of this chapter has been on English, but other languages function very differently in how they represent this kind of morphology. We also have left unexplained the situations where certain modals and tensed auxiliaries behave as a class. Both modals and tensed auxiliaries precede negation, as in the examples (79a and b), and both tensed auxiliaries and modals can undergo subject/auxiliary inversion to form yes/no questions as in (79c and d).</p>
<ul>
<li><ol type="a">
<li>Fiona must not eat the sautéed candy canes.</li>
</ol></li>
</ul>
<p>Fiona has not eaten the sautéed candy canes.</p>
<p>Can Fiona eat sautéed candy canes?</p>
<p>Has Fiona eaten sautéed candy canes?</p>
<p>The account of auxiliaries given in this chapter is completely silent on the behaviors in (79). In order to account for these facts as well as for patterns in other languages, we need the additional technology afforded us by movement. This is discussed in the next few chapters.</p>
<h1 id="chapter-10">Chapter 10</h1>
<h2 id="learning-objectives-9">Learning Objectives</h2>
<p>After reading chapter 10 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Understand the distinction between D-structure and S-structure.</p></li>
<li><p>Determine whether a language is verb-raising or not.</p></li>
<li><p>Discuss the interaction between V T and T C.</p></li>
<li><p>Explain the evidence for V T movement in French and Irish.</p></li>
<li><p>Discuss the position of tensed English auxiliaries as compared to main verbs.</p></li>
<li><p>Explain how the VP-internal subject hypothesis accounts for VSO languages.</p></li>
<li><p>Discuss the whens, wheres, and whys of do-support.</p></li>
</ul>
<h2 id="conclusions-9">Conclusions</h2>
<h1 id="chapter-11">Chapter 11</h1>
<h2 id="learning-objectives-10">Learning Objectives</h2>
<p>After reading chapter 11 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Draw the theta grids of raising predicates like is likely and seem.</p></li>
<li><p>Draw trees indicating DP movement of embedded subjects.</p></li>
<li><p>Explain how the Case filter motivates the movement of NPs out of infinitival clauses into main clauses.</p></li>
<li><p>Describe how the passive voice head affects the introduction of external arguments and Case assignment by verbs.</p></li>
<li><p>Show passive DP movement in a tree.</p></li>
</ul>
<h2 id="conclusions-10">Conclusions</h2>
<p>In this chapter, we’ve looked at situations where DPs don’t appear in the positions we expect them to (given our knowledge of theta theory). We have argued that these sentences involve movement of DPs to various specifier positions. The motivation for this comes from Case. The Case filter requires all DPs to check a Case in a specific structural position. We looked at two situations where DPs don’t get Case in their D-structure position. In raising structures, a DP is in the specifier of an embedded clause with non-finite T. In this position, it can’t receive Case so it raises to the specifier of the finite T in the higher clause. We also looked at passive structures. The passive morpheme does two things: it takes the role of external argument and it absorbs the verb’s ability to assign accusative Case. This results in a structure where there is no subject DP, and the object cannot receive Case in its base position. The DP must move to the specifier of T to get Case.</p>
<h1 id="chapter-12">Chapter 12</h1>
<h2 id="learning-objectives-11">Learning Objectives</h2>
<p>After reading chapter 12 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Explain the motivation for wh-movement.</p></li>
<li><p>Draw the tree indicating wh-movement.</p></li>
<li><p>Identify various complementizer types.</p></li>
<li><p>Draw a tree for a relative clause.</p></li>
<li><p>Identify various island types.</p></li>
<li><p>Explain why wh-island sentences are ungrammatical according to the Minimal Link Condition.</p></li>
<li><p>Explain, using the MLC, why certain instances of DP movement and head-to-head movement are ungrammatical.</p></li>
</ul>
<h2 id="conclusions-11">Conclusions</h2>
<p>In this chapter, we looked at a third kind of movement transformation: Wh-movement. This process targets wh-phrases and moves them to the specifier of CPs. This movement is triggered by the presence of a [+WH] feature in C. Wh-movement of a DP is always from a Case position to the specifier of CP. Wh-movement is not totally unrestricted; there is a locality constraint on the movement: the MLC. Movement must be local, where local is defined in terms of closest potential landing site. We saw further that the MLC might be extended to other types of movement.</p>
<p>　　In the next chapter, we’re going to continue this trend and look at movement processes in general and the similarities between them, as well as briefly delve into the interaction between the syntax and the formal interpretation (semantics) of the sentence.</p>
<p>　　</p>
<h1 id="chapter-13">Chapter 13</h1>
<h2 id="learning-objectives-12">Learning Objectives</h2>
<p>After reading chapter 13 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Understand and apply the Y-model of grammar.</p></li>
<li><p>Explain and apply the Principle of Full Interpretation.</p></li>
<li><p>Distinguish between overt and covert movement.</p></li>
<li><p>Identify whether a language has overt or covert movement for any given movement type.</p></li>
<li><p>Explain the difference between LF and PF.</p></li>
<li><p>Understand scope and how it applies to the universal and existential quantifiers.</p></li>
</ul>
<h2 id="conclusions-12">Conclusions</h2>
<p>In this chapter we made the big jump from three movement rules with different but similar motivations to a single rule with a single motivation (Full Interpretation). We also claimed that cross-linguistic variation in movement, when we assume a universal semantics, requires that movement can both be overt (before SPELLOUT) and covert (after SPELLOUT). The Y model with Saussurean interface levels (LF and PF) allows this to occur. We also looked very briefly at an example from quantifier scope that provides independent support for the notion of covert movement.</p>
<h1 id="chapter-14">Chapter 14</h1>
<h2 id="learning-objectives-13">Learning Objectives</h2>
<p>After reading chapter 14 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify shifted objects, and explain how split VPs account for them.</p></li>
<li><p>Draw the tree for particle constructions, prepositional ditransitives and double object ditransitives.</p></li>
<li><p>Explain how passives and actives have different selectional properties for the functional category that assigns accusative Case.</p></li>
</ul>
<h2 id="conclusions-13">Conclusions</h2>
<h1 id="chapter-15">Chapter 15</h1>
<h2 id="learning-objectives-14">Learning Objectives</h2>
<p>After reading chapter 15 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Distinguish between raising and control predicates.</p></li>
<li><p>Distinguish between subject-to-subject raising (SSR) and subject control constructions.</p></li>
<li><p>Distinguish between subject-to-object raising (SOR) and object control constructions.</p></li>
<li><p>Apply the idiom, clausal subject, and expletive tests.</p></li>
<li><p>Draw theta grids and trees of SSR, SOR, OC, and SC sentences.</p></li>
<li><p>Describe the restrictions on control of PRO.</p></li>
<li><p>Explain why PRO is null.</p></li>
<li><p>Distinguish between PRO and pro.</p></li>
<li><p>Determine how a language has set the null subject parameter.</p></li>
</ul>
<h2 id="conclusions-14">Conclusions</h2>
<p>　We started this chapter with the observation that certain sentences, even though they look alike on the surface, can actually have very different syntactic trees. We compared subject-to-subject raising constructions to subject control constructions, and subject-to-object raising constructions to object control constructions. You can test for these various construction types by working out their argument structure, and using the idiom test. Next under consideration was the issue of what kind of DP PRO is. We claimed that it only showed up in Caseless positions. We also saw that it didn’t meet any of the binding conditions, and suggested it is subject, instead, to control theory. Control theory is a bit of a mystery, but may involve syntactic, thematic, and pragmatic features. We closed the chapter by comparing two different kinds of null subject categories: PRO and pro. PRO is Caseless and is subject to the theory of control. On the other hand, pro takes Case and is often “licensed” by rich agreement morphology on the verb.</p>
<ul>
<li>This is not a universally true statement. Many Asian languages allow pro-drop even though they don’t have rich agreement systems. For discussion, see Huang (1989).</li>
</ul>
<h1 id="chapter-16">Chapter 16</h1>
<h2 id="learning-objectives-15">Learning Objectives</h2>
<p>After reading chapter 16 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Identify cases of VP ellipsis (including antecedent-contained deletion and pseudogapping) and sluicing.</p></li>
<li><p>Explain the licensing conditions on ellipsis and sluicing.</p></li>
<li><p>Provide arguments for and against the PF-deletion hypothesis and for and against the LF-copying analysis of ellipsis.</p></li>
<li><p>Explain the difference between strict and sloppy pronoun identity and why it occurs.</p></li>
<li><p>Demonstrate how movement of a DP through either QR or DP movement explains antecedent-contained deletion.</p></li>
<li><p>Show how DP movement of objects explains the non-constituent deletion effects in pseudogapping.</p></li>
</ul>
<h2 id="conclusions-15">Conclusions</h2>
<p>Ellipsis, the syntax of missing elements, is a tempting playground for syntacticians. What could be more appealing than trying to deduce the properties of something you know has to be there because you can interpret it, but at the same time has no overt expression in what we see or write? We’ve looked at a few different kinds of ellipsis (VP ellipsis, ACD, pseudogapping, and sluicing) and investigated what they have in common and what they don’t.</p>
<p>　　In the chapters leading up to this one, I’ve tried to give you a consistent set of analyses that follow from a sequence of hypothesis testing and hypothesis revision, building from phrase structure to X-bar theory to movement rules. This chapter has been deliberately different. Here, I’ve tried to give you a taste of what a working syntactician faces each day. We’ve looked at a series of phenomena and for each one posited a couple of conflicting hypotheses. So for example, looking at how elided material gets its meaning we considered two different hypotheses: LF copying and PF deletion. We looked at conflicting evidence that shows that either of them might be right or wrong. We did the same with ACD and pseudogapping. This kind of investigation is the bread and butter of what syntacticians do each day. They look at a puzzling set of data, consider different hypotheses and the predictions they make and weigh the evidence one way or another.</p>
<p>　　</p>
<h1 id="chapter-17">Chapter 17</h1>
<h2 id="learning-objectives-16">Learning Objectives</h2>
<p>After reading chapter 17 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Explain why the binding conditions appear to hold both before and after movement.</p></li>
<li><p>Describe the copy theory of movement.</p></li>
<li><p>Explain how the copy theory of movement explains the ordering paradox.</p></li>
<li><p>Explain the data that shows that pronouns and anaphors seem to have different binding domains.</p></li>
<li><p>Be able to identify the new binding domains for pronouns and anaphors making reference to “potential antecedents”.</p></li>
</ul>
<h2 id="conclusions-16">Conclusions</h2>
<h1 id="chapter-18">Chapter 18</h1>
<h2 id="learning-objectives-17">Learning Objectives</h2>
<p>After reading chapter 18 you should walk away having mastered the following ideas and skills:</p>
<ul>
<li><p>Compare and contrast the syntax-free and radical pro-drop hypotheses approaches to polysynthesis.</p></li>
<li><p>Identify data that supports the movement analysis of incorporation.</p></li>
<li><p>Explain the movement approach to scrambling.</p></li>
<li><p>Compare and contrast the three approaches to non-configurationality (the dual-structure approach, the pronominal argument hypothesis, and the movement approach).</p></li>
</ul>
<h2 id="conclusions-17">Conclusions</h2>
<p>Polysynthesis, incorporation, scrambling, and non-configurationality are certainly serious challenges to innate generative grammar. But they aren’t insurmountable ones. A nuanced approach to investigating languages with these phenomena shows that they might have more in common with more familiar languages than we might think at first glance. For each phenomenon, I have given brief sketches of some common analyses within generative grammar. As in the previous few chapters, I’ve left it open as to whether these analyses are right or not. I encourage you to discuss these phenomena and the hypotheses about them with your fellow students and professors and try to figure out what parts of them are right and what are wrong. We started this book with the observation that syntax was a science. We propose hypotheses, test them, revise them in some cases and discard them in others. As in any other science there are plenty of open questions and unresolved issues. I hope the past few chapters have given you a taste for this.</p>
<p>　　In part 5 of this book – which is only available on the website for this book, I offer a brief description of two alternative approaches to syntax. Again like any science, we have competing approaches to difficult questions. As you work your way to becoming a syntactician, it’s worth taking your time to consider alternatives and test your hypotheses against new and challenging data like that found in this chapter.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>句法</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title>语义学框架</title>
    <url>/semantics-concepts/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2021/04/29/cwbXFoxvJr14Wf5.png"/></p>
<p>See <a href="https://share.mubu.com/doc/7e-TFn3zFCV">mubu</a> for more details.</p>
<h1 id="chapter-1">Chapter 1</h1>
<p>In this chapter we will introduce some important concepts for the study of semantics. In 1.1 we place the notion of linguistic meaning in the wider context of human communication and behaviour. Section 1.2 then examines some of the vocabulary that English and other languages use for ordinary talk about meaning in language and related phenomena. A consideration of how this everyday nontechnical vocabulary varies cross-linguistically can show some of the important different aspects of linguistic meaning. In section 1.3 the semiotic triangle of mind, world and language is discussed, followed in 1.4 by an introduction to five fundamental concepts:</p>
<ul>
<li><p>lexemes;</p></li>
<li><p>sense and reference;</p></li>
<li><p>denotation and connotation;</p></li>
<li><p>compositionality; and</p></li>
<li><p>levels of meaning.</p></li>
</ul>
<p>Next (1.5), we introduce the concepts of object language and metalanguage, and distinguish a number of different possible relations between the language in which meanings are described (the ‘metalanguage’) and the language whose meanings are described (the ‘object language’). We will then consider three different identifications of meaning: meanings as objects in the world (referents: 1.6.1), as objects in the mind (concepts: 1.6.2), and as brain states (1.6.3). An alternative identification is the notion of meanings as uses, discussed in 1.6.4. To end the chapter, we consider a view of meaning on which meanings are unobservable, hypothetical constructs posited to explain facts about language use (1.7).</p>
<h1 id="chapter-2">Chapter 2</h1>
<p>This chapter considers the role of definition in the description of meaning, through four main questions:</p>
<ul>
<li><p>What units need to receive definition?</p></li>
<li><p>What forms should the definitions take?</p></li>
<li><p>Can definitions be grounded in a set of semantic primitives?</p></li>
<li><p>What is the place of definition in semantics generally?</p></li>
</ul>
<p>We begin by contrasting the types of definition that might appear in dictionaries from the types that interest a theoretical semantic analysis (2.1). Before any definition can begin, we have to confront an initial question: what are the meaning-bearing units of the language for which definitions are required? We explore this question by looking at meaning on, above and below the word level in 2.2, paying particular attention to certain problematic cases. The next section distinguishes definition of things (real definition) from definition of meanings (nominal definition), and cognitive from extensional definitions, and discusses some differences of opinion in linguistics as to what the proper objects of linguistic definition are (2.3.1). We then distinguish different possible definitional strategies, including</p>
<ul>
<li><p>definition by ostension (2.3.2)</p></li>
<li><p>definition by synonymy (2.3.3)</p></li>
<li><p>definition by context and typical exemplar (2.3.4)</p></li>
<li><p>definition by genus and differentia (2.3.5).</p></li>
</ul>
<p>The test of truth preserving substitutability is introduced as a standard criterion of definitional adequacy (2.4), and we discuss the problem of definitional circularity and the question of semantic primitives (2.5).</p>
<p>　We then exemplify the extreme difficulty involved in couching successful definitions of words (2.6), before finally devoting some discussion to the relationship between definition and understanding (2.7). 　</p>
<h1 id="chapter-3">Chapter 3</h1>
<p>Linguistic expressions can only occur in particular contexts; as a result, working out what role context plays in the determination of meaning is an important part of semantic analysis. This chapter considers one essential type of context: the external or real-world context to which linguistic expressions refer. 　We begin by discussing an important distinction: the distinction between what a word inherently means, and what it can be used to mean in a particular context, showing that this distinction is often not self-evident. We then distinguish the different types of task a hearer must perform to correctly understand a linguistic expression in its context (3.1).</p>
<p>　In 3.2 we begin the treatment of external context by considering the relation between sense and reference, discussing - the origins of this distinction in Frege; - its applications in linguistics; and</p>
<ul>
<li>the nature of deictic expressions, which can be seen as a bridge between language and its surrounding external context.</li>
</ul>
<p>In 3.3. we discuss, and reject, a possible distinction between knowledge of a word’s inherent, linguistic meaning (dictionary knowledge) and knowledge of facts about the word’s external context (encyclopaedic knowledge).</p>
<h1 id="chapter-4">Chapter 4</h1>
<p>Following the treatment of external context in the previous chapter, this chapter considers the interpersonal context of linguistic action in which any utterance is placed.</p>
<p>　Section 4.1 introduces the notion of illocutionary force, which refers to the different interpersonal functions or speech acts which a linguistic expression may be made to perform (stating, questioning, ordering, requesting, advising, warning, promising, etc.).</p>
<p>　Section 4.2 considers the role of speaker’s intention and hearer’s inference in meaning: in general, the meaning of an expression can often be described as whatever it was that the speaker intended the hearer to understand in using the expression; the hearer’s task, on this picture, is to make inferences about what this intention was.</p>
<p>　In 4.3 we discuss the Gricean theory of implicature, which is the theory of how meanings may be implied rather than explicitly stated. In 4.4 and 4.5 we turn to an exploration of the principles which have been proposed as governing the operation of implicature in conversation. Section 4.6 considers an important alternative tradition in the analysis of interpersonal context, Relevance Theory, and 4.7 discusses, in general terms, the interrelation between semantics and pragmatics, the branch of linguistics in which the relations between language and context are specifically studied. 　</p>
<h1 id="chapter-5">Chapter 5</h1>
<p>The different sections of this chapter follow three logical steps in meaning analysis. In 5.1, some of the different possible semantic relations among words are exemplified and discussed. We concentrate on those relations which are of most use for semantic description: - antonymy (oppositeness; 5.1.1), - meronymy ( part of-ness; 5.1.2),</p>
<ul>
<li><p>the class-inclusion relations of hyponymy and taxonomy (kind of-ness; 5.1.3–4) and</p></li>
<li><p>synonymy (5.1.5).</p></li>
</ul>
<p>These meaning relations can be seen as reflecting the presence of various isolable components in the meanings of the related words; accordingly, Section 5.2 introduces the possibility of analysing senses as composed of bundles of semantic components, and considers the wider applicability of componential analysis as well as the problems it faces. The third section (5.3) discusses the necessity for a theory of meaning to specify the number of senses associated with a lexeme in a rigorous way. In 5.3.1 we distinguish the case where a single lexeme possesses several related meanings (polysemy) from two other cases: the case where it possesses only a single meaning (monosemy) and the case where it possesses two unrelated meanings (homonymy). Section 5.3.2 then shows that any attempt to make these definitions rigorous confronts serious problems, the implications of which are discussed in 5.3.3.</p>
<h1 id="chapter-6">Chapter 6</h1>
<p>Logic is the study of the nature of valid inferences and reasoning. The logical tradition constitutes one of the major strands in the study of meaning, and some knowledge of its background is indispensable in linguistic semantics. In this chapter we will study some basic logical tools and concepts. Our aim is twofold:</p>
<ul>
<li>first, to understand the ways in which some types of meaning can be represented in logical symbolism</li>
<li>second, to appreciate the advantages and disadvantages of this type of representation.</li>
</ul>
<p>We begin by introducing the ideas of validity, soundness and logical form (6.1): these define the context and aims of a logical approach to language. In 6.2 we present an exposition of the basic principles of propositional logic, the logic of basic sentences, including a treatment of the principal logical operators: and, not, or and if . . . then. In 6.3 we discuss the extent to which these logical concepts overlap with the meanings of their ordinary language equivalents. Section 6.4 introduces predicate logic, the logic of expressions like some and all. In 6.5 we discuss the ways in which the concept of a model allows us to describe reference using logical techniques. Section 6.6 contains a discussion of the sentence relations of entailment, presupposition and contradiction. This leads to a discussion of meaning postulates in Section 6.7, which use the sentence relations introduced in 6.6 as part of a non-decompositional approach to meaning. In 6.8 Russell’s theory of descriptions is discussed. This is a proposal for the analysis of noun phrases containing the definite article, and provides an instructive example of the advantages and problems of applying logical tools to the analysis of natural language.</p>
<p>　We end the chapter in 6.9 with a short discussion of the controversies surrounding the use of logic as an aid in the analysis of natural language. 　</p>
<h1 id="chapter-7">Chapter 7</h1>
<p>This chapter considers meaning from the perspective of the cognitive operations which the mind can be hypothesized to perform in using language. We begin by introducing the idea that words in natural language can be seen as categories, and discuss two different models of the way categories work, the classical view of categorization and the prototype view (7.1), exploring the advantages and problems of each. We then discuss cognitive approaches to meaning, which developed out of the prototypical model of categorization. These approaches have introduced a rich model of the cognitive architecture underlying language (7.2).</p>
<h1 id="chapter-8">Chapter 8</h1>
<p>In the previous chapter we looked at some proposals about the types of cognitive operation that underlie semantic ability. In this chapter, we examine some attempts to formalize and model the conceptual representations involved in language. In 8.1 we examine Jackendoff’s conceptual semantics, a theory about the cognitive structures behind language and the modes of their interaction. This is followed by a discussion of the treatment of meaning in computational linguistics, which uses computer models of language as an aid to understanding the mental processes involved in language production and understanding (8.2). We will concentrate on the aspects of computational linguistics which give insight into the nature of the task of meaning-processing. We specifically look at WordNet, an online lexical database, at the problems of word-sense disambiguation, and at Pustejovsky’s solution to this in his model of qualia structure.</p>
<h1 id="chapter-9">Chapter 9</h1>
<p>This chapter and the next investigate a range of semantic phenomena which are relevant to morphosyntax. This chapter focuses on morphosyntactic categories such as noun and verb and tense and aspect. The major questions are these: - Does a word’s meaning determine its grammatical category?</p>
<ul>
<li>How can we describe the meanings of major verbal categories like tense and aspect?</li>
</ul>
<p>We begin with a discussion of the meaning of lexical categories (parts of speech), exploring the possible semantic contribution made by a word’s categorization as noun, verb, adjective, and so on (9.1). Section 9.2 focuses on the verb, investigating the semantics of tense and aspect: two central dimensions of verb meaning with major consequences on the verbal and clausal levels.</p>
<h1 id="chapter-10">Chapter 10</h1>
<p>This chapter discusses the semantics of the clause, particularly the relationship between a verb and its noun participants. This relationship is called the verb’s argument structure. There are three basic questions:</p>
<ul>
<li><p>What principles determine which of the noun phrases associated with a transitive verb will be expressed as subject and which as object?</p></li>
<li><p>Can verbs be grouped into classes about which argument structure generalizations can be made?</p></li>
<li><p>Can constructions have meanings on their own?</p></li>
</ul>
<p>We begin by looking at the semantics of argument structure, a central topic in investigation of the way semantics and syntax are connected. We introduce and motivate the notion of thematic role, and go on to consider the modifications this notion has undergone in research into argument structure (10.1). We then consider argument structure alternations (10.2), the name for situations where a single verb can take several different argument structures. Lastly, we consider construction grammar, which attributes many apparently lexical meanings to the grammatical constructions in which they occur (10.3).</p>
<h1 id="chapter-11">Chapter 11</h1>
<p>Variation is one of the most immediately obvious facts about meaning. Everyone is aware of how the meaning of identical expressions can differ from one person to another, sometimes significantly. There are two aspects of meaning variation: a synchronic and a diachronic (historical) one; we examine each in turn in this chapter. After a quick tour of some important preliminary questions (11.1), we begin diachronically by illustrating the traditional categories with which meaning change has been described, and we consider some of the shortcomings of this approach (11.2.1). We then move on to more recent studies of the pathways and mechanisms of semantic change (11.2.2) and a brief discussion of grammaticalization, the process by which full lexical words are converted into grammatical morphemes (11.2.3). The second half of the chapter discusses synchronic meaning variation. We start by examining the subtle types of semantic variation which exist within a single language community at any one time. Powerful new tools developed within corpus linguistics allow this kind of variation to be studied in a way that was not previously available: these are illustrated in 11.3. We then look at the field of semantic typology, which studies possible constraints on meaning variation and seeks out possible semantic universals in various semantic fields such as the body, colour, space and motion (11.4). Lastly, we consider the implications of these studies for the question of the influences between language and cognition, discussing the famous Sapir–Whorf or linguistic relativity hypothesis (11.5).</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>语义</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>semantics</tag>
      </tags>
  </entry>
  <entry>
    <title>第0章：为什么要学习微积分？</title>
    <url>/calculus-I-0/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>本章的目的是诱使你学习一些微积分。</p>
<p><strong>主题</strong></p>
<p>0.1 你应该知道什么</p>
<p>0.2 什么是微积分，我们为什么要学习它？</p>
<span id="more"></span>
<h1 id="你应该知道什么">0.1 你应该知道什么</h1>
<p>要学习微积分，你必须要有呼吸的能力。没有这种能力，你很快就会死亡，无法继续下去。 除此之外，你还需要熟悉两个概念：一个是数的概念，一个是函数的概念。</p>
<p><strong>假设我已经忘记了我所知道的关于数字和函数的一切？</strong> 不要担心。我们将回顾它们的属性。</p>
<p><strong>如果我知道所有关于数和函数的知识呢？</strong> 那么你已经了解了微积分，不需要再继续了。</p>
<p>在提醒自己关于数和函数的知识之前，你可以问以下问题。</p>
<p><strong>什么是微积分？</strong> <strong>我为什么要学习它？</strong></p>
<h1 id="什么是微积分为什么我们要研究它">0.2 什么是微积分，为什么我们要研究它？</h1>
<p>微积分是关于事物如何变化的研究。它提供了一个对存在变化的系统进行建模的框架，以及一种推导这种模型的预测的方法。</p>
<p><strong>我已经待了一段时间，或多或少知道事物是如何变化的。微积分能给我带来什么？</strong></p>
<p>我确信你对事物的变化有很多了解。而且你对微积分有一个定性的概念。例如，运动速度的概念是一个直接来自微积分的概念，尽管它肯定在微积分之前就已经存在了，而且你对它了解很多。</p>
<p><strong>那么，微积分为我增加了什么？</strong></p>
<p>它为我们提供了一种方法来构建相对简单的变化的定量模型，并推断其结果。</p>
<p><strong>达到什么目的呢？</strong></p>
<p>有了它，你就有能力找到变化的条件对正在研究的系统的影响。通过研究这些，你可以学习如何控制系统，使其做你想做的事情。微积分使工程师和你有能力对系统进行建模和控制，从而使他们（以及可能是你）对物质世界拥有非凡的力量。</p>
<p>微积分的发展及其在物理学和工程学中的应用，可能是现代科学发展的最重要因素，超过了阿基米德时代的水平。而这也是工业革命以及随之而来的一切，包括过去几个世纪几乎所有的重大进步的原因。</p>
<p><strong>你是否试图声称我将知道足够的微积分来建立系统模型，并推导出足够的控制它们的方法？</strong></p>
<p>如果你在1990年问我这个问题，我会说没有。现在，对于一些非微不足道的系统，在你使用你的笔记本电脑或台式电脑的情况下，它是在可能的范围内。</p>
<p><strong>好吧，但是微积分模型是如何变化的？微积分是什么样子的？</strong></p>
<p>微积分的基本思想是通过研究“瞬时”变化来研究变化，我们指的是微小时间间隔内的变化。</p>
<p><strong>这有什么好处呢？</strong></p>
<p>事实证明，这种变化往往比有限时间间隔的变化要简单得多。这意味着它们更容易建模。事实上，微积分是由牛顿发明的，他发现加速度，也就是物体的速度变化，可以通过他相对简单的运动定律来建模。</p>
<p><strong>那又如何？</strong></p>
<p>这就给我们留下了一个问题，即从物体的速度或加速度的信息中推导出关于物体运动的信息。而微积分的细节涉及速度和加速度所代表的概念与位置所代表的概念之间的相互关系。</p>
<p><strong>那么，在学习微积分的过程中，我们要学习什么呢？</strong></p>
<p>首先，你必须有一个框架来描述诸如位置、速度和加速度等概念。</p>
<p>单变量微积分，也就是我们开始学习的，可以处理一个物体沿固定路径的运动。更普遍的问题是，当运动可以发生在表面或空间时，可以用多变量微积分处理。我们研究这后一个主题时，要找到使用一维思想和方法来处理更普遍问题的巧妙技巧。所以单变量微积分也是一般问题的关键。</p>
<p>当我们处理一个沿着路径运动的物体时，它的位置随时间变化，我们可以用一个单一的数字来描述它在任何时候的位置，这个数字可以是距离该路径上的某个固定点（称为我们坐标系的原点）的一些单位。我们在这个距离上加一个符号，如果物体在原点后面，这个符号将是负的）。</p>
<p>然后，物体的运动由其在相关时间点的数字位置集来描述。</p>
<p>我们用来描述运动的位置和时间集合就是我们所说的函数。在所有应用微积分的系统中，类似的函数被用来描述所关注的量。</p>
<p>这里的课程从回顾数字和函数及其属性开始。毫无疑问，你对其中的大部分内容都很熟悉，所以我们试图增加一些不熟悉的材料，让你在看的时候保持注意力。</p>
<p><strong>如果我读到这样的东西，我就会陷入困境。我必须这样做吗？</strong></p>
<p>我很想让你看看，因为这是我写的，但如果你不愿意，你无疑可以跳过它，在需要时再参考它。但是你会错过新的信息，这样做可能会使你永远受到困扰。（虽然我对此表示怀疑）。</p>
<p><strong>那么在数字和函数之后是什么呢</strong>？</p>
<p>一个典型的微积分课程包括以下主题。</p>
<ol type="1">
<li>如何找到各种函数的瞬时变化（称为 "导数"）。这样做的过程被称为 "微分"）。</li>
<li>如何使用导数来解决各种问题。</li>
<li>如何从一个函数的导数回到函数本身。这个过程被称为 "积分"）。</li>
<li>研究某些种类的函数积分的详细方法。</li>
<li>如何使用积分来解决各种几何问题，如某些区域的面积和体积的计算。</li>
</ol>
<p>在这样的课程中还有一些其他的标准课题。其中包括用幂级数来描述函数，以及研究无限级数何时 "收敛 "到一个数字。</p>
<p><strong>那么，这让我有什么权力去做什么呢？</strong></p>
<p>它并没有真正这样做。问题是，这些课程最初是在几个世纪前设计的，它们的目的不是增强能力（在当时是完全不可能的），而是让听众熟悉一些思想、概念和符号，以便理解更高级的工作。数学家、科学家和工程师在各种背景下使用微积分的概念，并使用行话和符号，如果你不学习微积分，你将完全无法理解。学习微积分的目的通常是让你具备 "数学素养"，以便与这些更高级的工作联系起来。</p>
<p><strong>那么，为什么要对赋权一统胡说八道呢？</strong></p>
<p>本课程将努力做到与众不同，在实现其他通常目标的同时，也致力于增强能力。它可能不会成功，但至少会尝试。</p>
<p><strong>那么，它将如何尝试演绎这一奇迹呢？</strong></p>
<p>传统的微积分课程强调用代数方法进行微分和积分。我们将描述这些方法，但也将展示如何在计算机电子表格上以可接受的工作量进行微分和积分（以及普通微分方程的解法）。我们还将提供一些小程序，这些小程序可以自动进行同样的操作，而且工作量更小。通过这些小程序或电子表格，你可以比以前更容易和更灵活地应用微积分的工具。</p>
<p>还有一些更高级的程序，如MAPLE和Mathematica，它们可以让你以类似的方式做更多的事情。有了它们，你可以在各种各样的背景下推导出各种模型的结果。一旦你理解了微积分，它们可以使微积分的使用变得更加容易，但它们提供的答案是给定的输入，这并不提供对它们如何做的理解)。</p>
<p>另外，我们将更多地强调建模系统。有了关于建模的想法和解决它们导致的微分方程的方法，你可以实现我们所声称的授权。</p>
<p><strong>我将能够利用这一点达到一些有价值的目的？</strong></p>
<p>好吧，可能不会。但你可能会。还有，你可能会被激起去学习更多关于你想研究的系统或数学的知识，以提高你这样做的机会。另外，你可能会比现在更好地理解模型的可能后果。另外你可能会爱上微积分的概念和思想。</p>
<p><strong>那么，关于数字的介绍性章节有哪些内容呢？</strong></p>
<p>我们从自然数<span class="math inline">\((1,2,3,...)\)</span>开始，并注意到减法、除法和取平方根的运算是如何引导我们将数系统扩展到包括负数、分数（称为有理数）和复数。我们还描述了十进制扩展（描述 "实数"），并研究了可数性的概念。我们也嘀咕了复数。</p>
<p><strong>在关于函数的章节里呢？</strong></p>
<p>我们从函数的抽象定义开始（作为一组参数-值对），然后描述标准函数。这些是通过从同一函数（值=参数）和指数函数开始，并对它们进行各种运算得到的。</p>
<p><strong>运算，什么运算？</strong></p>
<p>这些是加法、减法、乘法、除法、替换和反转。</p>
<p><strong>但什么是指数函数，什么是置换和反转？</strong></p>
<p>这里有一句话的答案：如果你想知道更多的内容，请阅读本章!</p>
<p>指数函数是用微积分神秘地定义的：它是自己的导数的函数，定义为在参数0处的值为1，然而，它原来是你以前见过的东西。而且，它与三角学的正弦函数有着密切的关系。</p>
<p>将一个函数<span class="math inline">\(f\)</span>替换成另一个函数<span class="math inline">\(g\)</span>，会产生一个新的函数，这个函数被定义为在参数<span class="math inline">\(x\)</span>处有<span class="math inline">\(f\)</span>的值，而<span class="math inline">\(f\)</span>在参数<span class="math inline">\(x\)</span>处的值就是<span class="math inline">\(g\)</span>的值，这比听起来要简单。例如，假设<span class="math inline">\(y=x^2\)</span>，而<span class="math inline">\(x=2z\)</span>，那么<span class="math inline">\(y(x(z))\)</span>就是<span class="math inline">\((2z)^2\)</span>。</p>
<p>一个函数的逆函数是通过将其值与参数交换而得到的函数。例如平方函数，通常写成<span class="math inline">\(x^2\)</span>有一个平方根函数作为其逆函数。</p>
<p>用路易斯-卡罗尔（Lewis Carroll）写给他侄子的威廉神父的不朽名言来说，他是一位数学家：</p>
<blockquote>
<p>我已经回答了三个问题，这就够了。</p>
<p>圣人说，不要给自己找借口。</p>
<p>你认为我可以整天听这些东西吗？</p>
<p>快走吧，否则我就把你踢下楼去！</p>
</blockquote>
<h1 id="原文链接">原文链接</h1>
<p>http://www-math.mit.edu/~djk/calculus_beginners/index.html</p>
]]></content>
      <categories>
        <category>数学</category>
        <category>微积分</category>
      </categories>
      <tags>
        <tag>calculus</tag>
      </tags>
  </entry>
  <entry>
    <title>语义学术语大全</title>
    <url>/semantics-terms/</url>
    <content><![CDATA[<h1 id="chapter-1">Chapter 1</h1>
<h3 id="the-meaningfulness-of-language-is-an-instance-of-the-meaningfulness-of-behaviour">The meaningfulness of language is an instance of the meaningfulness of behaviour</h3>
<p>The meaningfulness of language can be seen as just one instance of the meaningfulness of human behaviour and communication in general, and is one of the systems of structured meaningfulness studied in semiotics.</p>
<h3 id="meaning-is-a-very-vague-term">‘Meaning’ is a very vague term</h3>
<p>‘Meaning’ is a very vague term: in English it refers to a variety of different relations between the world, language and speakers. Most languages do not have precise equivalents for the English term ‘meaning’, and some use a very different stock of lexical resources to talk about meaning-like phenomena.</p>
<h3 id="the-semiotic-triangle">The semiotic triangle</h3>
<p>For the purposes of linguistics, we can isolate three particularly important factors relevant to the study of meaning: the psychology of speakers, which creates and interprets language, the referent of the language expression as projected by the language user’s psychology, and the linguistic expression itself: these three points constitute the semiotic triangle.</p>
<h3 id="lexemes">Lexemes</h3>
<p>In providing a semantic description of a language, we do not need to treat all the variant morphological forms of a single word separately. Instead, we describe the meanings of a language’s lexemes, or the abstract units which unite all the morphological variants of a single word.</p>
<h3 id="sense-reference-denotation-and-connotation">Sense, reference, denotation and connotation</h3>
<p>There are several different aspects of the meaning of a lexeme: its referent on any one occasion of use, its denotation, which is the set of all its referents, and its sense, or the abstract, general meaning which can be translated from one language to another, paraphrased, or defined in a dictionary. Connotation names those aspects of meaning which do not affect a word’s sense, reference or denotation, but which have to do with secondary factors such as its emotional force, its level of formality, its character as a euphemism, etc.</p>
<h3 id="compositionality">Compositionality</h3>
<p>Meaning is often compositional, which means that the meanings of sentences are made up, or composed, of the meanings of their constituent lexemes.</p>
<h3 id="sentence-and-utterance-meaning">Sentence and utterance meaning</h3>
<p>Sentence meaning is the compositional meaning of the sentence as constructed out of the meanings of its individual component lexemes. Utterance meaning is the meaning which the words have on a particular occasion of use in the particular context in which they occur. Semantics studies sentence meaning, whereas pragmatics studies utterance meaning and other aspects of language use.</p>
<h3 id="object-language-and-metalanguage">Object language and metalanguage</h3>
<p>In analysing meaning we distinguish the object language, or the language whose meanings are being described, from the metalanguage, the language in which we describe these meanings.</p>
<h3 id="explanations-of-meaning-in-terms-of-meanings-are-circular">Explanations of meaning in terms of meanings are circular</h3>
<p>When we propose a definition in a metalanguage as an analysis of the meaning of an object language term, the more basic questions, ‘what is meaning?’ and ‘what is it to understand a meaning?’ are left unanswered. All definitions of meaning in language, therefore, are ultimately circular because they use one kind of meaning to explain another.</p>
<h3 id="four-ways-of-breaking-the-circle">Four ways of breaking the circle</h3>
<p>　There are four important answers to the question ‘what is meaning?’: the referential/denotational theory of meaning, the conceptual theory of meaning, the brain states theory and the use theory. We do not have to categorically choose between these theories. Instead, recognizing that the notion of meaning in linguistics is a way of talking about the factors which explain language use, we can see referents, concepts, brain states and uses as all relevant to this task.</p>
<h1 id="chapter-2">Chapter 2</h1>
<h3 id="meaning-definition-and-the-mental-lexicon">Meaning, definition and the mental lexicon</h3>
<p>The concept of a word’s meaning is closely linked to the concept of definition. Many linguists identify the task of linguistic semantics with the task of describing the entries stored in the mental lexicon, a stock of words and meanings stored in long-term memory: the definition</p>
<p>of a word is part of its entry in the mental lexicon, and the process of matching a meaning with a word-form is assumed to be analogous to that involved in consulting a dictionary. In order to serve the purposes of serious linguistic description, the definitions in the lexicon must be much more detailed than is usual in ordinary dictionaries.</p>
<h3 id="what-units-need-to-receive-definition">What units need to receive definition?</h3>
<p>Any attempt to analyse the meanings of language must specify what the meaning-bearing units are. Individual lexemes are the central examples of units with individually describable meanings. Morphemes also have meanings, as do phrasal verbs and compounds.</p>
<p>　Ambiguities about the level of grammatical structure to which meaning is correctly attributed are not infrequent: sound symbolism and idioms exemplify cases where the correct level for the analysis of a meaning may not be clear.</p>
<h3 id="real-and-nominal-definition">Real and nominal definition</h3>
<h3 id="we-can-distinguish-two-types-of-definition">We can distinguish two types of definition:</h3>
<p>definition of the essence of a thing (real definition), or definition of the meaning of a word (nominal definition).</p>
<p>Most linguists take nominal definition to be the type that is of interest to linguistic semantic research.</p>
<h3 id="cognitive-and-extensional-definition">Cognitive and extensional definition</h3>
<p>A nominal definition may be of two types:</p>
<p>cognitive (aimed to inculcate an understanding of the word’s correct use), or extensional (aimed at delimiting the denotation of the word).</p>
<p>​</p>
<h1 id="chapter-3">Chapter 3</h1>
<h3 id="the-basic-question-meaning-and-context">The basic question: meaning and context</h3>
<p>One of the main questions to be answered by any theory of meaning concerns the scope of an expression’s meaning: how much of the total effect of an expression is to be attributed to its meaning, and how much to the context in which it occurs?</p>
<p>We can distinguish two essential types of context:</p>
<p>the external or real-world context to which linguistic expressions refer, and the interpersonal context of linguistic action in which any utterance is placed.</p>
<h3 id="external-context-sense-and-reference">External context: sense and reference</h3>
<p>Frege distinguished an expression’s reference, which concerns the entities which the expression is about, from its sense, which is the way in which we grasp or understand its referent. In the Fregean view, two crucial features of sense are as follows:</p>
<p>sense is what our minds ‘grasp’ when we understand the meaning of a word; sense determines reference; words’ referents are identified through their senses.</p>
<p>Truth has a central place in Frege’s semantics. To know the sense of a sentence is, for Frege, to know how the sentence could be assigned a value as true or false: to know what the conditions are that would make it true or false. Knowledge of a sentence’s truth conditions allows us to determine, by looking at the sentence’s referents, whether the world actually is the way the sentence represents it, and thus whether or not the sentence is therefore true.</p>
<h3 id="predication-and-deixis">Predication and deixis</h3>
<p>As well as referring, linguistic expressions can often be used to predicate (attribute properties). Verbs, for example, are characteristically limited to this function. Deictic expressions (otherwise known as deictics or indexicals) are defined as those which make reference to some aspect of the context of utterance as an essential part of their meaning. Examples of deictics in English include the words I, you and here. The languages of the world show a large variety of deictic systems.</p>
<h3 id="knowledge-of-meaning-and-knowledge-of-facts">Knowledge of meaning and knowledge of facts</h3>
<p>Since reference is an important part of the meaning of many words, many linguists have wanted to distinguish knowledge we have of a word’s meaning (sense) from knowledge we might have about its referent. This is the distinction between lexical (‘dictionary’) knowledge and factual (‘encyclopaedic’) knowledge.</p>
<p>　The distinction enables an economical description of word meanings, but is often criticized: the boundary between dictionary and encyclopaedia seems to be so highly permeable as to be nonexistent.</p>
<h1 id="chapter-4">Chapter 4</h1>
<h3 id="interpersonal-context">Interpersonal context</h3>
<p>The relations between language and context are not limited to those in which a linguistic expression describes a preexisting world. The assertion of facts about the world is just one of the acts which we can use language to perform: we also ask questions, issue orders and make requests. In these types of speech act, truth is not a relevant parameter in the appreciation of meaning.</p>
<h3 id="austin-and-searle-on-speech-acts">Austin and Searle on speech acts</h3>
<p>Austin’s theory of speech acts distinguished three types of act we perform in any utterance:</p>
<p>the locutionary act is the act of saying something, i.e. the act of expressing the basic, literal meanings of the words chosen</p>
<p>the illocutionary act is the act performed in saying something, i.e. the act of using words to achieve such goals as warning, promising, guaranteeing, etc.</p>
<p>the perlocutionary act is the act performed by saying something, i.e. the act of producing an effect in the hearer by means of the utterance.</p>
<p>Considerations of truth and falsity are simply irrelevant for many types of illocutionary act. Austin distinguished constative utterances like snow is white, which have the illocutionary force of simply stating something, from performative utterances like I apologize, which themselves bring about the state of affairs they mention. Fregean truth conditions are relevant to constatives but not to performatives. Instead of truth conditions, performative utterances have felicity conditions. Typical felicity conditions for many types of constative and performative utterance were described by Searle.</p>
<h3 id="grice-on-implicature">Grice on implicature</h3>
<p>Grice recast the study of the relations between language and context by highlighting the central role of intention to meaning, and developed a theory of implicature and conversational maxims which described the relation between sentence and utterance (speaker) meaning. Grice’s main contribution is the four conversational maxims of Quality, Quantity, Manner and Relevance. Many implied meanings result from speakers’ deliberate infringement of these maxims.</p>
<h3 id="relevance-theory">Relevance theory</h3>
<p>Relevance Theory, finally, represents a third tradition which challenges some of the central presuppositions of the study of meaning. According to Relevance Theorists, the production and understanding of utterances is explained as the result of a universal comprehension procedure which consists in selecting the most relevant aspects of a word’s meaning in a given situation. There is no distinction between literal and non-literal meaning, and what meanings are activated by a word is highly dependent on the particular context in which it is uttered.</p>
<h1 id="chapter-5">Chapter 5</h1>
<p>As well as knowing a word’s definitional meaning, a competent speaker knows how it relates to other words of the language. Five important types of lexical relation have been identified.</p>
<h3 id="antonymy">Antonymy</h3>
<p>Antonymy (oppositeness) may be characterized as a relationship of incompatibility between two terms with respect to some given dimension of contrast. The principal distinction to be made in discussion of antonymy is between gradable (e.g. hot–cold) and non-gradable (e.g. married–unmarried) antonyms, i.e. antonyms which do and do not admit a midpoint.</p>
<h3 id="meronymy">Meronymy</h3>
<p>Meronymy is the relation of part to whole: hand is a meronym of arm, seed is a meronym of fruit, blade is a meronym of knife. Not all languages seem to have an unambiguous means of lexicalizing the concept PART OF, but meronymy is often at the origin of various polysemy patterns in languages.</p>
<h3 id="hyponymy-and-taxonomy">Hyponymy and taxonomy</h3>
<p>Hyponymy and taxonomy (kind of-ness) define different types of class-inclusion hierarchies; hyponymy is an important structural principle in many languages with classifiers, while taxonomy has been argued to be basic to the classification and naming of biological species.</p>
<h3 id="synonymy">Synonymy</h3>
<p>Synonymy is frequently claimed to exist between different expressions of the same language, but genuine lexical synonyms prove extremely hard to find: once their combinatorial environments have been fully explored, proposed lexical synonyms often prove not to be such.</p>
<h3 id="componential-analysis">Componential analysis</h3>
<p>The importance of appreciating a lexeme’s semantic relations in order to understand its meaning is one of the motivations for a componential approach to semantic analysis. Componential analysis analyses meaning in terms of binary features (i.e. features with only two possible values, + or –), and represents a translation into semantics of the principles of structuralist phonological analysis. As a type of definitional analysis, componential analysis inherits the failings of traditional</p>
<p>definitions, and words for which it proves hard to couch definitions are also hard to analyse componentially.</p>
<h3 id="polysemy-and-monosemy">Polysemy and monosemy</h3>
<p>Theoretical and ordinary description of meaning would both be impossible without the recognition of separate senses within the same word. Words with several related senses are described as polysemous. Polysemy contrasts simultaneously with monosemy, the case where a word has a single meaning, and homonymy, the case where two unrelated words happen to share the same phonological form. In spite of the intuitive obviousness of these distinctions, there are many instances where it is not clear whether a word should be analysed as polysemous or monosemous, and no absolute criteria have ever been proposed which will successfully discriminate them.</p>
<h1 id="chapter-6">Chapter 6</h1>
<h3 id="the-nature-and-importance-of-logic">The nature and importance of logic</h3>
<p>Logic investigates the properties of valid arguments and chains of reasoning, and specifies the conditions which arguments must meet in order to be valid. It is important to linguists for three principal reasons:</p>
<ul>
<li><p>it constitutes one of the oldest and most developed traditions of the study of meaning</p></li>
<li><p>it is at the heart of formal and computational theories of semantics</p></li>
<li><p>certain logical concepts, like ¬ or ), provide an interesting point of contrast with their natural language equivalents.</p></li>
</ul>
<h3 id="logical-form-validity-and-soundness">Logical form, validity and soundness</h3>
<p>Logic analyses the underlying logical structure of arguments, known as their logical form. This is independent of the way in which the argument happens to be phrased in any given language. We distinguished between valid and sound arguments:</p>
<ul>
<li><p>Valid arguments are ones in which, if the premises are true, the conclusion must also be true.</p></li>
<li><p>Sound arguments are valid arguments which have true premises.</p></li>
</ul>
<h3 id="propositional-logic">Propositional logic</h3>
<p>　Propositional logic is the branch of logic that studies relations between propositions. A proposition is something which serves as the premise or conclusion of an argument. In propositional logic, special importance is given to the four propositional connectives or operators not, and, or and if . . . then. These connectives are truth-functional. This means that whether the propositions to which they are added are true or not depends solely on the truth of the original propositions. The values or meanings of the operators can be specified in the form of truth tables, which display the way in which logical connectives affect the truth of the propositions in which they appear.</p>
<h3 id="logic-as-representation-and-as-perfection-of-meaning">Logic as representation and as perfection of meaning</h3>
<p>The truth-functional definitions of the propositional connectives are quite often counterintuitive and unnatural. None of the operators corresponds perfectly with any English equivalent. The clash between the meanings of the logical connectives and their ordinary language equivalents reveals a contrast between two different interpretations of the nature of logic: logic as a representation and as a perfection of meaning.</p>
<h3 id="predicate-logic">Predicate logic</h3>
<p>‘Some’ and ‘all’ are the basic notions of predicate logic. Predicate logic studies the logical form of propositions involving three kinds of expression:</p>
<ul>
<li><p>singular terms or individual constants, which refer to individuals (whether things or people). Singular terms are symbolized by lower case letters.</p></li>
<li><p>predicates, which represent properties or relations, such as ‘primate’, ‘hairy’ or ‘adore’. Predicates are symbolized by upper case letters.</p></li>
<li><p>quantifiers, like ‘some’ ( ) and ‘all’ ( ).</p></li>
</ul>
<p>　Predicates have a certain number of arguments. An argument is the individual or individuals to which the property or relation expressed by the predicate is attributed.</p>
<p>　　is called the existential quantifier. ( x) is read as ‘there is at least one x, such that’. is called the universal quantifier. ( x) is read ‘for every x, it is the case that’. Quantification may be single or multiple. A singly quantified proposition contains only a single quantifier. A multiply quantified proposition contains several. Propositions with both the universal and the existential quantifiers allow for the disambiguation of sentences like everyone loves someone.</p>
<h3 id="reference-truth-and-models">Reference, truth and models</h3>
<p>For logical approaches to semantics, reference and truth are the principal semantic facts: the most important thing about the meaning of a word is what it refers to, and the most important thing about a sentence is whether or not it is true. The model of a set of logical formulae is a set of statements showing what each expression of the formula refers to in some possible world (6.5). The referent of a logical expression is called its extension:</p>
<ul>
<li><p>The extension of an individual constant (singular term) is simply the individual entity which the constant picks out.</p></li>
<li><p>The extension of a one-place predicate is the entire set of individuals to which the predicate applies. The predicate ‘tall’, for instance, applies to all tall entities.</p></li>
<li><p>The extension of a two-place predicate like ‘respect’ will be the set of all pairs of individuals such that the first respects the second.</p></li>
</ul>
<p>In general, we can say that the extension of an n-place predicate is an ordered n-tuple of entities.</p>
<h3 id="relations-between-propositions">Relations between propositions</h3>
<p>Entailment is the relation between propositions where the truth of the first guarantees the truth of the second. Presupposition is the relation between two propositions p and q, such that both p and ¬p entail q. A contradictory is a pair of propositions which always have opposite truth values. Pairs of propositions which cannot both be true but can both be false are called contraries. Pairs of propositions which cannot be simultaneously false, but can be simultaneously true are called subcontraries.</p>
<h3 id="meaning-postulates">Meaning postulates</h3>
<p>The theory of meaning postulates uses logical notions to describe the relations which a word has with other members of the same vocabulary, and constitutes a possible alternative to the decompositional modes of meaning analysis.</p>
<h3 id="russell-on-definite-descriptions">Russell on definite descriptions</h3>
<p>Russell’s theory of definite descriptions offers an analysis in logical terms of the meaning of propositions involving the English determiner the, according to which such propositions contain disguised quantifications.</p>
<h3 id="is-logic-relevant-to-the-semantics-of-natural-language">Is logic relevant to the semantics of natural language?</h3>
<p>In the course of the chapter we saw a number of reasons to doubt that logical tools provide an appropriate model of the meanings involved in natural language. These include: - the existence of incompatibilities between logical operators and their natural language equivalents</p>
<ul>
<li>the orientation of logic to reference and truth, which are only some of the considerations relevant to natural language.</li>
</ul>
<p>However, many linguists in favour of logical approaches to semantics would claim that:</p>
<ul>
<li><p>in its attention to declarative sentences a logical approach promises a formalization of an important subset of natural language sentences, and that</p></li>
<li><p>a logical approach permits a degree of rigour and formalization which entirely outstrips that of the more descriptive approaches to meaning.</p></li>
</ul>
<h1 id="chapter-7">Chapter 7</h1>
<h3 id="two-views-of-categorization">Two views of categorization</h3>
<p>Categorization is a fundamental psychological process: the human mind can class different things in the world in the same category, and denote them with a single term. Since words can be seen as the names of categories, categorization has been a major focus of investigations of word meaning. Linguists and psychologists typically contrast two views of categorization:</p>
<ul>
<li><p>the classical view, on which membership of a given category is an either-or property, with no in-between cases. For example, on the classical view, something either is or is not a flower, or a lie, or red.</p></li>
<li><p>the prototype view, on which a category is structured in terms of a central tendency. On this view, categories like FLOWER, LIE or RED each have more and less central (prototypical) members.</p></li>
</ul>
<h3 id="problems-with-classical-categorization">Problems with classical categorization</h3>
<p>The classical view of categorization is rejected by many semanticists since it seems unable to account for basic semantic phenomena, such as the following:</p>
<ul>
<li><p>There are categories in which some members are better exemplars of the category than others.</p></li>
<li><p>There are categories in which the boundaries of membership are not clearcut: it is not always possible to say whether or not something is a member of the category.</p></li>
</ul>
<h3 id="problems-with-prototype-categorization">Problems with prototype categorization</h3>
<p>The prototype view is certainly able to account for these facts, but is open to several questions and problems:</p>
<ul>
<li><p>how do we identify the relevant attributes in a category?</p></li>
<li><p>how can we account for the boundaries of prototype categories?</p></li>
<li><p>how much of the vocabulary is structured according to prototype categories? - prototype semantics may simply absolve the semanticist from the serious effort of lexical description</p></li>
<li><p>the evidence for prototypes may be metalinguistic in nature</p></li>
</ul>
<h3 id="cognitivist-approaches-to-semantics">Cognitivist approaches to semantics</h3>
<p>Cognitivist approaches to semantics are directly inspired by prototype theory. These approaches have four important commitments:</p>
<ul>
<li><p>an identification between meaning and conceptual structure</p></li>
<li><p>a rejection of the syntax–semantics distinction</p></li>
<li><p>a rejection of the semantics–pragmatics distinction</p></li>
<li><p>a rejection of a modular approach to language</p></li>
</ul>
<h3 id="icms-and-image-schemas">ICMs and image schemas</h3>
<p>A central notion in cognitive semantics is that linguistic meaning depends on encyclopaedic knowledge structures stored in long-term memory. Lakoff (1987) calls these idealized cognitive models (ICMs), and sees prototype effects as explained by them. ICMs can be thought of as theories of particular subjects – the implicit knowledge we have about the objects, relations and processes named in language. The knowledge structures typically involve image schemas, such as CONTAINMENT, SOURCE-PATH-GOAL, FORCE, BALANCE and so on. These are organizing structures of our experience and understanding at the level of bodily perception and movement. They are usually represented diagrammatically. Image schemas are particularly useful as representations of the meanings of prepositions.</p>
<h3 id="metaphor">Metaphor</h3>
<p>Metaphor is stressed in much cognitive semantics as an inherent aspect of language structure. Cognitive semantics shows that metaphor is not the exception in language: metaphorical ways of talking are just as widespread as ‘literal’ ones. The normal way of referring to many domains of meaning, such as that of obligation in English, is metaphorical. Lakoff and Johnson’s conceptual theory of metaphor proposes that metaphor is a cognitive process which helps us to conceptualize our experience by setting up correspondences between easily understood things like burdens and hard to understand things like obligations. A metaphorical mapping allows knowledge about the metaphor’s source or vehicle domain (burdens) to be applied to the target (obligations) in a way that fundamentally determines or influences the conceptualization of the target.</p>
<h3 id="metonymy">Metonymy</h3>
<p>Another important cognitive process is metonymy: the concepts related by a metonymy can be understood as contiguous to (neighbouring) each other, either conceptually or in the real world.</p>
<h3 id="semantic-extension-and-radial-categories">Semantic extension and radial categories</h3>
<p>Metaphor and metonymy constitute the principal mechanisms of semantic extension, as seen in expressions like head of a queue, head of cattle and so on. This type of structure, where there is a central case and conventionalized variations on it which cannot be predicted by general rules, is called a radial structure. For head, the central case is represented by uses consistent with the ICM described above, and the variations are the metaphoric and metonymic extensions.</p>
<h3 id="problems-with-cognitive-semantics">Problems with cognitive semantics</h3>
<p>Cognitive approaches to semantics have proven very popular, but can be criticized for three main reasons:</p>
<ul>
<li><p>the ambiguity of diagrammatic representations</p></li>
<li><p>the problem of determining a lexical item’s core meaning, and</p></li>
<li><p>the indeterminate and speculative nature of the analyses.</p></li>
</ul>
<h1 id="chapter-8">Chapter 8</h1>
<h3 id="jackendoff-conceptual-semantics">Jackendoff: Conceptual Semantics</h3>
<p>Jackendoff’s Conceptual Semantics shares with cognitive semantics a commitment to analysing meaning as inherently linked to conceptualization. Its most important difference from cognitive semantics is that it uses a formalism.</p>
<h3 id="decomposition-and-conceptual-primitives">Decomposition and conceptual primitives</h3>
<p>Jackendoff claims that a decompositional method is necessary to explore conceptual structure, in which the concepts underlying word meaning are broken down into their smallest elements: conceptual primitives envisaged as the semantic equivalents of phonological features. Conceptual Semantics posits ‘a finite set of mental primitives and a finite set of principles of mental combination’ governing their interaction. The conceptual structure of a lexical item is an element with zero or more open argument slots, which are filled by the syntactic complements of the lexical item. Jackendoff’s system permits interesting connections to be made between apparently unrelated meanings, but can be criticized for the apparently somewhat arbitrary nature of the conceptual constituents it recognizes.</p>
<h3 id="modelling-meaning-computationally">Modelling meaning computationally</h3>
<p>Computers come closer than any other artificial system to matching the complexity and interconnectedness of the human brain, and it has often been assumed that the attempt to simulate human linguistic ability computationally will teach us important lessons about the way language is processed in real-life minds/brains. This is particularly true of studies of words and their meanings, though it is important not to push the mind–computer analogy too far.</p>
<h3 id="wordnet-and-the-lexicon">WordNet and the lexicon</h3>
<p>The most comprehensive attempt to model lexical knowledge on a computer is WordNet, an online lexical database which sets out to represent and organize lexical semantic information in a psychologically realistic form that facilitates maximally efficient digital manipulation. The main organizational unit in WordNet is the synset. Synsets are groupings of near-synonyms, arranged into hyponymic/taxonomic trees called inheritance hierarchies. Each term in an inheritance hierarchy ‘inherits’ the information associated with its hypernyms: this gives the user immediate access to the full range of information associated with a lexical item.</p>
<h3 id="word-sense-disambiguation">Word sense disambiguation</h3>
<p>One of the hardest problems in computer simulations of natural language processing is the problem of word-sense disambiguation. Computers must know how to distinguish between the different senses of ambiguous words like bank if they are to be able to process language correctly. We discussed two main approaches to this task:</p>
<ul>
<li><p>selectional restriction approaches, which use selectional restrictions to weed out improperly formed semantic representations; and</p></li>
<li><p>the contextual approach, in which the computer assesses the words surrounding the target word, and chooses the appropriate sense on the basis of the other words in this immediate context.</p></li>
</ul>
<p>Both approaches are in their infancy and programs using them significantly underperform humans.</p>
<h3 id="pustejovsky-and-qualia-structure">Pustejovsky and qualia structure</h3>
<p>　Pustejovsky attempts to solve a number of problems in word-sense disambiguation by proposing a richer structure for nominal entries in the lexicon. He claims that the meaning of nouns is best modelled by the notion of qualia structure. A noun’s qualia structure consists of four roles, the constitutive, formal, telic and agentive roles. The key element of Pustejovsky’s theory is that each of these roles can operate independently within the semantics of a clause. For example, we know that a fast car is one that moves quickly, and a fast motorway is one on which cars can move quickly, since fast applies to the telic role of the noun: the role that refers to the function or purpose which the referent fulfils.</p>
<h1 id="chapter-9">Chapter 9</h1>
<h3 id="what-parts-of-speech-a-language-has-is-a-matter-of-interpretation">What parts of speech a language has is a matter of interpretation</h3>
<p>There are usually several different plausible interpretations of the part of speech categories (lexical categories, grammatical categories) of any language.</p>
<h3 id="morphological-and-distributional-criteria-for-parts-of-speech">Morphological and distributional criteria for parts of speech</h3>
<p>Some languages have clear morphological criteria for assigning words to parts of speech: we can divide up the classes on the basis of what affixes they appear with. In languages without clear morphology, a distributional approach to parts of speech can sometimes be used to classify parts of speech on the basis of the way they pattern in sentences. Both morphological and distributional methods for part of speech classification are unreliable. In any case, both presuppose a preexisting decision about how the parts of speech are to be defined.</p>
<h3 id="semantic-definitions-of-parts-of-speech">Semantic definitions of parts of speech</h3>
<p>The same problem affects semantic definitions of parts of speech: we cannot appeal to thingness, eventhood and so on as criteria for grammatical category, since they are not known independently of the very grammatical features which they are supposed to establish.</p>
<h3 id="multicategoriality">Multicategoriality</h3>
<p>Many languages show widespread multicategoriality (roots which may appear as different parts of speech). We can think of nouns and verbs as ‘slots’ or contexts available in each clause, each of which comes associated with the appropriate grammatical machinery. The grammatical slots themselves can be seen as the carriers of the nounhood or verbhood which the word ends up acquiring.</p>
<h3 id="hopper-and-thompson-parts-of-speech-and-discourse-function">Hopper and Thompson: parts of speech and discourse function</h3>
<p>Hopper and Thompson suggest that parts of speech can be understood as prototype categories defined by their discourse functions. The difference in the grammatical options available to a given occurrence of a noun or verb correlates with its discourse function in a given context – the closer the noun or verb is to playing its prototypical discourse role, the closer it comes to exhibiting the full range of grammatical possibilities of its class. It is open to question whether the discourse function definition of parts of speech is any less problematic than the semantic definitions it replaces.</p>
<h3 id="tense">Tense</h3>
<p>Tense is the name of the class of grammatical markers used to signal the location of situations in time. Three basic temporal divisions are relevant to the representation of time in language: what is happening now, what will happen afterwards, and what has already happened. Some languages display a three-way division between past, present and future, with each tense marked separately on the verb. Others have a two-way distinction; either between past and non-past, or (more rarely) future and non-future. Perfect tenses are often described in terms of relevance to the speech situation, but this definition is problematic.</p>
<h3 id="aspect">Aspect</h3>
<p>Aspect is the grammatical category which expresses differences in the way time is presented in events. Aspectual categories express the internal temporal constituency of an event; whether the event is viewed from the distance, as a single unanalysable whole (perfective aspect), or from closeup, so that the distinct stages of the event can be seen individually (imperfective aspect). The perfective/imperfective distinction has nothing to do with the actual nature of the event, but is all about how the event is construed by the speaker. In particular, it is independent of the actual duration of the event in question.</p>
<h3 id="tense-is-deictic-aspect-isnt">Tense is deictic, aspect isn’t</h3>
<p>A major difference between tense and aspect is that tense is deictic and aspect isn’t.</p>
<p>　The particular time reference of any tense therefore has to be anchored deictically in the moment of utterance.</p>
<h3 id="aktionsart">Aktionsart</h3>
<p>Aktionsart is the term for an event’s inherent aspectual classification.</p>
<h3 id="many-researchers-claim-that-events-can-be-classified-into-five-basic">Many researchers claim that events can be classified into five basic</h3>
<p>Aktionsart classes:</p>
<ul>
<li><p>states</p></li>
<li><p>activities</p></li>
<li><p>accomplishments</p></li>
<li><p>achievements, and</p></li>
<li><p>semelfactives</p></li>
</ul>
<p>These classes show significant interaction effects with perfective and imperfective meanings. The five Aktionsart classes can be summarized along the dimensions of whether they are static (whether they refer to unchanging states or to occurrences), telicity (whether they have an inherent endpoint) and punctuality (whether they are conceived of as consisting of internal temporal parts):</p>
<p>State [+static], [−telic], [−punctual]</p>
<p>Activity [−static], [−telic], [−punctual]</p>
<p>Achievement [−static], [+telic], [+punctual]</p>
<p>Accomplishment [−static], [+telic], [−punctual]</p>
<p>Semelfactive [−static], [−telic], [+punctual]</p>
<h3 id="the-internal-structure-of-achievements">The internal structure of achievements</h3>
<p>Botne showed that achievement verbs have a more complex temporal-ity than was originally assumed. What is punctual about achievement verbs like find, die, notice, or recognize is their nucleus; in addition to this nucleus, a verb may contain a preceding onset phase or a subsequent coda phase. Languages differ in the temporal phases surrounding the central nucleus.</p>
<h3 id="tense-and-aspect-less-languages">Tense and aspect-less languages</h3>
<p>Some languages lack any grammatical means of expressing tense– aspect contrasts. In such languages, the relevant contrasts will be achieved through non-grammatical (lexical and pragmatic) means. The complete absence of grammatical coding of tense and aspect is not uncommon in the languages of the world.</p>
<h1 id="chapter-10">Chapter 10</h1>
<h3 id="the-linking-problem">The linking problem</h3>
<p>The linking problem is the problem of accounting for the relations between a verb and its associated noun phrases. According to the traditional generative understanding, the lexical entries for verbs include a specification of the types of argument they have associated with them. It was assumed that the possible arguments of all verbs could be classified into a small number of classes, called thematic roles, theta-roles, participant roles or semantic roles. Typical roles include agent, patient/ theme, goal, source, location, instrument, beneficiary and experiencer. Some roles are more likely to be coded as subject, and others as object. It was suggested that it is possible to rank the different roles in an order which shows their relative accessibility to subject position.</p>
<h3 id="problems-with-thematic-roles">Problems with thematic roles</h3>
<p>There are three main problems with thematic roles:</p>
<ul>
<li><p>The arguments of many verbs seem hard to assign to any of the conventional thematic roles.</p></li>
<li><p>There are also many occasions where an argument could be assigned to several thematic roles.</p></li>
<li><p>It has not proven possible to formulate a universal thematic hierarchy ranking these roles.</p></li>
</ul>
<h3 id="proto-roles">Proto-roles</h3>
<p>Dowty (1991) suggested that the different participant roles are cluster concepts, like Roschean prototypes, and that thematic roles are based on entailments of verb-meanings. The argument with the most Proto-Agent entailments will be coded as subject, and the one with the most Proto-Patient entailments as object.</p>
<h3 id="thematic-roles-and-conceptual-structure">Thematic roles and conceptual structure</h3>
<p>Jackendoff’s theory of semantic representation dispenses completely with theta-roles, and derives argument structure directly from the semantics of the verb. This means that the thematic hierarchy can be completely restated in terms of underlying conceptual configurations. In Jackendoff’s theory of conceptual structure, selectional restrictions are also specified directly by the conceptual structure: they are not extra information which needs to be learnt in addition to the meaning of the verbs themselves.</p>
<h3 id="verb-classes-and-alternations">Verb classes and alternations</h3>
<p>Many verbs show several different argument structures. These different types of argument structure are known as alternations. They include the causative, middle, resultative, conative and others. Levin and Hovav proposed that which alternations a verb participates in is explained by its underlying semantic structure. On their theory, verbs fall into semantically defined classes, which all show similar syntactic behaviour with respect to their alternations.</p>
<h3 id="the-meaning-of-constructions">The meaning of constructions</h3>
<p>Words are not the only meaning-bearing units in grammar. Semantic representations are also associated with constructions. Goldberg put forward a constructional account of the syntax–semantics interface, in which arguments can be subcategorized by the construction itself. Not all verbs can appear in all constructions. A verb’s meaning determines whether it is compatible with a given construction. The constructional account reduces the proliferation of verb-senses.</p>
<h1 id="chapter-11">Chapter 11</h1>
<h3 id="diachronic-and-cross-linguistic-meaning-comparison-presupposes-a-correct-metalanguage">Diachronic and cross-linguistic meaning comparison presupposes a correct metalanguage</h3>
<p>Claims about the variation or change of given meanings necessitate a particular metalanguage in which the meanings can be described, a situation which immediately introduces complications since there is not yet any agreement about what the correct metalanguage for semantic description is. Linguistics in general, and semantic theory in particular, assume that languages are mutually translatable in a way that preserves important meaning components.</p>
<h3 id="importance-of-polysemy-in-meaning-change">Importance of polysemy in meaning change</h3>
<p>　Meaning change crucially involves polysemy. A word does not suddenly change from meaning A to meaning B in a single move; instead, the change happens via an intermediate stage in which the word has both A and B among its meanings.</p>
<h3 id="the-traditional-classification-of-semantic-change">The traditional classification of semantic change</h3>
<p>The traditional classification of semantic change recognized the following six types:</p>
<ul>
<li><p>Specialization (narrowing), in which a word narrows its range of reference</p></li>
<li><p>Generalization (broadening), in which a word’s meaning changes to encompass a wider class of referents</p></li>
<li><p>Pejorization, in which a word takes on a meaning with a less favourable evaluative force</p></li>
<li><p>Ameliorization, in which a word takes on a meaning with a more favourable evaluative force</p></li>
<li><p>Metonymy, the process of sense-extension in which a word shifts to a contiguous meaning</p></li>
<li><p>Metaphor, changes based on similarity or analogy</p></li>
</ul>
<h3 id="conventionalization-of-implicature">Conventionalization of implicature</h3>
<p>Much modern work on semantic change examines pathways and regularities of semantic change, stressing the role of the conventionalization of implicature. This is the theory that semantic change occurs through the progressive strengthening of the implicatures of expressions in particular contexts, until the implicated meaning becomes part of the expression’s literal meaning. These explanations can supersede ones based on the traditional categories.</p>
<h3 id="subjectification">Subjectification</h3>
<p>　An important tendency in semantic change is subjectification. This is the tendency for meanings to ‘become increasingly based in the speaker’s subjective belief state/attitude toward the proposition’.</p>
<p>　 Perception verbs and the mind-as-body metaphor</p>
<p>Viberg (1984) found a strong cross-linguistic hierarchy governed the polysemies of perception verbs. The visual modality of perception is always the source but never the target of processes of polysemy. The mind-as-body metaphor is a possible explanation of the extension of ‘see’ verbs to ‘know/understand’ in Indo-European languages, but is not universal: many languages draw their ‘know’ verb from verbs for hearing.</p>
<h3 id="grammaticalization">Grammaticalization</h3>
<p>One particular context for semantic change is grammaticalization, the process by which open-class content words turn into closed-class function forms. They do this by losing elements of their meaning, and by a restriction in their possible grammatical contexts. Study of these processes has revealed a number of regular pathways which recur again and again in the world’s languages linking particular open-class lexemes with particular grammaticalized functions.</p>
<h3 id="corpus-studies-of-meaning-variation">Corpus studies of meaning variation</h3>
<p>The seeds of semantic change are found in synchronic meaning variation in everyday discourse. Corpora are useful for semantic analysis because they can reveal unsuspected patterns of collocation (regular word combination). Many words cluster in predictable collocational patterns. Studies of collocation can give surprising results: for example, corpus investigation reveals that cause is not used neutrally, but has a strong tendency to be associated with negative events.</p>
<h3 id="semantic-typology">Semantic typology</h3>
<p>Because of the problems of determining universals of sense, semantic typology concentrates on the question of cross-linguistic regularities in denotation or extension (11.4).</p>
<h3 id="typology-of-body-part-reference">Typology of body-part reference</h3>
<p>The human body is a basic and universal aspect of our experience, but there are remarkably few cross-linguistic generalizations that hold about the semantics of body-part terms.</p>
<h3 id="typology-of-colour-reference">Typology of colour-reference</h3>
<p>Colour terms have been an important site of cross-linguistic investigation. Berlin and Kay hypothesized that each language has a set of basic colour terms (BCTs). Basic colour terms in all languages target a restricted range of colours, but the boundaries between these targets vary widely. The number of BCTs in a language makes it possible to predict exactly what the basic colour terms are, and Berlin and Kay proposed seven types of language, classified according to the number of BCTs. Berlin and Kay’s findings have been broadly confirmed, but</p>
<p>there are some significant counterexamples to their typology, as well as fundamental criticisms of their methodology which cast doubt on the significance of their approach.</p>
<h3 id="deictic-motion-typology">Deictic motion typology</h3>
<p>Meanings in the domain of deictic motion, the actions expressed in</p>
<p>English as ‘coming’ and ‘going’, also vary widely cross-linguistically.</p>
<h3 id="typology-of-motion-path-and-manner-lexicalization">Typology of motion, path and manner lexicalization</h3>
<p>　Talmy (1985) compared how different languages lexicalize the four elements of motion, path, figure and manner in motion verbs. Talmy proposed a major typological division between verb-framed and satellite-framed languages. In verb-framed languages the path component is lexicalized in the verb root itself. In satellite-framed languages it is lexicalized in a satellite element. As well as verb- and satellite-framed languages, some linguists claim that there is a third type, equipollent languages, in which both path and manner are treated in the same way. Talmy’s typology has been widely discussed, and the distinction between verb- and satellite-framing is often invoked as a way of characterizing how different languages distribute motion information in the clause. It has been challenged, however, on the grounds that it artificially targets an abstract motion component in verbs whose meaning is actually much less abstract.</p>
<h3 id="the-typology-of-spatial-reference">The typology of spatial reference</h3>
<p>A frame of reference is ‘the internally consistent system of projecting regions of space onto a figure-ground relationship in order to establish specification of location’ (Pederson et al. 1998: 571). Languages with a relative frame of reference use spatial expressions with meanings like ‘in front of me/behind me’ and ‘to my left/right’. Other languages contain an absolute frame of reference. This is a system of spatial location which does not depend on the position of a speech participant, but is anchored instead in unchanging features of the geography, like uphill/ downhill distinctions, or in the cardinal directions (north, south, east, west). The least common frame of reference in the languages of the world is the intrinsic frame of reference. This system only makes reference to intrinsic features of figure and ground: ‘the man is at the side of the tree, the tree is at the chest/face/back of the man’ and so on.</p>
<h3 id="the-relation-of-language-and-thought">The relation of language and thought</h3>
<p>Differences in semantic typology raise the question of the influence between language and thought. Whorf believed that the grammatical categories of one’s language determine the categories of broader cognition. This idea is known as linguistic determinism or the linguistic relativity hypothesis. Thinking in general must be distinguished from thinking for speaking. The grammatical categories of a language must determine thinking for speaking. The interesting question is whether they also determine general cognition. Research has found a statistically very highly reliable correlation between the prevailing frame of spatial reference used in a language and the types of response in non-linguistic cognitive tasks. These suggest a limited influence of language on general cognition. There are many other domains, however, where such an effect is not observed.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>语义</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>semantics</tag>
      </tags>
  </entry>
  <entry>
    <title>PDF相关知识大全</title>
    <url>/pdf-explained/</url>
    <content><![CDATA[<h1 id="pdf是什么">Pdf是什么</h1>
<p>Portable Document Format，由Adobe 公司于1993 年提出的一种文件规范，主要是为了让同一份文件在不同的装置、硬体或作业系统上皆有相同的呈现效果，达到跨平台皆可使用的目的。</p>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title>句法学框架及术语大全</title>
    <url>/syntax-terms/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2021/04/29/5UCyu3mlzBNXD8Y.png"/></p>
<p>See <a href="https://share.mubu.com/doc/2aerL3sh4SV">mubu</a> for more details.</p>
<h1 id="chapter-1">Chapter 1</h1>
<ul>
<li><p><strong>Syntax:</strong> The level of linguistic organization that mediates between sounds and meaning, where words are organized into phrases and sentences.</p></li>
<li><p><strong>Language (capital L):</strong> The psychological ability of humans to produce and understand a particular language. Also called the Human Language Capacity or i-Language. This is the object of study in this book.</p></li>
<li><p><strong>language (lowercase l)</strong>: A language like English or French. These are the particular instances of the human Language. The data sources we use to examine Language are languages. Also called e-language.</p></li>
<li><p><strong>Generative Grammar:</strong> A theory of linguistics in which grammar is viewed as a cognitive faculty. Language is generated by a set of rules or procedures. The version of generative grammar we are looking at here is primarily the Principles and Parameters approach (P&amp;P) touching occasionally on Minimalism.</p></li>
<li><p><strong>The Scientific Method:</strong> Observe some data, make generalizations about that data, draw a hypothesis, test the hypothesis against more data.</p></li>
<li><p><strong>Falsifiable Prediction:</strong> To prove that a hypothesis is correct you have to look for the data that would prove it wrong. The prediction that might prove a hypothesis wrong is said to be falsifiable.</p></li>
<li><p><strong>Grammar:</strong> Not what you learned in school. This is the set of rules that generate a language.</p></li>
<li><p><strong>Prescriptive Grammar:</strong> The grammar rules as taught by so-called “language experts”. These rules, often inaccurate descriptively, prescribe how people should talk/write, rather than describe what they actually do.</p></li>
<li><p><strong>Descriptive Grammar:</strong> A scientific grammar that describes, rather than prescribes, how people talk/write.</p></li>
<li><p><strong>Anaphor:</strong> A word that ends in -self or -selves (a better definition will be given in chapter 5).</p></li>
<li><p><strong>Antecedent:</strong> The noun an anaphor refers to.</p></li>
<li><p>**Asterisk (*)**: The mark used to mark syntactically ill-formed (unacceptable or ungrammatical) sentences. The hash mark, pound, or number sign (#) is used to mark semantically strange, but syntactically well-formed, sentences.</p></li>
<li><p><strong>Gender (grammatical):</strong> Masculine vs. Feminine vs. Neuter. Does not have to be identical to the actual sex of the referent. For example, a dog might be female, but we can refer to it with the neuter pronoun it. Similarly, boats don’t have a sex, but are grammatically feminine.</p></li>
<li><p><strong>Number:</strong> The quantity of individuals or things described by a noun. English distinguishes singular (e.g., a cat) from plural (e.g., cats). Other languages have more or less complicated number systems.</p></li>
<li><p><strong>Person:</strong> The perspective of the participants in the conversation. The speaker or speakers (I, me, we, us) are called the first person. The addressee(s) (you) is called the second person. Anyone else (those not involved in the conversation) (he, him, she, her, it, they, them) is referred to as the third person.</p></li>
<li><p><strong>Case:</strong> The form a noun takes depending upon its position in the sentence. We discuss this more in chapter 11.</p></li>
<li><p><strong>Nominative:</strong> The form of a noun in subject position (I, you, he, she, it, we, they).</p></li>
<li><p><strong>Accusative:</strong> The form of a noun in object position (me, you, him, her, it, us, them).</p></li>
<li><p><strong>Corpus</strong> (pl. Corpora): A collection of real-world language data.</p></li>
<li><p><strong>Native Speaker Judgments (Intuitions):</strong> Information about the subconscious knowledge of a language. This information is tapped by means of the grammaticality judgment task.</p></li>
<li><p><strong>Semantic Judgment:</strong> A judgment about the meaning of a sentence, often relying on our knowledge of the context in which the sentence was uttered.</p></li>
<li><p><strong>Syntactic Judgment:</strong> A judgment about the form or structure of a sentence.</p></li>
<li><p><strong>Garden Path Sentence:</strong> A sentence with a strong ambiguity in structure that makes it hard to understand.</p></li>
<li><p><strong>Center Embedding:</strong> A sentence in which a relative clause consisting of a subject and a verb is placed between the main clause subject and verb. E.g., The house [Bill built] leans to the left.</p></li>
<li><p><strong>Parsing:</strong> The mental tools a listener uses to process and understand a sentence.</p></li>
<li><p><strong>Competence:</strong> What you know about your language.</p></li>
<li><p><strong>Performance:</strong> The real-world behaviors that are a consequence of what you know about your language.</p></li>
<li><p><strong>Learning:</strong> The gathering of conscious knowledge (like linguistics or chemistry).</p></li>
<li><p><strong>Acquisition:</strong> The gathering of subconscious information (like language).</p></li>
<li><p><strong>Innate:</strong> Hardwired or builtin, an instinct.</p></li>
<li><p><strong>Recursion:</strong> The ability to embed structures iteratively inside one another. Allows us to produce sentences we’ve never heard before.</p></li>
<li><p><strong>Universal Grammar (UG):</strong> The innate (or instinctual) part of each language’s grammar.</p></li>
<li><p><strong>The Logical Problem of Language Acquisition:</strong> The proof that an infinite system like human language cannot be learned on the basis of observed data – an argument for UG.</p></li>
<li><p><strong>Underdetermination of the Data:</strong> The idea that we know things about our language that we could not have possibly learned – an argument for UG.</p></li>
<li><p><strong>Universal:</strong> A property found in all the languages of the world.</p></li>
<li><p><strong>Bioprogram Hypothesis:</strong> The idea that creole languages share similar features because of an innate basic setting for language.</p></li>
<li><p><strong>Observationally Adequate Grammar:</strong> A grammar that accounts for observed real-world data (such as corpora).</p></li>
<li><p><strong>Descriptively Adequate Grammar:</strong> A grammar that accounts for observed real-world data and native speaker judgments.</p></li>
<li><p><strong>Explanatorily Adequate Grammar:</strong> A grammar that accounts for observed real-world data and native speaker judgments and offers an explanation for the facts of language acquisition.</p></li>
</ul>
<p>　</p>
<h1 id="chapter-2">Chapter 2</h1>
<ul>
<li><p>Parts of Speech (a.k.a. Word Class, Syntactic Categories): The labels we give to constituents (N, V, Adj, Adv, D, P, C, T, Neg, Conj). These determine the position of the word in the sentence.</p></li>
<li><p><strong>Distribution:</strong> Parts of speech are determined based on their distribution. We have both morphological distribution (what affixes are found on the word) and syntactic distribution (what other words are nearby).</p></li>
<li><p><strong>Complementary Distribution:</strong> When you have two categories and they never appear in the same environment (context), you have complementary distribution. Typically complementary distribution means that the two categories are subtypes of a larger class.</p></li>
<li><p><strong>Parts of speech that are Open Class can take new members or coinages</strong>: N, V, Adj, Adv.</p></li>
<li><p><strong>Parts of speech that are Closed Class don’t allow new coinages:</strong> D, P, Conj, C, T, Neg, and the pronoun and anaphor subcategories of N.</p></li>
<li><p>Lexical Categories express the content of the sentence. N (including pronouns), V, Adj, Adv.</p></li>
<li><p><strong>Functional Categories contain the grammatical information in a sentence:</strong> D, P, Conj, T, Neg, C.</p></li>
<li><p><strong>Subcategories:</strong> The major parts of speech can often be divided up into subtypes. These are called subcategories.</p></li>
<li><p>Feature Notations on major categories are a mechanism for indicating subcategories.</p></li>
<li><p>Plurality refers to the number of nouns. It is usually indicated in English with an -s suffix. Plural nouns in English do not require a determiner.</p></li>
<li><p>Count vs. Mass: Count nouns can appear with determiners and the quantifier many. Mass nouns appear with much and usually don’t have articles.</p></li>
<li><p>The Predicate defines the relation between the individuals being talked about and some fact about them, as well as relations among the arguments.</p></li>
<li><p><strong>Argument Structure:</strong> The number of arguments that a predicate takes.</p></li>
<li><p>The Arguments are the entities that are participating in the predicate relation.</p></li>
<li><p><strong>Intransitive:</strong> A predicate that takes only one argument.</p></li>
<li><p><strong>Transitive:</strong> A predicate that takes two arguments.</p></li>
<li><p><strong>Ditransitive:</strong> A predicate that takes three arguments.</p></li>
</ul>
<h1 id="chapter-3">Chapter 3</h1>
<ul>
<li><p><strong>Constituent:</strong> A group of words that function together as a unit.</p></li>
<li><p><strong>Hierarchical Structure:</strong> Constituents in a sentence are embedded inside of other constituents.</p></li>
<li><p><strong>Syntactic Trees and Bracketed Diagrams:</strong> These are means of representing constituency. They are generated by rules.</p></li>
<li><p>English Phrase Structure Rules</p></li>
</ul>
<ol type="a">
<li>CP</li>
</ol>
<ol start="3" type="A">
<li>TP</li>
</ol>
<ol start="2" type="a">
<li>TP</li>
</ol>
<p>{NP/CP} (T) VP</p>
<p>​</p>
<ol start="3" type="a">
<li><p>VP (AdvP+) V (NP) ({NP/CP}) (AdvP+) (PP+) (AdvP+)</p></li>
<li><p>NP (D) (AdjP+) N (PP+) (CP)</p></li>
</ol>
<ul>
<li>PP P (NP)</li>
</ul>
<ol start="6" type="a">
<li><p>AdjP (AdvP) Adj</p></li>
<li><p>AdvP (AdvP) Adv</p></li>
<li><p>XP XP conj XP</p></li>
</ol>
<ul>
<li><p>**X X conj X</p></li>
<li><p><strong>Head:</strong>** The word that gives its category to the phrase.</p></li>
<li><p><strong>Recursion:</strong> The possibility of loops in the phrase structure rules</p></li>
</ul>
<p>that allow infinitely long sentences, and explain the creativity of language.</p>
<ul>
<li><p><strong>The Principle of Modification:</strong> If an XP (that is, a phrase with some category X) modifies some head Y, then XP must be a sister to Y (i.e., a daughter of YP).</p></li>
<li><p><strong>Constituency Tests:</strong> Tests that show that a group of words functions as a unit. There are four major constituency tests given here: movement, coordination, standalone, and replacement.</p></li>
</ul>
<h1 id="chapter-4">Chapter 4</h1>
<ul>
<li><p><strong>Branch:</strong> A line connecting two parts of a tree.</p></li>
<li><p><strong>Node:</strong> The end of a branch.</p></li>
<li><p><strong>Label:</strong> The name given to a node (e.g., N, NP, TP, etc.).</p></li>
<li><p><strong>(Proper) Domination:</strong> Node A dominates node B if and only if A is higher up in the tree than B and if you can trace a branch from A to B going only downwards.</p></li>
<li><p><strong>Immediate Domination:</strong> Node A immediately dominates node B if there is no intervening node G that is dominated by A, but dominates B. (In other words, A is the first node that dominates B.)</p></li>
<li><p>A is the Mother of B if A immediately dominates B.</p></li>
<li><p>B is the Daughter of A if B is immediately dominated by A.</p></li>
<li><p><strong>Sisters:</strong> Two nodes that share the same mother.</p></li>
<li><p><strong>Root Node (revised):</strong> The node that dominates everything, but is dominated by nothing. (The node that is no node’s daughter.)</p></li>
<li><p><strong>Terminal Node (revised):</strong> A node that dominates nothing. (A node that is not a mother.)</p></li>
<li><p><strong>Non-terminal Node (revised):</strong> A node that dominates something. (A node that is a mother.)</p></li>
<li><p><strong>Exhaustive Domination:</strong> Node A exhaustively dominates a set of terminal nodes {B, C, ..., D} provided it dominates all the members of the set (so that there is no member of the set that is not dominated by and there is no terminal node G dominated by A that is not a member of the set.</p></li>
<li><p><strong>Constituent:</strong> A set of terminal nodes exhaustively dominated by a particular node.</p></li>
<li><p><strong>Constituent of:</strong> A is a constituent of B if and only if B dominates A.</p></li>
<li><p><strong>Immediate Constituent of:</strong> A is an immediate constituent of B if and only if B immediately dominates A.</p></li>
<li><p><strong>Sister Precedence:</strong> Node A sister-precedes node B if and only if both are immediately dominated by the same node, and A appears to the left of B.</p></li>
<li><p><strong>Precedence:</strong> Node A precedes node B if and only if neither A dominates B nor B dominates A and A or some node dominating A sister-precedes B or some node dominating B.</p></li>
<li><p><strong>No Crossing Branches Constraint:</strong> If node X precedes another node Y then X and all nodes dominated by X must precede Y and all nodes dominated by Y.</p></li>
<li><p><strong>Immediate Precedence:</strong> A immediately precedes B if there is no node G that follows A but precedes B.</p></li>
<li><p><strong>C-command (informal):</strong> A node c-commands its sisters and all the daughters (and granddaughters, and great-granddaughters, etc.) of its sisters.</p></li>
<li><p><strong>C-command (formal):</strong> Node A c-commands node B if every node dominating A also dominates B and neither A nor B dominates the other.</p></li>
<li><p><strong>Symmetric C-command:</strong> A symmetrically c-commands B if A c-commands B and B c-commands A.</p></li>
<li><p><strong>Asymmetric C-command:</strong> A asymmetrically c-commands B if A c-commands B but B does not c-command A.</p></li>
<li><p><strong>Government:</strong> Node A governs node B if A c-commands B, and there is no node G such that G is c-commanded by A and G asymmetrically c-commands B.</p></li>
<li><p><strong>Phrase-government:</strong> If A is a phrase, then the categories that count for G in the above definition must also be phrases.</p></li>
<li><p><strong>Head-government:</strong> If A is a head (word), then the categories that count for G in the above definition must also be heads.</p></li>
<li><p><strong>Subject (preliminary):</strong> NP or CP daughter of TP.</p></li>
<li><p><strong>Object of Preposition (preliminary):</strong> NP daughter of PP.</p></li>
<li><p><strong>Direct Object:</strong></p></li>
</ul>
<p>With verbs of type V[NP__NP], V[NP__ CP] and V[NP__ NP PP], the NP or CP daughter of VP.</p>
<p>With verbs of type V[NP __ NP {NP/CP}], an NP or CP daughter of VP that is preceded by an NP daughter of VP.</p>
<ul>
<li><strong>Indirect Object (preliminary):</strong></li>
</ul>
<p>With verbs of type V[NP__ NP PP], the PP daughter of VP immediately preceded by an NP daughter of VP.</p>
<p>With verbs of type V[NP __ NP {NP/CP}], the NP daughter of VP immediately preceded by V (i.e., the first NP daughter of VP).</p>
<ul>
<li><strong>Oblique:</strong> any NP/PP in the sentence that is not a subject, direct object of a preposition, direct object, or indirect object.</li>
</ul>
<h1 id="chapter-5">Chapter 5</h1>
<ul>
<li><p><strong>R-expression:</strong> An NP that gets its meaning by referring to an entity in the world.</p></li>
<li><p><strong>Anaphor:</strong> An NP that obligatorily gets its meaning from another NP in the sentence.</p></li>
<li><p><strong>Pronoun:</strong> An NP that may (but need not) get its meaning from another NP in the sentence.</p></li>
<li><p><strong>Antecedent:</strong> The element that binds a pronoun, anaphor or R-expression. When this element c-commands another coindexed NP, it is a binder of that NP.</p></li>
<li><p><strong>Index:</strong> A subscript mark that indicates what an NP refers to.</p></li>
<li><p><strong>Coindexed:</strong> Two NPs that have the same index (i, j, k, etc.) are said to be coindexed.</p></li>
<li><p><strong>Corefer:</strong> Two NPs that are coindexed are said to corefer (refer to the same entity in the world).</p></li>
<li><p><strong>Binding:</strong> A binds B if and only if A c-commands B and A and B are coindexed. A is the binder, B is the bindee.</p></li>
<li><p><strong>Locality Constraint:</strong> A constraint on the grammar, such that two syntactic entities must be “local” or near to one another.</p></li>
<li><p><strong>Binding Domain:</strong> The clause (for our purposes).</p></li>
<li><p><strong>Free:</strong> Not bound.</p></li>
<li><p>The Binding Principles</p></li>
<li><p><strong>Principle A:</strong> An anaphor must be bound in its binding domain.</p></li>
<li><p><strong>Principle B:</strong> A pronoun must be free in its binding domain.</p></li>
<li><p><strong>Principle C:</strong> An R-expression must be free.</p></li>
</ul>
<h1 id="chapter-6">Chapter 6</h1>
<ul>
<li><p><strong>Specifier:</strong> Sister to X', daughter of XP.</p></li>
<li><p><strong>Adjunct:</strong> Sister to X', daughter of X'.</p></li>
<li><p><strong>Complement:</strong> Sister to X, daughter of X'.</p></li>
<li><p><strong>Head:</strong> The word that gives its category to the phrase.</p></li>
<li><p><strong>Projection:</strong> The string of elements associated with a head that bear the same category as the head (N, N', N', N', NP, etc.).</p></li>
<li><p><strong>Maximal Projection:</strong> The topmost projection in a phrase (XP).</p></li>
<li><p><strong>Intermediate Projection:</strong> Any projection that is neither the head nor the phrase (i.e., all the X' levels).</p></li>
<li><p><strong>One-replacement:</strong> Replacement of an N' node with one.</p></li>
<li><p><strong>Do-so-replacement:</strong> Replacement of a V' with do so.</p></li>
</ul>
<ol start="24" type="a">
<li>Specifier Rule: XP (YP) X' or XP X' (YP)</li>
</ol>
<ol start="11" type="i">
<li><p>Adjunct Rule: X' X' (ZP) or X' (ZP) X'</p></li>
<li><p>Complement Rule: X' X (WP) or X' (WP) X</p></li>
</ol>
<ul>
<li><p><strong>Additional Rules:</strong></p></li>
<li><p>**CP (C) TP</p></li>
</ul>
<p>XP XP Conj XP</p>
<p>TP</p>
<p>X'</p>
<p>NP (T) VP</p>
<p>X' Conj X'</p>
<p>X X Conj X</p>
<ul>
<li><p><strong>Parameterization:</strong>** The idea that there is a fixed set of possibilities in terms of structure (such as the options in the X-bar framework), and people acquiring a language choose from among those possibilities.</p></li>
<li><p><strong>Principle of Modification (revised):</strong> If a YP modifies some head X, then YP must be a sister to X or a projection of X (i.e., X’ or XP).</p></li>
</ul>
<h1 id="chapter-7">Chapter 7</h1>
<ul>
<li><strong>Determiner Phrase (DP):</strong> D is not in the specifier of NP. D heads its</li>
</ul>
<p>own phrase: [DP [D' D NP]].</p>
<ul>
<li><strong>Complementizer Phrase (CP):</strong> C is the head of CP and is obligatory in all clauses, although sometimes phonologically null:</li>
</ul>
<p>[CP [C' C TP ]].</p>
<ul>
<li><p><strong>Tense Phrase (TP):</strong> T is the head of TP and is obligatory in all clauses. Sometimes it involves lowering of the affix to the V. The subject DP occupies the specifier position: [TP DPsubject [T' T VP ]].</p></li>
<li><p><strong>Free Genitive/of-Genitive:</strong> Possessed of the possessor.</p></li>
<li><p><strong>Construct Genitive/’s-Genitive:</strong> Possessor ’s possessed.</p></li>
<li><p><strong>Subject:</strong> A DP that has the property indicated by the predicate phrase. What the sentence is about. In most sentences, this surfaces in the specifier of TP.</p></li>
<li><p><strong>Predicate Phrase:</strong> A group of words that attributes a property to the subject. (In most sentences this is the VP, although not necessarily so.)</p></li>
<li><p><strong>Clause:</strong> A subject and a predicate phrase (always a CP in our system).</p></li>
<li><p><strong>Root, Matrix, or Main Clause:</strong> A clause (CP) that isn’t dominated by anything.</p></li>
<li><p><strong>Embedded Clause/Subordinate Clause:</strong> A clause inside of another.</p></li>
<li><p><strong>Specifier Clause:</strong> An embedded clause in a specifier position.</p></li>
<li><p><strong>Adjunct Clause:</strong> An embedded clause in an adjunct position.</p></li>
<li><p><strong>Complement Clause:</strong> An embedded clause in a complement position.</p></li>
<li><p><strong>Tenseless or Non-finite Clause:</strong> A clause that isn’t tensed (e.g., I want [Mary to leave]).</p></li>
<li><p><strong>Tensed or Finite Clause:</strong> A clause that is tensed.</p></li>
<li><p><strong>Yes/No Question:</strong> A question that can be answered with a yes, a no or a maybe.</p></li>
<li><p><strong>Subject-Aux Inversion:</strong> A means of indicating a yes/no question. Involves movement of T to Ø[+Q] complementizer for morpho-phonological reasons.</p></li>
<li><p><strong>Affix Lowering:</strong> An old analysis of how past and present tense</p></li>
</ul>
<p>suffixes get on the verb: The lowering of inflectional suffixes to attach to their verb. Now largely replaced by an analysis where T is null and selects for a VP complement that is correctly inflected.</p>
<h1 id="chapter-8">Chapter 8</h1>
<ul>
<li><p><strong>Selectional Restrictions:</strong> Semantic restrictions on arguments.</p></li>
<li><p><strong>Thematic Relations:</strong> Semantic relations between a predicate and an argument – used as a means of encoding selectional restrictions.</p></li>
<li><p><strong>Agent:</strong> The doer of an action (under some definitions must be capable of volition).</p></li>
<li><p><strong>Experiencer:</strong> The argument that perceives an event or state.</p></li>
<li><p><strong>Theme:</strong> The element that is perceived, experienced, or undergoing the action or change of state</p></li>
<li><p><strong>Goal:</strong> The end point of a movement.</p></li>
<li><p><strong>Recipient:</strong> A special kind of goal, found with verbs of possession</p></li>
<li><p><strong>Source:</strong> The starting point of a movement.</p></li>
<li><p><strong>Location:</strong> The place where an action or state occurs.</p></li>
<li><p><strong>Instrument:</strong> A tool with which an action is performed.</p></li>
<li><p><strong>Beneficiary:</strong> The entity for whose benefit the action is performed.</p></li>
<li><p><strong>Proposition:</strong> The thematic relation assigned to clauses.</p></li>
<li><p><strong>Theta Role:</strong> A bundle of thematic relations associated with a particular argument (DPs, PPs, or CPs).</p></li>
<li><p><strong>Theta Grid:</strong> The schematic representation of the argument structure of a predicate, where the theta roles are listed.</p></li>
<li><p><strong>External Theta Role:</strong> The theta role associated with subjects.</p></li>
<li><p><strong>Internal Theta Role:</strong> The theta role associated with other arguments.</p></li>
<li><p><strong>The Theta Criterion:</strong></p></li>
</ul>
<p>Each argument is assigned one and only one theta role.</p>
<p>Each theta role is assigned to one and only one argument.</p>
<ul>
<li><p><strong>Lexical Item:</strong> Another way of saying “word”. A lexical item is an entry in the mental dictionary.</p></li>
<li><p><strong>The Projection Principle:</strong> Lexical information (like theta roles) is syntactically represented at all levels.</p></li>
<li><p><strong>Expletive (or Pleonastic) Pronoun:</strong> A pronoun (usually it or there) without a theta role. Usually found in subject position.</p></li>
<li><p><strong>Extended Projection Principle (EPP):</strong> All clauses must have subjects. Lexical information is syntactically represented.</p></li>
<li><p><strong>Expletive Insertion:</strong> Insert an expletive pronoun into the specifier of TP.</p></li>
<li><p><strong>The Lexicon:</strong> The mental dictionary or list of words. Contains all irregular and memorized information about language, including the argument structure (theta grid) of predicates.</p></li>
<li><p><strong>The Computational Component:</strong> The combinatorial, rule-based part of the mind. Where the rules and filters are found.</p></li>
</ul>
<h1 id="chapter-9">Chapter 9</h1>
<ul>
<li><p>Theta Grids can contain material other than theta roles, such as features.</p></li>
<li><p><strong>[±FINITE]:</strong> A feature of complementizers that indicates if the clause is finite or not. That is [+FINITE].</p></li>
<li><p><strong>[±Q]:</strong> A feature of complementizers that indicates if the clause is a question or not. If and whether are [+Q].</p></li>
<li><p><strong>[±INFINITIVE]:</strong> Not to be confused with [±FINITE], this is a feature of T nodes. To is [+INFINITIVE].</p></li>
<li><p><strong>[±PLURAL]:</strong> A feature of N heads indicating number.</p></li>
<li><p><strong>[±PROPER]:</strong> The feature associated with proper names.</p></li>
<li><p><strong>[±PRONOUN]:</strong> The feature associated with pronouns.</p></li>
<li><p><strong>[±COUNT]:</strong> This feature distinguishes count nouns from mass nouns.</p></li>
<li><p>Tense refers to the time of an event relative to the time at which the sentence is either spoken or written.</p></li>
<li><p><strong>The Event Time:</strong> The time at which the event described by the predicate occurs.</p></li>
<li><p><strong>The Assertion Time:</strong> The time at which the sentence is said.</p></li>
<li><p><strong>Past Tense:</strong> The event time happened before the assertion time.</p></li>
<li><p><strong>Present Tense:</strong> The event time is the same as the assertion time.</p></li>
<li><p><strong>Future Tense:</strong> The event time happens after the assertion time.</p></li>
<li><p><strong>Preterite:</strong> the special form of verbs in the past tense.</p></li>
<li><p><strong>Futurates:</strong> the future tense usage of a present tense verb.</p></li>
<li><p><strong>Aspect:</strong> a temporal relation that makes reference to some point other than the speech time, then looking at when the event happens relative to that reference point.</p></li>
<li><p><strong>Perfect:</strong> the aspect when the time of the event occurs before some reference point. Haveperf + participle.</p></li>
<li><p><strong>Participle:</strong> A particular form of the verb used in perfects and passives. It is often formed by suffixing –en or –ed, although other irregular methods are found too. Same thing as past participle.</p></li>
<li><p><strong>Gerund:</strong> A particular form of the verb used in progressives. It is normally formed by suffixing –ing. Traditionally called the present participle.</p></li>
<li><p><strong>Progressive:</strong> An aspect where the event time and the reference time overlap and the event is ongoing. beprog + gerund.</p></li>
<li><p><strong>Voice:</strong> An inflection that indicates the number of arguments and position of arguments that a verb uses.</p></li>
<li><p><strong>Active:</strong> A type of voice where the agent or experiencer of the sentence is in subject position and the theme is in the object position. Actives in English are unmarked morphologically.</p></li>
<li><p><strong>Passive:</strong> A type of voice where the theme of the sentence is in subject position. Passives are always marked in English by the combination of a be auxiliary and a participle.</p></li>
<li><p><strong>Mood:</strong> An inflectional category that refers to the speaker’s perspective on the event, indicating possibility, probability, necessity, or obligation.</p></li>
<li><p><strong>Possessive Have:</strong> A main verb use of have, which indicates possession.</p></li>
<li><p><strong>Copular Be:</strong> A main verb use of be, where the subject is attributed a certain property or is identified with a particular role.</p></li>
<li><p><strong>Main Verb Do:</strong> The use of the verb do to indicate accomplishments.</p></li>
<li><p><strong>Modals:</strong> Verbs that can only appear before negation and never take tense inflection. Auxiliaries, by contrast, can follow negation and can bear tense inflection.</p></li>
<li><p>[FORM] Features indicate the form of complements. Possible values include bare, participle, gerund, preterite, and present.</p></li>
<li><p><strong>Do-support:</strong> The use of the auxiliary do to bear tense features in the context of negation. This do is of category T.</p></li>
<li><p><strong>Affix Hopping:</strong> An alternative analysis of multiple auxiliary constructions, where affixes associated with particular tenses, aspects, and voice are generated as part of the same word as the relevant auxiliary, but then “hop” one verbal element to the right.</p></li>
</ul>
<h1 id="chapter-10">Chapter 10</h1>
<ul>
<li><p><strong>Transformation:</strong> A rule that takes an X-bar-generated structure and changes it in restricted ways.</p></li>
<li><p><strong>D-structure:</strong> The level of the derivation created by the base. No transformations have yet applied.</p></li>
<li><p><strong>S-structure:</strong> The output of transformations. The form you perform judgments on.</p></li>
<li><p><strong>V T Movement:</strong> Move the head V to the head T (motivated by morphology).</p></li>
<li><p><strong>Verb Movement Parameter:</strong> All verbs raise (French) or only auxiliaries raise (English).</p></li>
<li><p><strong>The VP-internal Subject Hypothesis:</strong> Subjects are generated in the specifier of the voice-headed VP.</p></li>
<li><p><strong>T C Movement:</strong> Move T to C when there is a phonologically empty Ø[+Q] complementizer.</p></li>
<li><p><strong>Do-support:</strong> When there is no other option for supporting inflectional affixes, insert the dummy verb do into T.</p></li>
</ul>
<h1 id="chapter-11">Chapter 11</h1>
<ul>
<li><p><strong>The Locality Constraint on Theta Role Assignment:</strong> Theta roles are assigned within the clause containing the predicate that introduces them (i.e., the VP or other predicate).</p></li>
<li><p><strong>DP Movement:</strong> Move a DP to a specifier position.</p></li>
<li><p><strong>Raising:</strong> A specific instance of DP movement. The DP moves from the specifier of an embedded non-finite T to the specifier of a finite T in the main clause where it can get Case.</p></li>
<li><p><strong>case (lowercase c):</strong> The special form DPs get depending upon their place in the sentence.</p></li>
<li><p><strong>Case (capital C):</strong> Licensing for DPs: NOM is found on the specifier of finite T. ACC is found on the complement of transitive V.</p></li>
<li><p><strong>The Case Filter:</strong> All DPs must be marked with Case.</p></li>
<li><p><strong>Passives:</strong> A particular verb form where the external argument (often the agent or experiencer) is suppressed and the theme appears in subject position. The movement of the theme is also an instance of DP movement.</p></li>
<li><p><strong>Burzio’s Generalization:</strong> If a verb does not have an external argument (i.e., is passive or unaccusative), then it can’t assign accusative Case.</p></li>
<li><p><strong>Unaccusatives:</strong> Inherently passive verbs like arrive.</p></li>
</ul>
<h1 id="chapter-12">Chapter 12</h1>
<ul>
<li><p><strong>Wh-movement:</strong> Move a wh-phrase to the specifier of CP to check a [+WH] feature in C.</p></li>
<li><p><strong>Wanna-contraction:</strong> The contraction of want and to, which does not apply across a wh-trace.</p></li>
<li><p><strong>That-trace Effect:</strong> Movement of a wh-phrase from subject position in English is disallowed when that trace is preceded by the complementizer that.</p></li>
<li><p><strong>That-trace Filter:</strong> *[CP that t …]</p></li>
<li><p><strong>Relative Clause:</strong> A CP that modifies a noun. These always have a “missing” element in them that corresponds to some kind of wh-element.</p></li>
<li><p><strong>Factive Clause:</strong> A clause that is the complement to a factive verb (know, claim, recall, etc.), or to a factive noun (knowledge, claim, fact, recollection, etc.).</p></li>
<li><p><strong>Operator (Op):</strong> The wh-element in relative clauses without an overt wh-phrase.</p></li>
<li><p><strong>Restrictive Relative Clause:</strong> A relative clause that restricts the meaning of a noun as a modifier. Adjoined to N’.</p></li>
<li><p><strong>Nonrestrictive Relative Clause:</strong> A relative clause that adds additional parenthetical commentary about a noun. Adjoined to D’.</p></li>
<li><p><strong>Island:</strong> A phrase that contains (dominates) the wh-phrase, and that you may not move out of.</p></li>
<li><p><strong>The Complex DP Constraint:</strong> *whi [ … [DP … ti … ] …]</p></li>
<li><p><strong>Wh-island Constraint:</strong> *whi [ … [CP whk [ … ti … ] … ] …]</p></li>
<li><p><strong>The Subject Condition:</strong> *whi … [TP [CP … ti … ] T …]</p></li>
<li><p><strong>Coordinate Structure Constraint:</strong> <em>whi … [XP [XP … ti … ] conj [XP … ]] … or </em>whi … [XP [XP … ] conj [XP … ti … ]] … or *whi … [XP [XP … ] conj ti] …</p></li>
</ul>
<p>or *whi … [XP ti conj [XP … ]] …</p>
<ul>
<li><p><strong>Minimal Link Condition (MLC) (intuitive version):</strong> Move to the closest potential landing site.</p></li>
<li><p><strong>Whin-situ:</strong> When a wh-phrase does not move.</p></li>
<li><p><strong>Echo Questions and Intonational Questions:</strong> Question forms that are licensed by the phonology (intonation and stress) and not by the syntax, although they may involve a special C.</p></li>
</ul>
<h1 id="chapter-13">Chapter 13</h1>
<ul>
<li><p><strong>Move (very informal version):</strong> Move something somewhere.</p></li>
<li><p><strong>Full Interpretation:</strong> Features must be checked in a local configuration.</p></li>
<li><p><strong>Local Configuration:</strong></p></li>
</ul>
<p>[WH], [NOM] features: specifier–head configuration.</p>
<p>[ACC] features: head–complement configuration.</p>
<p>[PAST] etc., [Q] features: head–head configuration.</p>
<ul>
<li><p><strong>Logical Form (LF):</strong> The semantic/interpretive system.</p></li>
<li><p><strong>Phonetic Form (PF):</strong> The overt component of grammar.</p></li>
<li><p><strong>SPELLOUT:</strong> The point at which the derivation divides into form (PF) and meaning structures (LF).</p></li>
<li><p><strong>Overt Movement:</strong> Movement between D-structure and SPELLOUT.</p></li>
<li><p><strong>Covert Movement:</strong> Movement between SPELLOUT and LF.</p></li>
<li><p><strong>Universal Quantifier ( ):</strong> A word such as every, each, all, any. Identifies all the members of a set.</p></li>
<li><p><strong>Existential Quantifier ( ):</strong> A word like some, or a. Identifies at least one member of a set.</p></li>
<li><p><strong>Scope:</strong> A quantifier’s scope is the range of material it c-commands.</p></li>
<li><p>Wide vs. Narrow Scope: Wide scope is when one particular</p></li>
</ul>
<p>quantifier c-commands another quantifier. Narrow scope is the opposite.</p>
<ul>
<li><strong>Quantifier Raising (QR):</strong> A covert instance of Move that moves quantifiers.</li>
</ul>
<h1 id="chapter-14">Chapter 14</h1>
<ul>
<li><p><strong>Light Verbs (Little v):</strong> the higher part of a complex verb, usually meaning CAUSE (or LOCATE, in the case of ditransitive double object verbs).</p></li>
<li><p><strong>Object Shift:</strong> the phenomenon where accusatively marked objects shift leftwards.</p></li>
<li><p><strong>AgrO:</strong> the head that checks accusative Case in the split VP system.</p></li>
</ul>
<p>　</p>
<h1 id="chapter-15">Chapter 15</h1>
<ul>
<li><p><strong>PRO (Big PRO):</strong> A null (silent) DP found in Caseless positions.</p></li>
<li><p><strong>pro (Little pro or Baby pro):</strong> A null (silent) DP often found in languages with “rich” agreement. pro does get Case.</p></li>
<li><p><strong>Clausal Subject Construction:</strong> A sentence where a clause appears in the specifier of TP.</p></li>
<li><p><strong>Extraposition (Expletive Subject):</strong> A sentence where there is an expletive in the subject position and a clausal complement.</p></li>
<li><p><strong>Subject-to-subject Raising:</strong> A kind of DP movement where the subject of an embedded non-finite clause moves to the specifier of TP of the main clause to get nominative Case.</p></li>
<li><p><strong>Subject-to-object Raising (also called Exceptional Case Marking or ECM):</strong> A kind of DP movement where the subject of an embedded non-finite clause moves to the specifier of AgrO in the main clause to get accusative Case.</p></li>
<li><p><strong>Control Theory:</strong> The theory that governs how PRO gets its meaning.</p></li>
<li><p><strong>Pragmatics:</strong> The science that looks at how language and knowledge of the world interact.</p></li>
<li><p><strong>Subject Control (also called Equi):</strong> A sentence where there is a PRO in the embedded non-finite clause that is controlled by the subject argument of the main clause.</p></li>
<li><p><strong>Object Control:</strong> A sentence where there is a PRO in the embedded</p></li>
</ul>
<p>non-finite clause that is controlled by the object argument of the main clause.</p>
<ul>
<li><p>Obligatory vs. Optional Control: Obligatory control is when the PRO must be controlled. Optional control is when the DP can be controlled or not.</p></li>
<li><p><strong>PROarb:</strong> Uncontrolled PRO takes an “arbitrary” reference.</p></li>
<li><p><strong>Null Subject Parameter:</strong> The parameter switch that distinguishes languages like English, which require an overt subject, from languages like Italian that don’t, and allow pro.</p></li>
</ul>
<h1 id="chapter-16">Chapter 16</h1>
<ul>
<li><p><strong>Ellipsis:</strong> A construction that omits a constituent when it is identical to a string that has previously been uttered.</p></li>
<li><p><strong>VP Ellipsis:</strong> A process that omits a VP (or vP) under identity with a previously uttered identical VP, normally in a conjunction. (E.g., I will eat a squid sandwich and you will too.)</p></li>
<li><p><strong>Antecedent-Contained Deletion (ACD):</strong> A kind of ellipsis where the antecedent of the ellipsis contains the ellipsis site. (E.g., She read every book that I did.)</p></li>
<li><p><strong>Pseudogapping:</strong> A variety of ellipsis where the accusative object is not omitted, but the rest of the VP is. (E.g., Dan can’t prove Paul innocent but he can prove Della innocent.)</p></li>
<li><p><strong>Comparative Deletion:</strong> The deletion in a comparative construction; often more than just a VP is missing. We did not attempt an account of comparative deletion in this chapter. (E.g., I’ve read more books than you [have read books].)</p></li>
<li><p><strong>Comparative Subdeletion:</strong> A kind of comparative deletion that is effectively equivalent to one kind of pseudogapping. (E.g., I’ve eaten more popcorn than you have eaten fries.)</p></li>
<li><p><strong>Stripping:</strong> An ellipsis process where only one argument remains and the rest of the clause is elided. (E.g., Frank read the Times last night, or maybe the Post.)</p></li>
<li><p><strong>N-ellipsis:</strong> The deletion of some part of a DP, typically including the N head. (E.g., I read these three books not those two ____.)</p></li>
<li><p><strong>Sluicing:</strong> A kind of ellipsis, where a TP is elided after a wh-phrase (E.g., I saw someone come into the room, but I don’t remember who ____.)</p></li>
<li><p><strong>LF-copying hypothesis:</strong> The idea that VP ellipsis consists of a null pronominal VP that is replaced by a copy of its antecedent after SPELLOUT and before LF.</p></li>
<li><p><strong>PF-deletion hypothesis:</strong> The idea that VP ellipsis targets a fully structured VP, which is deleted under identity with an antecedent after SPELLOUT and before PF.</p></li>
<li><p><strong>Sloppy Identity:</strong> In an ellipsis structure an elided pronoun or anaphor takes its reference from a local subject (e.g., where John loves his father and Bill does too has an interpretation where Bill loves Bill’s father).</p></li>
<li><p><strong>Strict Identity:</strong> In an ellipsis structure an elided pronoun or anaphor takes its reference from the subject in the antecedent clause (e.g., where John loves his father and Bill does too has an interpretation where Bill loves John’s father).</p></li>
<li><p><strong>Preposition Stranding:</strong> The phenomenon in English and related languages where prepositions do not move with the wh-phrase. (E.g., Who did you take a picture of?)</p></li>
<li><p><strong>Sag’s Analysis of ACD:</strong> To avoid infinite regress, the quantified DP undergoes covert QR to the top of the clause, which places it outside of the VP that is its antecedent.</p></li>
<li><p><strong>Hornstein’s Analysis of ACD:</strong> To avoid infinite regress, the DP undergoes DP movement to the specifier of AgrOP, which places it outside of the (lowest) VP that is its antecedent.</p></li>
<li><p><strong>Lasnik’s Analysis of Pseudogapping (Ellipsis Analysis):</strong> To explain why accusative objects survive ellipsis in pseudogapping structures, the accusative object moves to the specifier of AgrOP, which places it outside the VP that is elided. Normal VP ellipsis is really vP ellipsis.</p></li>
<li><p><strong>Across-the-Board Movement (ATB):</strong> The movement, typically of a wh-phrase, that appears to originate in two different conjoined VPs or clauses (e.g., Who did [Evan despise ti] and [Calvin adore ti?])</p></li>
<li><p><strong>Agbayani and Zoerner’s Analysis of Pseudogapping (ATB Analysis):</strong> Pseudogapping is not VP ellipsis; instead, it is the overt ATB head movement of a V head in two different phrases into a single little v node. The second trace of this movement corresponds to the “ellipsis” site in pseudogapping.</p></li>
</ul>
<h1 id="chapter-17">Chapter 17</h1>
<ul>
<li><p><strong>The Copy Theory of Movement:</strong> Movement is a two-part operation. First, the moved element is copied and put into the surface position; second, the original is made silent.</p></li>
<li><p><strong>Chain:</strong> The moved copy and all its traces.</p></li>
<li><p><strong>Potential Antecedent:</strong> A DP in the specifier of TP or another DP. The potential antecedent cannot be the anaphor or pronoun itself, nor can it be a DP that contains the anaphor or pronoun.</p></li>
<li><p><strong>Binding Principle A (final):</strong> One copy of an anaphor in a chain must be bound within the smallest CP or DP containing it and a potential antecedent.</p></li>
<li><p><strong>Binding Principle B (Dialect 1):</strong> A pronoun must be free within the smallest CP or DP containing it.</p></li>
<li><p><strong>Binding Principle B (Dialect 2):</strong> A pronoun must be free within the smallest DP (with a filled specifier) or CP containing it.</p></li>
</ul>
<h1 id="chapter-18">Chapter 18</h1>
<ul>
<li><p><strong>Polysynthesis:</strong> The phenomenon where all the required arguments of a verb surface as morphemes on that verb.</p></li>
<li><p><strong>Syntax-free Hypothesis:</strong> A hypothesis where polysynthetic languages are said to lack a syntactic component. All the work of the grammar is done by the morphology instead.</p></li>
<li><p><strong>Polysynthesis Parameter:</strong> Every argument of a head must be related to a morpheme in the word that contains the head.</p></li>
<li><p><strong>Radical Pro-drop Hypothesis:</strong> A hypothesis about polysynthetic languages, where the morphemes on a verb are agreement morphemes related to null pro arguments.</p></li>
<li><p><strong>Incorporation:</strong> A phenomenon where the direct object appears as part of the inflected verb.</p></li>
<li><p><strong>The Empty Category Principle:</strong> Traces must be c-commanded by the moved element (their antecedents).</p></li>
<li><p><strong>Scrambling:</strong> A movement rule that positions DPs in specifier of FocusP or TopicP.</p></li>
<li><p><strong>Topics:</strong> DPs that represent given (old) information in the discourse.</p></li>
<li><p><strong>Information Focus:</strong> The new information in a discourse.</p></li>
<li><p><strong>Contrastive Focus:</strong> A phrase that is presented in contrast to a previously expressed idea.</p></li>
<li><p><strong>Non-configurationality:</strong> Non-configurational languages exhibit very free word order, discontinuous constituents, and missing DP arguments.</p></li>
<li><p><strong>The Dual-Structure Hypothesis:</strong> The idea that syntax is divided into two structures.</p></li>
<li><p><strong>Hale’s Phrase Structure Rule for Non-configurational Languages:</strong> TP XTZ+</p></li>
<li><p><strong>The Pronominal Argument Parameter:</strong> Only pronouns can function as the arguments of predicates.</p></li>
</ul>
]]></content>
      <categories>
        <category>语言学</category>
        <category>句法</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作</title>
    <url>/pandas/</url>
    <content><![CDATA[<h1 id="代码">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.set_option(<span class="string">&#x27;display.max_colwidth&#x27;</span>,<span class="literal">None</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>英文句子切分的要点和工具</title>
    <url>/sentence-segmentation/</url>
    <content><![CDATA[<h1 id="英文句子切分的规则">英文句子切分的规则</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line">text = <span class="string">&quot;This \n This is Ms. Right&#x27;s first sentence.\n\nNext: is numbered list？\n1.Hello World!\n2.Hello World2!\n3.Hello World!&quot;</span></span><br><span class="line">text_sentences = nlp(text)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> text_sentences.sents:</span><br><span class="line">    print(<span class="string">&#x27;句子：&#x27;</span>+sentence.text)</span><br></pre></td></tr></table></figure>
<p>一般句子以<code>.?!</code>结尾，但<code>.</code>有时也出现在句中。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>句子切分</category>
      </categories>
      <tags>
        <tag>sentence segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>信息抽取-Pilot Study</title>
    <url>/ir-project-0/</url>
    <content><![CDATA[<p>这个周末要把项目雏形做出来，在这里记录实验全过程、用到的资料和数据等，代码我放到了github。</p>
<h1 id="第一步建立学术文献语料库">第一步：建立学术文献语料库</h1>
<h2 id="下载pdf文件">1.1 下载pdf文件</h2>
<p>在pilot study中我们只选取ACL中前100篇top cited papers，获取文章列表及pdf的<a href="https://github.com/MissFreak/academic-information-retrieval/blob/main/corpus/get-papers.py">完整代码</a>。</p>
<table>
<colgroup>
<col style="width: 88%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="header">
<th>title</th>
<th>citation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Neural Machine Translation of Rare Words with Subword Units</td>
<td>3461</td>
</tr>
<tr class="even">
<td>Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.</td>
<td>2405</td>
</tr>
<tr class="odd">
<td>End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.</td>
<td>1823</td>
</tr>
<tr class="even">
<td>Get To The Point: Summarization with Pointer-Generator Networks.</td>
<td>1351</td>
</tr>
<tr class="odd">
<td>OpenNMT: Open-Source Toolkit for Neural Machine Translation.</td>
<td>1286</td>
</tr>
</tbody>
</table>
<span id="more"></span>
<h2 id="格式转化与数据清洗">1.2 格式转化与数据清洗</h2>
<p>把pdf转化为txt，去掉hyphen和newline。</p>
<p>然后切分句子，把这些句子保存在csv中。</p>
<h1 id="第二步抽取术语">第二步：抽取术语</h1>
]]></content>
      <categories>
        <category>项目</category>
        <category>信息抽取</category>
      </categories>
  </entry>
  <entry>
    <title>spaCy超强指南之文本预处理和语言特征表</title>
    <url>/spacy-1/</url>
    <content><![CDATA[<h1 id="语言特征表token属性">语言特征表：token属性</h1>
<p>https://spacy.io/api/token#attributes</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&quot;I took a walk happily yesterday.&quot;</span></span><br><span class="line">doc = nlp(sentence)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> tok <span class="keyword">in</span> doc:</span><br><span class="line">    row = <span class="string">&quot;\t&quot;</span>.join([<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> [tok.text, tok.pos_, tok.dep_, tok.head, tok.left_edge, tok.right_edge, tok.lemma_, tok.morph, tok.suffix_, tok.is_stop, tok.tag_]])</span><br><span class="line">    print(row)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<table>
<colgroup>
<col style="width: 8%" />
<col style="width: 7%" />
<col style="width: 3%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 39%" />
<col style="width: 2%" />
<col style="width: 4%" />
<col style="width: 2%" />
<col style="width: 4%" />
</colgroup>
<thead>
<tr class="header">
<th>text</th>
<th>pos_</th>
<th>dep_</th>
<th>head</th>
<th>left_edge</th>
<th>right_edge</th>
<th>lemma_</th>
<th>morph</th>
<th>suffix</th>
<th>is_stop</th>
<th>tag_</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>I</td>
<td>PRON</td>
<td>nsubj</td>
<td>took</td>
<td>I</td>
<td>I</td>
<td>I</td>
<td>Case=Nom|Number=Sing|Person=1|PronType=Prs</td>
<td>I</td>
<td>True</td>
<td>PRP</td>
</tr>
<tr class="even">
<td>took</td>
<td>VERB</td>
<td>ROOT</td>
<td>took</td>
<td>I</td>
<td>.</td>
<td>take</td>
<td>Tense=Past|VerbForm=Fin</td>
<td>ook</td>
<td>False</td>
<td>VBD</td>
</tr>
<tr class="odd">
<td>a</td>
<td>DET</td>
<td>det</td>
<td>walk</td>
<td>a</td>
<td>a</td>
<td>a</td>
<td>Definite=Ind|PronType=Art</td>
<td>a</td>
<td>True</td>
<td>DT</td>
</tr>
<tr class="even">
<td>walk</td>
<td>NOUN</td>
<td>dobj</td>
<td>took</td>
<td>a</td>
<td>walk</td>
<td>walk</td>
<td>Number=Sing</td>
<td>alk</td>
<td>False</td>
<td>NN</td>
</tr>
<tr class="odd">
<td>happily</td>
<td>ADV</td>
<td>advmod</td>
<td>took</td>
<td>happily</td>
<td>happily</td>
<td>happily</td>
<td></td>
<td>ily</td>
<td>False</td>
<td>RB</td>
</tr>
<tr class="even">
<td>yesterday</td>
<td>NOUN</td>
<td>npadvmod</td>
<td>took</td>
<td>yesterday</td>
<td>yesterday</td>
<td>yesterday</td>
<td>Number=Sing</td>
<td>day</td>
<td>False</td>
<td>NN</td>
</tr>
<tr class="odd">
<td>.</td>
<td>PUNCT</td>
<td>punct</td>
<td>took</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>PunctType=Peri</td>
<td>.</td>
<td>False</td>
<td>.</td>
</tr>
</tbody>
</table>
<h1 id="文本预处理">文本预处理</h1>
<p>spaCy的功能：把原始文本处理和标记并返回一个<a href="https://spacy.io/api/doc"><code>Doc</code></a>对象。以下是处理的常见操作，其中lemmatization和形态学是spacy使用的：</p>
<table>
<colgroup>
<col style="width: 38%" />
<col style="width: 61%" />
</colgroup>
<thead>
<tr class="header">
<th>特征</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>规范化（normalization）</td>
<td>transforming a text into a canonical (standard) form, e.g. :) to smile, and goooood to good.</td>
</tr>
<tr class="even">
<td>词形还原（lemmatization）</td>
<td>Base form of the token, with no inflectional suffixes. Lemmatization is typically seen as much more informative than simple stemming, which is why Spacy has opted to only have Lemmatization available instead of Stemming.</td>
</tr>
<tr class="odd">
<td>词干提取（stemming）</td>
<td>reducing inflection in words (e.g. thanks, troubling) to their root form (e.g. thank, troubl).</td>
</tr>
<tr class="even">
<td>形态学（morphology）</td>
<td>"you": Case=Nom|Person=2|PronType=Prs</td>
</tr>
</tbody>
</table>
<h1 id="其他文本预处理的资料">其他文本预处理的资料</h1>
<p>https://medium.com/<span class="citation" data-cites="datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908">@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908</span></p>
<p>https://towardsdatascience.com/stemming-vs-lemmatization-2daddabcb221</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>spaCy</tag>
      </tags>
  </entry>
  <entry>
    <title>有限状态机</title>
    <url>/finite-state-machines/</url>
    <content><![CDATA[<p>有限状态机（Finite-state machine, FSM），是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。主要用来是描述对象在它的生命周期内所经历的状态序列，以及如何响应来自外界的各种事件。 <span id="more"></span></p>
<p>参考：</p>
<p>https://www.huaweicloud.com/articles/ff04e6e5d7386b32d58f9d5015104ce9.html</p>
<p>https://www.cnblogs.com/21207-iHome/p/6085334.html</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title>文本编码格式大全</title>
    <url>/encoding/</url>
    <content><![CDATA[<p><strong>字符——（翻译过程）——二进制数字</strong></p>
<p>这个过程实际就是一个字符如何对应一个特定数字的标准，这个标准称之为字符编码。</p>
<h2 id="字符编码发展史与编码类型">字符编码发展史与编码类型</h2>
<h3 id="ascii-码美国标准信息交换码">ASCII 码（美国标准信息交换码）</h3>
<p>ASCII 码一共规定了128个字符的编码，比如空格SPACE是32（二进制00100000），大写的字母A是65（二进制01000001）。这128个符号（包括32个不能打印出来的控制符号），只占用了一个字节的后面7位，最前面的一位统一规定为0。</p>
<span id="more"></span>
<h3 id="非-ascii-编码">非 ASCII 编码</h3>
<p>英语用128个符号编码就够了，但是用来表示其他语言，128个符号是不够的。比如，在法语中，字母上方有注音符号，它就无法用 ASCII 码表示。于是，一些欧洲国家就决定，利用字节中闲置的最高位编入新的符号。比如，法语中的é的编码为130（二进制10000010）。这样一来，这些欧洲国家使用的编码体系，可以表示最多256个符号。</p>
<h3 id="gb2312-码">GB2312 码</h3>
<p>中文字符的每个字节最高位规定为 1，这便是 GB2312 编码。</p>
<h3 id="unicode">Unicode</h3>
<p>可以想象，如果有一种编码，将世界上所有的符号都纳入其中。每一个符号都给予一个独一无二的编码，那么乱码问题就会消失。<u>这就是 Unicode，就像它的名字都表示的，这是一种所有符号的编码。</u></p>
<p>Unicode 当然是一个很大的集合，现在的规模可以容纳100多万个符号。每个符号的编码都不一样，比如，<code>U+0639</code>表示阿拉伯字母<code>Ain</code>，<code>U+0041</code>表示英语的大写字母<code>A</code>，<code>U+4E25</code>表示汉字<code>严</code>。具体的符号对应表，可以查询<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.unicode.org%2F">unicode.org</a>，或者专门的<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.chi2ko.com%2Ftool%2FCJK.htm">汉字对应表</a>。</p>
<h3 id="unicode-的问题">Unicode 的问题</h3>
<p>这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。</p>
<h3 id="utf-8">UTF-8</h3>
<p><strong>重复一遍，这里的关系是，UTF-8 是 Unicode 的实现方式之一。</strong></p>
<p>UTF-8 最大的一个特点，就是它是一种<strong>变长</strong>的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。</p>
<p>UTF-8 的编码规则很简单，只有二条：</p>
<p>1）对于单字节的符号，字节的第一位设为<code>0</code>，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。</p>
<p>2）对于<code>n</code>字节的符号（<code>n &gt; 1</code>），第一个字节的前<code>n</code>位都设为<code>1</code>，第<code>n + 1</code>位设为<code>0</code>，后面字节的前两位一律设为<code>10</code>。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。</p>
<p>参考：</p>
<p><a href="https://www.jianshu.com/p/51b40820848a">Python编码避坑指南——编码基础知识</a></p>
<p><a href="http://www.cnblogs.com/ysocean/p/6850811.html">Java 字符编码与解码</a></p>
<p><a href="https://blog.csdn.net/sinat_36972314/article/details/79745438">Python常见字符编码及其之间的转换</a></p>
<p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896">字符串和编码python</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>预处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>编程方法论</title>
    <url>/programming-methodology/</url>
    <content><![CDATA[<h1 id="编程方法总结">编程方法总结</h1>
<p>首先，本着大道至简的原理，使用sublime text和terminal来查看和运行最终脚本。</p>
<p>其次，由于缺乏自动提示等插件，可以使用jupyter notebook作为打草稿的工具。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>代码</category>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>methodology</tag>
      </tags>
  </entry>
  <entry>
    <title>信息论知识与资源汇总</title>
    <url>/information-theory-1/</url>
    <content><![CDATA[<h1 id="信息">信息</h1>
<p>信息是事物运动状态或存在方式不确定性的描述。不确定性大小 <span class="math inline">\(f(p(x))\)</span> 应该满足以下3个条件：</p>
<ol type="1">
<li><p><span class="math inline">\(f(1)=0\)</span></p></li>
<li><p><span class="math inline">\(f(p(x))\)</span> 是单调减函数</p></li>
<li><p>独立可加性，独立事件应具有增量的信息：<span class="math inline">\(f(p(x) p(y))=f(p(x))+f(p(y))\)</span></p></li>
</ol>
<p>因此，我们定义一个事件 X = x 的 <strong>自信息(self-information)</strong> 为： <span class="math display">\[
I(x) = -logP(x)
\]</span></p>
<p>使用底数为 2 的对数，单位是 <strong>比特(bit)</strong> 或者 <strong>香农(shannons)</strong>。</p>
<p>我们主要使用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。我们可以用 <strong>香农熵(Shannon entropy)</strong> 来对整个概率分布中的不确定性总量进行量化:</p>
<p><span class="math display">\[
H(x) = E_{x \sim P}[I(x)] = -E_{x \sim P}[logP(x)]
\]</span></p>
<p>二值随机变量的熵由 $(p − 1) log(1 − p) − p log p $给出。</p>
<span id="more"></span>
<h2 id="课程">课程</h2>
<p>信息论与编码理论：https://www.icourse163.org/course/XIDIAN-1002199004</p>
<h2 id="书目">书目</h2>
<p>《信息简史》</p>
<p>《信息论与编码理论》</p>
<h2 id="参考">参考</h2>
<p>https://blog.csdn.net/lyxleft/article/details/84867306</p>
<p>https://www.icourse163.org/learn/XDU-1002199004?tid=1463141462#/learn/content?type=detail&amp;id=1240328941&amp;sm=1</p>
]]></content>
      <categories>
        <category>数学</category>
        <category>信息论</category>
      </categories>
      <tags>
        <tag>information theory</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫代码-全</title>
    <url>/scraping/</url>
    <content><![CDATA[<h1 id="常用框架工具和代码">常用框架、工具和代码</h1>
<h2 id="框架">框架</h2>
<p>熟悉scrapy框架、pyspider框架等。</p>
<h1 id="期待完成的项目">期待完成的项目</h1>
<h2 id="多媒体">多媒体</h2>
<h3 id="爬取网站音视频文件并整合">爬取网站音视频文件并整合</h3>
<p>https://www.naturalreaders.com/online/</p>
<h2 id="文本">文本</h2>
<h3 id="爬取aminer文献的标题和pdf文件">爬取Aminer文献的标题和pdf文件</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;ACL.html&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    html = f.read()</span><br><span class="line"><span class="comment"># print(&#x27;读取到以下html内容：&#123;&#125;...&#x27;.format(html[:20]))</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lst = re.split(<span class="string">&#x27;&lt;/?script.*&gt;&#x27;</span>, html)</span><br><span class="line"><span class="keyword">for</span> string <span class="keyword">in</span> lst:</span><br><span class="line">	<span class="keyword">if</span> <span class="string">&quot;g_initialProps&quot;</span> <span class="keyword">in</span> string:</span><br><span class="line">		json_file = re.sub(<span class="string">&#x27;undefined&#x27;</span>, <span class="string">&#x27;&quot;undefined&quot;&#x27;</span>, string.split(<span class="string">&#x27;;&#x27;</span>, <span class="number">1</span>)[<span class="number">1</span>].split(<span class="string">&#x27;= &#x27;</span>)[-<span class="number">1</span>].rsplit(<span class="string">&#x27;;&#x27;</span>, <span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">dict_json = json.loads(json_file)</span><br><span class="line">top_cited_papers = dict_json[<span class="string">&#x27;rank&#x27;</span>][<span class="string">&#x27;confInfo&#x27;</span>][<span class="string">&#x27;top_cited_papers&#x27;</span>]</span><br><span class="line"></span><br><span class="line">lst_of_ids = [paper[<span class="string">&#x27;id&#x27;</span>] <span class="keyword">for</span> paper <span class="keyword">in</span> top_cited_papers]</span><br><span class="line">lst_of_titles = [paper[<span class="string">&#x27;title&#x27;</span>] <span class="keyword">for</span> paper <span class="keyword">in</span> top_cited_papers]</span><br><span class="line">print(<span class="built_in">len</span>(lst_of_ids))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(lst_of_ids[<span class="number">1</span>:])):</span><br><span class="line">	page_link = <span class="string">&#x27;https://www.aminer.cn/pub/&#123;&#125;/&#x27;</span>.<span class="built_in">format</span>(lst_of_ids[i])</span><br><span class="line">	headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: ua.random&#125; <span class="comment"># 更换headers来应对反爬虫</span></span><br><span class="line">	bs = BeautifulSoup(requests.get(page_link).content, <span class="string">&quot;lxml&quot;</span>)</span><br><span class="line">	pdf_link = <span class="string">&quot;http://&quot;</span> + bs.find(<span class="string">&#x27;iframe&#x27;</span>)[<span class="string">&#x27;src&#x27;</span>].split(<span class="string">&#x27;file=//&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">	print(pdf_link)</span><br><span class="line">	<span class="keyword">with</span> <span class="built_in">open</span>(lst_of_titles[i] +<span class="string">&#x27;.pdf&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">		print(<span class="string">&#x27;Downloading...&#x27;</span>)</span><br><span class="line">		headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: ua.random&#125;</span><br><span class="line">		response = requests.get(pdf_link, headers=headers)</span><br><span class="line">		f.write(response.content)</span><br><span class="line">	print(<span class="string">&#x27;Downloaded: &#x27;</span>+ lst_of_titles[i])</span><br><span class="line">	time.sleep(random.random()*<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>web scraping</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（下）</title>
    <url>/information-retrieval-3/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<h1 id="抽取时间">17.3 抽取时间</h1>
<p>时间和日期是一种特别重要的命名实体，它在自动问答、日历和私人助理应用中起着重要的作用。为了对时间和日期进行推理，在我们提取了这些时间表达式后，必须对它们进行归一化处理——将其转换为标准格式，这样我们才能对它们进行推理。在本节中，我们将同时考虑时间表达式的提取和归一化。</p>
<span id="more"></span>
<h2 id="时间表达式的提取">17.3.1 时间表达式的提取</h2>
<p>时间表达式是指绝对时间点、相对时间、持续时间以及这些的集合。<strong>绝对</strong>（absolute）时间表达式是指那些可以直接相对映射到日历日期、一天中的时间或两者都有的表达式。<strong>相对</strong>（Relative）时间表达式通过其他一些参考点映射到特定的时间（如从上周二开始一周的持续时间）。最后，<strong>持续时间</strong>（durations）表示不同粒度的时间跨度（秒、分、天、周、世纪等）。图17.11列出了这些类别中的一些时间表达式样本。</p>
<table>
<caption>Figure 17.11 Examples of absolute, relational and durational temporal expressions.</caption>
<thead>
<tr class="header">
<th>Absolute</th>
<th>Relative</th>
<th>Durations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>April 24, 1916</td>
<td>yesterday</td>
<td>four hours</td>
</tr>
<tr class="even">
<td>The summer of ’77</td>
<td>next semester</td>
<td>three weeks</td>
</tr>
<tr class="odd">
<td>10:15 AM</td>
<td>two weeks from yesterday</td>
<td>six days</td>
</tr>
<tr class="even">
<td>The 3rd quarter of 2006</td>
<td>last quarter</td>
<td>the last three quarters</td>
</tr>
</tbody>
</table>
<p>时间表达式是以时间词汇触发器<u>lexical triggers</u>为中心的语法结构。词汇触发器可以是名词、专有名词、形容词和副词；完整的时间表达式由它们的短语投射组成：名词短语、形容词短语和副词短语。图17.12提供了示例。</p>
<table>
<caption>Figure 17.12 Examples of temporal lexical triggers.</caption>
<thead>
<tr class="header">
<th>Category</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Noun</td>
<td>morning, noon, night, winter, dusk, dawn</td>
</tr>
<tr class="even">
<td>Proper Noun</td>
<td>January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet</td>
</tr>
<tr class="odd">
<td>Adjective</td>
<td>recent, past, annual, former</td>
</tr>
<tr class="even">
<td>Adverb</td>
<td>hourly, daily, monthly, yearly</td>
</tr>
</tbody>
</table>
<p>让我们看看TimeML标注方案，其中时间表达式用XML标签、TIMEX3和该标签的各种属性进行标注（Pustejovsky et al.2005，Ferro et al.2005）。下面的示例说明了此方案的基本用法（我们将对属性的讨论推迟到第17.3.2节）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A fare increase initiated &lt;TIMEX3&gt;last week&lt;&#x2F;TIMEX3&gt; by UAL Corp’s United Airlines was matched by competitors over &lt;TIMEX3&gt;the weekend&lt;&#x2F;TIMEX3&gt;, marking the second successful fare increase in &lt;TIMEX3&gt;two weeks&lt;&#x2F;TIMEX3&gt;.</span><br></pre></td></tr></table></figure>
<p>时间表达式识别任务包括查找与这些时间表达式对应的所有文本跨度的开始和结束。基于规则的时间表达式识别方法使用级联自动机来识别复杂度不断提高的模式。首先标记词性，然后根据包含触发词（如二月）或类（如月份）的模式，从前一阶段的结果中识别出越来越大的语块。图17.13给出了一个基于规则的系统的片段。</p>
<figure class="highlight perl"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yesterday/today/tomorrow</span></span><br><span class="line">$string =˜ s/((($OT+the$CT+\s+)?$OT+day$CT+\s+$OT+(before|after)$CT+\s+)?$OT+$TERelDayExpr$CT+</span><br><span class="line">(\s+$OT+(morning|afternoon|evening|night)$CT+)?)/&lt;TIMEX$tever TYPE=\<span class="string">&quot;DATE\&quot;&gt;$1</span></span><br><span class="line"><span class="string">&lt;\/TIMEX$tever&gt;/gio;</span></span><br><span class="line"><span class="string">$string =˜ s/($OT+\w+$CT+\s+)&lt;TIMEX$tever TYPE=\&quot;DATE\&quot;[ˆ&gt;]*&gt;($OT+(Today|Tonight)$CT+)</span></span><br><span class="line"><span class="string">&lt;\/TIMEX$tever&gt;/$1$4/gso;</span></span><br><span class="line"><span class="string"># this (morning/afternoon/evening)</span></span><br><span class="line"><span class="string">$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+this$CT+\s*$OT+(morning|afternoon|evening)$CT+)/</span></span><br><span class="line"><span class="string">&lt;TIMEX$tever TYPE=\&quot;DATE\&quot;&gt;$1&lt;\/TIMEX$tever&gt;/gosi;</span></span><br><span class="line"><span class="string">$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+last$CT+\s*$OT+night$CT+)/&lt;TIMEX$tever</span></span><br><span class="line"><span class="string">TYPE=\&quot;DATE\&quot;&gt;$1&lt;\/TIMEX$tever&gt;/gsio;</span></span><br></pre></td></tr></table></figure>
<p>: Figure 17.13 Perl fragment from the GUTime temporal tagging system in Tarsqi (Verhagen et al., 2005).</p>
<p>序列标记方法遵循与命名实体标记相同的IOB方案，使用I、O和B标签标记TIMEX3分隔的时间表达式的内部、外部或开头的单词，如下所示：</p>
<p>A O fare O increase O initiated O last B week I by O UAL O Corp’s... O</p>
<p>从标记（token）及其上下文中提取特征，并训练统计序列标注器（可以使用任何序列模型）。图17.14列出了时间标注中使用的标准特征。时间表达式识别器使用通常的recall、precision和F-measure进行评估。所有这些非常词汇化的方法的一个主要困难是避免触发假阳性的表达式：</p>
<blockquote>
<p>(17.15) 1984 tells the story of Winston Smith...</p>
<p>(17.16) ...U2’s classic Sunday Bloody Sunday</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>Feature</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Token</td>
<td>The target token to be labeled</td>
</tr>
<tr class="even">
<td>Tokens in window</td>
<td>Bag of tokens in the window around a target</td>
</tr>
<tr class="odd">
<td>Shape</td>
<td>Character shape features</td>
</tr>
<tr class="even">
<td>POS</td>
<td>Parts of speech of target and window words</td>
</tr>
<tr class="odd">
<td>Chunk tags</td>
<td>Base phrase chunk tag for target and words in a window</td>
</tr>
<tr class="even">
<td>Lexical triggers</td>
<td>Presence in a list of temporal terms</td>
</tr>
</tbody>
</table>
<hr />
<p>: Figure 17.14 Typical features used to train IOB-style temporal expression taggers.</p>
<h2 id="时间归一化">17.3.2 时间归一化</h2>
<p>时间归一化是将时间表达式映射到特定时间点或持续时间的时间规范化的过程。时间点与日历日期、一天中的时间或两者都对应。持续时间主要由时间长度组成，但也可能包括关于起点和终点的信息。标准化时间用ISO 8601标准中的值属性表示，该标准用于编码时间值（ISO86012004）。图17.15再现了我们前面的示例，其中添加了值属性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;TIMEX3 i d &#x3D; ’ ’ t 1 ’ ’ t y p e &#x3D;”DATE” v a l u e &#x3D;” 2007 −07 −02 ” f u n cti o n I n D o c u m e nt &#x3D;”CREATION TIME”</span><br><span class="line">&gt; J u l y 2 , 2007 &lt;&#x2F;TIMEX3&gt; A f a r e i n c r e a s e i n i t i a t e d &lt;TIMEX3 i d &#x3D;” t 2 ” t y p e &#x3D;”DATE”</span><br><span class="line">v a l u e &#x3D;” 2007 −W26” a nc h o rTime ID &#x3D;” t 1 ”&gt;l a s t week&lt;&#x2F;TIMEX3&gt; by U nit e d A i r l i n e s was</span><br><span class="line">matc he d by c o m p e t i t o r s o v e r &lt;TIMEX3 i d &#x3D;” t 3 ” t y p e &#x3D;”DURATION” v a l u e &#x3D;”P1WE”</span><br><span class="line">a nc h o rTime ID &#x3D;” t 1 ”&gt; t h e weekend &lt;&#x2F;TIMEX3&gt; , ma r ki n g t h e s e c o n d s u c c e s s f u l f a r e</span><br><span class="line">i n c r e a s e i n &lt;TIMEX3 i d &#x3D;” t 4 ” t y p e &#x3D;”DURATION” v a l u e &#x3D;”P2W” a nc h o rTime ID &#x3D;” t 1 ”&gt; two</span><br><span class="line">weeks &lt;&#x2F;TIMEX3&gt;.</span><br></pre></td></tr></table></figure>
<p>: Figure 17.15 TimeML markup including normalized values for temporal expressions.</p>
<p>此文本的日期行或文档日期为2007年7月2日。这种表达式的ISO表示为YYYY-MM-DD，在本例中为2007-07-02。我们示例文本中的时间表达式的编码都从这个日期开始，这里显示为VALUE属性的值。</p>
<p>正文中的第一个时间表达式是指一年中的某个特定星期。在ISO标准中，周数从01到53，一年中的第一周是一年中第一个星期四。这些周用模板yyyywnn表示。我们的文件日期的ISO周是第27周；因此上周的值表示为 “2007-W26”。</p>
<p>下一个时间表达式是周末。ISO周从周一开始；因此，周末发生在一周的末尾，并且完全包含在一周内。周末被视为持续时间，因此value属性的值必须是一个长度。持续时间根据模式Pnx表示，其中n是表示长度的整数，x表示单位，如P3Y表示三年，P2D表示两天。在本例中，一个周末被捕获为P1WE。在这种情况下，也有足够的信息锚定这个特定的周末作为一个特定的一周的一部分。这些信息编码在ANCHORTIMEID属性中。最后，短语two weeks还表示作为P2W捕获的持续时间。图17.16描述了表示其他时间和持续时间的一些基本方法。更多细节请参考ISO8601（2004）、Ferro等人（2005）和Pustejovsky等人（2005）。当前大多数时间标准化方法都是基于规则的（Chang和Manning 2012，Strotgen和Gertz 2013）。匹配时态表达式的模式与语义分析过程相关联。就像构图一样</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>认知语言学大纲</title>
    <url>/cognitive-linguistics/</url>
    <content><![CDATA[<p><strong><em>*Syllabus (Spring*</em></strong> <strong><em>*S*</em><em>*emester, 2021)*</em></strong></p>
<p><strong><em>*COURSE TITLE:*</em></strong> <strong><em>*Introduction to C*</em><em>*ognitive*</em></strong> <strong><em>*Linguistics*</em></strong> <strong><em>*认知语言学*</em></strong></p>
<figure>
<img src="file:///C:\Users\13607\AppData\Local\Temp\ksohtml15088\wps1.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><em>*INSTRUCTOR: LAN CHUN EMAIL:*</em></strong> beiwailanchun@126.com</p>
<p><strong><em>*STUDENTS: SEIS postgraduate*</em></strong> grade 2018</p>
<p><strong><em>*TIME: 10:10-12:00,Thursday*</em></strong> <strong><em>*PLACE*</em><em>*：S*</em><em>*EIS Building 417*</em></strong></p>
<p><strong><em>*OFFICE:*</em></strong> Room 202, SEIS Building <strong><em>*OFFICE HOURS:*</em></strong> 10:10-12.00, Friday</p>
<figure>
<img src="file:///C:\Users\13607\AppData\Local\Temp\ksohtml15088\wps2.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><em>*I. OBJECTIVES*</em></strong></p>
<p>This course aims at introducing to students the basic concepts, theories, approaches, research methods and hot issues of cognitive linguistics. It is expected that at the end of the course, students can have a general idea of the intricate relationships between language and cognition and of the various cognitive perspectives of doing linguistic research. Students are also expected to be able to apply the basic theoretical frameworks and methods of cognitive linguistics to the analysis of authentic data from different languages and cultures.</p>
<p><strong><em>*II. CLASS SCHEDULE*</em></strong></p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th><strong><em>*Week*</em></strong></th>
<th><strong><em>*Date*</em></strong></th>
<th><strong><em>*Content*</em></strong></th>
<th><strong><em>*Assignment*</em></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Mar. 4</td>
<td>An overview of cognitive science</td>
<td>Assigned reading 1: <strong>Metaphors We Live By</strong></td>
</tr>
<tr class="even">
<td>2</td>
<td>Mar. 11</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Mar. 18</td>
<td>The theory of prototype and basic-level categories</td>
<td>Assigned reading 2:<strong>Women, Fire and Dangerous Things: What Categories Reveal about the Mind</strong></td>
</tr>
<tr class="even">
<td>4</td>
<td>Mar. 25</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>April 1</td>
<td>Objectivism, subjectivism and embodied philosophy</td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>April 8</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>April 15</td>
<td>The prominence view and the attentional view of cognitive linguistics</td>
<td>Assigned reading 3:<strong>Cognitive Poetics: An Introduction</strong></td>
</tr>
<tr class="even">
<td>8</td>
<td>April 22</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>9</td>
<td>April 29</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>10</td>
<td>May 6</td>
<td>Conceptual metaphor and conceptual metonymy</td>
<td></td>
</tr>
<tr class="odd">
<td>11</td>
<td>May 13</td>
<td>Mini-research projects on assigned topics</td>
<td></td>
</tr>
<tr class="even">
<td>12</td>
<td>May 20</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>13</td>
<td>May 27</td>
<td>Workshop</td>
<td></td>
</tr>
<tr class="even">
<td>14</td>
<td>June 3</td>
<td>Cognitive grammar and cognitive semantics</td>
<td>Preparing for final exam/term paper</td>
</tr>
<tr class="odd">
<td>15</td>
<td>June 10</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>16</td>
<td>June 17</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>17</td>
<td>Exam Week</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>18</td>
<td>Exam Week</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong><em>*III. COURSE REQUIREMENTS*</em></strong></p>
<p>Students are expected to attend all the lectures, finish all the assigned reading and written work properly, contribute actively to both in-class and outside-class group discussions and team work, and take the final exam / submit a term paper at the end of the course.</p>
<p><strong><em>*IV. TEACHING APPROACH*</em></strong></p>
<p>This is a lecture-based course. The instructor will prepare detailed handouts and will use PPT to assist her teaching. Group discussions will be organized in class. Students are also expected to carry out mini research projects in small groups on assigned topics.</p>
<p><strong><em>*V. COURSE MATERIALS*</em></strong></p>
<p><strong><em>*1. Required Materials (or Readings)*</em></strong></p>
<p>蓝纯 2005 《认知语言学与隐喻研究》 北京：外语教学与研究出版社。</p>
<p>Evans, V. &amp; M. Green. 2006. <strong>Cognitive Linguistics: An Introduction</strong>. Berlin: Lawrence Erlbaum Associates Publishers.</p>
<p>Ungerer, F., &amp; Schmid, H.-J. 2006. <strong>An Introduction to Cognitive Linguistics</strong>. Beijing: FLTRP.</p>
<p><strong><em>*2. Recommended Materials (or Readings)*</em></strong></p>
<p>Croft, William and Alan Cruse. 2004. <strong>Cognitive Linguistics</strong>. Cambridge: Cambridge University Press.</p>
<p>Evans, Vyvyan, Benjamin K. Bergen and Jörg Zinken (eds.). 2007. <strong>The Cognitive Linguistic Reader</strong>. London: Equinox Publishing Company.</p>
<p>Lakoff, George, &amp; Johnson, Mark, 1980. <strong>Metaphors We Live By</strong>. Chicago: University of Chicago Press.</p>
<p>Johnson, Mark. 1987. <strong>The Body in the Mind</strong>. Chicago: University of Chicago Press.</p>
<p>Lakoff, George. 1987. <strong>Women, Fire, and Dangerous Things</strong>. Chicago: University of Chicago Press.</p>
<p>Langacker, Ronald W. 1987. <strong>Foundations of Cognitive Grammar: Theoretical Prerequisites</strong>. Stanford: Stanford University Press.</p>
<p>Langacker, Ronald W. 1991. <strong>Foundations of Cognitive Grammar: Descriptive Application</strong>. Stanford: Stanford University Press.</p>
<p>Lefrancois, G. R. 2004. <strong>Theories of Human Learning</strong>. Beijing: FLTRP.</p>
<p>Stockwell, Peter. 2002. <strong>Cognitive Poetics: An Introduction</strong>. London/New York: Routledge.</p>
<p>Talmy, Leonard. 2000. <strong>Toward a Cognitive Semantics</strong>. New York: MIT Press.</p>
<p><strong><em>*VI. ASSESSMENT*</em></strong></p>
<p>Students’ final score will consist of the following parts:</p>
<p><strong>1.</strong> <strong><em>*Post-class Work/Quiz/*</em></strong> <strong><em>*Assignments/*</em><em>*…*</em><em>*: 30%*</em></strong></p>
<p><strong>2.</strong> <strong><em>*C*</em><em>*lass*</em></strong> <strong><em>*Participation*</em></strong> <strong><em>*and Attendance: 20%*</em></strong></p>
<p>(<strong><em>*Absence of one third of class time*</em></strong>, excused or unexcused, will disqualify you from earning <strong><em>*credits for the course*</em></strong>. Showing up late for class or leaving before class is dismissed will also be penalized. <strong><em>*Two such records*</em></strong> will take one point off from your final marks.)</p>
<p><strong>3.</strong> <strong><em>*Final Exam and/or Term Paper: 50%*</em></strong></p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>认知语言学</category>
      </categories>
      <tags>
        <tag>linguistics</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（中）</title>
    <url>/information-retrieval-2/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<h1 id="关系抽取算法">17.2 关系抽取算法</h1>
<p>关系抽取的算法主要有五类：<strong>手写模式</strong>、<strong>监督机器学习</strong>、<strong>半监督</strong>（通过<strong>bootstrapping</strong>和通过<strong>远程监督</strong>）以及<strong>无监督</strong>。我们将在接下来的章节中分别介绍这些算法。</p>
<h2 id="使用模式抽取关系">17.2.1 使用模式抽取关系</h2>
<p>最早并且现在依然常用的关系抽取算法是词法-句法模式（ <strong>lexico-syntactic pattern</strong>），由Hearst（1992a）第一个开发，因此通常被称为<u>Hearst patterns</u>。例如以下句子：</p>
<blockquote>
<p>Agar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use.</p>
<p>琼脂是一种由红藻（如Gelidium）混合制备的物质，用于实验室或工业用途。</p>
</blockquote>
<p>赫斯特指出，大多数人类读者不会知道Gelidium是什么，但他们可以很容易推断出它是一种红藻（的<u>下义词</u><em>hyponym</em>）。她认为以下词法-句法模式：</p>
<p><span class="math display">\[
N P_{0} \text { such as } N P_{1}\left\{, N P_{2} \ldots,(\text { and } \mid \text { or }) N P_{i}\right\}, i \geq 1
\]</span></p>
<p>意味着以下语义 <span class="math display">\[
\forall N P_{i}, i \geq 1, \text { hyponym }\left(N P_{i}, N P_{0}\right)
\]</span> 让我们可以推断出 <span class="math display">\[
hyponym(Gelidium,red algae)
\]</span></p>
<table>
<caption>Figure 17.5 Hand-built lexico-syntactic patterns for finding hypernyms, using {} to mark optionality (Hearst 1992a, Hearst 1998).</caption>
<colgroup>
<col style="width: 40%" />
<col style="width: 59%" />
</colgroup>
<thead>
<tr class="header">
<th>NP {, NP}* {,} (and|or) other NP<sub>H</sub></th>
<th>temples, treasuries, and other important <strong>civic buildings</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NP<sub>H</sub> such as {NP,}* {(or|and)} NP</td>
<td><strong>red algae</strong> such as Gelidium</td>
</tr>
<tr class="even">
<td>such NPH as {NP,}* {(or|and)} NP</td>
<td>such <strong>authors</strong> as Herrick, Goldsmith, and Shakespeare</td>
</tr>
<tr class="odd">
<td>NP<sub>H</sub> {,} including {NP,}* {(or|and)} NP</td>
<td><strong>common-law countries</strong>, including Canada and England</td>
</tr>
<tr class="even">
<td>NP<sub>H</sub> {,} especially {NP}* {(or|and)} NP</td>
<td><strong>European countries</strong>, especially France, England, and Spain</td>
</tr>
</tbody>
</table>
<span id="more"></span>
<p>图17.5显示了Hearst(1992a, 1998)提出的五种模式，用于推断上下位关系；我们用NP<sub>H</sub>表示parent/hyponym。现代版本的基于模式的方法通过增加命名实体约束来扩展它。例如，如果我们的目标是回答关于“谁在哪个组织中担任什么职务”的问题，我们可以使用如下模式。</p>
<p><img src="https://i.loli.net/2021/03/18/iyZINETdVrXG3cn.png" width="500"/></p>
<p>手工构建的模式具有高精度的优势，并且可以针对特定领域进行定制。但另一方面，它们通常是低召回率的，而且如果要创建所有可能的模式，工作量很大。</p>
<h2 id="通过监督学习进行关系抽取">17.2.2 通过监督学习进行关系抽取</h2>
<p>监督机器学习的关系抽取方法遵循的是一种大家现在应该很熟悉的方案。选择一组固定的关系和实体，用这些关系和实体手动标注训练语料，然后用标注后的文本来训练分类器标注一个未出现过的测试集。</p>
<p>最直接的方法，如图17.6所示：(1) 找到命名的实体对(通常在同一个句子中)；(2) 为每对实体进行关系分类。分类器可以使用任何有监督的技术（逻辑回归、RNN、Transformer、随机森林等）。</p>
<p>一个可选的中间过滤分类器可以用来加快处理速度，通过二元决策判定给定的一对实体是否相关（通过任何关系）。它的训练对象是直接从标注语料中的所有关系中抽取的正例，以及从未标注关系的句内实体对中生成的负例。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">FINDRELATIONS</span><span class="params">(words)</span> returns relations</span></span><br><span class="line">	relations←nil</span><br><span class="line">	entities←FINDENTITIES(words)</span><br><span class="line">	forall entity pairs &lt;e1, e2&gt; in entities <span class="keyword">do</span></span><br><span class="line">	<span class="keyword">if</span> RELATED?(e1, e2)</span><br><span class="line">		relations←relations+CLASSIFYRELATION(e1, e2)</span><br></pre></td></tr></table></figure>
<p>基于特征的监督关系分类器。让我们考虑基于特征的分类器（如逻辑回归或随机森林）的样本特征，对这个句子中的<em>美国航空公司</em>（Mention 1，或M1）和<em>Tim Wagner</em>（Mention 2，M2）之间的关系进行分类。</p>
<blockquote>
<p>(17.5) <strong>American Airlines</strong>, a unit of AMR, immediately matched the move, spokesman <strong>Tim Wagner</strong> said</p>
</blockquote>
<p>这些包括词的特征（如词嵌入、one-hot、无论是否抽取词干<em>stemming</em>）：</p>
<blockquote>
<ul>
<li><p>M1和M2的核心词（headwords）及其连接 （concatenation）</p>
<p><strong>Airlines Wagner Airlines-Wagner</strong></p></li>
<li><p>M1和M2的词袋和ngram</p>
<p><strong>American, Airlines, Tim, Wagner, American Airlines, Tim Wagner</strong></p></li>
<li><p>特定位置的词和ngram</p>
<p>M2: <strong>-1 spokesman</strong></p>
<p>M2: <strong>+1 said</strong></p></li>
<li><p>M1和M2之间的词袋或ngram</p>
<p><strong>a, AMR, of, immediately, matched, move, spokesman, the, unit</strong></p></li>
</ul>
</blockquote>
<p>以及<strong>命名实体</strong>特征：</p>
<blockquote>
<ul>
<li><p>命名实体类型及其连接（concatenation）</p>
<p>M1: <strong>ORG</strong>, M2: <strong>PER</strong>, M1M2: <strong>ORG-PER</strong></p></li>
<li><p>M1和M2的实体级别（从集合NAME、NOMINAL、PRONOUN中选择）。</p>
<p>M1: <strong>NAME</strong> [it or he would be <strong>PRONOUN</strong>]</p>
<p>M2: <strong>NAME</strong> [the company would be <strong>NOMINAL</strong>]</p></li>
<li><p>参数之间的实体数量（在本例中，对于AMR）。</p></li>
</ul>
</blockquote>
<p><strong>句法结构</strong>是一个有用的信号，通常表示为依存关系或穿越实体之间的树的成分<strong>句法路径</strong>。</p>
<blockquote>
<ul>
<li><p>M1和M2之间的成分路径</p>
<p><strong>NP ↑ NP ↑ S ↑ S ↓ NP</strong></p></li>
<li><p>依存树路径</p>
<p><strong>Airlines ←<sub>subj</sub> matched ←<sub>comp</sub>said →<sub>subj</sub> Wagner</strong></p></li>
</ul>
</blockquote>
<p><strong>神经监督关系分类器</strong> 关系抽取的神经模型同样将这个任务视为监督分类。让我们考虑一个应用于TACRED关系抽取数据集和任务的典型系统 (Zhang et al., 2017) 。在TACRED中，提供一个句子和其中的两个spans：主语，即人或组织；宾语，即任何其他实体。任务是从42个TAC关系中分配一个关系（或者没有关系）。</p>
<p>一个典型的Transformer-encoder算法，如图17.7所示，简单的说就是把一个像BERT这样的预训练编码器，在句子表示的基础上增加一个线性层（例如BERT [CLS] token），这个线性层被微调为1-of-N分类器，以分配43个标注中的一个。BERT编码器的输入是部分去词汇化的；主语和宾语实体在输入中被其NER标注所取代。这有助于保持系统不会对单个词项过度拟合 (Zhang et al., 2017) 。当使用BERT类型的Transformers进行关系抽取时，使用类似RoBERTa (Liu et al., 2019) 或SPANbert (Joshi et al., 2020) 这样的BERT版本会很有帮助，这些版本没有用[SEP]标记分隔的两个序列，而是将单个长序列的句子形成输入。</p>
<p><img src="https://i.loli.net/2021/03/18/mpbfZs9QJoPnv7a.png" width="700"/></p>
<p>一般来说，如果测试集与训练集足够相似，并且有足够多的手工标注数据，监督关系抽取系统可以获得很高的准确率。但对一个庞大的训练集进行标注是非常昂贵的，而且监督模型也很脆弱：它们不能很好地泛化到不同的文本类型。由于这个原因，关系抽取的很多研究都集中在我们接下来要讲的半监督和无监督方法上。</p>
<h2 id="通过-bootstrapping-进行半监督关系抽取">17.2.3 通过 Bootstrapping 进行半监督关系抽取</h2>
<p>监督机器学习假设我们有大量的标注数据。不幸的是，这很昂贵。但假设我们只有一些高精度的<u>种子模式</u>（seed patterns），就像第17.2.1节中的那些，或者可能有一些<u>种子元组</u>（seed tuples）。这已经足够boostrap一个分类器了！<u>Bootstrapping</u>的过程是获取种子对中的实体，然后找到包含这两个实体的句子（通过网络或者我们使用的任何数据集）。从所有这些句子中，我们抽取并归纳实体周围的上下文，以学习新的模式。图17.8给出了一个基本算法。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">BOOTSTRAP</span><span class="params">(Relation R)</span> returns <span class="keyword">new</span> relation tuples</span></span><br><span class="line">	tuples←Gather a set of seed tuples that have relation R</span><br><span class="line">	iterate</span><br><span class="line">		sentences←find sentences that contain entities in tuples</span><br><span class="line">		patterns←generalize the context between <span class="keyword">and</span> around entities in sentences</span><br><span class="line">		newpairs←use patterns to grep <span class="keyword">for</span> more tuples</span><br><span class="line">		newpairs←newpairs with high confidence</span><br><span class="line">		tuples←tuples + newpairs</span><br><span class="line">	<span class="keyword">return</span> tuples</span><br></pre></td></tr></table></figure>
<p>假设我们需要创建一个航空公司/枢纽对的列表，我们只知道Ryanair在Charleroi有一个枢纽。我们可以使用这个种子事实来发现新的模式，在我们的语料库中找到这个关系的其他提及。我们搜索Ryanair、Charleroi和hub在某种程度上接近的术语。也许我们会发现以下一组句子。</p>
<blockquote>
<p>(17.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all weekend flights out of the airport.</p>
<p>以沙勒罗瓦为枢纽的低价航空公司Ryanair取消了所有周末从机场起飞的航班。</p>
<p>(17.7) All flights in and out of Ryanair’s hub at Charleroi airport were grounded on Friday...</p>
<p>所有进出沙勒罗伊机场枢纽的航班都在周五停飞。</p>
<p>(17.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000 passengers had already been affected.</p>
<p>瑞安航空的核心枢纽查勒罗伊的一位发言人估计，已经有8000名乘客受到影响。</p>
</blockquote>
<p>从这些结果中，我们可以利用实体提及之间的上下文单词、提及一之前的词、提及二之后的词，以及两个提及的命名实体类型，还有其他可能的特征，来抽取如下的通用模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F; [ORG], which uses [LOC] as a hub &#x2F; </span><br><span class="line">&#x2F; [ORG]’s hub at [LOC] &#x2F; </span><br><span class="line">&#x2F; [LOC], a main hub for [ORG] &#x2F;</span><br></pre></td></tr></table></figure>
<p>这些新的模式可以继而用来搜索更多的元组。Bootstrapping系统还为新的元组分配了<u>置信度值</u>（confidence values），以避免<u>语义漂移</u>（semantic drift）。在语义漂移中，一个错误的模式会导致错误元组的引入，而这些元组又会导致不正确的模式的建立，使得抽取关系的意义发生漂移。</p>
<p>比如下面的例子。</p>
<blockquote>
<p>(17.9) Sydney has a ferry hub at Circular Quay.</p>
<p>(17.9) 悉尼在环形码头有一个轮渡枢纽。</p>
</blockquote>
<p>如果被接受为正例，这个表达式可能导致元组<span class="math inline">\(&lt;Sydney,CircularQuay&gt;\)</span>的错误引入。基于这个元组的模式可能会将更多的错误传播到数据库中。</p>
<p>模式的置信度值基于两个因素的平衡：模式相对于当前元组集的表现，以及模式就在文档集中产生的匹配数量而言的生产率。更正式的表达是，给定一个文档集 <span class="math inline">\(D\)</span>，一个当前的元组集 <span class="math inline">\(T\)</span>，以及一个引入的模式 <span class="math inline">\(p\)</span>，我们需要跟踪两个因素：</p>
<ul>
<li><p><span class="math inline">\(hits(p)\)</span>：在<span class="math inline">\(D\)</span>中查找时，<span class="math inline">\(p\)</span>所匹配的<span class="math inline">\(T\)</span>中的元组集</p></li>
<li><p><span class="math inline">\(finds(p)\)</span>：<span class="math inline">\(p\)</span>在<span class="math inline">\(D\)</span>中找到的元组的总集</p></li>
</ul>
<p>下面的公式平衡了这些考虑因素 (Riloff and Jones, 1999)： <span class="math display">\[
\operatorname{Conf}_{R \log F}(p)=\frac{|\operatorname{hits}(p)|}{|\operatorname{finds}(p)|} \log (|\operatorname{finds}(p)|)
\]</span> 这个度量标准一般经过标准化处理，产生一个概率。我们可以通过结合<span class="math inline">\(D\)</span>中与该元组相匹配的所有模式<span class="math inline">\(P&#39;\)</span>中支持该元组的证据来评估所提出的新元组的置信度 (Agichtein and Granoisy-or vano, 2000)。结合这种证据的一种方法是<u>noisy-or</u>技术。假设一个给定的元组是由<span class="math inline">\(P\)</span>中的一个模式子集支持的，每个模式都有自己的置信度，正如上面的度量所示。在noisy-or模型中，我们做了两个基本假设。首先，如果一个提出的元组是假的，它的所有支持模式一定是错误的；其次，它们各自失败的来源都是独立的。如果我们不严格地将我们的置信度视为概率，那么任何单个模式<span class="math inline">\(p\)</span>失败的概率为<span class="math inline">\(1-Conf(p)\)</span>；一个元组的所有支持模式出错的概率是它们单个失败概率的乘积，因此我们对一个新元组的置信度有以下公式： <span class="math display">\[
\operatorname{Conf}(t)=1-\prod_{p \in P^{\prime}}(1-\operatorname{Conf}(p))
\]</span> 在bootstrapping过程中，为接受新的模式和元组设置保守的置信度阈值，有助于防止系统偏离目标关系。</p>
<h2 id="远程监督式关系抽取">17.2.4 远程监督式关系抽取</h2>
<p>虽然手工标注文本的关系标注成本很高，但是有一些方法可以找到间接的训练数据来源。<u>远程监督</u>方法 <em>distant supervision</em> (Mintz et al., 2009) 结合了bootstrapping和监督学习的优点。远程监督不是只用少量的种子，而是使用一个大型数据库来获取大量的种子实例，从所有这些例子中创建大量的噪声模式特征（noisy pattern features），然后将它们结合在一个监督分类器中。例如，假设我们要学习人与出生城市之间的出生地关系。在基于种子的方法中，我们一开始可能只有5个例子。但基于维基百科的数据库，如<a href="https://en.wikipedia.org/wiki/DBpedia">DBPedia</a>或<a href="https://en.wikipedia.org/wiki/Freebase_(database)">Freebase</a>，有数万个许多关系的例子；包括超过10万个出生地的例子（<span class="math inline">\(&lt;Edwin Hubble，Marshfield&gt;\)</span>，<span class="math inline">\(&lt;Albert Einstein，Ulm&gt;\)</span>等）。下一步是在大量的文本上运行命名实体标注器-——Mintz et al. (2009) 使用了维基百科上的80万篇文章，并抽取出所有具有与元组相匹配的两个命名实体的句子，比如：</p>
<blockquote>
<p>...Hubble was born in Marshfield...</p>
<p>...Einstein, born (1879), Ulm...</p>
<p>...Hubble’s birthplace in Marshfield...</p>
</blockquote>
<p>现在可以从这些数据中抽取训练实例，每个元组&lt;关系，实体1，实体2&gt;都有一个相同的训练实例。因此，每个如下的元组都会有一个训练实例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;born-in, Edwin Hubble, Marshfield&gt;</span><br><span class="line">&lt;born-in, Albert Einstein, Ulm&gt;</span><br><span class="line">&lt;born-year, Albert Einstein, 1879&gt;</span><br></pre></td></tr></table></figure>
<p>然后我们可以应用基于特征或神经网络的分类。对于基于特征的分类，标准的监督关系抽取特征，比如两个提及的命名实体标注，提及之间的词和依存路径，以及相邻的词。每个元组都会有从许多训练实例中收集到的特征；像<span class="math inline">\(&lt;born-in,Albert Einstein,Ulm&gt;\)</span>这样的单个训练实例的特征向量将含有来自许多提到Einstein和Ulm的不同句子的词法和句法特征）。</p>
<p>因为远程监督有非常大的训练集，所以它也能够使用非常丰富的特征，这些特征是这些单个特征的联合体。所以我们会抽取出成千上万的模式，这些模式会将实体类型与中间的单词或依存路径结合在一起，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PER was born in LOC </span><br><span class="line">PER, born (XXXX), LOC </span><br><span class="line">PER’s birthplace in LOC</span><br></pre></td></tr></table></figure>
<p>回到我们的运行示例，对于这个句子：</p>
<blockquote>
<p>(17.12) American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said</p>
</blockquote>
<p>我们将学习丰富的连词（conjunction）特征，比如：</p>
<blockquote>
<p>M1 = ORG &amp; M2 = PER &amp; nextword=“said”&amp; path= <strong>NP ↑ NP ↑ S ↑ S ↓ NP</strong></p>
</blockquote>
<p>结果是一个监督分类器，它有大量丰富的特征集用于检测关系。由于并不是每个测试句子都会有一个训练关系，所以分类器还需要能够将一个例子标记为无关系。这个标注是通过随机选择在任何Freebase关系中都未出现的实体对，抽取它们的特征，并为每个这样的元组建立一个特征向量来训练的。最终的算法在如图17.9所示。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">function DISTANT <span class="title">SUPERVISION</span><span class="params">(Database D, Text T)</span> returns relation classifier C</span></span><br><span class="line"><span class="function">	foreach relation R</span></span><br><span class="line"><span class="function">		foreach <span class="title">tuple</span> <span class="params">(e1,e2)</span> of entities with relation R in D</span></span><br><span class="line">			sentences←Sentences in T that contain e1 and e2</span><br><span class="line">			f←Frequent features in sentences</span><br><span class="line">			observations←observations + <span class="keyword">new</span> training tuple (e1, e2, f, R)</span><br><span class="line">		C←Train supervised classifier on observations</span><br><span class="line">	<span class="keyword">return</span> C</span><br></pre></td></tr></table></figure>
<p>远程监督享有我们所探讨的每一种方法的优势。像监督分类一样，远程监督使用一个具有大量特征的分类器，并通过详细的手工创建的知识进行监督。和基于模式的分类器一样，它可以利用高精度的证据来证明实体之间的关系。事实上，远程监督系统学习到的模式就像早期关系抽取器手工构建的模式。例如Snow et al. (2005)的is-a或hypernym抽取系统使用来自WordNet的hypernym/hyponym NP对作为远程监督，然后从大量的文本中学习新的模式。他们的系统精确地产生了Hearst（1992a）最初的5个模板模式，但也导致了包括这4个模式在内的7万个额外的模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NPH like NP Many hormones like leptin... </span><br><span class="line">NPH called NP ...using a markup language called XHTML </span><br><span class="line">NP is a NPH Ruby is a programming language... </span><br><span class="line">NP, a NPH IBM, a company with a long...</span><br></pre></td></tr></table></figure>
<p>这种同时使用大量特征的能力意味着，与基于种子的系统中模式的迭代扩展不同，没有语义漂移的发生。与无监督分类一样，它不使用有标注的训练语料库，而是依赖于非常大量的无标注数据，因此对训练语料库中的体裁（genre）问题不敏感。远程监督的另一个优势是，它可以创建训练元组，以便与神经网络分类器一起使用，而神经网络分类器不需要特征。远程监督的主要问题是，它往往会产生低精度的结果，因此目前的研究重点是如何提高精度。此外，远程监督只能帮助抽取已经存在的足够大的数据库的关系。如果要在没有数据集的情况下抽取新的关系，或者新领域的关系，就必须使用纯无监督的方法。</p>
<h2 id="关系抽取的评估">17.2.6 关系抽取的评估</h2>
<p><strong>有监督</strong>的关系抽取系统是通过使用带有人类标注的gold-standard关系的测试集并计算精度、召回率和 F-measure 来评估的。标注精度和召回率要求系统正确分类关系，而未标注的方法只是衡量系统检测相关实体的能力。</p>
<p><strong>半监督</strong>和<strong>无监督</strong>方法更难评估，因为它们是从网络或大型文本中抽取全新的关系。由于这些方法使用了非常多的文本，所以通常不可能只在一个小的标注测试集上运行它们，因此不可能预先标注出一个正确的关系实例的gold set。</p>
<p>对于这些方法，可以通过从输出中随机抽取一个关系样本来（仅仅是）近似精确性，并让人检查这些关系的准确性。通常，这种方法侧重于从文本中抽取的<strong>元组</strong>，而不是关系的<strong>提及</strong>；系统不需要检测每一个关系的提及来正确评分。相反，评估是基于系统完成任务时数据库中包含的元组集。也就是说，我们想知道系统是否能发现 Ryanair 在 Charleroi 有一个枢纽；我们并不关心它发现了多少次。那么，估计的精度<span class="math inline">\(\hat{P}\)</span> 就是： <span class="math display">\[
\hat{P}=\frac{\# \text { of correctly extracted relation tuples in the sample }}{\text { total } \# \text { of extracted relation tuples in the sample. }}
\]</span> 译：P = 样本中正确提取的关系元组 / 样本中提取的关系元组总数</p>
<p>另一种能给我们提供一点召回信息的方法是计算不同召回级别的精度。假设我们的系统能够对它所产生的关系进行排序（按照概率或置信度），我们可以分别计算前1000个新关系、前10000个新关系、前100000个新关系的精度等等。在每一种情况下，我们都会从该集合中随机抽取一个样本。这将向我们展示当我们抽取越来越多的元组时，精度曲线的表现。但是没有办法直接评估召回率。</p>
<hr />
<p><strong>本章剩余内容见：《自然语言处理综论》第17章-信息抽取（下）</strong></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>Git和Github常用操作大全</title>
    <url>/git/</url>
    <content><![CDATA[<blockquote>
<p>Git是目前世界上最先进的分布式版本控制系统（没有之一）。Git的功能有版本控制（版本管理、远程仓库、分支协作）。GitHub是一个非常流行的全球代码托管平台.</p>
</blockquote>
<p>参考：<a href="https://blog.csdn.net/Python_Ai_Road/article/details/109476021">30分钟吃掉Git和GitHub常用操作</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">git init</span><br><span class="line">git status</span><br><span class="line"></span><br><span class="line">git add -A </span><br><span class="line">git add . </span><br><span class="line">git add <span class="built_in">next</span>/_config.yml</span><br><span class="line"></span><br><span class="line">git commit -m <span class="string">&quot;comment&quot;</span> </span><br><span class="line"></span><br><span class="line">git remote add origin https://github.com/XX/XX</span><br><span class="line">git push -u origin master</span><br><span class="line"></span><br><span class="line">git clone https://github.com/XX/XX  ../XX</span><br><span class="line">    </span><br><span class="line">git remote -v</span><br><span class="line">git remote rm origin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除git</span></span><br><span class="line">rm -rf .git</span><br><span class="line"></span><br><span class="line">git reset HEAD^ <span class="comment">#可以回退到上一个版本。</span></span><br><span class="line">git reset HEAD^^ <span class="comment">#可以回退到上上个版本。</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
]]></content>
      <categories>
        <category>代码</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>从小白到起飞，一站解决Atom编辑器各种骚操作</title>
    <url>/atom/</url>
    <content><![CDATA[<blockquote>
<p>一站到底解决IDE搭建问题！从基础设置、插件安装、snippets填充、到github版本控制、代码调试……本文将不断更新以求完善！</p>
</blockquote>
<h2 id="安装文本编辑器atom">安装文本编辑器Atom</h2>
<p>入坑Sublime、VSCode无数次后，还是回头选择了Atom IDE，因为它颜值惊艳、操作便捷、界面简单，除了许多编辑器都有的<strong>代码折叠</strong>和<strong>自动补全</strong>，还自带原生Markdown支持！！！这漂亮的实时预览和代码高亮真让人春心荡漾！并且，插件非常丰富！ <span id="more"></span> 于是，果断选择Atom作为本博主的<strong>创作神器</strong>，为Python开发之旅保驾护航！</p>
<p>官网一键安装：<a href="https://atom.io/">AtomSetup-x64</a></p>
<p>确认操作系统无误，点击download，打开下载好的AtomSetup-x64.exe，极速体验！</p>
<figure>
<img src="https://pic2.zhimg.com/v2-e2e305deea69303a19d71da452138945_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>缺点是启动速度不如sublime</em></p>
<h2 id="安装插件失足卡顿的惨痛经历">安装插件：失足卡顿的惨痛经历</h2>
<p>此时你一定急不可耐地冲向Install a Package，风风火火地下载了一堆插件：minimap用来预览全貌，atom-beautify用来格式化（想到令人头痛的html），file-icons小图标好可爱呀，material主题貌似很热门吼，markdown-preview-enhanced吊打我的Typora呢（Typora是我常用的md编辑器），script可以运行代码嗷，autocomplete-python自动补全呢，python-autopep8调格式也不错哟，linter-flake8检查语法错误呢，Hydrogen简直是jupyter的孪生姐妹……你在界面乐此不疲地倒腾……</p>
<p>A few hours later...</p>
<p>突然，你意识到此时的atom一片混乱卡顿缓慢，再也不是当初清纯活泼的模样！你蹙起眉，两行清泪润湿了乌黑的下眼眶！</p>
<p>一怒之下，你卸载了atom，并剿杀了一切软件残留：<a href="https://cn.compbs.com/how-uninstall-atom-windows">如何彻底删除atom</a>，<a href="https://www.coder.work/article/552966">如何更彻底地删除</a>！</p>
<h2 id="正确的打开方式是什么">正确的打开方式是什么？</h2>
<p>正文从这里开始！</p>
<p>那么，配置环境和安装插件的正确方式是什么呢？</p>
<ol type="1">
<li>打开Editor Settings，<strong>勾选Scroll Past End和Show Indent Guide，设置Tab Length为4，Font Size为20</strong>，护眼第一啦！</li>
<li>主题我喜欢atom自带的<strong>One Light，</strong>打开themes-&gt;One Light UI-&gt;Settings-&gt;<strong>Font Size设置为15</strong>，语法主题我选择<strong>Monokai</strong>。还是为了护眼！码字时还可以随时ctrl+和ctrl-调整字体大小。</li>
<li>下载插件：<strong>minimap、file-icons、atom-beautify、markdown-preview-enhanced、python-autopep8、markdown-writer</strong>。大道至简，这是我目前选择的基础组件，可能以后会更新！进入python-autopep8-&gt;Settings，勾选Format On Save。</li>
</ol>
<figure>
<img src="https://pic3.zhimg.com/v2-62a1e257adc2ca9978dee539798d3342_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>atom-material-syntax-dark语法主题也好看，只是Python语法高亮略丑，就不放了</em></p>
<p>个人认为，<strong>编辑器只需提供高效、舒适的代码体验即可</strong>，各种花里胡哨的功能比较鸡肋，反而会加重的负担。</p>
<p><a href="https://atom.io/themes/list?direction=desc&amp;sort=downloads">atom热门主题排行榜</a>：material、monokai、seti。</p>
<p><a href="https://atom.io/packages/list?direction=desc&amp;sort=stars">atom热门插件排行榜</a>：minimap、file-icons、atom-beautify、linter、script。</p>
<p><strong>下面重点来了，下载插件的方式！！！</strong></p>
<p>打开cmd，执行以下命令（apm是Atom Package Manager的缩写）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install autopep8</span><br><span class="line">apm install monokai, minimap, file-icons, atom-beautify, markdown-preview-enhanced, python-autopep8</span><br></pre></td></tr></table></figure>
<p>如果一起下载速度太慢，也可以apm install <package_name>分开下载。</p>
<p>晃悠了一杯茶的时间，已经全部done啦！Voila！</p>
<p>重启Atom，Ctrl+Shift+P打开Settings-&gt;Packages，是不是整齐陈列着我们要的插件呢？（不知为什么ctrl+逗号不能打开settings）</p>
<p>码上行动叭！！！</p>
<figure>
<img src="https://pic3.zhimg.com/v2-873f85f52366e6ab44b3eda548930102_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>这是最终的windows界面</em></p>
<p>另外，有童鞋选择github上面的源码git clone，然后cd进文件夹、npm install来安装，也不错呢~</p>
<p>对了，卸载插件的话，apm uninstall <package_name> 就好啦！</p>
<p>----------2021-03-04更新---------</p>
<h2 id="代码填充功能snippets">代码填充功能Snippets</h2>
<p>当我们需要重复使用一套模板时，不如试试<a href="https://www.jianshu.com/p/2ee34d8da142">Atom自带的Snippets代码块功能</a>，这种快捷填充，省去了重复码字的时间。打开终端输入下面指令，用atom打开snippets.cson：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">atom C:\Users\用户名\.atom\snippets.cson</span><br></pre></td></tr></table></figure>
<p>可以看到目前空空如也，在这个文件里面打“snip”，然后敲下tab键，会跳出来用于创建snippets的snippets（像套娃一样耶）。换上你想存储的代码块吧，示例如下：</p>
<figure>
<img src="https://pic1.zhimg.com/v2-d5d6ec618868382045aa1044eae6e988_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以后每次想插入代码块，直接输我们预设的“暗号”，然后按tab键即可，迅如闪电呀！</p>
<p><strong>注意</strong>：snippets只在相应语言中有效！并且，除了常见的Python、Java、JS等，<strong>其他有些语言是不支持snippets功能的</strong>，此时有两种方法：</p>
<ol type="1">
<li>通过apm install language-语言，安装相应语言的支持包；</li>
<li>将.source.语言 改成* 。</li>
</ol>
<p>我比较建议第二种方法。</p>
<p>另外，mardown中输入table、img、L等按下tab键，可以快捷插入表格、图片、链接等。</p>
<figure>
<img src="https://pic2.zhimg.com/v2-4d43fef791abc2195246b9621b37f4b9_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>成功啦，美滋滋~</p>
<h2 id="代码调试">代码调试</h2>
<p>暂时用不到，先占个坑，下次完善！</p>
<h2 id="使用github远程版本控制">使用Github远程版本控制</h2>
<p>又解锁atom连接github和git啦！赶紧更新日志！</p>
<p>话说，atom本来就是github开发的编辑器好嘛！我们来测试下好用吗！</p>
<p>Github注册无须多言吧，我创建了一个新的repository，命名blog，用来云端存储我的写作素材与稿件。这时我们看到页面提供了一系列指令：</p>
<figure>
<img src="https://pic1.zhimg.com/v2-6daf82cf178c578fd4cf4c19da68cb88_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>这里其实是blog仓库，但我为了演示又重新建了名为test的仓库</em></p>
<p>我们只需要打开windows系统的git bash，一句一句输入。我输入的是以下指令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd blog</span><br><span class="line">echo &quot;#blog&quot; &gt;&gt; README.md</span><br><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;first commit&quot;</span><br><span class="line">git branch -M main</span><br><span class="line">git remote add origin https:&#x2F;&#x2F;github.com&#x2F;MissFreak&#x2F;blog.git</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>
<p>成功后，在Atom打开blog这个文件夹（也就是你想要进行版本控制的项目），我们看到右侧显示github登录界面，提示我们打开github.atom.io，复制GitHub token到Atom的登录表单。Success！</p>
<figure>
<img src="https://pic1.zhimg.com/v2-6e1aa37602728c2eb6bdc8701b013270_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>接下来就随心所欲地增改文件，然后stage all-&gt;commit to main-&gt;pull 吧！</p>
<p>终于不用在终端敲指令，也可以和Github Desktop说拜拜啦！</p>
<figure>
<img src="https://pic4.zhimg.com/v2-713ea328565e6babf7e9559c20bc2e8b_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>参考资料：</strong><a href="https://flight-manual.atom.io/using-atom/sections/github-package/">Atom Documentation-GitHub package</a></p>
<h2 id="你可能不知道的快捷键">你可能不知道的快捷键</h2>
<table>
<thead>
<tr class="header">
<th>功能</th>
<th>快捷键</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>在所有项目文件中查找字符串</td>
<td>Ctrl + Shift + f</td>
</tr>
<tr class="even">
<td>打开导航栏的File、Edit、View等</td>
<td>Alt+导航栏首字母</td>
</tr>
<tr class="odd">
<td>向上/下移动该行</td>
<td>Ctrl + up/down</td>
</tr>
</tbody>
</table>
<p><a href="https://yanyinhong.github.io/2017/07/23/Atom-keyboard-shortcuts/">Windows环境下的Atom快捷键</a></p>
<p><a href="https://www.itread01.com/content/1549978931.html">mac下Atom编辑器快捷键大全</a></p>
<figure>
<img src="https://pic4.zhimg.com/v2-e0ed2bebdccf67e44a7c2bb61ff7b26b_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><em>在项目所有文件中搜索“正则”</em></p>
<p>----------2021-03-04更新---------</p>
<h2 id="html神器">HTML神器</h2>
<p>又屁颠屁颠跑来更新啦，虽然只有我一个人自娱自乐。</p>
<p>Python开发网站离不开的插件，亲测好用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apm install atom-html-preview, pigments, highlight-selected, autoclose-html-plus, color-picker, color-tabs</span><br></pre></td></tr></table></figure>
<p>效果如下：</p>
<figure>
<img src="https://pic3.zhimg.com/v2-692783c713218dc64a68c8faca88c096_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="markdown神器">Markdown神器</h2>
<p>还有，大家赶紧把这款神器mardown-preview-enhanced用起来吧，功能齐全到不可想象！<strong>可以折叠一级、二级、三级标题，专注于当前标题下的内容！（但是我刚用了貌似会卡）</strong></p>
<p>可以运行代码，并渲染运行结果：</p>
<figure>
<img src="https://pic3.zhimg.com/v2-12f773fb3a40b67afa0a432aa3dbc9b2_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>各种流程图、结构图：</p>
<figure>
<img src="https://pic2.zhimg.com/v2-9c4fd8ff9abd02d7cbb3f6566bf0549d_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>甚至制作幻灯片：</p>
<figure>
<img src="https://pic4.zhimg.com/v2-cdcf68324e69b094ea314250405b9e9b_b.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>还有太多亮瞎眼的操作不一一列举，详情请戳：https://shd101wyy.github.io/markdown-preview-enhanced/#/zh-cn/</p>
<p>VScode也可以用~真的很想把sublime扔进垃圾桶了！虽然sublime打开速度确实快！</p>
<h2 id="terminal-神器">Terminal 神器</h2>
<p>我又发现了一款终端程序包，terminal-plus！可以更改主题、查看终端当前运行的命令进程！用颜色标记状态图标和排序！从文本编辑器插入并运行文本！还有太多功能大家自己查阅！再次挖到宝贝啦！</p>
<p><a href="https://github.com/jeremyramin/terminal-plus">jeremyramin/terminal-plus</a></p>
<figure>
<img src="https://pic1.zhimg.com/v2-a579e162c502f89da0d9036ac21abfb8_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>总结：我目前安装的插件和主题如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── atom-beautify@0.33.4</span><br><span class="line">├── atom-html-preview@0.2.6</span><br><span class="line">├── autoclose-html-plus@0.27.2</span><br><span class="line">├── color-picker@2.3.0</span><br><span class="line">├── color-tabs@0.1.8</span><br><span class="line">├── file-icons@2.1.46</span><br><span class="line">├── highlight-selected@0.17.0</span><br><span class="line">├── markdown-preview-enhanced@0.18.8</span><br><span class="line">├── markdown-writer@2.11.11</span><br><span class="line">├── minimap@4.39.9</span><br><span class="line">├── monokai@0.27.0</span><br><span class="line">├── pigments@0.40.6</span><br><span class="line">├── python-autopep8@0.1.3</span><br><span class="line">└── python-tools@0.6.9</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>atom</tag>
        <tag>editor</tag>
      </tags>
  </entry>
  <entry>
    <title>语料库资源大全</title>
    <url>/corpus/</url>
    <content><![CDATA[<h1 id="现存语料库">现存语料库</h1>
<h2 id="词语">词语</h2>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">条目</th>
<th style="text-align: left;">长度</th>
<th style="text-align: left;">数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">汉字</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">16142</td>
</tr>
<tr class="even">
<td style="text-align: left;">词语</td>
<td style="text-align: left;">2~3</td>
<td style="text-align: left;">264434</td>
</tr>
<tr class="odd">
<td style="text-align: left;">成语</td>
<td style="text-align: left;">4~5</td>
<td style="text-align: left;">31648</td>
</tr>
<tr class="even">
<td style="text-align: left;">歇后语</td>
<td style="text-align: left;">6+</td>
<td style="text-align: left;">14032</td>
</tr>
</tbody>
</table>
<p>新华网成语、歇后语和词语：https://github.com/pwxcoo/chinese-xinhua</p>
<p>查询接口：https://github.com/netnr/zidian</p>
<p>近反义词：https://github.com/guotong1988/chinese_dictionary</p>
<span id="more"></span>
<h2 id="句子">句子</h2>
<p>微博短句、句子迷</p>
<h3 id="idea">idea</h3>
<p>除了欣赏，也可以收集美句美文，并表达不同心情和志趣。</p>
<h2 id="文章">文章</h2>
<p>新闻：https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset</p>
<p>微信公众号语料库：https://github.com/nonamestreet/weixin_public_corpus</p>
<p>每行文章，是JSON格式，名称是微信公众号名字，帐户是微信公众号ID，标题是译文，内容是正文。</p>
<h3 id="idea-1">idea</h3>
<p>文本分类！主题建模！可以按照风格分类！</p>
<h2 id="名字">名字</h2>
<p>豆瓣影视和书籍的名字</p>
<p>微信、知乎文章标题</p>
<p>书名号</p>
<h3 id="idea-2">idea</h3>
<p>抓取所有名字，动态展现，且按照不同风格分类！就叫标题网！</p>
<p>功能：帮助大家取名、让标题更吸引人、更有文采！并研究怎样的文章更吸引读者。</p>
<h2 id="诗词">诗词</h2>
<p>中华古诗词数据库：</p>
<p>https://github.com/chinese-poetry/chinese-poetry</p>
<h2 id="维基百科">维基百科</h2>
<p>信息检索？</p>
<h2 id="学术语料">学术语料</h2>
<p>信息检索？</p>
<h1 id="抓取原则">抓取原则</h1>
<h2 id="频率tf-idf">频率tf-idf</h2>
<h2 id="打分">打分</h2>
<ul>
<li>通过文章和书籍的阅读量和点赞</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>语料库</category>
      </categories>
      <tags>
        <tag>corpus</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第14章-依存分析（上）</title>
    <url>/dependency-parsing-1/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/14.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<p>前两章的重点是<strong>上下文无关语法</strong>（context-free grammars）<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>，及其用于自动生成基于成分的表征。 我们在这里介绍另一种称为<strong>依存语法</strong>（dependency grammars）的语法形式主义派系，它在当代语音和语言处理系统中极其重要。 在这些形式主义中，短语成分（phrasal constituents）和短语结构规则（phrase-structure rules）并不直接发挥作用。相反，一个句子的句法结构，仅根据句中单词（或词元lemmas）以及单词间存在的一组关联的有向二元语法关系来描述。</p>
下图展示了标准的依存分析风格的句法图示。
<center>
<img src="https://i.loli.net/2021/03/17/3XaE6umlQIgpZrs.png"  alt="" width="700" />
</center>
<p>在句子上方，用从<strong>头部</strong>（heads）到<strong>依存项</strong>（dependents）的有向的标记弧来表示单词之间的关系。我们称之为<strong>类型依存结构</strong>（typed dependency structure），因为标签是从固定的语法关系清单中提取的。它还包括一个根（root）节点，显式地标记句法树的根，即整个结构的中心。</p>
<span id="more"></span>
<p>图14.1显示了与第12章中给出的相应短语结构分析相同的依存分析及树形结构。注意依存分析中没有对应短语成分或词汇类别的节点；<strong>其内部结构仅由句子中词汇项之间的定向关系组成。</strong>这些关系<strong>直接编码重要信息</strong>，这些信息往往隐藏在更复杂的短语结构分析中。例如，动词prefer的<strong>论元</strong>（arguments）在依存结构中直接链接到它，而在短语结构树中它们与主动词的连接不那么紧密。类似地，flight的修饰语morning和Denver在依存结构中直接与之链接。</p>
<center>
<img src="https://i.loli.net/2021/03/17/c1yRPAQCefvESZW.png"  alt="" width="700" />
</center>
<p><strong>依存语法的一个主要优势是能够处理形态丰富、词序相对自由（free word order）的语言。</strong>例如，捷克语的词序可能比英语灵活得多；宾语可能出现在位置状语之前或之后。短语结构语法会需要为解析树中每个可能出现这样一个状语短语的位置单独制定一条规则。基于依存关系的方法只用一种连接类型来表示这种特殊的状语关系。因此，依存分析的方法抽象出了词序信息，只表示解析所需的信息。</p>
<p>使用依存分析的另一个实际性的动机是，<strong>头部-依存（head-dependent）关系</strong>提供了一种近似于谓词及其论元之间的语义关系，这使得它们对指代消歧、自动问答和信息提取之类的许多应用都能产生直接的帮助。基于成分（constituent-based）的语法解析也提供了类似的信息，但通常必须通过诸如第12章讨论的中心语规则等技术从树中提炼出来。</p>
<p>在下面的章节，我们将更详细地讨论依存分析中使用的关系清单，以及这些依存结构的形式基础。然后我们将继续讨论用于自动生成这些结构的主流算法派系。最后，我们将讨论如何评估依存分析器，并指出它们在语言处理中应用的一些方式。</p>
<h1 id="依存关系">14.1 依存关系</h1>
<p>传统语言学的语法关系概念为构成这些依存结构的二元语法关系提供了基础。这些头关系（head relations）的参数由一个<strong>头部</strong>（heads）和一个<strong>依存项</strong>（dependents）组成。在第12章和附录C中，我们已经在成分结构的语境下中讨论过头部的依存项这个概念。在那里，一个成分的头部是一个更大成分的中心组织词（例如名词短语中的关键名词，或动词短语中的动词）。成分中其余的词都是其头部的直接或间接的依存项。在基于依存关系的方法中，通过直接将头部与紧靠头部的词连接起来，绕过成分结构，使头部-依存关系变得明确。</p>
<p>除了指定头部-依存对，依存语法还允许我们根据依存项相对于头部的作用，进一步划分语法关系种类或<strong>语法功能</strong>（grammatical function）。我们熟悉的主语、直接宾语和间接宾语等概念都是可能会想到的关系种类。在英语中，这些概念虽然与一个词在句中的位置和成分类型密切相关，但不起决定性作用，因此与短语结构树中提供的信息重复累赘。然而，在更灵活的语言中，直接编码这些语法关系中的信息是至关重要的，因为基于短语的成分句法提供的帮助很小。</p>
<p>毫不奇怪，语言学家们已经发明了远远超出我们熟悉的主语和宾语概念的关系分类学。虽然不同的理论之间大相径庭，但有足够的共性使其发展出一个在计算上有用的标准。<strong>通用依存关系</strong>（Universal Dependencies）项目（Nivre et al.，2016）提供了一个语言驱动的、利于计算的、跨语言适用的依存关系清单。</p>
<table>
<caption>图14.2 通用依存关系集中的部分依存关系 (de Marneffe et al., 2014)</caption>
<thead>
<tr class="header">
<th>Clausal Argument Relations</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NSUBJ</td>
<td>Nominal subject</td>
</tr>
<tr class="even">
<td>DOBJ</td>
<td>Direct object</td>
</tr>
<tr class="odd">
<td>IOBJ</td>
<td>Indirect object</td>
</tr>
<tr class="even">
<td>CCOMP</td>
<td>Clausal complement</td>
</tr>
<tr class="odd">
<td>XCOMP</td>
<td>Open clausal complement</td>
</tr>
<tr class="even">
<td><strong>Nominal Modifier Relations</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="odd">
<td>NMOD</td>
<td>Nominal modifier</td>
</tr>
<tr class="even">
<td>AMOD</td>
<td>Adjectival modifier</td>
</tr>
<tr class="odd">
<td>NUMMOD</td>
<td>Numeric modifier</td>
</tr>
<tr class="even">
<td>APPOS</td>
<td>Appositional modifier</td>
</tr>
<tr class="odd">
<td>DET</td>
<td>Determiner</td>
</tr>
<tr class="even">
<td>CASE</td>
<td>Prepositions, postpositions and other case markers</td>
</tr>
<tr class="odd">
<td><strong>Other Notable Relations</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td>CONJ</td>
<td>Conjunct</td>
</tr>
<tr class="odd">
<td>CC</td>
<td>Coordinating conjunction</td>
</tr>
</tbody>
</table>
<p>图14.2显示了这项工作中的关系子集。图 14.3 提供了一些例句来说明选定的关系。通用依存方案中所有关系的来由超出了本章的范围，但常用关系的核心集可以分成两组：描述与谓语（通常是动词）有关的句法角色的子句关系（clausal relations），以及对头部修饰词进行分类的修饰关系（modifier relations）。</p>
<table>
<caption>Figure 14.3 Examples of core Universal Dependency relations.</caption>
<thead>
<tr class="header">
<th>Relation</th>
<th>Examples with <em>head</em> and dependent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NSUBJ</td>
<td><strong>United</strong> <em>canceled</em> the flight.</td>
</tr>
<tr class="even">
<td>DOBJ</td>
<td>United <em>diverted</em> the <strong>flight</strong> to Reno.</td>
</tr>
<tr class="odd">
<td></td>
<td>We <em>booked</em> her the first <strong>flight</strong> to Miami.</td>
</tr>
<tr class="even">
<td>IOBJ</td>
<td>We <em>booked</em> <strong>her</strong> the flight to Miami.</td>
</tr>
<tr class="odd">
<td>NMOD</td>
<td>We took the <strong>morning</strong> <em>flight</em>.</td>
</tr>
<tr class="even">
<td>AMOD</td>
<td>Book the <strong>cheapest</strong> <em>flight</em>.</td>
</tr>
<tr class="odd">
<td>NUMMOD</td>
<td>Before the storm JetBlue canceled <strong>1000</strong> <em>flights</em>.</td>
</tr>
<tr class="even">
<td>APPOS</td>
<td><em>United</em>, a <strong>unit</strong> of UAL, matched the fares.</td>
</tr>
<tr class="odd">
<td>DET</td>
<td><strong>The</strong> <em>flight</em> was canceled.</td>
</tr>
<tr class="even">
<td></td>
<td><strong>Which</strong> <em>flight</em> was delayed?</td>
</tr>
<tr class="odd">
<td>CONJ</td>
<td>We <em>flew</em> to Denver and <strong>drove</strong> to Steamboat.</td>
</tr>
<tr class="even">
<td>CC</td>
<td>We flew to Denver <strong>and</strong> <em>drove</em> to Steamboat.</td>
</tr>
<tr class="odd">
<td>CASE</td>
<td>Book the flight <strong>through</strong> <em>Houston</em>.</td>
</tr>
</tbody>
</table>
<p>参考以下例句，子句关系NSUBJ和DOBJ分别表示主语和谓语cancel的直接宾语，而NMOD、DET和CASE关系表示名词flights和Houston的修饰语。</p>
<p><img src="https://i.loli.net/2021/03/17/XgwzFkLVSdaIfDj.png" /></p>
<h1 id="依存形式主义">14.2 依存形式主义</h1>
<p>在最普通的形式中，我们讨论的依存关系结构仅仅是有向图，即由一组顶点<span class="math inline">\(V\)</span>和一组有序的顶点对<span class="math inline">\(A\)</span>组成的结构<span class="math inline">\(G=(V, A)\)</span>，我们称之为弧（arcs）。</p>
<p>大多数情况下，我们假设顶点集<span class="math inline">\(V\)</span>完全对应于给定句子中单词的集合。然而，它们也可能对应于标点符号，或者当处理形态复杂的语言时，顶点集可能由词干和词缀组成。弧线集<span class="math inline">\(A\)</span>捕获了<span class="math inline">\(V\)</span>中元素之间的头部-依存关系和语法功能关系。</p>
<p>对这些依存结构的进一步限制是针对底层语法理论或形式主义的。其中比较常见的限制是，这些结构必须是连接的、有一个指定的根节点，并且是无环或平面的。与本章讨论的解析方法最相关的是对有根树的常见的、以计算为目的的限制。也就是说，<strong>依存树</strong>（dependency tree）是一个满足以下约束的有向图。</p>
<ol type="1">
<li>有一个指定的根结点，它没有传入弧。</li>
<li>除根节点外，每个顶点恰好有一个传出弧。</li>
<li>从根节点到<span class="math inline">\(V\)</span>中的每个顶点有一条唯一的路径。</li>
</ol>
<p>综上所述，这些约束条件保证了每个词都有一个头，依存结构是连接的，并且有唯一的根节点，从这个根节点可以沿着唯一的定向路径到句子中的每个词。</p>
<h2 id="投射性">14.2.1 投射性</h2>
<p><strong>投射性</strong>（projectivity）的概念施加了一个额外的约束条件，这个约束条件来自于输入（input）中词的顺序。如果在句子中<strong>存在一条从头部到位于头部和依存项之间的每个词的路径</strong>，那么就说这条从头部到依存项的弧线具有投射性。如果组成依存树的所有弧线都有投射性，那么就可以说它是投射的。到目前为止，我们看到的所有依存树都是投射的。然而，有许多完全合乎规则的结构会生成非投射树，特别是在词序相对灵活的语言中。</p>
<p>请看下面的例子。</p>
<p><img src="https://i.loli.net/2021/03/17/2Jicfx1AThkBFId.png" /></p>
<p>在这个例子中，从flight到它的修饰词was的弧线是非投射的，因为从flight到中间的单词this和morning没有路径。正如我们从这张图中看到的，投射性（和非投射性）可以通过画树的方式来检测。<strong>如果能画出没有交叉边的依存树，那么它就是投射性的。</strong>在这里，如果不跨越连接morning和它的头部的弧线，就无法将flight和它的依存项was联系起来。</p>
<p>我们对投射性的关注来自于两个相关的问题。首先，最广泛使用的英语依存关系树库是通过使用中心语查找规则从短语结构树库中自动导出的（第12章）。以这种方式生成的树是保证投射性的，因为它们是由上下文无关语法生成的。第二，最广泛使用的一系列解析算法存在计算上的限制。第14.4节中讨论的基于转换的方法只能生成投射树，因此任何具有非投射结构的句子都必然会出错。这个限制是第14.5节中描述的更灵活的基于图的解析方法的动机之一。</p>
<h1 id="依存树库">14.3 依存树库</h1>
<p>与基于成分的方法一样，树库（treebanks）在依存分析器（dependency parsers）的开发和评估中起着至关重要的作用。<strong>依存树库</strong>（dependency treebanks）的创建方法与第12章中讨论的方法类似——让人类标注者直接为给定的语料库生成依存结构，或者使用自动解析器（automatic parsers ）提供初始解析，然后让标注者手动修正这些解析器。我们也可以用一个确定性过程<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>（deterministic process）通过标注中心语规则将现有的基于成分的树库翻译成依存树。</p>
<p>大多形态丰富的语言（如捷克语、印地语和芬兰语）都已经建立了直接标注的依存树库用于依存分析，其中捷克语的Prague依存树库（Bejcek et al., 2013）是最著名的工作。主流英语依存树库主要是从现有资源中提取出来的，比如Penn树库的华尔街日报部分（Marcus等人，1993）。最近的OntoNotes项目（Hovy et al. 2006, Weischedel et al. 2011）扩展了这种方法，超越了传统的新闻文本，涵盖了英语、汉语和阿拉伯语的电话对话、网络日志、usenet新闻组、广播和脱口秀。</p>
<p>从成分结构到依存结构的翻译过程有两个子任务：识别结构中所有的头部-依存关系，以及正确识别这些关系的种类。第一个任务主要依赖于第12章中讨论的中心语规则（head rules）的使用，这些规则最早是为词汇化概率解析器（ lexicalized probabilistic parsers）而开发的(Magerman 1994, Collins 1999, Collins 2003)。下面是Xia和Palmer（2001）提出的一个简单有效的算法。</p>
<ol type="1">
<li>使用适当的中心语规则，标记短语结构中每个节点的头部子节点。</li>
<li>在依存结构中，让每个非头部子节点的头部依存于头部子节点的头部。</li>
</ol>
<p>当一个短语结构解析包含了额外的语法关系和函数标签形式的信息时，如在Penn Treebank的情况下，这些标签可以用来标记生成的树的边。当应用于图14.4中的解析树时，这种算法将产生例14.4中的依存结构。这些提取方法的主要缺点是它们受到原始结构树中存在的信息的限制。其中最重要的问题是未能将形态学信息与短语结构树整合在一起，不能轻易地表示非宾语结构，以及大多数名词短语缺乏内部结构，这反映在大多数树库语法通常所使用的平面规则中。由于这些原因，除了英语之外，大多数依存树库都是直接靠人类标注者开发的。</p>
<hr />
<p><strong>本章剩余内容见：<a href="http://nlpcourse.cn/dependency-parsing-2/">《自然语言处理综论》第14章-依存分析（中）</a></strong></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>也称为短语结构语法 (phrase-structure grammar)<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>译者注：对应<a href="https://baike.baidu.com/item/随机过程">随机过程（<em>Stochastic Process</em>）</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>dependency</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>英文依存句法分析</title>
    <url>/dependency-parsing/</url>
    <content><![CDATA[<blockquote>
<p>依存分析是根据句子中单词之间的依存关系来分析句子语法结构的过程。</p>
</blockquote>
<p>在依存分析中，各种标签代表了一个句子中两两词语之间的关系。例如，在'rainy weather'这个短语中，rainy是修饰名词weather的形容词。weather -&gt; rainy 形成了依存关系，其中weather是head（中心词），而rainy则是dependent（依赖）。该依存关系用amod标签表示，即形容词修饰语。我们可以用依存关系箭头标注语法关系： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     root</span><br><span class="line">      |</span><br><span class="line">      | +-------dobj---------+</span><br><span class="line">      | |                    |</span><br><span class="line">nsubj | |   +------det-----+ | +-----nmod------+</span><br><span class="line">+--+  | |   |              | | |               |</span><br><span class="line">|  |  | |   |      +-nmod-+| | |      +-case-+ |</span><br><span class="line">+  |  + |   +      +      || + |      +      | |</span><br><span class="line">I  prefer  the  morning   flight  through  Denver</span><br></pre></td></tr></table></figure></p>
<p>也可以表示为依存句法树（Dependency Tree Graph）：</p>
<p><img src="https://i.loli.net/2021/03/16/JtlAKsfoXhZwLjP.png" /></p>
<p>图片来源：<a href="https://zhuanlan.zhihu.com/p/66268929">CS224N笔记(五):Dependency Parsing</a></p>
<h1 id="通用依存关系">通用依存关系</h1>
<p>截至目前，UD项目的通用依存关系共有37个，这些关系的完整<a href="https://universaldependencies.org/u/dep/">列表</a>可以在这里查看并深入研究。</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Nominals</strong></th>
<th><strong>Clauses</strong></th>
<th><strong>Modifier words</strong></th>
<th><strong>Function Words</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Core arguments</strong></td>
<td><a href="https://universaldependencies.org/u/dep/nsubj.html">nsubj</a> <a href="https://universaldependencies.org/u/dep/obj.html">obj</a> <a href="https://universaldependencies.org/u/dep/iobj.html">iobj</a></td>
<td><a href="https://universaldependencies.org/u/dep/csubj.html">csubj</a> <a href="https://universaldependencies.org/u/dep/ccomp.html">ccomp</a> <a href="https://universaldependencies.org/u/dep/xcomp.html">xcomp</a></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Non-core dependents</strong></td>
<td><a href="https://universaldependencies.org/u/dep/obl.html">obl</a> <a href="https://universaldependencies.org/u/dep/vocative.html">vocative</a> <a href="https://universaldependencies.org/u/dep/expl.html">expl</a> <a href="https://universaldependencies.org/u/dep/dislocated.html">dislocated</a></td>
<td><a href="https://universaldependencies.org/u/dep/advcl.html">advcl</a></td>
<td><a href="https://universaldependencies.org/u/dep/advmod.html">advmod</a>* <a href="https://universaldependencies.org/u/dep/discourse.html">discourse</a></td>
<td><a href="https://universaldependencies.org/u/dep/aux_.html">aux</a> <a href="https://universaldependencies.org/u/dep/cop.html">cop</a> <a href="https://universaldependencies.org/u/dep/mark.html">mark</a></td>
</tr>
<tr class="odd">
<td><strong>Nominal dependents</strong></td>
<td><a href="https://universaldependencies.org/u/dep/nmod.html">nmod</a> <a href="https://universaldependencies.org/u/dep/appos.html">appos</a> <a href="https://universaldependencies.org/u/dep/nummod.html">nummod</a></td>
<td><a href="https://universaldependencies.org/u/dep/acl.html">acl</a></td>
<td><a href="https://universaldependencies.org/u/dep/amod.html">amod</a></td>
<td><a href="https://universaldependencies.org/u/dep/det.html">det</a> <a href="https://universaldependencies.org/u/dep/clf.html">clf</a> <a href="https://universaldependencies.org/u/dep/case.html">case</a></td>
</tr>
<tr class="even">
<td><strong>Coordination</strong></td>
<td><strong>MWE</strong></td>
<td><strong>Loose</strong></td>
<td><strong>Special</strong></td>
<td><strong>Other</strong></td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/dep/conj.html">conj</a> <a href="https://universaldependencies.org/u/dep/cc.html">cc</a></td>
<td><a href="https://universaldependencies.org/u/dep/fixed.html">fixed</a> <a href="https://universaldependencies.org/u/dep/flat.html">flat</a> <a href="https://universaldependencies.org/u/dep/compound.html">compound</a></td>
<td><a href="https://universaldependencies.org/u/dep/list.html">list</a> <a href="https://universaldependencies.org/u/dep/parataxis.html">parataxis</a></td>
<td><a href="https://universaldependencies.org/u/dep/orphan.html">orphan</a> <a href="https://universaldependencies.org/u/dep/goeswith.html">goeswith</a> <a href="https://universaldependencies.org/u/dep/reparandum.html">reparandum</a></td>
<td><a href="https://universaldependencies.org/u/dep/punct.html">punct</a> <a href="https://universaldependencies.org/u/dep/root.html">root</a> <a href="https://universaldependencies.org/u/dep/dep.html">dep</a></td>
</tr>
</tbody>
</table>
<p>此外还有一些基于特定语言的依存关系。斯坦福依存分析定义了接近50个依存关系，具体的定义可参考：<a href="https://nlp.stanford.edu/software/dependencies_manual.pdf">Stanford typed dependencies manual</a>。</p>
<p>关于依存句法分析，也可以参考Daniel Jurafsky的经典NLP书籍Speech and Language Processing相关<a href="https://web.stanford.edu/~jurafsky/slp3/14.pdf">章节</a>。</p>
<span id="more"></span>
<h1 id="工具推荐">工具推荐</h1>
<h2 id="stanford-parser句法分析">Stanford Parser句法分析</h2>
<p>比较有名的工具是Stanford Parser，我们可以在<a href="http://nlp.stanford.edu:8080/corenlp/">这里</a>在线使用并进行可视化。</p>
<p>或者安装斯坦福的Python NLP软件包<a href="https://stanfordnlp.github.io/stanza/installation_usage.html">Stanza</a>，里面集成了<a href="https://stanfordnlp.github.io/stanza/depparse.html">依存关系分析</a>工具。</p>
<p>如何解决download()下载异常的问题可以参考<a href="https://blog.csdn.net/superyangtze/article/details/105252193">这里</a>。</p>
<p>我们可以进行词性标注：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanza</span><br><span class="line"><span class="comment"># stanza.download(&#x27;en&#x27;)</span></span><br><span class="line">nlp = stanza.Pipeline(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">doc = nlp(<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> doc.sentences:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence.words:</span><br><span class="line">        print(word.text, word.lemma, word.pos)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">It it PRON</span></span><br><span class="line"><span class="string">took take VERB</span></span><br><span class="line"><span class="string">me I PRON</span></span><br><span class="line"><span class="string">more more ADJ</span></span><br><span class="line"><span class="string">than than ADP</span></span><br><span class="line"><span class="string">two two NUM</span></span><br><span class="line"><span class="string">hours hour NOUN</span></span><br><span class="line"><span class="string">to to PART</span></span><br><span class="line"><span class="string">translate translate VERB</span></span><br><span class="line"><span class="string">a a DET</span></span><br><span class="line"><span class="string">few few ADJ</span></span><br><span class="line"><span class="string">pages page NOUN</span></span><br><span class="line"><span class="string">of of ADP</span></span><br><span class="line"><span class="string">English English PROPN</span></span><br><span class="line"><span class="string">. . PUNCT</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>以及依存句法分析： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanza</span><br><span class="line"><span class="comment"># stanza.download(&#x27;en&#x27;)</span></span><br><span class="line">nlp = stanza.Pipeline(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">doc = nlp(<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span>)</span><br><span class="line">print(*[<span class="string">f&#x27;id: <span class="subst">&#123;word.<span class="built_in">id</span>&#125;</span>\tword: <span class="subst">&#123;word.text&#125;</span>\thead id: <span class="subst">&#123;word.head&#125;</span>\thead: <span class="subst">&#123;sent.words[word.head-<span class="number">1</span>].text <span class="keyword">if</span> word.head &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&quot;root&quot;</span>&#125;</span>\tdeprel: <span class="subst">&#123;word.deprel&#125;</span>&#x27;</span> <span class="keyword">for</span> sent <span class="keyword">in</span> doc.sentences <span class="keyword">for</span> word <span class="keyword">in</span> sent.words], sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="built_in">id</span>: <span class="number">1</span>	word: It	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: expl</span><br><span class="line"><span class="built_in">id</span>: <span class="number">2</span>	word: took	head <span class="built_in">id</span>: <span class="number">0</span>	head: root	deprel: root</span><br><span class="line"><span class="built_in">id</span>: <span class="number">3</span>	word: me	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: iobj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">4</span>	word: more	head <span class="built_in">id</span>: <span class="number">6</span>	head: two	deprel: advmod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">5</span>	word: than	head <span class="built_in">id</span>: <span class="number">4</span>	head: more	deprel: fixed</span><br><span class="line"><span class="built_in">id</span>: <span class="number">6</span>	word: two	head <span class="built_in">id</span>: <span class="number">7</span>	head: hours	deprel: nummod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">7</span>	word: hours	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: obj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">8</span>	word: to	head <span class="built_in">id</span>: <span class="number">9</span>	head: translate	deprel: mark</span><br><span class="line"><span class="built_in">id</span>: <span class="number">9</span>	word: translate	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: csubj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">10</span>	word: a	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: det</span><br><span class="line"><span class="built_in">id</span>: <span class="number">11</span>	word: few	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: amod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">12</span>	word: pages	head <span class="built_in">id</span>: <span class="number">9</span>	head: translate	deprel: obj</span><br><span class="line"><span class="built_in">id</span>: <span class="number">13</span>	word: of	head <span class="built_in">id</span>: <span class="number">14</span>	head: English	deprel: case</span><br><span class="line"><span class="built_in">id</span>: <span class="number">14</span>	word: English	head <span class="built_in">id</span>: <span class="number">12</span>	head: pages	deprel: nmod</span><br><span class="line"><span class="built_in">id</span>: <span class="number">15</span>	word: .	head <span class="built_in">id</span>: <span class="number">2</span>	head: took	deprel: punct</span><br></pre></td></tr></table></figure> ## Spacy依存句法分析</p>
<p>我们也可以使用Spacy进行依存句法分析并画出句法树。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $python -m spacy download en_core_web_sm</span></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp=spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line">text=<span class="string">&#x27;It took me more than two hours to translate a few pages of English.&#x27;</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(text):</span><br><span class="line">	print(token.text,<span class="string">&#x27;=&gt;&#x27;</span>,token.dep_,<span class="string">&#x27;=&gt;&#x27;</span>,token.head.text)</span><br></pre></td></tr></table></figure>
<p>输出结果如下，对比stanza的结果，还是有明显差异，例如此处it被标记为<code>nsubj</code>，但其实这种情况下it是虚词expletive（我们都学过形式主语），不担任谓词的语义角色，因此应该标记为expl。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">It &#x3D;&gt; nsubj &#x3D;&gt; took</span><br><span class="line">took &#x3D;&gt; ROOT &#x3D;&gt; took</span><br><span class="line">me &#x3D;&gt; dative &#x3D;&gt; took</span><br><span class="line">more &#x3D;&gt; amod &#x3D;&gt; two</span><br><span class="line">than &#x3D;&gt; quantmod &#x3D;&gt; two</span><br><span class="line">two &#x3D;&gt; nummod &#x3D;&gt; hours</span><br><span class="line">hours &#x3D;&gt; dobj &#x3D;&gt; took</span><br><span class="line">to &#x3D;&gt; aux &#x3D;&gt; translate</span><br><span class="line">translate &#x3D;&gt; xcomp &#x3D;&gt; took</span><br><span class="line">a &#x3D;&gt; det &#x3D;&gt; pages</span><br><span class="line">few &#x3D;&gt; amod &#x3D;&gt; pages</span><br><span class="line">pages &#x3D;&gt; dobj &#x3D;&gt; translate</span><br><span class="line">of &#x3D;&gt; prep &#x3D;&gt; pages</span><br><span class="line">English &#x3D;&gt; pobj &#x3D;&gt; of</span><br><span class="line">. &#x3D;&gt; punct &#x3D;&gt; took</span><br></pre></td></tr></table></figure>
<p>画图的方法参考<a href="https://spacy.io/usage/visualizers">spacy可视化工具</a>，如果是在sublime text之类的编辑器内运行代码，我们需要打开浏览器地址http://localhost:5000/查看画图的结果。如果是jupyter notebook，则可以直接显示图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">doc = nlp(<span class="string">&quot;It took me more than two hours to translate a few pages of English.&quot;</span>)</span><br><span class="line">displacy.serve(doc, style=<span class="string">&quot;dep&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2021/03/16/dInSpW2aBw1Dg4u.png" /></p>
<p>spacy中的依存关系有45个，完整的标签如下，具体的定义可以参考：<a href="https://nlp.stanford.edu/software/dependencies_manual.pdf">Stanford typed dependencies manual</a>。</p>
<table>
<thead>
<tr class="header">
<th>标签</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ROOT</code></td>
<td>root</td>
</tr>
<tr class="even">
<td><code>acl</code></td>
<td>clausal modifier of noun (adjectival clause)</td>
</tr>
<tr class="odd">
<td><code>acomp</code></td>
<td>adjectival complement</td>
</tr>
<tr class="even">
<td><code>advcl</code></td>
<td>adverbial clause modifier</td>
</tr>
<tr class="odd">
<td><code>advmod</code></td>
<td>adverbial modifier</td>
</tr>
<tr class="even">
<td><code>agent</code></td>
<td>agent</td>
</tr>
<tr class="odd">
<td><code>amod</code></td>
<td>adjectival modifier</td>
</tr>
<tr class="even">
<td><code>appos</code></td>
<td>appositional modifier</td>
</tr>
<tr class="odd">
<td><code>attr</code></td>
<td>attribute</td>
</tr>
<tr class="even">
<td><code>aux</code></td>
<td>auxiliary</td>
</tr>
<tr class="odd">
<td><code>auxpass</code></td>
<td>auxiliary (passive)</td>
</tr>
<tr class="even">
<td><code>case</code></td>
<td>case marking</td>
</tr>
<tr class="odd">
<td><code>cc</code></td>
<td>coordinating conjunction</td>
</tr>
<tr class="even">
<td><code>ccomp</code></td>
<td>clausal complement</td>
</tr>
<tr class="odd">
<td><code>compound</code></td>
<td>compound</td>
</tr>
<tr class="even">
<td><code>conj</code></td>
<td>conjunct</td>
</tr>
<tr class="odd">
<td><code>csubj</code></td>
<td>clausal subject</td>
</tr>
<tr class="even">
<td><code>csubjpass</code></td>
<td>clausal subject (passive)</td>
</tr>
<tr class="odd">
<td><code>dative</code></td>
<td>dative</td>
</tr>
<tr class="even">
<td><code>dep</code></td>
<td>unclassified dependent</td>
</tr>
<tr class="odd">
<td><code>det</code></td>
<td>determiner</td>
</tr>
<tr class="even">
<td><code>dobj</code></td>
<td>direct object</td>
</tr>
<tr class="odd">
<td><code>expl</code></td>
<td>expletive</td>
</tr>
<tr class="even">
<td><code>intj</code></td>
<td>interjection</td>
</tr>
<tr class="odd">
<td><code>mark</code></td>
<td>marker</td>
</tr>
<tr class="even">
<td><code>meta</code></td>
<td>meta modifier</td>
</tr>
<tr class="odd">
<td><code>neg</code></td>
<td>negation modifier</td>
</tr>
<tr class="even">
<td><code>nmod</code></td>
<td>modifier of nominal</td>
</tr>
<tr class="odd">
<td><code>npadvmod</code></td>
<td>noun phrase as adverbial modifier</td>
</tr>
<tr class="even">
<td><code>nsubj</code></td>
<td>nominal subject</td>
</tr>
<tr class="odd">
<td><code>nsubjpass</code></td>
<td>nominal subject (passive)</td>
</tr>
<tr class="even">
<td><code>nummod</code></td>
<td>numeric modifier</td>
</tr>
<tr class="odd">
<td><code>oprd</code></td>
<td>object predicate</td>
</tr>
<tr class="even">
<td><code>parataxis</code></td>
<td>parataxis</td>
</tr>
<tr class="odd">
<td><code>pcomp</code></td>
<td>complement of preposition</td>
</tr>
<tr class="even">
<td><code>pobj</code></td>
<td>object of preposition</td>
</tr>
<tr class="odd">
<td><code>poss</code></td>
<td>possession modifier</td>
</tr>
<tr class="even">
<td><code>preconj</code></td>
<td>pre-correlative conjunction</td>
</tr>
<tr class="odd">
<td><code>predet</code></td>
<td>predeterminer</td>
</tr>
<tr class="even">
<td><code>prep</code></td>
<td>prepositional modifier</td>
</tr>
<tr class="odd">
<td><code>prt</code></td>
<td>particle</td>
</tr>
<tr class="even">
<td><code>punct</code></td>
<td>punctuation</td>
</tr>
<tr class="odd">
<td><code>quantmod</code></td>
<td>modifier of quantifier</td>
</tr>
<tr class="even">
<td><code>relcl</code></td>
<td>relative clause modifier</td>
</tr>
<tr class="odd">
<td><code>xcomp</code></td>
<td>open clausal complement</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>dependency</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式进阶</title>
    <url>/regex-1/</url>
    <content><![CDATA[<p>至此，我们已经建立了正则表达式的思维框架（参见「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">正则表达式入门</a>」），以及如何用python中的re模块编译正则表达式来匹配文本（参见「<a href="https://mp.weixin.qq.com/s/ibNb0rOSnBr4YC0PzCyIBA">Python实操篇</a>」）。尽管此时我们已经可以流畅地编写和使用它，但我们需要一些进阶知识，让我们的正则表达式更准确和精练。本文使用的语言依然是python。 <span id="more"></span></p>
<p><strong>全文概览：</strong></p>
<p>1.分组与捕获：<code>MatchObject.group()</code>的奥秘</p>
<p>2.四种类型的环视：匹配位置，而非匹配文本</p>
<p>3.贪婪与非贪婪：匹配优先 VS 忽略优先</p>
<h1 id="分组与捕获">分组与捕获</h1>
<p>在「<a href="https://mp.weixin.qq.com/s/ibNb0rOSnBr4YC0PzCyIBA">Python实操篇</a>」我们讲到<code>MatchObject</code>的方法<code>group()</code>可以返回匹配的整个字符串，但这并不全面。本节，我们将谈谈括号的一个关键功能：捕获（capturing）。</p>
<p><strong>捕获</strong>就是用括号提取文本以便后续访问，我们可以把这个过程想象成“闰土捕鸟”，把每个括号里的鸟儿都抓起来放进“笼子”。</p>
<p>而捕获进所有“笼子”里的内容，正是通过<code>MatchObject.groups()</code>访问。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.search(<span class="string">r&quot;(\w+) (\w+)&quot;</span>, <span class="string">&quot;John Smith&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：(&#x27;John&#x27;, &#x27;Smith&#x27;)</span></span><br></pre></td></tr></table></figure>
<p>我们可以通过索引来访问每个“笼子”，<code>group(0)</code>返回整个正则表达式匹配的文本，相当于编号为0的隐式捕获，而<code>group(1)</code>、<code>group(2)</code>则按顺序返回显式捕获分组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = re.search(<span class="string">r&quot;(\w+) (\w+)&quot;</span>, <span class="string">&quot;John Smith&quot;</span>)</span><br><span class="line">m.group(<span class="number">0</span>) <span class="comment"># 输出：&#x27;John Smith&#x27;</span></span><br><span class="line">m.group(<span class="number">1</span>)  <span class="comment"># 输出：&#x27;John&#x27;</span></span><br><span class="line">m.group(<span class="number">2</span>)  <span class="comment"># 输出：&#x27;Smith&#x27;</span></span><br></pre></td></tr></table></figure>
<p>你可能会想到「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">入门篇</a>」讲过的<strong>反向引用</strong>，在python中我们依然可以使用序号<code>\1</code>、<code>\2</code>来引用捕获的组别。下面这个例子将“数字-字母”组成的产品ID进行替换，变成了“字母-数字”的形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\d+)-(\w+)&quot;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&quot;\2-\1&quot;</span>, <span class="string">&quot;1-a\n20-baer\n34-afcr&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;a-1\nbaer-20\nafcr-34&#x27;</span></span><br></pre></td></tr></table></figure>
<p>我们也可以给每个“笼子”取上名字，即<strong>命名捕获</strong>，在python正则表达式中表示为<code>(?P&lt;name&gt;pattern)</code>，依然是通过<code>group()</code>访问单个“笼子”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(?P&lt;first&gt;\w+) (?P&lt;last&gt;\w+)&quot;</span>)</span><br><span class="line">match = pattern.search(<span class="string">&quot;John Smith&quot;</span>)</span><br><span class="line">match.group(<span class="string">&quot;first&quot;</span>) <span class="comment"># 输出：&#x27;John&#x27;</span></span><br><span class="line">match.group(<span class="string">&quot;last&quot;</span>) <span class="comment"># 输出：&#x27;Smith&#x27;</span></span><br></pre></td></tr></table></figure>
<p>需要注意，在<code>sub()</code>替换操作中，如果用名称来引用组别，我们需要写成<code>\g&lt;name&gt;</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(?P&lt;country&gt;\d+)-(?P&lt;id&gt;\w+)&quot;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&quot;\g&lt;id&gt;-\g&lt;country&gt;&quot;</span>, <span class="string">&quot;1-a\n20-baer\n34-afcr&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;a-1\nbaer-20\nafcr-34&#x27;</span></span><br></pre></td></tr></table></figure>
<p>当然，很多时候我们使用括号不是为了捕获，而仅仅是为了<strong>分组</strong>，即用于构建子表达式、多选结构或者量词作用的对象。此时，我们可以使用<strong>非捕获型括号</strong><code>(?:)</code>，告诉正则引擎，不需要提取括号内的任何信息。非捕获型括号能够提高效率、节约内存，不浪费咱们的“笼子”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.search(<span class="string">r&quot;gr(a|e)y&quot;</span>, <span class="string">&quot;gray&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：(&#x27;a&#x27;,)</span></span><br><span class="line">re.search(<span class="string">r&quot;gr(?:a|e)y&quot;</span>, <span class="string">&quot;gray&quot;</span>).groups()</span><br><span class="line"><span class="comment"># 输出：()</span></span><br></pre></td></tr></table></figure>
<h1 id="四种类型的环视">四种类型的环视</h1>
<p><strong>环视</strong>（Look Around）是正则表达式最强大的技术之一。我们可以把环视想象成前视镜和后视镜，<strong>顺序环视</strong>就是向前看（从左到右），<strong>逆序环视</strong>就是向后看（从右到左）。通过左右环视，我们进行“闰土捕鸟”的位置会更精确，确保匹配内容的上下文满足特定要求。</p>
<p>以下是四种类型的环视：</p>
<table>
<thead>
<tr class="header">
<th>类型</th>
<th>正则表达式</th>
<th>匹配成功的条件</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>肯定顺序环视</td>
<td>(?=……)</td>
<td>子表达式能够匹配右侧文本</td>
</tr>
<tr class="even">
<td>否定顺序环视</td>
<td>(?!……)</td>
<td>子表达式不能匹配右侧文本</td>
</tr>
<tr class="odd">
<td>肯定逆序环视</td>
<td>(?&lt;=……)</td>
<td>子表达式能够匹配左侧文本</td>
</tr>
<tr class="even">
<td>否定逆序环视</td>
<td>(?&lt;!……)</td>
<td>子表达式不能匹配左侧文本</td>
</tr>
</tbody>
</table>
<p>我们要注意，环视相当于作用于匹配位置的附加条件、不占用任何字符。因此它和分界符类似，是一种<strong>零宽度断言</strong>（zero-width assertions）。我们拿肯定顺序环视举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?=fox)&#x27;</span>)</span><br><span class="line">result = pattern.search(<span class="string">&quot;The quick brown fox jumps over the lazy dog&quot;</span>)</span><br><span class="line">result.span()</span><br><span class="line"><span class="comment">#输出 (16, 16)</span></span><br></pre></td></tr></table></figure>
<p>我们发现，表达式<code>(?=fox)</code>只匹配fox之前的位置，也就是索引16。</p>
<p><img src="C:\Users\13607\AppData\Roaming\Typora\typora-user-images\image-20210218222948086.png" alt="image-20210218222948086" style="zoom: 33%;" /></p>
<p>环视的作用不可小觑，我们可以精准定位，并剔除多余字符，保留更加干净、准确的匹配文本。下面的例子定位<code>&lt;p&gt;</code>标签的同时，只匹配标签里的内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;=&lt;p&gt;)[^&lt;]+(?=&lt;p&gt;)&#x27;</span>)</span><br><span class="line">pattern.search(<span class="string">&quot;&lt;p&gt;test&lt;p&gt;&quot;</span>).group(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;test&#x27;</span></span><br></pre></td></tr></table></figure>
<p>环视的另一典型应用就是将文本变成逗号分隔的货币形式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;\d&#123;1,3&#125;(?=(\d&#123;3&#125;)+(?!\d))&#x27;</span>)</span><br><span class="line">pattern.sub(<span class="string">r&#x27;\g&lt;0&gt;,&#x27;</span>, <span class="string">&quot;123456789&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：&#x27;123,456,789&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="贪婪与非贪婪">贪婪与非贪婪</h1>
<p>在「<a href="http://mp.weixin.qq.com/s?__biz=MzIzMDY0NDQ1Ng==&amp;mid=2247484919&amp;idx=1&amp;sn=7309a9bf1be78ea3250838724ebaa81c&amp;chksm=e8b10a70dfc683666f6d87e6fda6f51863dbaf565ac522b6d01d80059c01a821af70bc279438&amp;scene=21#wechat_redirect">入门篇</a>」中，我们接触了量词，但并未讲到贪婪与非贪婪的区别。</p>
<p>在python的re模块中，量词默认为<strong>贪婪模式</strong>：尽可能多地匹配更长的字符串。这就是为什么，<code>.*</code>通常会匹配到一行文本的末尾。如果要采用<strong>非贪婪模式</strong>，我们可以在量词后添加一个额外的问号，例如<code>??</code>、<code>*?</code>或<code>+?</code>，使得匹配的长度最小。这两种模式又称为<strong>匹配优先</strong>和<strong>忽略优先</strong>。</p>
<ul>
<li><p>贪婪量词（Greedy quantifiers）：<code>?</code>, <code>*</code>, <code>+</code>, <code>&#123;num,num&#125;</code></p></li>
<li><p>非贪婪/懒惰量词（Lazy quantifiers）：<code>??</code>, <code>*?</code>, <code>+?</code> , <code>&#123;num,num&#125;?</code></p></li>
</ul>
<p>比如，如果要匹配引号内的内容，会得到下面的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = <span class="string">&#x27;The name &quot;McDonald\&#x27;s&quot; ! is said &quot;makudonarudo&quot;in Japanese.&#x27;</span></span><br><span class="line">re.search(<span class="string">r&#x27;&quot;.*&quot;&#x27;</span>, s).group()</span><br><span class="line"><span class="comment"># 输出：&#x27;&quot;McDonald\&#x27;s&quot; is said &quot;makudonarudo&quot;&#x27;</span></span><br><span class="line">re.search(<span class="string">r&#x27;&quot;.*?&quot;&#x27;</span>, s).group()</span><br><span class="line"><span class="comment"># 输出：&#x27;&quot;McDonald\&#x27;s&quot;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>事实上，还有第三种量词，但目前python并不支持：</p>
<ul>
<li>占有量词（Possessive quantifiers）：<code>?+</code>, <code>*+</code>, <code>++</code>, <code>&#123;num,num&#125;+</code></li>
</ul>
<p>占有量词与贪婪量词类似，只是它们从不会交还已经匹配的字符。当然，我们也可以用固化分组<code>(?&gt;...)</code>来代替，比如<code>!.++</code>其实等同于<code>(?&gt;!.+)</code>。然而，固化分组在python中同样不被支持，所以此处不过多讨论。</p>
<p>以上です！</p>
<p>整理完python正则表达式的进阶知识，相信你已经能得心应手解决很多问题。但是，要真正打造高效、规范、美妙的正则表达式，我们仍需要了解正则引擎的原理，以及一些平衡法则、以及测试和优化的技巧，我们下篇再谈！</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title>纯文本文件的处理</title>
    <url>/txt/</url>
    <content><![CDATA[<h1 id="正则表达式">正则表达式</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">file_path = <span class="string">r&#x27;D:\books\Psychology_of_Language.txt&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    txt_string = f.read()</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;第一 初见.*第二 再见&#x27;</span>, re.DOTALL)</span><br><span class="line">cha1 = re.search(pattern, txt_string).group()</span><br><span class="line">print(re.findall(<span class="string">r&#x27;蓝色&#x27;</span>, cha1))</span><br><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;第一 初见[^蓝色]*(蓝色)*.*第二 再见&#x27;</span>, re.DOTALL)</span><br><span class="line">match = re.search(pattern, txt_string)</span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">    print(match.groups())</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="获取文章目录">获取文章目录</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">file_path = <span class="string">r&#x27;D:\project\hexo-final\source\_posts&#x27;</span></span><br><span class="line"><span class="comment"># 这里要修改</span></span><br><span class="line"><span class="comment"># weblink = &#x27;https://github.com/MissFreak/writings/blob/main/&#x27;</span></span><br><span class="line">weblink = <span class="string">&#x27;http://nlpcourse.cn/&#x27;</span></span><br><span class="line">lst = os.listdir(file_path)</span><br><span class="line">post_list = []</span><br><span class="line"><span class="comment"># 这里要修改</span></span><br><span class="line">md_name = <span class="string">&#x27;all-posts.md&#x27;</span></span><br><span class="line"><span class="comment"># md_name = &#x27;README.md&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain the titles and categories of all posts</span></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> lst:</span><br><span class="line">	<span class="comment"># make sure it is a markdown file</span></span><br><span class="line">	<span class="keyword">if</span> filename[-<span class="number">3</span>:] == <span class="string">&#x27;.md&#x27;</span>:</span><br><span class="line">		<span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(file_path, filename), encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">			s = f.read()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># create a dict that stores attributes: title and category_1 and category_2</span></span><br><span class="line">		post_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment"># get the title</span></span><br><span class="line">		match_title = re.search(<span class="string">r&#x27;(?&lt;=title: ).*(?=\n)&#x27;</span>, s)</span><br><span class="line">		<span class="keyword">if</span> match_title:</span><br><span class="line">			title = match_title.group().strip(<span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			title = filename</span><br><span class="line"></span><br><span class="line">		<span class="comment"># convert into linked title</span></span><br><span class="line">        <span class="comment"># 这里要修改</span></span><br><span class="line">		<span class="comment"># linked_title = &#x27;[&#123;&#125;](&#123;&#125;&#123;&#125;)&#x27;.format(title, weblink, filename)</span></span><br><span class="line">		linked_title = <span class="string">&#x27;[&#123;&#125;](&#123;&#125;&#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(title, weblink, filename[:-<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">		<span class="comment"># get the categories</span></span><br><span class="line">		match_categories = re.search(<span class="string">r&#x27;categories:\n- (.*)\n- (.*)\n&#x27;</span>, s)</span><br><span class="line">		<span class="keyword">try</span>:</span><br><span class="line">			category_1 = match_categories.group(<span class="number">1</span>)</span><br><span class="line">			category_2 = match_categories.group(<span class="number">2</span>)</span><br><span class="line">		<span class="keyword">except</span>:</span><br><span class="line">			category_1 = <span class="string">&#x27;未分类&#x27;</span></span><br><span class="line">			category_2 = <span class="string">&#x27;未分类&#x27;</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># save into a list</span></span><br><span class="line">		post_dict[<span class="string">&#x27;title&#x27;</span>] = linked_title</span><br><span class="line">		post_dict[<span class="string">&#x27;category_1&#x27;</span>] = category_1</span><br><span class="line">		post_dict[<span class="string">&#x27;category_2&#x27;</span>] = category_2</span><br><span class="line">		post_list.append(post_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sort and group by the first category</span></span><br><span class="line">get_first_category = <span class="keyword">lambda</span> dct: dct.get(<span class="string">&#x27;category_1&#x27;</span>)</span><br><span class="line">group_1 = itertools.groupby(<span class="built_in">sorted</span>(post_list, key=get_first_category), get_first_category)</span><br><span class="line">num_post = <span class="built_in">len</span>(post_list)</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;---\ntitle: 所有文章目录\n---\n&lt;center&gt;目前共有&#123;&#125;篇文章：&lt;/center&gt;\n&lt;!-- more --&gt;\n\n&#x27;</span>.<span class="built_in">format</span>(num_post)</span><br><span class="line"><span class="keyword">for</span> k1,v1 <span class="keyword">in</span> group_1:</span><br><span class="line">	<span class="comment"># add the first category into content</span></span><br><span class="line">	content += (<span class="string">&#x27;# &#x27;</span>+k1+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">	<span class="comment"># sort and group by the first category</span></span><br><span class="line">	get_second_category = <span class="keyword">lambda</span> dct: dct.get(<span class="string">&#x27;category_2&#x27;</span>)</span><br><span class="line">	group_2 = itertools.groupby(<span class="built_in">sorted</span>(v1, key=get_second_category), get_second_category)</span><br><span class="line">	<span class="keyword">for</span> k2,v2 <span class="keyword">in</span> group_2:</span><br><span class="line">		<span class="comment"># add the second category into content</span></span><br><span class="line">		<span class="keyword">if</span> k2 != <span class="string">&#x27;未分类&#x27;</span>:</span><br><span class="line">			content += (<span class="string">&#x27;- _&#x27;</span>+k2+<span class="string">&#x27;_\n&#x27;</span>)</span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> v2:</span><br><span class="line">				<span class="comment"># add the title into content</span></span><br><span class="line">				content += (<span class="string">&#x27;\t- &#x27;</span>+i[<span class="string">&#x27;title&#x27;</span>]+<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> v2:</span><br><span class="line">				<span class="comment"># add the title into content</span></span><br><span class="line">				content += (<span class="string">&#x27;- &#x27;</span>+i[<span class="string">&#x27;title&#x27;</span>]+<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">print(content)</span><br><span class="line"><span class="comment"># write the content into md file</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(file_path, md_name), <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	f.write(content)</span><br></pre></td></tr></table></figure>
<h1 id="分离章节">分离章节</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">file_path = <span class="string">r&#x27;D:\books\Psychology_of_Language.txt&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    txt_string = f.read()</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;\ng\n&#x27;</span>)</span><br><span class="line">chapters = pattern.split(txt_string)[<span class="number">6</span>:]</span><br><span class="line"></span><br><span class="line">cha8 = chapters[<span class="number">7</span>]</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;\nn\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>, cha8)</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;\n&#123;2,&#125;&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, cha8)</span><br><span class="line">cha8 = re.sub(<span class="string">r&#x27;-&#x27;</span>, <span class="string">&#x27;&#x27;</span>, cha8)</span><br><span class="line"><span class="comment"># print(repr(cha8))</span></span><br><span class="line">print(cha8)</span><br></pre></td></tr></table></figure>
<h1 id="去除字符">去除字符</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，\n 请加入，谢谢\n\n\n&#x27;</span></span><br><span class="line">print(ss.strip(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，\n 请加入，谢谢\n\n\n&#x27;</span></span><br><span class="line">print(ss.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>))</span><br><span class="line">ss = <span class="string">&#x27;我的电话是18827038663，也是微信号，请加入，谢谢啦啦嗯&#x27;</span></span><br><span class="line">print(ss.strip(<span class="string">&#x27;嗯啦&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h1 id="表格pytablewriter">表格：<a href="https://github.com/thombashi/pytablewriter/">pytablewriter</a></h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MarkdownTableWriter, ExcelXlsxTableWriter, UnicodeTableWriter, JavaScriptTableWriter, JsonTableWriter, HtmlTableWriter</span><br></pre></td></tr></table></figure>
<p>先获得matrix：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tempo.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	s = f.read()</span><br><span class="line">matrix = [i.split(<span class="string">&#x27;: &#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> s.split(<span class="string">&#x27;\n&#x27;</span>)] <span class="comment"># 行分隔符和列分隔符</span></span><br><span class="line">print(matrix)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tempo2.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	s = f.read()</span><br><span class="line">matrix = [i.split(<span class="string">&#x27; &#x27;</span>, <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> s.split(<span class="string">&#x27;\n&#x27;</span>)] <span class="comment"># split on first occurrence</span></span><br><span class="line">print(matrix)</span><br></pre></td></tr></table></figure>
<p>然后生成相应的表格：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line">writer = pytablewriter.MarkdownTableWriter()</span><br><span class="line">writer.value_matrix = matrix</span><br><span class="line">writer.write_table()</span><br></pre></td></tr></table></figure>
<h2 id="markdown表格">markdown表格</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># s = spacy.explain(&#x27;``&#x27;)</span></span><br><span class="line"><span class="comment"># print(repr(s))</span></span><br><span class="line">lst1 = <span class="string">&#x27;`$`, `&#x27;</span><span class="string">&#x27;`, `,`, `-LRB-`, `-RRB-`, `.`, `:`, `ADD`, `AFX`, `CC`, `CD`, `DT`, `EX`, `FW`, `HYPH`, `IN`, `JJ`, `JJR`, `JJS`, `LS`, `MD`, `NFP`, `NN`, `NNP`, `NNPS`, `NNS`, `PDT`, `POS`, `PRP`, `PRP$`, `RB`, `RBR`, `RBS`, `RP`, `SYM`, `TO`, `UH`, `VB`, `VBD`, `VBG`, `VBN`, `VBP`, `VBZ`, `WDT`, `WP`, `WP$`, `WRB`, `XX`&#x27;</span>.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">lst2 = [spacy.explain(i.strip(<span class="string">&#x27; `&#x27;</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> lst1]</span><br><span class="line"><span class="comment"># print(lst2)</span></span><br><span class="line"></span><br><span class="line">matrix = np.column_stack((lst1, lst2)).tolist()</span><br><span class="line"><span class="comment"># print(type(matrix))</span></span><br><span class="line"><span class="comment"># print(matrix)</span></span><br><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line">writer = pytablewriter.MarkdownTableWriter()</span><br><span class="line">writer.value_matrix = matrix</span><br><span class="line">writer.write_table()</span><br></pre></td></tr></table></figure>
<h2 id="csv">csv</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pytablewriter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    writer = pytablewriter.CsvTableWriter()</span><br><span class="line">    writer.headers = [<span class="string">&quot;int&quot;</span>, <span class="string">&quot;float&quot;</span>, <span class="string">&quot;str&quot;</span>, <span class="string">&quot;bool&quot;</span>, <span class="string">&quot;mix&quot;</span>, <span class="string">&quot;time&quot;</span>]</span><br><span class="line">    writer.value_matrix = [</span><br><span class="line">        [<span class="number">0</span>,   <span class="number">0.1</span>,      <span class="string">&quot;hoge&quot;</span>, <span class="literal">True</span>,   <span class="number">0</span>,      <span class="string">&quot;2017-01-01 03:04:05+0900&quot;</span>],</span><br><span class="line">        [<span class="number">2</span>,   <span class="string">&quot;-2.23&quot;</span>,  <span class="string">&quot;foo&quot;</span>,  <span class="literal">False</span>,  <span class="literal">None</span>,   <span class="string">&quot;2017-12-23 45:01:23+0900&quot;</span>],</span><br><span class="line">        [<span class="number">3</span>,   <span class="number">0</span>,        <span class="string">&quot;bar&quot;</span>,  <span class="string">&quot;true&quot;</span>,  <span class="string">&quot;inf&quot;</span>, <span class="string">&quot;2017-03-03 33:44:55+0900&quot;</span>],</span><br><span class="line">        [-<span class="number">10</span>, -<span class="number">9.9</span>,     <span class="string">&quot;&quot;</span>,     <span class="string">&quot;FALSE&quot;</span>, <span class="string">&quot;nan&quot;</span>, <span class="string">&quot;2017-01-01 00:00:00+0900&quot;</span>],</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    writer.write_table()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="其他格式">其他格式</h2>
<p><a href="https://pytablewriter.readthedocs.io/en/latest/pages/examples/table_format/text/json.html">JSON</a></p>
<p><a href="https://pytablewriter.readthedocs.io/en/latest/pages/examples/table_format/text/html.html">HTML</a></p>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>txt</tag>
      </tags>
  </entry>
  <entry>
    <title>好用的API集锦</title>
    <url>/wiki-api/</url>
    <content><![CDATA[<h1 id="wikipedia-python库">Wikipedia Python库</h1>
<p><strong>Wikipedia</strong>是一个Python库，可轻松访问和解析Wikipedia中的数据，搜索Wikipedia、获取文章摘要、从页面获取链接和图像等数据等等。Wikipedia封装了<a href="https://www.mediawiki.org/wiki/API">MediaWiki API，</a>因此您可以专注于使用Wikipedia数据，而不是获取数据。 <span id="more"></span></p>
]]></content>
      <tags>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>如何在curl和python中使用API</title>
    <url>/flask-api-3/</url>
    <content><![CDATA[<p>除了自己构建API，我们也可能需要使用其他网站提供的API。 <span id="more"></span></p>
<h1 id="在curl中使用api">在curl中使用API</h1>
<p>我们以学术检索网站<a href="https://api.semanticscholar.org/">semantic scholar</a>为例，查找某论文的信息，并保存为JSON文件<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;api.semanticscholar.org&#x2F;v1&#x2F;paper&#x2F;10.1038&#x2F;nrn3241 &gt; paper1.json</span><br></pre></td></tr></table></figure>
<p>但此时的json数据结构并不整洁，我们用python格式化json字符串并保存在paper2.json<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m json.tool paper1.json paper2.json</span><br></pre></td></tr></table></figure>
<p>或者一步到位<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;api.semanticscholar.org&#x2F;v1&#x2F;paper&#x2F;10.1038&#x2F;nrn3241 | python -mjson.tool &gt; paper3.json</span><br></pre></td></tr></table></figure>
<p>如果仅仅是想显示为Pretty Print打印出来，把后面的filename.json去掉即可。</p>
<h1 id="用python获取api数据">用python获取API数据</h1>
<p>依旧是这个例子，我们将返回的数据转成字典格式<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">&quot;https://api.semanticscholar.org/v1/paper/10.1038/nrn3241&quot;</span>)</span><br><span class="line">response_dic = response.json() <span class="comment"># if the result was written in JSON format, if not it raises an error</span></span><br></pre></td></tr></table></figure>
<p>更多requests相关参考<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>。</p>
<p><strong>参考文献：</strong></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://www.ruanyifeng.com/blog/2019/09/curl-reference.html">curl 的用法指南</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="https://docs.python.org/3/library/json.html">python json官方文档</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="https://skorks.com/2013/04/the-best-way-to-pretty-print-json-on-the-command-line/">在命令行上漂亮地打印JSON的最佳方法</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><a href="https://jzchangmark.wordpress.com/2016/06/12/%E9%80%8F%E9%81%8E-curl%E3%80%81python%E3%80%81postman-%E4%BE%86-request-api/">透过curl、Python、Postman 来Request API</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a href="https://www.w3schools.com/python/ref_requests_response.asp">python请求</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>一个人开发信息检索与抽取网站的全过程</title>
    <url>/flask-web/</url>
    <content><![CDATA[<h1 id="我的网站开发学习">我的网站开发学习</h1>
<p>我不会使用css或js文件，因为我只专注于python后端开发，前端基本都省去，只用bootstrap，但是要精通bootstrap哟！</p>
<p>我的网站是动态呈现，所以一定要用flask或者django，我现在先不考虑部署到服务器，只是本地使用！！！</p>
<p>计划：</p>
<ol type="1">
<li>完成检索功能，包括错误检测。检索成语、句子、词语等等。</li>
<li>其他小插件的完善</li>
<li>重点是毕业论文</li>
</ol>
<span id="more"></span>
<h1 id="搜索框">搜索框</h1>
<p>首先我们把boostrap加入html页面，直接使用<a href="https://www.bootstrapcdn.com/">BootstrapCDN</a>跳过下载，将Bootstrap编译的CSS和JS的缓存版本交付给我们的项目。我们使用最简单的主题即可，也可以使用其他主题（比如<a href="https://bootswatch.com/minty/">minty</a>）。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">title</span>&gt;</span>Document<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">href</span>=<span class="string">&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css&quot;</span> <span class="attr">integrity</span>=<span class="string">&quot;sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u&quot;</span> <span class="attr">crossorigin</span>=<span class="string">&quot;anonymous&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">form</span> <span class="attr">class</span>=<span class="string">&quot;form my-2 my-lg-0&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">class</span>=<span class="string">&quot;form-control mr-sm-2&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text&quot;</span> <span class="attr">placeholder</span>=<span class="string">&quot;Search&quot;</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">button</span> <span class="attr">class</span>=<span class="string">&quot;btn btn-secondary my-2 my-sm-0&quot;</span> <span class="attr">type</span>=<span class="string">&quot;submit&quot;</span>&gt;</span>Search<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js&quot;</span> <span class="attr">integrity</span>=<span class="string">&quot;sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa&quot;</span> <span class="attr">crossorigin</span>=<span class="string">&quot;anonymous&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;idiom.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    idiom_list = json.load(read_file)</span><br><span class="line"></span><br><span class="line">idiom_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> idiom <span class="keyword">in</span> idiom_list:</span><br><span class="line">    idiom_dict[idiom[<span class="string">&#x27;word&#x27;</span>]] = idiom</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;idiom_dict.json&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">    json.dump(idiom_dict, write_file)</span><br><span class="line">print(<span class="string">&#x27;成功将原列表转换为key为成语的字典！&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>查看源代码，</p>
<p>参考：</p>
<p>https://www.bootdey.com/snippets/view/Search-Results#html</p>
<p><a href="https://blog.csdn.net/star_xing123/article/details/101271925">百度搜索框</a></p>
<p><a href="https://www.html.cn/qa/css3/12786.html">怎么在HTML中加入css样式？ - html中文网</a></p>
<p><a href="https://getbootstrap.com/docs/4.3/getting-started/introduction/">Bootstrap Introduction</a></p>
<p><a href="https://bootswatch.com/">Free themes for Bootstrap</a></p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo博客Next主题搭建全过程</title>
    <url>/hexo-next/</url>
    <content><![CDATA[<p>经过三次不明觉厉的error崩盘导致我重新初始化博客（不知道哪里出错所以只有推倒重来），2021年3月18日博客终于配置完毕。在此记录全过程。最后一次配置，我只修改了三个文件：_config.yml和next/_config.yml以及custom_file_path（当然还有css/images和source）。其中custom_file_path设置为styles.styl，用于<strong>修改文章内链接文本样式</strong>、<strong>文章内单行代码的样式设置</strong>等等，需要在custom_file_path中加入这个地址。 <span id="more"></span></p>
<h1 id="hexo博客">Hexo博客</h1>
<h2 id="博客的初始化">博客的初始化</h2>
<p>关于怎么部署参考：https://zhuanlan.zhihu.com/p/26625249</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd hexo-final</span><br><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo init</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></figure>
<p>完成以上步骤后，我们看到目前的插件如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-- hexo-generator-archive@1.0.0</span><br><span class="line">+-- hexo-generator-category@1.0.0</span><br><span class="line">+-- hexo-generator-index@2.0.0</span><br><span class="line">+-- hexo-generator-tag@1.0.0</span><br><span class="line">+-- hexo-renderer-ejs@1.0.0</span><br><span class="line">+-- hexo-renderer-marked@4.0.0</span><br><span class="line">+-- hexo-renderer-stylus@2.0.1</span><br><span class="line">+-- hexo-server@2.0.0</span><br><span class="line">+-- hexo-theme-landscape@0.0.3</span><br><span class="line">&#96;-- hexo@5.4.0</span><br></pre></td></tr></table></figure>
<p>我们需要安装其他插件，我使用pandoc渲染markdown，它也用于typora的渲染，可以很好地支持数学公式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-- hexo-browsersync@0.3.0</span><br><span class="line">+-- hexo-deployer-git@3.0</span><br><span class="line">+-- hexo-generator-index-pin-top@0.2.2</span><br><span class="line">+-- hexo-generator-searchdb@1.3.3</span><br><span class="line">+-- hexo-related-popular-posts@5.0.1</span><br><span class="line">+-- hexo-renderer-pandoc@0.3.0</span><br><span class="line">+-- hexo-symbols-count-time@0.7.1</span><br><span class="line">+-- hexo-hide-posts@0.1.1</span><br></pre></td></tr></table></figure>
<p>安装方式： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br><span class="line">npm install 插件 --save</span><br></pre></td></tr></table></figure> 安装完毕后，我们把_config.yml和next/_config.yml以及css/images和source中的文件（注意有_data/styles.styl，这是我的个性化设置）复制粘贴过来即可。</p>
<h2 id="使用hexo还是hugo">使用Hexo还是Hugo</h2>
<p>3月19日，我又尝试了hugo，的确渲染速度很快、有很多漂亮的主题，并且很多主题都有详细的文档，而且hugo是按照文件夹结构生成侧边栏的目录，非常适合做文档和电子书！但缺点是很多主题不能折叠侧边栏，提供沉浸式阅读体验，所以我还是放弃了hugo，可能以后遇到喜欢的主题还是会迁移到hugo吧。</p>
<h2 id="最终我的博客页面外观">最终我的博客页面外观</h2>
<p>我花了很多时间去研究怎样让博客更美观和简洁，其实有很多是无用功，所以3月19日我正式敲定了目前的版本，并不再继续进行博客主题的个性化配置。以下是博客文章的截图：</p>
<p><img src="https://i.loli.net/2021/03/19/MjEKeWoSuidzVxT.png"/></p>
<p>主色调为蓝绿色，辅以紫色，我把最终的配置文件备份在github地址：_config.yml，next/_config.yml，next/source/css/images和source（source中有styles.styl和我写的文章），并不再修改。</p>
<p>但是我经常用typora撰写笔记，因此<code>_post</code>文件夹下经常会更新，所以我写完文章有时会打开server，确认渲染无误，然后保存好写的文章。每隔一天或一周，我就hexo d一下，这样不至于操作太频繁。我可能也会使用<a href="https://novnan.github.io/Hexo/hexo-draft/">draft功能</a>。同时，我将_post文件夹同步在github上，经常push到这个repo。由此，我的博客配置和文章都得以云端保存了。</p>
<h2 id="百度和谷歌搜索">百度和谷歌搜索</h2>
<p>使用dns配置完成了百度认证，使用html完成了谷歌认证，但目前还不能搜到我的网站。不过这不重要，以后再publicize吧。</p>
<h1 id="我的写作工具">我的写作工具</h1>
<p>我用Typora写作，记录零碎的想法、翻译、笔记、代码等，但是<strong>上传网站后访问速度有点慢</strong>，所以一般我会用本地服务器查看笔记。手机的话，我会使用知乎或微信或csdn查看，所以<strong>重要的文章需要同步到各个网站</strong>。可能之后网站优化就好了~</p>
<p>各大同步地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">知乎、微信、csdn、hexo、github</span><br></pre></td></tr></table></figure>
<p>好啦，终于不用浪费时间在网站美工和找工具上啦！专心写文章吧！</p>
]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第17章-信息抽取（上）</title>
    <url>/information-retrieval-1/</url>
    <content><![CDATA[<center>
<i>英文原文链接：https://web.stanford.edu/~jurafsky/slp3/17.pdf</i> <br> <i>译者：鸽鸽（自己学习使用，非商业用途）</i>
</center>
<hr />
<blockquote>
<p><em>I am the very model of a modern Major-General,</em> <em>I’ve information vegetable, animal, and mineral,</em> <em>I know the kings of England, and I quote the fights historical</em> <em>From Marathon to Waterloo, in order categorical...</em></p>
<p>Gilbert and Sullivan, Pirates of Penzance</p>
</blockquote>
<p>假设你是一家跟踪航空公司股票的投资公司分析师。你的任务是确定航空公司机票价格上涨的公告与他们股票第二天的表现之间的关系（如果有的话）。关于股票价格的历史数据很容易得到，但是航空公司的公告呢？你至少需要知道航空公司的名称、提议票价上涨的性质、公告的日期，可能还需要知道其他航空公司的反应。幸运的是，这些都可以在新闻文章中找到，比如这一条。</p>
<span id="more"></span>
<p>美国联合航空公司（United Airlines）周五表示，以高燃油价格为理由，已将飞往一些城市的往返机票价格提高了6美元，而这些航班也由成本较低的航空公司提供服务。发言人蒂姆-瓦格纳（Tim Wagner）表示，AMR Corp.旗下的美国航空（American Airlines）立即配合这一举措。美国联合航空公司（UAL Corp.）旗下的联合航空公司（United）表示，涨价于周四生效，适用于该公司与折扣航空公司竞争的大部分航线，如芝加哥至达拉斯、丹佛至旧金山等。</p>
<p>本章介绍了从文本中提取有限种类的语义内容的技术。这种信息提取过程（IE）将嵌入文本中的非结构化信息提取信息转换为结构化数据，例如用于填充关系数据库以实现进一步处理。关系提取与填充关系数据库有着密切的联系。事实上，知识图谱，即结构化关系数据库的数据集，是搜索引擎向用户展示信息的一种常见方式。</p>
<p>接下来，我们讨论与事件相关的三个任务。事件提取是找到这些实体参与的事件，比如在我们的样本文本中，<em>United</em>（”美联航“）和<em>American</em>（”美航“）的票价上涨以及报道事件<em>said</em>（”说“）和<em>cite</em>（”引用“）。需要指代消歧（第22章）来弄清文本中哪些事件提及是指同一个事件；在我们的运行示例中，<em>increase</em>的两次出现和短语<em>the move</em>都是指同一个事件。</p>
<p>为了弄清文本中事件发生的时间，我们提取时间表达式，比如一周中的几天（周五和周四）；相对表达式，比如从现在起或明年起；以及时刻，比如下午3点半。这些表达式必须归一化到特定的日历日期或时间上，以便定位事件的时间。在我们的示例任务中，这将使我们能够将周五与美联航宣布的时间联系起来，将周四与前一天的票价上涨联系起来，并生成一条时间线，其中美联航的宣布紧随票价上涨，而美航的宣布紧随这两个事件。</p>
<p>最后，许多文本描述了反复出现的模式化事件或情境。模板填充的任务就是在文档中找到这样的情况，并填充到模板槽中。这些槽位填充物可能由直接从文本中提取的文本片段组成，也可能是通过额外处理从文本元素中推断出来的概念，如时间、金额或本体实体。我们的航空公司文本就是这种模式化情境的一个例子，因为航空公司经常会提高票价，然后等着看竞争对手是否跟进。在这种情况下，我们可以将美联航确定为最初提高票价的牵头航空公司，6美元为金额，周四为涨价日期，而美航则为跟风的航空公司，从而得到如下的填充模板。</p>
<figure>
<img src="C:\Users\13607\AppData\Roaming\Typora\typora-user-images\image-20210321212530121.png" alt="image-20210321212530121" /><figcaption aria-hidden="true">image-20210321212530121</figcaption>
</figure>
<h1 id="关系抽取">17.1 关系抽取</h1>
<p>假设我们已经检测到了样本文本中的命名实体（也许使用了第8章的技术），并想识别出检测到的实体之间的关系：</p>
<blockquote>
<p>以高油价为由，[ORG联合航空公司]表示，[时间周五]它已将飞往一些同样由低成本航空公司服务的城市的航班的票价每往返提高了[MONEY 6美元]。发言人[per蒂姆-瓦格纳]表示，[ORG美国航空]是[ORG AMR Corp.]的一个单位，立即配合这一举措。ORG美联航]，[ORG UAL Corp.]的一个单位，说增加生效[时间周四]，适用于它与折扣航空公司竞争的大多数航线，如[LOC芝加哥]到[LOC达拉斯]和[LOC丹佛]到[LOC旧金山]。<em>(机翻结果)</em></p>
</blockquote>
<p><img src="https://i.loli.net/2021/03/18/senloyDcQAGipId.png" width="700"/></p>
<p>例如，这段文本告诉我们，Tim Wagner是American Airlines的发言人，United是UAL Corp.的一个单位，American是AMR的一个单位。这些二元关系是更通用关系的实例，比如part-of或employes，它们在新闻风格的文本中出现得相当频繁。图17.1列出了ACE关系提取评估中使用的17种关系，图17.2显示了一些关系的样例。我们还可以提取更多的特定领域的关系，比如航空路线的概念。例如从这个文本中我们可以得出美联航有到芝加哥、达拉斯、丹佛和旧金山的航线。</p>
<center>
<img src="https://i.loli.net/2021/03/18/B5QEbCoyKd6ZsFi.png"  alt="" width="700" />
</center>
<table>
<caption>Figure 17.2 Semantic relations with examples and the named entity types they involve.</caption>
<thead>
<tr class="header">
<th>Relations</th>
<th>Types Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Physical-Located</td>
<td>PER-GPE He was in Tennessee</td>
</tr>
<tr class="even">
<td>Part-Whole-Subsidiary</td>
<td>ORG-ORG XYZ, the parent company of ABC</td>
</tr>
<tr class="odd">
<td>Person-Social-Family</td>
<td>PER-PER Yoko鈥檚 husband John</td>
</tr>
<tr class="even">
<td>Org-AFF-Founder</td>
<td>PER-ORG Steve Jobs, co-founder of Apple...</td>
</tr>
</tbody>
</table>
<p>这些关系很好地对应了我们在第15章中引入的模型理论概念，为逻辑形式的含义提供了基础。也就是说，一个关系由一系列有序的元组组成，基于一个领域的元素。在大多数标准的信息提取应用中，领域元素对应于文本中出现的命名实体，对应于指代消歧产生的基础实体，或者对应于从领域本体中选择的实体。图 17.3 显示了一个基于模型的视图，可以从我们的运行示例中提取实体和关系的集合。</p>
<table>
<colgroup>
<col style="width: 60%" />
<col style="width: 39%" />
</colgroup>
<thead>
<tr class="header">
<th>Domain领域</th>
<th><span class="math inline">\(D = {a,b, c,d, e, f,g,h,i}\)</span>元素</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>United, UAL, American Airlines, AMR</td>
<td><span class="math inline">\(a,b, c,d\)</span></td>
</tr>
<tr class="even">
<td>Tim Wagner</td>
<td><span class="math inline">\(e\)</span></td>
</tr>
<tr class="odd">
<td>Chicago, Dallas, Denver, and San Francisco</td>
<td><span class="math inline">\(f,g,h,i\)</span></td>
</tr>
<tr class="even">
<td><strong>Classes类别</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>United, UAL, American, and AMR are organizations</td>
<td><span class="math inline">\(Org = {a,b, c,d}\)</span></td>
</tr>
<tr class="even">
<td>Tim Wagner is a person</td>
<td><span class="math inline">\(Pers = {e}\)</span></td>
</tr>
<tr class="odd">
<td>Chicago, Dallas, Denver, and San Francisco are places</td>
<td><span class="math inline">\(Loc = { f,g,h,i}\)</span></td>
</tr>
<tr class="even">
<td><strong>Relations关系</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>United is a unit of UAL</td>
<td><span class="math inline">\(PartOf = {&lt;a,b&gt;,&lt;c,d&gt;}\)</span></td>
</tr>
<tr class="even">
<td>American is a unit of AMR</td>
<td></td>
</tr>
<tr class="odd">
<td>Tim Wagner works for American Airlines</td>
<td><span class="math inline">\(OrgAff = {&lt;c, e&gt;}\)</span></td>
</tr>
<tr class="even">
<td>United serves Chicago, Dallas, Denver, and San Francisco</td>
<td><span class="math inline">\(Serves = {&lt;a, f&gt;,&lt;a,g&gt;,&lt;a,h&gt;,&lt;a,i&gt;}\)</span></td>
</tr>
</tbody>
</table>
<p>请注意这种模型理论的角度是如何将NER任务也包含在内的；命名实体识别对应于一类一元关系的识别。</p>
<p>许多其他领域都定义了关系集。例如美国国家医学图书馆的统一医学语言系统（UMLS）有一个网络，定义了134个大的主题类别、实体类型和54个实体之间的关系，示例如下。</p>
<table>
<thead>
<tr class="header">
<th>Entity</th>
<th>Relation</th>
<th>Entity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Injury</td>
<td>disrupts</td>
<td>Physiological Function</td>
</tr>
<tr class="even">
<td>Bodily Location</td>
<td>location-of</td>
<td>Biologic Function</td>
</tr>
<tr class="odd">
<td>Anatomical Structure</td>
<td>part-of</td>
<td>Organism</td>
</tr>
<tr class="even">
<td>Pharmacologic Substance</td>
<td>causes</td>
<td>Pathological Function</td>
</tr>
<tr class="odd">
<td>Pharmacologic Substance</td>
<td>treats</td>
<td>Pathologic Function</td>
</tr>
</tbody>
</table>
<p>给出这样一个医学句子：</p>
<blockquote>
<p>(17.1) Doppler echocardiography can be used to diagnose left anterior descending artery stenosis in patients with type 2 diabetes</p>
<p>多普勒超声心动图可以用来诊断2型糖尿病患者的左前降支动脉狭窄</p>
</blockquote>
<p>我们可以提取UMLS关系：</p>
<blockquote>
<p>Echocardiography, Doppler Diagnoses Acquired stenosis</p>
<p>超声心动图，多普勒诊断获得性狭窄</p>
</blockquote>
<p>维基百科也提供了大量的关系，这些关系来自于<u>信息框</u>(infoboxes)，即与某些维基百科文章相关的结构化表格。例如，维基百科上斯坦福（Stanford）的信息框包括诸如state = "California"或president = "Marc Tessier-Lavigne"这样的结构化事实。这些事实可以转化为president-of或located-in，或转化为一种称为<u>RDF</u>（Resource Description Framework，资源描述框架）的元语言中的关系。一个RDF triple是实体-关系-实体构成的元组，被称为主-谓-宾（subject-predicate-object）表达式。下面是一个RDF三元组的实例：</p>
<table>
<thead>
<tr class="header">
<th>subject</th>
<th>predicate</th>
<th>object</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Golden Gate Park</td>
<td>location</td>
<td>San Francisco</td>
</tr>
</tbody>
</table>
<p>例如众包的DBpedia (Bizer et al., 2009) 是一个从维基百科衍生出来的本体，包含超过20亿个RDF三元组。另一个来自维基百科信息框的数据集Freebase (Bollacker et al., 2008)，现在是Wikidata的一部分 (Vrandeciˇ c and Kr ´ otzsch, 2014)，涵盖了人员和他们的国籍或位置以及他们出现在的其他位置之间的关系。WordNet 或其他本体提供了有用的本体关系，描述了词或概念之间的层级关系。例如WordNet在类之间有is-a或hypernym的关系：</p>
<blockquote>
<p>Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...</p>
<p>长颈鹿是反刍动物是蹄类动物是哺乳动物是脊椎动物</p>
</blockquote>
<p>WordNet也有个体和类之间的Instance-of关系，例如San Francisco就和city处于Instance-of关系。提取这些关系是扩展或构建本体的重要步骤。</p>
<p>最后，有一些大型的数据集，其中包含了手动标注的句子和它们的关系，用于训练和测试关系提取器。TACRED数据集 (Zhang et al., 2017) 包含106,264个关于特定人或组织的关系三元组的例子，在来自年度TAC知识库人口（TAC KBP）挑战赛的新闻和网络文本的句子中标记得来。TACRED包含41种关系类型（如per:出生城市、org:子公司、org:成员、per:配偶），以及一个无关系标签；如图17.4所示。大约80%的例子都被标注为无关系；拥有足够的负数据对于训练监督分类器非常重要。</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="header">
<th>Example</th>
<th>Entity Types Label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Carey will succeed <strong>Cathleen P. Black</strong>, who held the position for 15 years and will take on a new role as <u>chairwoman</u> of Hearst Magazines, the company said.</td>
<td><strong>PERSON</strong>/<u>TITLE</u><br/>Relation: <em>per:title</em></td>
</tr>
<tr class="even">
<td><strong>Irene Morgan Kirkaldy</strong>, who was born and reared in <u>Baltimore</u>, lived on Long Island and ran a child-care center in Queens with her second husband, Stanley Kirkaldy.</td>
<td><strong>PERSON</strong>/<u>CITY</u><br/>Relation: <em>per:city of birth</em></td>
</tr>
<tr class="odd">
<td><strong>Baldwin</strong> declined further comment, and said JetBlue chief <u>executive</u> Dave Barger was unavailable.</td>
<td>Types: <strong>PERSON</strong>/<u>TITLE</u><br/>Relation: <em>no relation</em></td>
</tr>
</tbody>
</table>
<p>SemEval 2010任务8也推出了一个标准数据集，检测命名词（nominal）之间的关系 (Hendrickx et al., 2009)。该数据集有10,717个例子，每个例子都有一个命名词对（未分类），手工标注为9个定向关系之一，比如产品-生产者<em>product-producer</em> (一家工厂生产西装 <em>a factory manufactures suits</em>) ，或者成分-整体<em>component-whole</em> (我的公寓有一个大厨房 <em>my apartment has a large kitchen</em>)。</p>
<hr />
<p><strong>本章剩余内容见：<a href="http://nlpcourse.cn/information-retrieval-2/">《自然语言处理综论》第17章-信息抽取（中）</a></strong></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>信息抽取技术综述</title>
    <url>/information-retrieval/</url>
    <content><![CDATA[<p>信息抽取是指从非结构化或半结构化文本中寻找结构化信息的任务。它是文本挖掘的一项重要任务，在自然语言处理、信息检索和网络挖掘等各个领域得到了广泛的研究。 <span id="more"></span> <strong>信息抽取(Information Extraction, IE)的两个基本任务：</strong></p>
<ul>
<li><p><strong>命名实体识别</strong>：识别实体的名称，如人、组织和地点。</p></li>
<li><p><strong>关系抽取</strong>：提取实体之间的语义关系，如FounderOf和HeadquarteredIn。</p></li>
</ul>
<p>本文，我们将对过去几十年命名实体识别和关系抽取方面的主要工作进行综述。</p>
<h1 id="信息抽取系统">信息抽取系统</h1>
<p>命名实体识别、指代消歧和关系抽取等任务，是成熟的、特定领域的信息提取系统的基本支持组件。</p>
<p>例如，给定下面的英文句子，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In 1998, Larry Page and Sergey Brin founded Google Inc.</span><br><span class="line">1998年，Larry Page和Sergey Brin创立了Google公司。</span><br></pre></td></tr></table></figure>
<p>我们可以提取以下信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">FounderOf(Larry Page, Google Inc.),</span><br><span class="line">FounderOf(Sergey Brin, Google Inc.),</span><br><span class="line">FoundedIn(Google Inc., <span class="number">1998</span>).</span><br></pre></td></tr></table></figure>
<p>提取的信息可以被其他计算机系统（如搜索引擎和数据库管理系统）利用，为最终用户提供更好的服务。具体提取的信息类型和结构取决于特定应用的需要。下面给出一些信息提取的应用实例：</p>
<blockquote>
<ul>
<li>生物医学研究人员经常需要从大量的科学出版物中筛选出与特定基因、蛋白质或其他生物医学实体相关的发现。为了协助这项工作，基于关键词匹配的简单搜索可能并不足够，因为生物医学实体通常具有同义词和模糊的名称，因此很难准确地检索相关文档。因此，生物医学文献挖掘的一项关键任务是从文本中自动识别生物医学实体的提及，并将其与现有知识库（如FlyBase）中的相应条目链接起来。</li>
<li>金融专业人员经常需要从新闻文章中寻找特定的信息，以帮助他们进行日常决策。例如，一家金融公司可能需要知道在某个时间跨度内发生的所有公司收购，以及每次收购的细节。从文本中自动查找此类信息需要标准的信息提取技术，如命名实体识别和关系提取。</li>
<li>情报分析员审查大量文本，以搜索参与恐怖主义事件的人员、使用的武器和攻击目标等信息。虽然信息检索技术可以用来快速查找描述恐怖主义事件的文件，但需要信息提取技术来进一步确定这些文件中的具体信息单元。</li>
<li>随着网络的快速发展，搜索引擎已经成为人们日常生活中不可缺少的一部分，现在用户的搜索行为也更加了解。基于文档的词袋表示的搜索已经不能提供满意的结果。更高级的搜索问题，如实体搜索、结构化搜索和问题解答等，可以为用户提供更好的搜索体验。为了方便这些搜索功能，通常需要将信息提取作为一个预处理步骤，以丰富文档表示或填充基础数据库。</li>
</ul>
</blockquote>
<p>早期的信息提取系统，如参与MUCs的系统，通常是基于规则的系统（如[32，42]）。它们使用人类开发的语言提取模式来匹配文本和定位信息单元。它们可以在特定的目标域上取得很好的性能，但是设计好的提取规则需要耗费大量的人力，而且开发的规则对领域的依赖性很强。意识到这些人工开发系统的局限性，研究人员转而采用统计机器学习的方法。而随着信息提取系统被分解为命名实体识别等组件，许多信息提取子任务可以转化为分类问题，这些问题可以通过标准的监督学习算法，如支持向量机和最大熵模型来解决。由于信息提取涉及到识别扮演不同角色的文本片段，序列标签方法，如隐藏马尔科夫模型和条件随机场也得到了广泛的应用。</p>
<p>在本章中，我们将重点关注信息提取中最基本的两个任务，即命名实体识别和关系提取。这两个任务的最先进的解决方案都依赖于统计机器学习方法。我们还讨论了传统上没有引起太多关注的无监督信息提取。本章的其余部分组织如下。第2节讨论了当前命名实体识别的方法，包括基于规则的方法和统计学习方法。第3节讨论了完全监督环境和弱监督环境下的关系提取。然后，我们在第4节讨论了无监督的关系发现和开放的信息提取。在第5节中，我们讨论了信息提取系统的评估。最后我们在第6节中总结。</p>
<h2 id="模板填充"><strong>模板填充</strong></h2>
<p>如图的恐怖主义模板中，左边是槽位子集，右边是槽位填充值。</p>
<p>其中一些槽位填充值，如"Enrique Ormazabal Ormazabal"和 "商人"是直接从文本中提取的，而其他的槽位填充值，如抢劫、完成、枪支等则是根据文档从对应槽位的预定义值集中选择的。</p>
<p><img width=500 src="https://i.loli.net/2021/03/09/An4ryIzRKLDSTsg.png"/></p>
<h1 id="有监督的方法">有监督的方法</h1>
<p>传统的信息提取任务对提取信息的结构有明确的定义，例如命名实体的类型、关系的类型、或者模板槽。在某些场景下，我们事先并不知道我们想要提取的信息结构，而希望从大型语料库中挖掘这样的结构。例如，从一组地震新闻文章中，我们可能希望自动发现地震的日期、时间、震中、震级和伤亡是新闻文章中报道的最重要的信息。</p>
<p>最近已经有一些关于这类无监督信息提取问题的研究，但总体上沿着这个方向的工作仍然有限。另一个新的方向是开放信息提取，系统要从Web这样一个庞大的、多样化的语料库中提取所有有用的实体关系。这种系统的输出不仅包括关系中涉及的论据，还包括从文本中提取的关系的描述。最近在这个方向上取得的进展包括TextRunner[6]、Woe[66]和ReVerb[29]等系统。</p>
<h1 id="无监督的方法">无监督的方法</h1>
<h2 id="现存系统">现存系统</h2>
<p>TextRunner</p>
<p>Woe</p>
<p>ReVerb</p>
<h2 id="术语解释">术语解释</h2>
<p>国防高级研究计划局, Defense Advanced Research Projects Agency, DARPA</p>
<p>消息理解会议, Message Understanding Conferences, MUC (DARPA发起并资助)</p>
<p>DeJong的FRUMP计划</p>
<h2 id="监督学习算法">监督学习算法</h2>
<h2 id="序列标注方法">序列标注方法</h2>
<h1 id="命名实体识别">命名实体识别</h1>
<p><strong>命名实体</strong>：指代某种现实世界实体的词语序列，例如“California,” “Steve Jobs” and “Apple Inc.”。</p>
<p><strong>命名实体识别（NER）</strong>：从自由形式的文本中识别出命名实体，并将其分类为一组预定义的类型，如人名、组织和地点。</p>
<p>命名实体识别是信息提取中最基本的任务。关系和事件提取等更复杂的任务，需要准确的命名实体识别作为基础。</p>
<p>通常情况下，这项任务不能简单地通过与预先编译的地名录进行字符串匹配来完成，因为给定实体类型的命名实体通常不会形成一个封闭的集合，任何地名录都是不完整的。另一个原因是，命名实体的类型可能取决于上下文。例如，"JFK "可能指的是 "John F. Kennedy"这个人或者"JFK International Airport "这个地点，或任何其他具有相同缩写的实体。为了确定在特定文档中出现的 "JFK "的实体类型，必须考虑其上下文。</p>
<p>NER已有多个评估项目，包括自动内容提取(ACE)项目、2002年和2003年自然语言学习会议(CoNLL)的共享任务[63]、BioCreAtIvE(Critical Assessment of Information Extraction Systems in Biology)挑战评估[2]。</p>
<p>最常研究的命名实体类型是人名、组织和地点，这是由MUC-6首次定义的。这些类型足够通用，对许多应用领域都有用。日期、时间、货币值和百分比等表达式的提取，也是由MUC-6引入的，通常也是在NER下研究的，尽管严格来说这些表达式不是命名实体。除了这些一般的实体类型外，其他类型的实体通常是针对特定领域和应用而定义的。例如，GENIA语料库使用细粒度的本体对生物实体进行分类[52]。在在线搜索和广告中，产品名称的提取是一项有用的任务。</p>
<p>参考文献：</p>
<p>https://zhuanlan.zhihu.com/p/266056681</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>用python处理json数据</title>
    <url>/json/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://baike.baidu.com/item/JSON">JSON</a>(<a href="https://baike.baidu.com/item/JavaScript">JavaScript</a> Object Notation, JS 对象简谱) 是一种轻量级的数据交换格式。它基于 <a href="https://baike.baidu.com/item/ECMAScript">ECMAScript</a> (欧洲计算机协会制定的js规范)的一个子集，采用完全独立于编程语言的文本格式来存储和表示数据。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。 易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。——百度百科</p>
</blockquote>
<p>编码和解码JSON数据的过程相当于把同一样东西翻译成中文和日语。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_str = &#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;example&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">18</span>&#125;</span><br><span class="line"><span class="comment"># 序列化json</span></span><br><span class="line">json_str = json.dumps(params, sort_keys=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 反序列化json</span></span><br><span class="line">dict_json = json.loads(json_str)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="序列化json">序列化JSON</h1>
<h2 id="将python对象转换为json">将Python对象转换为JSON</h2>
<table>
<thead>
<tr class="header">
<th>Python</th>
<th>JSON格式</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>dict</code></td>
<td><code>object</code></td>
</tr>
<tr class="even">
<td><code>list</code>， <code>tuple</code></td>
<td><code>array</code></td>
</tr>
<tr class="odd">
<td><code>str</code></td>
<td><code>string</code></td>
</tr>
<tr class="even">
<td><code>int</code>，<code>long</code>，<code>float</code></td>
<td><code>number</code></td>
</tr>
<tr class="odd">
<td><code>True</code></td>
<td><code>true</code></td>
</tr>
<tr class="even">
<td><code>False</code></td>
<td><code>false</code></td>
</tr>
<tr class="odd">
<td><code>None</code></td>
<td><code>null</code></td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;president&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Zaphod Beeblebrox&quot;</span>,</span><br><span class="line">        <span class="string">&quot;species&quot;</span>: <span class="string">&quot;Betelgeusian&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">type</span>(data)</span><br><span class="line"><span class="comment">#&lt;class &#x27;dict&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存为json文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data_file.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> write_file:</span><br><span class="line">    json.dump(data, write_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者将序列化的JSON数据写入python字符串对象</span></span><br><span class="line">json_string = json.dumps(data)</span><br><span class="line"><span class="comment"># &lt;class &#x27;str&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="一些有用的关键字参数">一些有用的关键字参数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json.dumps(data, indent=<span class="number">4</span>, sort_keys = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>indent定义缩进的级别；如果<em>sort_keys</em>为true，则字典的输出将按key排序。</p>
<h1 id="反序列化json读取json数据">反序列化JSON：读取JSON数据</h1>
<h2 id="将json编码的数据转换为python对象">将JSON编码的数据转换为Python对象</h2>
<table>
<thead>
<tr class="header">
<th>JSON</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>object</code></td>
<td><code>dict</code></td>
</tr>
<tr class="even">
<td><code>array</code></td>
<td><code>list</code></td>
</tr>
<tr class="odd">
<td><code>string</code></td>
<td><code>str</code></td>
</tr>
<tr class="even">
<td><code>number</code> (int)</td>
<td><code>int</code></td>
</tr>
<tr class="odd">
<td><code>number</code> (real)</td>
<td><code>float</code></td>
</tr>
<tr class="even">
<td><code>true</code></td>
<td><code>True</code></td>
</tr>
<tr class="odd">
<td><code>false</code></td>
<td><code>False</code></td>
</tr>
<tr class="even">
<td><code>null</code></td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<h2 id="从文件中读取">从文件中读取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data_file.json&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">    data = json.load(read_file)</span><br><span class="line"><span class="built_in">type</span>(data)</span><br><span class="line"><span class="comment"># &lt;class &#x27;list&#x27;&gt;</span></span><br><span class="line"><span class="built_in">type</span>(data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="从字符串创建">从字符串创建</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_string = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    &quot;researcher&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;name&quot;: &quot;Ford Prefect&quot;,</span></span><br><span class="line"><span class="string">        &quot;species&quot;: &quot;Betelgeusian&quot;,</span></span><br><span class="line"><span class="string">        &quot;relatives&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                &quot;name&quot;: &quot;Zaphod Beeblebrox&quot;,</span></span><br><span class="line"><span class="string">                &quot;species&quot;: &quot;Betelgeusian&quot;</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">data = json.loads(json_string)</span><br></pre></td></tr></table></figure>
<h2 id="从api获取">从API获取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&quot;https://jsonplaceholder.typicode.com/todos&quot;</span>)</span><br><span class="line">todos = json.loads(response.text)</span><br><span class="line">todos == response.json()</span><br></pre></td></tr></table></figure>
<p>这里的伪json数据适合用来练习。</p>
<h1 id="遍历json字符串">遍历JSON字符串</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">items = data.items()</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> items:</span><br><span class="line">    print(<span class="built_in">str</span>(key) + <span class="string">&#x27;=&#x27;</span> + <span class="built_in">str</span>(value))</span><br></pre></td></tr></table></figure>
<h1 id="从网上下载的json文件">从网上下载的json文件</h1>
<p>有时候网上的json文件会有错误，比如某些字符串没有加双引号等等。我们要注意JSON中的名称-值对列表被花括号包裹，<strong>名称必须被双引号包裹，而值并不是总是需要被双引号包裹。当值是字符串时，必须使用双引号。</strong>而在json中，还有数字、布尔值（true false）、数组、对象、null等其他数据类型，而这些都不应该被双引号包裹。</p>
<p><strong>参考文献：</strong></p>
<p>《JSON必知必会》</p>
<p>https://www.jianshu.com/p/63dd4c77ad29</p>
<p>https://realpython.com/python-json/</p>
<p>https://blog.csdn.net/qq_33017925/article/details/87488200</p>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title>英文文献的命名实体识别（上）</title>
    <url>/named-entity-recognition/</url>
    <content><![CDATA[<p>命名实体识别（NER）是指自动识别文本中的命名实体并将其分类为预定义的类别。实体可以是人员、组织、位置、时间、数量、货币价值、百分比等的名称。作为信息抽取的基础步骤，NER从非结构化文本中提取关键元素，因此它是一项至关重要的技术。</p>
<p>我们可以创建自己的实体类别以适应不同的任务。现在有许多出色的开源库，包括<a href="https://www.nltk.org/">NLTK</a>，<a href="https://spacy.io/">SpaCy</a>和<a href="https://nlp.stanford.edu/software/CRF-NER.shtml">Stanford NER</a>。如何用这些工具实现，参考<a href="https://www.kdnuggets.com/2018/08/named-entity-recognition-practitioners-guide-nlp-4.html">命名实体识别：NLP从业人员指南</a>和<a href="https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da">NLTK和SpaCy命名实体识别</a>。也可以参考<a href="https://monkeylearn.com/blog/named-entity-recognition/">这篇</a>文章。</p>
<span id="more"></span>
<h2 id="学术文献">学术文献</h2>
<p><a href="https://www.frontiersin.org/articles/10.3389/fcell.2020.00673/full">用于生物医学信息提取的命名实体识别和关系检测</a></p>
<p><a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3321-4">使用具有上下文信息的深度神经网络进行生物医学命名实体识别</a></p>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0885230815300504">用于命名实体识别工具的多领域评估框架</a></p>
<p><a href="https://devopedia.org/named-entity-recognition#Wang-et-al.-2019">命名实体识别</a></p>
<p><a href="https://paperswithcode.com/task/named-entity-recognition-ner">带代码的文献</a></p>
<h1 id="如何训练ner分类器">如何训练NER分类器</h1>
<p>通常使用BIO表示法，该表示法区分实体的开始（B）和内部（I），O标记非实体。NER往往需要特定领域的训练，尤其是更细粒度的NER。</p>
<h2 id="评估">评估</h2>
<p><em>F-Score</em>是用于评估NER的一种常用度量，它是Precision和Recall的组合。Precision, recall, and F-Score are defined as follows (<a href="https://www.frontiersin.org/articles/10.3389/fcell.2020.00673/full#B21">Campos et al., 2012</a>):</p>
<p><span class="math display">\[
\begin{array}{c}
\text { Precision }=\frac{\text { Relevant Names Recognized }}{\text { Total Names Recognized }} \\
=\frac{\text { True Positives }}{\text { True Positives+False Positives }} \\
\text { Recall }=\frac{\text { Relevant Names Recognized }}{\text { Relevant Names in Corpus }} \\
=\frac{\text { True Positives }}{\text { True Positives+False Negatives }} \\
\text { F-score }=2 \times \frac{\text { Precision } \times \text { Recall }}{\text { Precision+Recáll }}
\end{array}
\]</span></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>PDF文件处理大全</title>
    <url>/pdf/</url>
    <content><![CDATA[<h1 id="pdfplumberpdf文件预处理">Pdfplumber：PDF文件预处理</h1>
<h2 id="用pdfminer把pdf转为txt">用pdfminer把pdf转为txt</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pdfminer.high_level <span class="keyword">import</span> extract_text</span><br><span class="line">file_path = <span class="string">r&#x27;D:\pdf-file\Psychology_of_Language.pdf&#x27;</span></span><br><span class="line">text = extract_text(file_path, page_numbers=<span class="built_in">range</span>(<span class="number">33</span>,<span class="number">35</span>))</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p><code>pdfminer.high_level.``extract_text</code>（<em>pdf_file</em>，<em>password =''</em>，<em>page_numbers = None</em>，<em>maxpages = 0</em>，<em>caching = True</em>，<em>codec ='utf-8'</em>，<em>laparams = None</em> ）</p>
<p>参考：https://pdfminersix.readthedocs.io/en/latest/reference/highlevel.html#api-extract-text</p>
<h2 id="更精确的转换除去换行符">更精确的转换（除去换行符）</h2>
<h3 id="少量文件转换">少量文件转换</h3>
<h4 id="先把pdf转换为word">先把pdf转换为word</h4>
<p>转换工具：https://pdf2doc.com/zh/</p>
<h4 id="然后把word转为txt">然后把word转为txt</h4>
<p>文件多的话用多线程PDF转Word：https://github.com/python-fan/pdf2word</p>
<h3 id="批量转换">批量转换</h3>
<p>还不知道，参考：https://www.zhihu.com/question/357994254</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdfplumber</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">file_path = <span class="string">&#x27;book.pdf&#x27;</span></span><br><span class="line">pdf = pdfplumber.<span class="built_in">open</span>(file_path)</span><br><span class="line">start_page = <span class="number">34</span></span><br><span class="line">end_page = <span class="number">35</span></span><br><span class="line">text_string = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> page_num <span class="keyword">in</span> <span class="built_in">range</span>(start_page-<span class="number">1</span>, end_page):</span><br><span class="line">    text_string += pdf.pages[page_num].extract_text()+<span class="string">&#x27; &#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;book.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text_string)</span><br></pre></td></tr></table></figure>
<h2 id="把表格保存为csv">把表格保存为csv</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdfplumber</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">file_path = <span class="string">&#x27;book.pdf&#x27;</span></span><br><span class="line">pdf = pdfplumber.<span class="built_in">open</span>(file_path)</span><br><span class="line"></span><br><span class="line">chars = []</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> pdf.pages[<span class="number">7</span>:<span class="number">166</span>]:</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> page.chars:</span><br><span class="line">        chars.append(char)</span><br><span class="line">width_unique = <span class="built_in">set</span>([char[<span class="string">&#x27;height&#x27;</span>] <span class="keyword">for</span> char <span class="keyword">in</span> chars])</span><br><span class="line">print(<span class="string">&#x27;The unique heights: &#x27;</span>+<span class="built_in">str</span>(width_unique))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码</category>
        <category>文本处理</category>
      </categories>
      <tags>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title>英文词性标记（POS Tagging）</title>
    <url>/pos-tagging/</url>
    <content><![CDATA[<h1 id="词性标记pos-tagging">词性标记（POS Tagging）</h1>
<p>POS标签大致分为两种：通用POS标签和细粒度POS标签。</p>
<h2 id="跨语言的通用pos标签">跨语言的通用POS标签</h2>
<p>通用依存关系（<a href="https://universaldependencies.org/">UD</a>）是一个致力于开发跨语言树库标注的项目，为不同语言标注一致的语法<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。 <span id="more"></span> 该标注方案基于（通用的）斯坦福依存关系（de Marneffe et al., 2006, 2008, 2014）、Google通用词性标签（Petrov et al., 2012）和Interset interlingua的形态语义标签集（Zeman, 2008）的发展进化。</p>
<p>该<a href="https://universaldependencies.org/u/pos/index.html">网站</a>列举了各个通用POS标签的详细解释。</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 14%" />
<col style="width: 34%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th>Tag</th>
<th>Category</th>
<th>Explanation</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/ADJ.html">ADJ</a></td>
<td>adjective</td>
<td>形容词，修饰名词或充当谓语</td>
<td>big, African, first</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/ADP.html">ADP</a></td>
<td>adposition</td>
<td>介词，包括前置词和后置词（prepositions and postpositions）</td>
<td>in, during</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/ADV.html">ADV</a></td>
<td>adverb</td>
<td>副词，修饰动词、形容词和副词本身，表示时间、地点、方向或方式</td>
<td>very, up, tomorrow, where, never</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/AUX_.html">AUX</a></td>
<td>auxiliary</td>
<td>助词，包括Tense auxiliaries, Passive auxiliaries, Modal auxiliaries, Verbal copulas</td>
<td>has, was, should</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/CCONJ.html">CCONJ</a></td>
<td>coordinating conjunction</td>
<td>并列连词</td>
<td>and, or, but</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/DET.html">DET</a></td>
<td>determiner</td>
<td>限定词，修饰名词</td>
<td>the, a, an</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/INTJ.html">INTJ</a></td>
<td>interjection</td>
<td>感叹词</td>
<td>ouch, yes</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/NOUN.html">NOUN</a></td>
<td>noun</td>
<td>普通名词</td>
<td>girl, tree, air</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/NUM.html">NUM</a></td>
<td>numeral</td>
<td>数词</td>
<td>2014, one, II</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/PART.html">PART</a></td>
<td>particle</td>
<td>小品词，与其他词语结合</td>
<td>'s, not</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/PRON.html">PRON</a></td>
<td>pronoun</td>
<td>代词，代替名词或名词短语</td>
<td>you, who, somebody, it</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/PROPN.html">PROPN</a></td>
<td>proper noun</td>
<td>专有名词，特定任务、地点或物体的名称</td>
<td>Mary, London, NATO</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/PUNCT.html">PUNCT</a></td>
<td>punctuation</td>
<td>标点符号是非字母字符和字符组</td>
<td>. , ()</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/SCONJ.html">SCONJ</a></td>
<td>subordinating conjunction</td>
<td>从属连词，引入从句的连词</td>
<td>since, that, who, if, while</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/SYM.html">SYM</a></td>
<td>symbol</td>
<td>符号，可以用普通单词代替（比如$换作美元）</td>
<td>$, %, ♥‿♥, 😝</td>
</tr>
<tr class="even">
<td><a href="https://universaldependencies.org/u/pos/VERB.html">VERB</a></td>
<td>verb</td>
<td>动词，表示事件和动作</td>
<td>run, runs, running</td>
</tr>
<tr class="odd">
<td><a href="https://universaldependencies.org/u/pos/X.html">X</a></td>
<td>other</td>
<td>其他</td>
<td>xfgh</td>
</tr>
</tbody>
</table>
<p>该项目的<a href="https://github.com/UniversalDependencies/docs">Github地址</a>涵盖了不同语言。</p>
<h2 id="特定语言的细粒度pos标签">特定语言的细粒度POS标签</h2>
<p>我们可以将通用POS标签细化，例如将英语中的NOUN（普通名词）进一步划分为复数普通名词（NNS），NN（单数普通名词），但这些标签是基于特定语言的。</p>
<p>我们可以打开<a href="https://spacy.io/usage/linguistic-features">spacy</a>，线上运行以下代码。其中pos表示<a href="https://universaldependencies.org/docs/u/pos/">通用POS标签集</a>的粗粒度<a href="https://universaldependencies.org/docs/u/pos/">词性</a>，而tag表示细粒度的词性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">doc = nlp(<span class="string">&quot;It took me more than two hours to translate a few pages of English.&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">	print(token.text, <span class="string">&#x27;=&gt;&#x27;</span>,token.pos_,<span class="string">&#x27;=&gt;&#x27;</span>,token.tag_)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">It &#x3D;&gt; PRON &#x3D;&gt; PRP</span><br><span class="line">took &#x3D;&gt; VERB &#x3D;&gt; VBD</span><br><span class="line">me &#x3D;&gt; PRON &#x3D;&gt; PRP</span><br><span class="line">more &#x3D;&gt; ADJ &#x3D;&gt; JJR</span><br><span class="line">than &#x3D;&gt; SCONJ &#x3D;&gt; IN</span><br><span class="line">two &#x3D;&gt; NUM &#x3D;&gt; CD</span><br><span class="line">hours &#x3D;&gt; NOUN &#x3D;&gt; NNS</span><br><span class="line">to &#x3D;&gt; PART &#x3D;&gt; TO</span><br><span class="line">translate &#x3D;&gt; VERB &#x3D;&gt; VB</span><br><span class="line">a &#x3D;&gt; DET &#x3D;&gt; DT</span><br><span class="line">few &#x3D;&gt; ADJ &#x3D;&gt; JJ</span><br><span class="line">pages &#x3D;&gt; NOUN &#x3D;&gt; NNS</span><br><span class="line">of &#x3D;&gt; ADP &#x3D;&gt; IN</span><br><span class="line">English &#x3D;&gt; PROPN &#x3D;&gt; NNP</span><br><span class="line">. &#x3D;&gt; PUNCT &#x3D;&gt; .</span><br></pre></td></tr></table></figure>
<p>有关spaCy模型在不同语言中分配的细粒度和粗粒度词性标签的列表，请参阅<a href="https://spacy.io/models">models目录中</a>记录的标签方案。</p>
<p>比如英语的所有细粒度<a href="https://spacy.io/models/en">标签</a>如下：</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="header">
<th>A</th>
<th>B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>$</code></td>
<td>symbol, currency</td>
</tr>
<tr class="even">
<td><code>''</code></td>
<td>closing quotation mark</td>
</tr>
<tr class="odd">
<td><code>,</code></td>
<td>punctuation mark, comma</td>
</tr>
<tr class="even">
<td>```<code>| opening quotation mark                    | |</code>-LRB-<code>| left round bracket                        | |</code>-RRB-<code>| right round bracket                       | |</code>.<code>| punctuation mark, sentence closer         | |</code>:<code>| punctuation mark, colon or ellipsis       | |</code>ADD<code>| email                                     | |</code>AFX<code>| affix                                     | |</code>CC<code>| conjunction, coordinating                 | |</code>CD<code>| cardinal number                           | |</code>DT<code>| determiner                                | |</code>EX<code>| existential there                         | |</code>FW<code>| foreign word                              | |</code>HYPH<code>| punctuation mark, hyphen                  | |</code>IN<code>| conjunction, subordinating or preposition | |</code>JJ<code>| adjective                                 | |</code>JJR<code>| adjective, comparative                    | |</code>JJS<code>| adjective, superlative                    | |</code>LS<code>| list item marker                          | |</code>MD<code>| verb, modal auxiliary                     | |</code>NFP<code>| superfluous punctuation                   | |</code>NN<code>| noun, singular or mass                    | |</code>NNP<code>| noun, proper singular                     | |</code>NNPS<code>| noun, proper plural                       | |</code>NNS<code>| noun, plural                              | |</code>PDT<code>| predeterminer                             | |</code>POS<code>| possessive ending                         | |</code>PRP<code>| pronoun, personal                         | |</code>PRP<span class="math inline">\(` | pronoun, possessive | | `RB` | adverb | | `RBR` | adverb, comparative | | `RBS` | adverb, superlative | | `RP` | adverb, particle | | `SYM` | symbol | | `TO` | infinitival &quot;to | | `UH` | interjection | | `VB` | verb, base form | | `VBD` | verb, past tense | | `VBG` | verb, gerund or present participle | | `VBN` | verb, past participle | | `VBP` | verb, non-3rd person singular present | | `VBZ` | verb, 3rd person singular present | | `WDT` | wh-determiner | | `WP` | wh-pronoun, personal | | `WP\)</span><code>| wh-pronoun, possessive                    | |</code>WRB<code>| wh-adverb                                 | |</code>XX`</td>
<td>unknown</td>
</tr>
</tbody>
</table>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/#:~:text=Dependency%20parsing%20is%20the%20process,tags%20are%20the%20dependency%20tags.">词性标记和依存关系分析</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>pos</tag>
      </tags>
  </entry>
  <entry>
    <title>信息抽取</title>
    <url>/information-retrieval-4/</url>
    <content><![CDATA[<h3 id="知识库-knowledge-base">11.1.1 知识库 (knowledge base)</h3>
<p>本章旨在教会你的机器人理解它所阅读的内容，并将知识存储在一个灵活的数据结构中，即将信息记录在一个知识库中，用于后期查询。除了识别文本中的数字和日期等简单任务之外，机器人还要提取更多关于世界的一般信息。</p>
<p>例如，它能够从自然语言文档中（比如维基百科）学习这句话：</p>
<blockquote>
<p>In 1983, Stanislav Petrov, a lieutenant colonel of the Soviet Air Defense Forces, saved the world from nuclear war.</p>
<p>1983年，斯坦尼斯拉夫·彼得罗夫，苏联防空部队的一名中校从核战争中拯救了世界。</p>
</blockquote>
<p>如果你在历史课上读完或听完这样的内容后做笔记，你可能会对事情进行解读，并在大脑中建立概念或词语之间的联系。你可能会把它还原成一个知识，那个你“从中得到的东西”。你希望你的机器人也能做同样的事情， “记下”它所学到的任何东西，比如斯坦尼斯洛夫·彼得罗夫是一名中校的事实或知识。</p>
<p>它可以存储在一个类似这样的数据结构中：</p>
<blockquote>
<p>('Stanislav Petrov', 'is-a', 'lieutenant colonel')</p>
</blockquote>
<p>这是知识图谱或知识库中两个<u>命名实体节点</u>('Stanislav Petrov'和'lieutenant colonel')和它们之间的<u>关系</u>或连接('is a')的例子。当这样的关系以符合知识图谱的<u>RDF标准</u>（关系描述格式）的形式存储时，它被称为<u>RDF三元模型(triplet)</u>。历史上，这些RDF triplet存储在XML文件中，但也可以存储在任何文件格式或数据库中，这些文件格式或数据库以 <u>(subject, relation, object)</u> 的形式保存，这些三元组的集合就是一个知识图谱。这有时也被语言学家称为<u>本体</u>（ontology），因为它存储的是关于词的结构化信息。但当图谱的目的是为了表示关于世界的事实而不仅仅是单词时，它就被称为<u>知识图谱</u>或<u>知识库</u>。</p>
<p>图11.1是你想从这样的句子中提取的知识图谱的图形表示。 图11.1顶部的“is-a”关系代表了一个不能直接从斯坦尼斯拉夫的陈述中提取的事实。但从一个军事组织成员的头衔是军衔这一事实可以推断出，“中校”是一个军衔。这种从知识图谱中推导出事实的逻辑操作叫做<u>知识推理</u>。也可以称为查询知识库，类似于查询关系型数据库。</p>
<p><img src="https://i.loli.net/2021/04/13/NoW1JAQhvMOaCTX.png"/></p>
<p>对于斯坦尼斯洛夫的军衔这一特殊推断或查询，你的知识图谱必须已经包含了关于军队和军衔的事实。也许你现在可以看到知识库是如何帮助机器理解一个语句的。如果没有这个知识基础，像这样简单的语句中的许多事实都会让你的聊天机器人“摸不着头脑”。你甚至可以说，关于职业等级的问题对于一个只知道如何根据随机分配的主题对文件进行分类的机器人来说，是“超乎寻常的"。如果你曾经与一个不懂“哪条路是向上的“的聊天机器人进行过互动，你就会明白。在人工智能研究中，最令人生畏的挑战之一是如何编译和高效查询常识性知识的知识图谱。</p>
<p><strong>机器很难找到常识性知识的语料库来阅读和学习。</strong>没有常识性知识的维基百科文章存在，你的机器就无法对其进行信息抽取。而有些知识是本能，是硬编码在我们的DNA中的，事物和人之间存在各种事实关系，比如“kindof"、"is-used-for"、"has-a"、"is-famous-for"、"was-born“和“has-profession"。</p>
<p>卡耐基梅隆大学永无止境的语言学习机器人NELL，几乎完全专注于提取“kind-of”关系信息的任务。大多数知识库都会对定义这些关系的字符串进行归一化处理，这样“kind of”和“type of”就会被分配一个归一化的字符串或ID来表示这个特定的关系。而有些知识库也会对知识库中代表obejct的名词进行解析。所以“Stanislav Petrov”这个bigram可能会被分配一个特定的ID。"Stanislav Petrov”的同义词，比如“S. Petrov”和“Lt Col Petrov"，也会被分配给同一个ID，如果NLP管道怀疑它们指的是同一个人。 知识库可以用来构建一种实用型的聊天机器人，称为问答系统（QA系统）。客服聊天机器人，包括大学TA机器人，几乎完全依靠知识库来生成他们的回答。问答系统对于帮助人类发现事实信息非常有用，它可以让人类的大脑自由地做自己擅长的事情，比如试图从这些事实中归纳总结。<strong>人类不善于准确地记住事实，但善于发现这些事实之间的联系和模式，这是机器尚未掌握的。</strong></p>
<h3 id="信息抽取">11.1.2 信息抽取</h3>
<p>所以你已经了解到“信息抽取“是将非结构化文本转换为存储在知识库或知识图谱中的结构化信息。信息抽取是被称为自然语言理解（NLU）的研究领域的一部分，尽管这个术语经常与自然语言处理同义使用。 信息抽取和NLU是一种不同于你在研究数据科学时可能想到的学习。它不仅仅是无监督的学习，甚至连“模型“本身，即关于世界如何运作的逻辑，都可以在没有人类干预的情况下组成。与其说是给机器钓鱼（事实），不如说是教它如何钓鱼（提取信息）。尽管如此，机器学习技术还是经常被用来训练信息抽取器。</p>
<h3 id="信息抽取作为ml特征提取">11.2.2 信息抽取作为ML特征提取</h3>
<p>模式匹配（和正则表达式）仍然是信息抽取的最先进的方法。即使使用机器学习方法来处理自然语言，你也需要进行特征工程。你需要创建词袋或词嵌入，以尝试将自然语言文本中近乎无限的意义可能性减少到机器可以轻松处理的向量中。信息抽取只是机器学习从非结构化自然语言数据中提取特征的另一种形式，比如创建一个词袋，或者在这个词袋上做PCA。而这些模式和特征即使在最先进的自然语言机器学习管道中仍然被采用，比如谷歌的Assistant、Siri、亚马逊Alexa和其他最先进的机器人。</p>
<p>信息抽取可以事先完成，以填充事实的知识库。或者，当聊天机器人被问到一个问题或搜索引擎被查询时，可以按需找到所需的语句和信息。当提前建立知识库时，可以优化数据结构，以便在更大的知识领域内进行更快的查询。预先构建的知识库可以让聊天机器人快速响应关于更广泛信息的问题。如果在聊天机器人被查询时实时检索信息，这通常被称为“搜索"。</p>
<p>Google和其他搜索引擎结合了这两种技术，查询知识库（knowledge base），如果没有找到必要的事实，就回落到文本搜索。你在学校里学到的许多自然语言语法规则可以被编码在正式的语法中，设计成对代表语言部分的单词或符号进行操作。而英语语言可以被认为是构成语言的单词和语法规则，或者你也可以把它看成是你可以说的所有可能的事情的集合，这些事情会被英语语言使用者认可为有效的语句。 而这就引出了<u>形式化语法</u>和<u>有限状态机</u>的另一个特点，它将在NLP中派上用场。</p>
<p>任何形式化语法都可以被机器以两种方式使用：</p>
<ul>
<li><p>识别与该语法相匹配的内容</p></li>
<li><p>生成新的符号序列</p></li>
</ul>
<p>你不仅可以使用模式（正则表达式）从自然语言中提取信息，还可以在聊天机器人中使用它们，让它“说“出与该模式相匹配的东西！我们将向你展示如何用有限状态机来实现这一点。我们在这里向你展示如何使用一个名为rstr4的包来实现这个功能，用于你的一些信息抽取模式。 这种形式化语法和有限状态机的模式匹配方法还有其他一些很棒的特性。</p>
<p>一个真正的有限状态机可以保证总是在有限时间内运行（停止）。它将始终告诉你是否在你的字符串中找到了匹配。它永远不会陷入永久循环......只要你不使用正则表达式引擎的一些高级功能，这些功能允许你“欺骗“并将循环纳入你的有限状态机。 所以，你要坚持使用不需要这些“回看“或“前瞻“作弊的正则表达式。你将确保你的正则表达式匹配器会处理每个字符，并且只有当它匹配时才会前进到下一个字符--有点像一个严格的列车员在座位上走动检查车票。如果你没有，列车员就会停下来，并宣布有问题，不匹配，他拒绝继续前进，或查看你的前面或后面，直到他解决这个问题。火车上的乘客没有“走回头路“或“重来"，也没有严格的正则表达式。</p>
<h2 id="值得提取的信息">11.3 值得提取的信息</h2>
<p>一些关键的定量信息值得“手工制作”正则表达式的努力：</p>
<blockquote>
<p>GPS位置｜日期｜价格｜数字</p>
</blockquote>
<p>其他重要的自然语言信息需要比正则表达式更复杂的模式：</p>
<blockquote>
<p>问题触发词｜问题目标词｜命名实体</p>
</blockquote>
<h3 id="提取gps位置">11.3.1 提取GPS位置</h3>
<p>GPS位置是典型的数字数据，你需要使用正则表达式从文本中提取。GPS位置是以一对经纬度的数字值来表示的。它们有时还包括第三个数字，即海拔高度，或海平面以上的高度，但你现在将忽略它。我们只提取十进制的经纬度对，用度数表示。这将适用于许多谷歌地图的URL。虽然URL在技术上不是自然语言，但它们通常是非结构化文本数据的一部分，你想提取这一点信息，这样你的聊天机器人就可以知道地方以及事物。 让我们使用之前例子中的十进制数字模式，但让我们更具限制性，确保该值在纬度（+/- 90度）和经度（+/- 180度）的有效范围内。你不能比北极（+90度）更北，也不能比南极（-90度）更南。而如果你从英国格林威治向东航行180度（+180度经度），你就会到达日期线，在那里你也是从格林威治向西180度（-180度）。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lat = <span class="string">r&#x27;([-]?[0-9]?[0-9][.][0-9]&#123;2,10&#125;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lon = <span class="string">r&#x27;([-]?1?[0-9]?[0-9][.][0-9]&#123;2,10&#125;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sep = <span class="string">r&#x27;[,/ ]&#123;1,3&#125;&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps = re.<span class="built_in">compile</span>(lat + sep + lon)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&#x27;http://...maps/@34.0551066,-118.2496763...&#x27;</span>)</span><br><span class="line">[(<span class="number">34.0551066</span>, -<span class="number">118.2496763</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&quot;https://www.openstreetmap.org/#map=10/5.9666/116.0566&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;5.9666&#x27;</span>, <span class="string">&#x27;116.0566&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re_gps.findall(<span class="string">&quot;Zig Zag Cafe is at 45.344, -121.9431 on my GPS.&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;45.3440&#x27;</span>, <span class="string">&#x27;-121.9431&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>数字数据很容易提取，特别是如果数字是机器可读字符串的一部分。URL和其他机器可读的字符串将经纬度等数字以可预测的顺序、格式和单位排列，以方便我们。这种模式仍然会接受一些不属于这个世界的经纬度值，但它可以满足你从OpenStreetMap等地图网络应用中复制的大多数URL的要求。 但是日期呢？正则表达式对日期有用吗？如果你想让你的日期提取器能在欧洲和美国工作，那该怎么办，因为那里的日/月顺序经常是相反的。</p>
<h3 id="提取日期">11.3.2 提取日期</h3>
<p>日期比 GPS 坐标更难提取。日期是一种比较自然的语言，类似的事情有不同的方言来表达。在美国，2017年的圣诞节是“12/25/17"。在欧洲，2017年的圣诞节是“25/12/17"。你可以检查你的用户的所在地，并假设他们和他们所在地区的其他人一样写日期。但这个假设可能是错误的。 因此，大多数日期和时间提取器都会尝试使用这两种日/月顺序，并检查以确保这是一个有效的日期。当我们读到这样的日期时，人脑就是这样工作的。即使你是一个美式英语的人，你在圣诞节前后在布鲁塞尔，你也可能会认出“25/12/17“是一个节日，因为一年只有12个月。</p>
<p><strong>这种在计算机编程中行之有效的“鸭子打字“ (duck-typing) 法<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>也可以用于自然语言。如果它看起来像一只鸭子，行为又像鸭子，那它可能就是一只鸭子。</strong>如果它看起来像个日期，行为又像个日期，那可能就是个日期。你会把这种“先试后买“的方法也用在其他自然语言处理任务上。你会尝试一堆选项，然后接受一个有效的选项。你会尝试你的提取器或生成器，然后你会在上面运行一个验证器，看看它是否合理。 对于聊天机器人来说，这是一个特别强大的方法，允许你结合多个自然语言生成器的优点。在第10章中，你使用LSTMs生成了一些聊天机器人的回复。为了改善用户体验，你可以生成很多回复，然后选择拼写、语法和情感最好的那个。我们将在第12章中详细讨论这个问题。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>us = <span class="string">r&#x27;((([01]?\d)[-/]([0123]?\d))([-/]([0123]\d)\d\d)?)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mdy = re.findall(us, <span class="string">&#x27;Santa came 12/25/2017. An elf appeared 12/12.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mdy</span><br><span class="line">[(<span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;25&#x27;</span>, <span class="string">&#x27;/2017&#x27;</span>, <span class="string">&#x27;20&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;12&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>列表理解可以用来为提取的数据提供一点结构，方法是将月份、日期和年份转换为整数，并用有意义的名称标记这些数字信息，如下面的列表所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates = [&#123;<span class="string">&#x27;mdy&#x27;</span>: x[<span class="number">0</span>], <span class="string">&#x27;my&#x27;</span>: x[<span class="number">1</span>], <span class="string">&#x27;m&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">2</span>]), <span class="string">&#x27;d&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">3</span>]),</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;y&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">4</span>].lstrip(<span class="string">&#x27;/&#x27;</span>) <span class="keyword">or</span> <span class="number">0</span>), <span class="string">&#x27;c&#x27;</span>: <span class="built_in">int</span>(x[<span class="number">5</span>] <span class="keyword">or</span> <span class="number">0</span>)&#125; <span class="keyword">for</span> x <span class="keyword">in</span> mdy]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates</span><br><span class="line">[&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">25</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">0</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>即使对于这些简单的日期，也不可能设计出一个能够解决第二个日期“12/12”中所有歧义的regex。在日期的语言中，有一些含糊不清的地方，只有人类能够利用圣诞节等知识和文本作者的意图来猜测解决。</p>
<p>比如“12/12“可能意味着：</p>
<blockquote>
<p>December 12th, 2017—month/day in the estimated year based on anaphora resolution</p>
<p>December 12th, 2018—month/day in the current year at time of publishing</p>
<p>December 2012—month/year in the year 2012</p>
</blockquote>
<p>因为在美国日期和我们的regex中，month/day在年份之前，"12/12“被假定为未知年份的12月12日。你可以使用内存中的结构化数据中的上下文来填充任何缺失的数字字段，如下面的列表所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(dates):</span><br><span class="line"><span class="meta">... </span><span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line"><span class="meta">... </span><span class="keyword">if</span> <span class="keyword">not</span> v:</span><br><span class="line"><span class="meta">... </span>d[k] = dates[<span class="built_in">max</span>(i - <span class="number">1</span>, <span class="number">0</span>)][k]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dates</span><br><span class="line">[&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/25/2017&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/25&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">25</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;mdy&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;my&#x27;</span>: <span class="string">&#x27;12/12&#x27;</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;y&#x27;</span>: <span class="number">2017</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">20</span>&#125;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> date</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>datetimes = [date(d[<span class="string">&#x27;y&#x27;</span>], d[<span class="string">&#x27;m&#x27;</span>], d[<span class="string">&#x27;d&#x27;</span>]) <span class="keyword">for</span> d <span class="keyword">in</span> dates]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>datetimes</span><br><span class="line">[datetime.date(<span class="number">2017</span>, <span class="number">12</span>, <span class="number">25</span>), datetime.date(<span class="number">2017</span>, <span class="number">12</span>, <span class="number">12</span>)]</span><br></pre></td></tr></table></figure>
<p>这是一种从自然语言文本中提取日期信息的基本但相当健壮的方法。要把它变成一个生产型的日期提取器，剩下的主要任务就是添加一些适合你的应用的异常捕获和上下文维护。如果你通过拉取请求将其添加到nlpia包(http://github.com/ totalgood/nlpia)中，我相信你的读者朋友们会很感激。而如果你为时间添加了一些提取器，好吧，那你就是相当的英雄了。 一些手工制作的逻辑有机会处理几个月甚至几天的边缘情况和自然语言名称。但是再复杂也无法解决“12/11“这个日期的歧义。这可能是12月11日，无论你在哪一年读到或听到它11月12日，如果你在伦敦或塔斯马尼亚州朗塞斯顿（一个联邦领土）听到它2011年12月，如果你在美国报纸上读到它2012年11月，如果你在欧盟报纸上读到它有些自然语言的歧义是无法解决的，即使是人脑。但是，让我们确保你的日期提取器可以通过在你的regex中颠倒月份和日期来处理欧洲的日/月顺序。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>eu = <span class="string">r&#x27;((([0123]?\d)[-/]([01]?\d))([-/]([0123]\d)?\d\d)?)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy = re.findall(eu, <span class="string">&#x27;Alan Mathison Turing OBE FRS (23/6/1912-7/6/1954) \</span></span><br><span class="line"><span class="string">... was an English computer scientist.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy</span><br><span class="line">[(<span class="string">&#x27;23/6/1912&#x27;</span>, <span class="string">&#x27;23/6&#x27;</span>, <span class="string">&#x27;23&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/1912&#x27;</span>, <span class="string">&#x27;19&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;7/6/1954&#x27;</span>, <span class="string">&#x27;7/6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/1954&#x27;</span>, <span class="string">&#x27;19&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy = re.findall(eu, <span class="string">&#x27;Alan Mathison Turing OBE FRS (23/6/12-7/6/54) \</span></span><br><span class="line"><span class="string">... was an English computer scientist.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dmy</span><br><span class="line">[(<span class="string">&#x27;23/6/12&#x27;</span>, <span class="string">&#x27;23/6&#x27;</span>, <span class="string">&#x27;23&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/12&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;7/6/54&#x27;</span>, <span class="string">&#x27;7/6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;/54&#x27;</span>, <span class="string">&#x27;&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>这个正则表达式正确地从维基百科的摘录中提取了图灵的生卒日期。但我作弊了，我把“6月”这个月份转换成了数字6，然后才在维基百科的那句话上测试正则表达式。所以这不是一个现实的例子。而且如果不指定世纪的话，你对年份的解析还是会有一些歧义的。54年是指1954年还是指2054年？你希望你的聊天机器人能够从未经修改的维基百科文章中提取日期，这样它就可以阅读名人的资料，学习导入日期。为了让您的正则表达式能够在更多的自然语言日期上发挥作用，例如维基百科文章中的日期，您需要在您的日期提取正则表达式中添加诸如“June"（及其所有缩写）这样的单词。 你不需要任何特殊符号来表示单词（按顺序排列的字符）。你可以在正则表达式中完全按照你希望它们在输入中的拼写来输入，包括大写。你所要做的就是在它们之间的正则表达式中加上一个OR符号(|)。你需要确保它可以处理美国的月/日顺序以及欧洲的顺序。你将把这两种可供选择的日期“拼法“添加到你的正则表达式中，在它们之间加上一个“大“的 OR (|)，作为正则表达式中决策树的分叉。 让我们使用一些命名组来帮助你识别年份，比如"'84'"是1984，"08“是2008。而且让我们尝试更精确地匹配4位数的年份，只匹配未来到2399年的年份和过去到0.6年的年份，请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_19xx = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr_19xx&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>, <span class="number">100</span>)) +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_20xx = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr_20xx&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;:02d&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)) + <span class="string">&#x27;|&#x27;</span> +</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;|&#x27;</span>.join(<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>, <span class="number">30</span>)) +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_cent = <span class="string">r&#x27;\b(?P&lt;yr_cent&gt;&#x27;</span> + <span class="string">&#x27;|&#x27;</span>.join(</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">40</span>)) + <span class="string">r&#x27;)&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_ccxx = <span class="string">r&#x27;(?P&lt;yr_ccxx&gt;&#x27;</span> + <span class="string">&#x27;|&#x27;</span>.join(</span><br><span class="line"><span class="meta">... </span><span class="string">&#x27;&#123;:02d&#125;&#x27;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>)) + <span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr_xxxx = <span class="string">r&#x27;\b(?P&lt;yr_xxxx&gt;(&#x27;</span> + yr_cent + <span class="string">&#x27;)(&#x27;</span> + yr_ccxx + <span class="string">r&#x27;))\b&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yr = (</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;\b(?P&lt;yr&gt;&#x27;</span> +</span><br><span class="line"><span class="meta">... </span>yr_19xx + <span class="string">&#x27;|&#x27;</span> + yr_20xx + <span class="string">&#x27;|&#x27;</span> + yr_xxxx +</span><br><span class="line"><span class="meta">... </span><span class="string">r&#x27;)\b&#x27;</span></span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>groups = <span class="built_in">list</span>(re.finditer(</span><br><span class="line"><span class="meta">... </span>yr,“<span class="number">0</span>, <span class="number">2000</span>, 01, <span class="string">&#x27;08, 99, 1984, 2030/1970 85 47 `66&quot;))</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; full_years = [g[&#x27;</span>y<span class="string">r&#x27;] for g in groups]</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; full_years</span></span><br><span class="line"><span class="string">[&#x27;</span><span class="number">2000</span><span class="string">&#x27;, &#x27;</span>01<span class="string">&#x27;, &#x27;</span>08<span class="string">&#x27;, &#x27;</span><span class="number">99</span><span class="string">&#x27;, &#x27;</span><span class="number">1984</span><span class="string">&#x27;, &#x27;</span><span class="number">2030</span><span class="string">&#x27;, &#x27;</span><span class="number">1970</span><span class="string">&#x27;, &#x27;</span><span class="number">85</span><span class="string">&#x27;, &#x27;</span><span class="number">47</span><span class="string">&#x27;, &#x27;</span><span class="number">66</span><span class="string">&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>哇！这可真够费劲的。这是一个很大的工作，只是为了在regex中而不是在Python中处理一些简单的年份规则。别担心，有一些包可以用来识别常见的日期格式。它们更精确 (更少的错误匹配) 和更通用 (更少的失误)。所以，你不需要自己能够编写复杂的正则表达式，比如这样。这个例子只是给你提供了一个模式，以备将来你需要使用正则表达式提取某种特定的数字。货币值和 IP 地址是一些例子，在这些例子中，一个更复杂的正则表达式（带有命名组）可能会派上用场。 让我们完成你的提取日期的正则表达式，为维基百科上的日期添加月份名称的模式，比如图灵生日中的“June“或“Jun"，如下面的列表所示。你能看到如何将这些正则表达式组合成一个更大的可以处理欧盟和美国日期格式的正则表达式吗？一个复杂的问题是，你不能为一个组重复使用相同的名称（正则表达式的括号部分）。所以你不能在月份和年份的命名正则表达式的美国和欧盟排序之间放一个OR。而且你需要在日、月、年之间包含一些可选的分隔符的模式。最后，你需要验证这些日期，看看它们是否可以变成有效的Python日期时间对象，如下面的列表所示。想想像Python-dateutil和datefinder这样的包是如何解决歧义和处理更多“自然“语言的日期，比如“今天“和“下周一"。如果你认为你能比这些包做得更好，就给他们发个拉请求吧! 如果你只是想要一个最先进的日期提取器，统计（机器学习）方法会让你更快地达到目的。Stanford Core NLP SUTime库(https:// nlp.stanford.edu/software/sutime.html)和Google的dateutil.parser.parse是最先进的。</p>
<h2 id="提取关系relation">11.4 提取关系（relation）</h2>
<p>到目前为止，你只研究了提取棘手的名词实例，如日期和GPS经纬度值。而且你主要是处理数字模式。现在是时候解决从自然语言中提取知识这个更难的问题了。</p>
<p>你想让你的机器人从阅读知识百科全书（如维基百科）中了解关于世界的事实。你希望它能够将这些日期和GPS坐标与它所阅读的实体联系起来。你的大脑可以从维基百科的这句话中提取什么知识呢？</p>
<blockquote>
<p>On March 15, 1554, Desoto wrote in his journal that the Pascagoula people ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5.</p>
<p>1554年3月15日，德索托在他的日记中写道，帕斯卡古拉人的范围是位于30.4，-88.5的最北边的利夫河和奇卡索河的交汇处。</p>
</blockquote>
<p>提取日期和GPS坐标也许能让你把这个日期和地点、德索托、帕斯卡古拉人以及这两条你念不出名字的河流联系起来。你希望你的机器（和你的大脑）能够将这些事实与更大的事实联系起来——例如，德索托是一个西班牙征服者，而帕斯卡古拉人是一个和平的美国土著部落。而且你希望日期和地点能与正确的“东西”联系起来：分别是德索托和两条河流的交汇处。</p>
<p>这就是大多数人听到自然语言理解这个词时想到的。要理解一个语句，你需要能够提取关键的信息，并将其与相关知识关联起来。对于机器来说，你将这些知识存储在一个图谱中，也称为知识库。你的知识图谱的边就是事物之间的关系。而你的知识图谱的节点就是在你的语料库中找到的名词或对象。 你要用来提取这些关系（或关系）的模式是一种subject - verb - object这样的模式。为了识别这些模式，你需要你的NLP管道知道句子中每个词的词性。</p>
<h3 id="词性pos标记">11.4.1 词性（POS）标记</h3>
<p>POS标记可以通过语言模型来完成，这些语言模型具有包含所有可能词性的单词字典。然后，他们可以对正确标记的句子进行训练，以识别新句子和字典中的其他单词中的词性。NLTK和spaCy都实现了POS标记功能。你在这里会使用spaCy，因为它更快、更准确。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> spacy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>en_model = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = (<span class="string">&quot;In 1541 Desoto wrote in his journal that the Pascagoula people&quot;</span> + <span class="string">&quot;ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent = en_model(sentence)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent.ents</span><br><span class="line">(<span class="number">1541</span>, Desoto, Pascagoula, Leaf, Chickasawhay, <span class="number">30.4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27; &#x27;</span>.join([<span class="string">&#x27;&#123;&#125;_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tok, tok.tag_) <span class="keyword">for</span> tok <span class="keyword">in</span> parsed_sent])</span><br><span class="line"><span class="string">&#x27;In_IN 1541_CD Desoto_NNP wrote_VBD in_IN his_PRP$ journal_NN that_IN the_DT Pascagoula_NNP people_NNS ranged_VBD as_RB far_RB north_RB as_IN the_DT confluence_NN of_IN the_DT Lea f_NNP and_CC Chickasawhay_NNP rivers_VBZ at_IN 30.4_CD ,_, -88.5_NFP ._.&#x27;</span></span><br></pre></td></tr></table></figure>
<p>因此，为了建立你的知识图谱，你需要弄清楚哪些对象（名词短语）应该配对。你想把日期“1554年3月15日“与命名实体Desoto配对。然后你可以将这两个字符串（名词短语）解析为指向你知识库中的对象。1554年3月15日可以转换为具有归一化表示的datetime.date对象。</p>
<p>spaCy-parsed句子还包含嵌套字典中的依存树。而spacy.displacy可以生成一个可扩展的矢量图形SVG字符串（或一个完整的HTML页面），它可以在浏览器中作为图像查看。这种可视化可以帮助您找到使用该树创建标签模式以进行关系提取的方法。请看下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> spacy.displacy <span class="keyword">import</span> render</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = <span class="string">&quot;In 1541 Desoto wrote in his journal about the Pascagoula.&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_sent = en_model(sentence)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;pascagoula.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="meta">... </span>f.write(render(docs=parsed_sent, page=<span class="literal">True</span>, options=<span class="built_in">dict</span>(compact=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>
<p>这个短句的依存树显示名词短语“The Pascagoula”是主语“Desoto”关系“met”的宾语（见图11.2）。两个名词都被标记为专有名词。</p>
<p><img src="https://i.loli.net/2021/04/13/oF3AcQ9CBVrps1X.png"/></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">token_dict</span>(<span class="params">token</span>):</span></span><br><span class="line"><span class="meta">... </span>	<span class="keyword">return</span> OrderedDict(ORTH=token.orth_, LEMMA=token.lemma_, POS=token.pos_, TAG=token.tag_, DEP=token.dep_)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">doc_dataframe</span>(<span class="params">doc</span>):</span></span><br><span class="line"><span class="meta">... </span>	<span class="keyword">return</span> pd.DataFrame([token_dict(tok) <span class="keyword">for</span> tok <span class="keyword">in</span> doc])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc_dataframe(en_model(<span class="string">&quot;In 1541 Desoto met the Pascagoula.&quot;</span>))</span><br><span class="line">         ORTH       LEMMA    POS  TAG    DEP</span><br><span class="line"><span class="number">0</span>          In          <span class="keyword">in</span>    ADP   IN   prep</span><br><span class="line"><span class="number">1</span>        <span class="number">1541</span>        <span class="number">1541</span>    NUM   CD   pobj</span><br><span class="line"><span class="number">2</span>      Desoto      desoto   NOUN   NN  nsubj</span><br><span class="line"><span class="number">3</span>         met        meet   VERB  VBD   ROOT</span><br><span class="line"><span class="number">4</span>         the         the    DET   DT    det</span><br><span class="line"><span class="number">5</span>  Pascagoula  Pascagoula  PROPN  NNP   dobj</span><br><span class="line"><span class="number">6</span>           .           .  PUNCT    .  punct</span><br></pre></td></tr></table></figure>
<p>现在您可以看到POS或TAG特性的序列，它们将构成一个良好的模式。如果您正在查找人员和组织之间的“has meet”关系，您可能希望允许使用诸如“PROPN met PROPN”、“PROPN met the PROPN”、“PROPN met the PROPN”、“PROPN met with the PROPN”和“PROPN frequency meeting with PROPN”之类的模式。您可以分别指定这些模式中的每一个，或者尝试使用一些*或者？专有名词之间的“任意词”模式的运算符：</p>
<blockquote>
<p>'PROPN ANYWORD? met ANYWORD? ANYWORD? PROPN'</p>
</blockquote>
<p>spaCy中的模式比前面的伪代码更强大、更灵活，因此您必须更详细地解释您想要匹配的word特性。在spaCy模式规范中，您可以使用字典来捕获要为每个标记或单词匹配的所有标记，如下面的清单所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = [[&#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, [&#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;],</span><br><span class="line"><span class="meta">... </span>	[&#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;meet&#x27;</span>&#125;],</span><br><span class="line"><span class="meta">... </span>	[&#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;, &#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;]]</span><br></pre></td></tr></table></figure>
<p>然后可以从解析的句子中提取所需的标记标记，如下面的清单所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;In 1541 Desoto met the Pascagoula.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher = Matcher(en_model.vocab)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher.add(<span class="string">&#x27;met&#x27;</span>, [pattern])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">[(<span class="number">12280034159272152371</span>, <span class="number">2</span>, <span class="number">6</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[<span class="number">0</span>][<span class="number">1</span>]:m[<span class="number">0</span>][<span class="number">2</span>]]</span><br><span class="line">Desoto met the Pascagoula</span><br></pre></td></tr></table></figure>
<p>所以你从创建pattern的原始句子中提取了一个匹配项，但是维基百科中类似的句子呢？请参见下面的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;October 24: Lewis and Clark met their first Mandan Chief, Big White.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">(<span class="number">12280034159272152371</span>, <span class="number">3</span>, <span class="number">11</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[<span class="number">1</span>]:m[<span class="number">2</span>]]</span><br><span class="line">Lewis <span class="keyword">and</span> Clark met their first Mandan Chief</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;On 11 October 1986, Gorbachev and Reagan met at a house&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher(doc)</span><br><span class="line">[]</span><br></pre></td></tr></table></figure>
<p>您需要添加第二个模式，以允许动词出现在主语和宾语名词之后，如下面的清单所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = en_model(<span class="string">&quot;On 11 October 1986, Gorbachev and Reagan met at a house&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pattern = [&#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, &#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;and&#x27;</span>&#125;, &#123;<span class="string">&#x27;TAG&#x27;</span>: <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;+&#x27;</span>&#125;, &#123;<span class="string">&#x27;IS_ALPHA&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;OP&#x27;</span>: <span class="string">&#x27;*&#x27;</span>&#125;, &#123;<span class="string">&#x27;LEMMA&#x27;</span>: <span class="string">&#x27;meet&#x27;</span>&#125;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matcher.add(<span class="string">&#x27;met&#x27;</span>, <span class="literal">None</span>, pattern)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = matcher(doc)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">[(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">9</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">11</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">7</span>, <span class="number">11</span>),</span><br><span class="line">(<span class="number">14332210279624491740</span>, <span class="number">5</span>, <span class="number">12</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc[m[-<span class="number">1</span>][<span class="number">1</span>]:m[-<span class="number">1</span>][<span class="number">2</span>]]</span><br><span class="line">Gorbachev <span class="keyword">and</span> Reagan met at a house</span><br></pre></td></tr></table></figure>
<p>所以现在你有了你的实体和关系。您甚至可以构建一个模式，该模式对中间的动词（“met”）限制较少，对两边的人和组的名称限制更多。这样做可能会让你识别出暗示一个人或一组人遇到了另一个人或另一组人的其他动词，比如动词“知道”，甚至是被动短语，比如“有过一次谈话”或“变得熟悉”。然后你可以使用这些新动词来为两边的新专有名词添加关系。但你可以看到你是如何偏离你的种子关系模式的最初含义的。这就是所谓的语义漂移。</p>
<p>幸运的是，spaCy不仅用词性和依存关系树信息来标记解析文档中的单词，而且还用词性模式匹配器（如清单11.18所示）将多个模式结合起来，以获得更健壮的模式匹配器该模式不匹配Wikipedia中句子的任何子字符串。添加附加图案而不删除以前的图案。这里的“met”是一个任意键。随便你怎么命名你的图案。“+”运算符增加了重叠替代匹配的数量。最长的匹配是匹配列表中的最后一个。357提取关系（relations）还提供Word2vec单词向量。您可以使用此向量来防止连接动词和两边的专有名词偏离种子模式的原始含义太远。</p>
<h3 id="实体名称规范化">11.4.2实体名称规范化</h3>
<p>实体的规范化表示通常是字符串，即使对于日期等数字信息也是如此。此日期的标准化ISO格式为“1541-01-01”。实体的标准化表示使您的知识库能够将世界上在同一日期发生的所有不同事件连接到图形中的同一节点（实体）。对其他命名实体也会这样做。</p>
<p>你应该纠正单词的拼写，并尝试解决物体、动物、人、地方等名称的歧义。规范化命名实体和解决歧义通常被称为共指消解或回指消解，特别是对于依存上下文的代词或其他“名称”。这类似于我们在第2章讨论的lemmatization。命名实体的规范化可确保拼写和命名变体不会因混淆、冗余的名称而污染实体名称的词汇表。例如，“Desoto”在一个特定的文档中至少可以用五种不同的方式来表达：</p>
<ul>
<li>“de Soto”</li>
<li>“Hernando de Soto”</li>
<li>“Hernando de Soto (c. 1496/1497–1542), Spanish conquistador”</li>
<li>https://en.wikipedia.org/wiki/Hernando_de_Soto (a URI)</li>
<li>A numerical ID for a database of famous and historical people</li>
</ul>
<p>类似地，您的规范化算法可以选择这些形式中的任何一种。知识图应该以相同的方式规范化每种实体，以防止同一类型的多个不同实体共享相同的名称。您不希望多个人名引用同一个自然人。更重要的是，无论是在向知识库中写入新事实时，还是在阅读或查询知识库时，规范化都应该始终如一地应用。如果在填充数据库之后决定更改规范化方法，则应该“迁移”或更改知识中现有实体的数据，以遵循新的规范化方案。无模式数据库（键值存储）与用于存储知识图或知识库的数据库一样，不能免除关系数据库的迁移责任。毕竟，无模式数据库是关系数据库的接口包装器。您的规范化实体还需要“is-a”关系来将它们连接到定义实体类型或类别的实体类别。这些“is-a”关系可以看作是标记，因为每个实体可以有多个“is-a”关系。就像7，这是积极研究的主题：https://nlp.stanford.edu/pubs/structuredVS.pdf。如果您想将人名或POS标签、日期和其他离散的数字对象合并到您的知识库中，则需要对它们进行规范化。实体之间的关系需要以正常的方式存储吗？</p>
<h3 id="关系规范化和提取">11.4.3关系规范化和提取</h3>
<p>现在需要一种方法来规范关系，以识别实体之间的关系类型。这样做可以让你找到日期和人之间的所有生日关系，或者历史事件发生的日期，例如“Hernando de Soto”和“Pascagola people”之间的遭遇。您需要编写一个算法来为您的关系选择正确的标签。这些关系可以有一个层次名称，例如“发生在/大约”和“发生在/确切地”以允许您找到特定的关系或关系类别。您还可以使用该关系的“置信度”、“概率、权重或标准化频率”（术语/词的ANLO  GOU到TF-IDF）的数值属性标记这些关系。每次从新文本中提取的事实证实或与数据库中存在的事实相矛盾时，都可以调整这些置信值。现在，您需要一种方法来匹配可以找到这些关系的模式。</p>
<h3 id="字型字模式">11.4.4字型字模式</h3>
<p>与正则表达式一样，而非字符。您没有字符类，而是有单词类。例如，您可能会有一个单词模式决定来匹配所有的单数名词（“NN”POS标记），8这通常是通过机器学习完成的。有些种子句子被标记了一些正确的关系（事实）从这些句子中提取。POS模式可以用来找到类似的句子，在这些句子中主语和宾语，甚至关系都可能发生变化。无论您希望匹配多少模式，您都可以使用spaCy包两种不同的方法来匹配O（1）（恒定时间）中的这些模式：”“。任何单词/标记序列模式的phrasether 9™POS标记序列模式匹配器10，以确保新句子中找到的新关系与原始种子（例如）真正类似（示例）关系，你经常需要约束主语，关系，和宾语的意思，以类似于种子句。最好的方法是用一些向量表示单词的意思。这个响吗？第4章讨论的词向量是目前应用最广泛的词义表示方法之一。它们有助于最小化语义漂移。</p>
<p>参考：</p>
<p><a href="https://livebook.manning.com/book/natural-language-processing-in-action/chapter-11/1">Chapter 11. Information extraction (named entity extraction and question answering) - Natural Language Processing in Action: Understanding, analyzing, and generating text with Python (manning.com)</a></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>https://www.pythonf.cn/read/129220<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>IR</tag>
      </tags>
  </entry>
  <entry>
    <title>心理语言学：资源和知识整理</title>
    <url>/psycho-linguistics/</url>
    <content><![CDATA[<h1 id="心理语言学术语glossary">心理语言学术语（Glossary）</h1>
<p>Absolute terms Spatial terms that refer to the location of an object in space irrespective of the location of a person (for example, north/south). Accommodation A phonological process in which elements that are shifted or deleted are adapted to their errorinduced environments. Acoustic phonetics The branch of phonetics that specifies the acoustic characteristics associated with each speech sound. Acquired dyslexia A form of reading disability in a previously literate person who has sustained brain damage. Active processing A collection of activities that includes relating new information to information we have in permanent memory, asking questions of the material, and writing summaries or outlines of the material. Active voice A sentence in which the surface structure subject is also the deep structure or logical subject of the sentence, such as The woman scolded the child. Addition A speech error in which linguistic material is added, as in I didn’t explain this clarefully enough [carefully enough]. Affricate A consonant that begins with complete closure of the vocal tract followed by gradual release of air pressure, such as the ch in church. Agent The thematic or semantic role corresponding to an individual who performs a given action, such as the manager in The manager opened the store. Agrammatic speech Speech in which there is a lack of grammatical structure, such as the absence of grammatical morphemes and function words. Agrammatism See agrammatic speech. Agraphia An aphasia characterized by the inability to write. Alexia An aphasia characterized by the inability to comprehend written or printed words. Alphabet A writing system in which each letter is supposed to represent a phoneme. Alveolar A consonant articulated at the alveolar ridge, such as the d in dog. Ambiguity A property of language in which a word or sentence may be interpreted in more than one way. See also deepstructure ambiguity, lexical ambiguity, and phrase ambiguity. American Sign Language (ASL) The form of sign language used in the United States. It is a complete language distinct from oral languages. Anaphor A linguistic expression that refers back to prior information in discourse. Anaphoric reference A form of reference cohesion in which one linguistic expression refers back to prior information in discourse. Angular gyrus Believed to serve as an association area in the brain that connects one region with another; particularly important for the association of visual stimuli with linguistic symbols. Damage to the angular gyrus leads to both alexia and agraphia. Animacy A semantic feature denoting whether an object is alive. Anomalous suspense In narrative comprehension, the experience of suspense when the reader already knows how a story will turn out. Antecedent Prior information in discourse. Anticipation A speech error in which a later word or sound takes the place of an earlier one. Anticipatory coarticulation Type of coarticulation in which the shape of the vocal tract for a given speech sound is influenced by upcoming sounds. Anticipatory retracing Selfrepair in which the speaker traces back to some point prior to an error. Previously correct material is repeated along with the corrected material. See also fresh start and instant repair. Aphasia A language or speech disorder caused by brain damage. Arbitrariness A feature of language in which there is no direct resemblance between words and their referents. Arcuate fasciculus The primary pathway in the brain between Wernicke’s area and Broca’s area. Articulatory phonetics The branch of phonetics that specifies the articulatory gestures associated with each speech sound. Aspiration A puff of air that accompanies the production of certain speech sounds. Aspiration is phonemic in some languages but not in English. Assertion A communicative act in which a person draws the attention of another person to a particular object—for example, a child showing a toy to an adult as if to say This is mine. Assertions may be made through words or gestures. Assimilation A phonological process in which one speech sound is replaced by another that is similar to sounds elsewhere in the utterance. Associative chain theory A theory favored by behaviorists that explains the formulation of a sentence as a chain of associations between the individual words in the sentence. Attemptsuppressing signal A cue given by a speaker to indicate to a listener that he or she is not finished. Attributive relations Relations between words that indicate the attributes of a given word, such as round as an attribute for ball. Auditory level A level of speech perception in which the speech signal is represented in terms of frequency, intensity, and temporal attributes. Automaticity A property of cognitive processes that do not require any processing capacity. Automatic process An activity that does not require any processing capacity. Autonoetic consciousness A form of consciousness in which one experiences time, as past, present, or future. Auxiliary verb A ‘‘helping verb.’’ A verb such as is, do, or can used in conjunction with the main verb in a sentence, such as Kim is gardening this afternoon. Baby talk See motherese. Background In discourse processing, information that was introduced or discussed earlier and is no longer the focus of discussion. Basic child grammar The grammatical characteristics of early child language, such as telegraphic speech, found in numerous languages. Basic color term A term that refers to color and that is only one morpheme, not contained within another color, and not restricted to a small number of referents. Basiclevel term A term that refers to a category in which there are broad similarities among exemplars. Behaviorism The doctrine that states that the proper concern of psychology should be the objective study of behavior rather than the study of the mind. Bilabial A consonant articulated at the mouth such as the b in big. Bilingual firstlanguage acquisition When children acquire two languages at the same time. Binaural perception A procedure in which the same stimulus is presented to the two ears. Blend A speech error in which two or more words are combined. Bottomup processing A process in which lowerlevel processes are carried out without influence from higherlevel processes (for example, perception of phonemes being uninfluenced by the words in which they appear). Bound morpheme A unit of meaning that exists only when combined or bound to a free morpheme. Bridging A process in which the listener or reader draws inferences to build a ‘‘bridge’’ between the current utterance and preceding utterances. Broca’s aphasia An aphasia characterized by deficits in language production. Also called expressive aphasia. Broca’s area A brain region in the frontal lobe of the left hemisphere. Damage to this region leads to Broca’s aphasia. Bystanders Individuals who are openly present but not part of a conversation. Cataphoric reference A form of reference cohesion in which one linguistic expression refers to information yet to be introduced in discourse. Categorical perception The inability to discriminate sounds within a phonemic category. Categorysize effect The fact that it takes longer to semantically verify a statement of the form An A is a B if B is a larger semantic category. Categoryspecific dissociations In aphasia, the selective inability to retrieve certain categories of words, such as fruits or vegetables, while retaining the ability to recognize and use other word categories. Childdirected speech Speech addressed to children. See also motherese. Childhood amnesia The inability of adults to remember the first few years of life. Also called infantile amnesia. Chunking Grouping individual pieces of information into larger units. A shortterm memory strategy. Closedclass words See function words. Coalescence A phonological process in which phonemes from different syllables are combined into a single syllable. Coarticulation The process of articulating more than one speech sound at a time. Codability The length of a verbal expression. Cognitive constraint A bias that children are assumed to use to infer the meanings of words. Cognitive economy A characteristic of semantic memory in which information is only represented once within a semantic network. Cognitive science The branch of science devoted to the study of the mind; consists of the fields of psychology, artificial intelligence, neuroscience, linguistics, philosophy, and adjacent disciplines. Coherence The degree to which different parts of a text are connected to one another. Coherence exists at both local and global levels of discourse. Cohesion Local coherence relations between adjacent sentences in discourse. GLOSSARY 423 Cohort model A model of auditory word recognition in which listeners are assumed to develop a group of candidates, a word initial cohort, and then determine which member of that cohort corresponds to the presented word. Common ground The shared understanding of those involved in the conversation. Communicative competence The skill associated with using a language appropriately and effectively in various social situations. Complement A noun phrase that includes a verb—for example, you sat down in I see you sat down. Complex sentence A sentence that expresses more than one proposition. Conceptual complexity See semantic complexity. Conceptual metaphor theory In figurative language comprehension, the position that we comprehend figurative language in terms of underlying conceptual metaphors. For example, we might comprehend the metaphor We’re spinning our wheels in terms of the conceptual metaphor LOVE IS A JOURNEY. Conduction aphasia An aphasia characterized by the inability to repeat what one has heard. Conjunctive cohesion A form of cohesion in which we express a relationship between sentences or phrases by using conjunctions such as and, or, and but. Connected discourse See discourse. Connectionist model A model of cognitive/linguistic processes that assumes (1) a vast interconnected network of information nodes in which each node influences and is influenced by a large number of adjacent nodes and 　parallel processing of information. Also called parallel distributed processing. Connotation The aspect of meaning suggested by a word but not strictly part of the word’s dictionary definition. See also denotation. Consonant A speech sound in which the vocal tract is partially or fully closed during production. Constituent A grammatical unit such as a noun or verb phrase. Constraintbased model A model of sentence comprehension in which we simultaneously use all available information (semantic, syntactic, contextual, and so forth) in our initial parsing of a sentence. Content word A word (such as a noun, a verb, or an adjective) that plays a primary role in the meaning of a sentence. See also function word. Contextconditioned variation The fact that the acoustic parameters associated with a given speech sound vary with its phonetic context. Contextualized language Language that is related to the immediate context. Contralateral The arrangement in the nervous system in which one half of the brain controls the other half of the body. Controlled process An activity that requires processing capacity. Convention A shared assumption about communication. Coordination A sentence in which two or more simple sentences are linked by a coordinating expression such as and, or, or but—for example, Lake Superior is beautiful, but it is cold. Also refers to words in the lexicon that are at the same level in a hierarchy, such as sparrow and robin. Copula The verb to be used as the main verb in a sentence such as Miguel is wonderful. Corpus callosum A band of fibers that connects the two cerebral hemispheres. Counterfactual reasoning The ability to reason about an event that is contrary to fact. Count noun A noun that takes the plural morpheme and refers to an object with clear boundaries, such as a stick. Also called an individual noun. Creole The language developed by children who have been exposed to a pidgin as their native language. Critical period hypothesis The view that there is a period early in life in which we are especially prepared to acquire a language. Decontextualized language Language that is separated in time or place from its referent. Deep structure The level of linguistic structure assumed in transformational grammar that expresses the underlying semantic meaning of a sentence. Deepstructure ambiguity A form of ambiguity in which a sentence may be derived from two different deep structures. Default value The value of a parameter that a child is hypothesized to be born with. Deferred imitation Imitation of a behavior that was observed some time earlier. De´ja` vu The erroneous feeling that one has experienced a particular event before. Deletion A speech error in which something is left out. Denotation The dictionary definition of a word. See also connotation. Dental A consonant articulated at the teeth, such as the th in thin. Derivation The series of linguistic rules needed to generate a sentence. Derivational morpheme A bound morpheme that is added to a free morpheme to create a new word. For example, ness turns good (an adjective) into goodness (a noun). Derivational theory of complexity The theory that states that the psychological complexity of a sentence is directly proportional to the length of its derivation. Descriptive adequacy The extent to which a grammar can provide a structural description of a sentence. See also explanatory adequacy and observational adequacy. Determiner A part of speech that quantifies or specifies a count noun, such as the in The cat ate the plant. Dichotic listening task An experimental task in which different stimuli are simultaneously presented to the two ears. Differentiation The number of words in a semantic domain. Discontinuous constituent A grammatical constituent in which some elements are separated, such as picked and up in George picked the baby up. Discourse A group of sentences combined in a meaningful manner. Discreteness A semantic feature denoting whether an object has definite outlines or boundaries. For example, a tree is + discrete, whereas air is – discrete. Dishabituation The recovery of the strength of a habituated response when a novel stimulus is presented. Displacement A feature of language in which words are separated in space and time from their referents. Distinctive feature The specification of the differences between speech sounds in terms of individual contrasts. Duality of patterning A feature of a communication system in which a small number of meaningless units can be combined into a large number of meaningful units. Duplex perception An experimental technique in which formant transitions are presented to one ear and steady states to the other. Eavesdroppers Individuals who listen in on conversations without the participants’ awareness. Elaboration The process of relating incoming information to information already stored in permanent memory. Ellipsis A form of cohesion in which a previous item is dropped from subsequent sentences but its presence is assumed. Empiricism The branch of philosophy that emphasizes the use of controlled observation and the belief that experience shapes human behavior. Episode A component of a story grammar. Episodic memory The division of permanent memory in which personally experienced information is stored. Evoked potential Measurement of electrical activity in a region of the brain following presentation of a stimulus. Exaptation Evolutionary process in which preexisting physical structures are used for new functions. Exchange A speech error in which two sounds or words change places with one another. Excitatory interaction In a connectionist model, the tendency for one unit’s activation to increase the activation of other units. Explanatory adequacy The extent to which a grammar can explain the facts of language acquisition. See also descriptive adequacy and observational adequacy. Explicit knowledge Knowledge of how to perform various acts. See also tacit knowledge. Expository discourse A type of discourse in which the writer’s goal is to convey information about the subject matter. Expressive aphasia See Broca’s aphasia. Expressive strategy A style of child language characterized by low noun/pronoun ratio, poor articulation, clear intonation, and relatively long utterances. Eye–voice span The lag between eye position and voice when reading aloud, about six or seven words. False recognition error When a subject believes that an item was presented during a study although it was not. Fast mapping The process of acquiring new words rapidly. Feature level A level of written language perception in which a visual stimulus is represented in terms of the GLOSSARY 425 physical features that comprise a letter of the alphabet, such as a vertical line, a curved line, and so on. Felicity condition A condition that must be present for a speech act to be understood as sincere or valid. Feral children Children who have grown up without human companionship in the wild. Figurative language Language that means one thing literally but is taken to mean something different. Fis phenomenon When a child mispronounces a word yet correctly distinguishes between child and adult versions of that word. Fixation The time spent focused at a given location during reading; the time between eye movements. Focal color The most representative example of a basic color. Foreground In discourse processing, information that is currently being discussed or explained. Formal complexity See syntactic complexity. Formant A concentrated band of energy found in the sound spectrograms of phonemes. Formant transition A rapid increase or decrease in frequency at the beginning of a formant. Free morpheme A unit of meaning that can stand alone. Fresh start A form of Selfrepair in which the speaker replaces the original syntactic structure with a new one. See also anticipatory retracing and instant repair. Fricative A consonant in which the vocal tract is partially closed during articulation, such as the f in fat. Functional magnetic resonance imaging (f MRI) A method of imaging brain structure and brain activity. Functional relations Relations among words that indicate what can be done with the referent of a word. For example, words such as sitting, rest, and rocking indicate what can be done with a chair. Function word A word such as an article, preposition, or conjunction that plays a secondary role in the meaning of a sentence. See also content word. Garden path sentence A sentence in which the comprehender assumes a particular meaning of a word or phrase but discovers later that the assumption was incorrect, forcing the comprehender to backtrack and reinterpret the sentence. Genre A category of discourse characterized by a particular form or content, such as the genre for fairy tales. Given information Information that the speaker assumes the listener already knows. Given/new strategy A comprehension strategy in which utterances are analyzed into given and new components and the new information is stored in memory with previously received information. Global structure See macrostructure. Glottis The opening between the vocal cords. Grammar In linguistics, a theory of language or set of hypotheses about how language is organized. Grammatical gender The grammatical property in which languages identify objects as masculine, feminine, and sometimes neuter. Grammatical morpheme See bound morpheme. Grapheme A printed letter of the alphabet. Ground In metaphor, the implied similarity between tenor and vehicle. Habituation The decline in a response to a stimulus following repeated presentation of the stimulus. Head parameter A grammatical feature that specifies the position of the head of a phrase (noun in noun phrase, verb in verb phrase, and so on). Hemispherectomy A surgical procedure in which one of the cerebral hemispheres is removed. Holistic processing A style of processing, associated with the right cerebral hemisphere, that is global in nature. Holophrase A oneword utterance used by a child to express more than the meaning attributed to the word by adults. Homesign A form of gestural communication invented by deaf children who are not exposed to a sign language. Hominids The family of species that includes modernday human beings. Also called hominins. Homophone A word that is pronounced the same as another word but means something different, such as to and two. Hypernymy A semantic relationship in which a word is a subordinate of another. For example, animal is a hypernym of dog. Hyponymy A semantic relationship in which a word is a superordinate of another. For example, collie is a hyponym of dog. Iconicity A characteristic of language in which words resemble their referents. Idiomorph A sound or sound sequence used consistently by a child to refer to someone or something even though it is not the sound sequence conventionally used in the language for that purpose. Illocutionary force In speech act theory, the action that is performed by a speaker in uttering a sentence. Immediacy principle The principle that we immediately interpret words as we encounter them. Incremental processing The notion that we are planning one portion of our utterance as we articulate another portion. Indirect speech act A speech act in which the literal utterance meaning is not the same as the speaker’s meaning. Induction A process of reasoning from the specific to the general. For instance, if all of the specific horses we have seen are brown, then we might induce that all horses are brown. Inference A proposition drawn by the listener or reader. Inflectional morpheme A bound morpheme that is added to a free morpheme to express grammatical contrasts in sentences. English examples include the plural and past tense morphemes. Inhibitory interaction In a connectionist model, the tendency for one unit’s activation to decrease the activation of other units. Initiationreplyevaluation sequence A form of discourse used in classrooms in which the teacher asks a student a question, the student answers, and the teacher evaluates the answer. Instantiation Identifying a general term with a specific meaning. Instant repair A form of Selfrepair in which the speaker traces back to an error that is then replaced with the correct word. See also anticipatory retracing and fresh start. Institutional setting A conversational setting in which participants engage in speech exchanges that resemble ordinary conversations but are limited by institutional rules. Interactional content Content of a sentence that conveys the speaker’s attitude toward the listener. Utterances that are high in interactional content include jokes, insults, and excessively polite speech. Interactive gesture A form of gesture used in conversation to convey interactional content, such as holding up one’s hands to indicate that one’s turn is not finished. Internal lexicon The storage of lexical information in memory. Interruption A period of simultaneous speech more than one word prior to the speaker’s projected completion point. See also overlap. Intersection search The process of retrieving information from a semantic network. Intonation The use of pitch to signal meaning. Intonational contour A pattern of pitch changes characteristic of an utterance as a whole, such as the rising intonation often found in questions. Intrinsic terms Spatial terms that refer to objects in relation to various object coordinates (such as behind the house, at the tip of the post). Ipsalateral The arrangement in the nervous system in which one half of the brain controls the same side of the body. Isolated children Children who have grown up without normal human interactions. Joint action An action carried out by an ensemble of people acting in coordination with one another. Examples include dancing and conversing. Kana Japanese syllabic symbols. Kanji Japanese logographic characters borrowed from Chinese. Lack of invariance The fact that there is no onetoone correspondence between speech cues and perception. Language Within linguistic theory, an infinite set of wellformed sentences. Language bioprogram A hypothesized innate grammar that is used by children whose environmental exposure to language is limited. The bioprogram is assumed to be suppressed in children whose language environment is normal. Language bioprogram hypothesis The hypothesis that children whose environmental exposure to language is limited use a backup linguistic system. Language transfer In secondlanguage acquisition, the process in which the first language influences the acquisition of a subsequent language. Laryngeal system The system of muscles that determines whether a speech sound is voiced or voiceless. GLOSSARY 427 Late closure strategy A strategy used in parsing that states that wherever possible we prefer to attach new items to the current constituent. Lateralization The extent to which a given psychological function is served by one hemisphere of the brain. Functions primarily served by one hemisphere are said to be lateralized to that hemisphere. Lemma Syntactic aspects of word knowledge. Letter level The level of written perception in which a visual stimulus is represented as a letter of the alphabet. Lexeme Phonological aspects of word knowledge. Lexical access The process of activating lexical items from semantic memory. Lexical ambiguity A form of ambiguity in which a word has more than one meaning. Lexical bias effect The finding that speech errors more commonly result in true words than would be expected by chance. Lexical cohesion The use of reiteration, synonymy, hyponymy, and other semantic relationships to link successive sentences in discourse. Lexical decision task An experimental task in which a subject sees a string of letters and must rapidly decide whether the string is a word. Lexicalfunctional grammar A grammar in which structural relationships are built into enriched lexical entries rather than with transformational rules. Lexicalinsertion rule A rule that governs how lexical entries are inserted into a tree structure during the derivation of a sentence. Lexicon The vocabulary of a language. See also internal lexicon. Linguistic creativity See linguistic productivity. Linguistic determinism The hypothesis that languages determine nonlinguistic cognitive processes such as the perception of shapes. Linguistic productivity The ability to create or comprehend an infinite number of new sentences that are grammatically correct; also called linguistic creativity. Linguistic relativity The hypothesis that the cognitive processes determined by language vary from language to language. Linguistics The branch of science that studies the origin, structure, and use of language. Local structure See microstructure. Locutionary act In speech act theory, the act of saying something. Logogen Structure in the internal lexicon that specifies the various attributes (semantic, orthographic, and so on) of a word. Logography An orthography in which spoken words are represented by visual symbols. Longitudinal investigation A method of studying child development in which a small number of children are studied over a period of years. Longterm memory See permanent memory. Macrostructure The global coherence relationships in discourse. Manner of articulation How a speech sound is articulated (for example, stop, fricative, and so on). Manual English A manual version of English, as in fingerspelling the letters of the English alphabet. See also American Sign Language. Mass noun A noun that does not take the plural morpheme and refers to objects without clear boundaries, such as air. Mean length of utterance in morphemes (MLU) An index of children’s language growth. It is computed by dividing the number of morphemes by the number of utterances. Mental model A mental representation of some aspect of the world. Meronymy A semantic relationship that pertains to the parts of an object referred to by a word; for example, for the word car, both engine and wheels are meronyms because they refer to parts of a car. Metalinguistic awareness The ability to think of language as an object. Metaphor A form of language in which a word or phrase that literally denotes one idea is interpreted to mean a different one and suggests a similarity between the two—for example, My head is an apple without a core. Microstructure The local coherence relationships in discourse. Minimal attachment strategy A principle used in parsing. It states that we prefer attaching new items into the phrase marker being constructed using the fewest syntactic nodes consistent with the rules of the language. Minimal response An utterance such as uhhuh or umhmm made by a listener during a conversation. Ordinarily minimal responses are taken as displays of interest in a speaker’s topic. Mispronunciation detection An experimental task in which subjects are presented auditorily with tapes that occasionally include mispronounced words. The subject’s task is to detect the mispronunciations. Modularity The degree to which language processing is independent of general cognitive processes such as memory and reasoning. Also refers to the degree to which an aspect of language is independent of other aspects of language. For example, parsing may be thought of as modular if there is a syntactic processor that operates independently of semantic and discourse processes. Morpheme The smallest unit of meaning in a language. Morphology The system of wordforming elements and processes in a language. Motherese A form of adulttochild speech characterized by relatively simple utterances, concrete referents, exaggerated intonation patterns, and a high proportion of directive utterances. Mutual exclusivity bias A cognitive constraint in which children assume that an object is ordinarily not given two different names. Narrative discourse A form of discourse in which settings, characters, and plot play a central role. Nasal A consonant in which air flows through the nasal cavity as in the n in nail. Nativism An approach to language acquisition that emphasizes the innate organization of language. Necessary condition A condition that must be present in order for a specified event to occur. Negative evidence Evidence that a particular linguistic expression (a word or sentence) is inappropriate or unacceptable. Negative evidence may be presented explicitly (No, that’s not a cow; that’s a dog) or implicitly (such as when adults repeat child utterances with corrections). Neurolinguistics The study of how linguistic information is processed in the brain. New information Information that the comprehender (reader or listener) is assumed not to know. Nonnutritive sucking A procedure used in research with infants in which sucks on a pacifier are recorded. Nullsubject parameter A grammatical feature that specifies whether a language permits sentences without subjects. Also called the prodrop parameter. Object permanence The awareness that objects that can no longer be seen still exist. Object relative clause A relative clause in which a wh clause modifies the object of a sentence. Observational adequacy The extent to which a grammar can distinguish between acceptable and unacceptable strings of words. See also descriptive adequacy and explanatory adequacy. Occipital lobe The visual center at the back of the brain. Openclass word See content word. Operating principle A preferred way of taking in or operating on information. Original word game A game in which adults teach children the names of words. Children point to an object and say, ‘‘What’s that?’’ and the adult supplies the name. Orthography The representation of a sound by written or printed symbols. Ostensive definition The process of defining a word by pointing to its referent. Overextension When a child uses a word to refer to a larger set of referents than an adult would—for example, calling a round clock a moon. Overhearers Individuals who are not part of a conversation. May be bystanders or eavesdroppers. Overlap A period of simultaneous speech during the last word of a speaker’s projected closing. See also interruption. Overregularization When a child applies a linguistic rule to cases that are exceptions to the rule—for example, saying goed instead of went. Paragrammatic speech Speech that is fluent but not coherent and that contains many irrelevant associations. Parallel distributed processing See connectionist model. Parallel processing When two or more processes take place at the same time. Parallel transmission The notion that different phonemes of the same syllable are encoded into the speech signal simultaneously. Parameter (1) In grammatical theory, a grammatical feature that is set in different ways in different languages. See also head parameter and nullsubject parameter. GLOSSARY 429 In American Sign Language, a dimension along which signs may differ, such as hand configuration, movement, and location. Parameter setting In grammatical theory, the notion that children are born with grammatical parameters that are preset to certain values. Language acquisition is seen as a matter of resetting these parameters to the values of one’s native language. Parietal lobe Middle brain region containing motor centers that control facial and speech muscles. Parsing The process of assigning words into grammatical categories. Partial report technique A method for studying the sensory stores. Subjects are briefly presented with an array of stimuli and asked to report only a portion of the array. Participants Individuals who are taking part in a conversation. Particlemovement transformation A transformational rule that accounts for the movement of particles such as up around noun phrases. Passive transformation A transformational rule that transforms the deep structure of an active sentence into the passive voice. Passive voice A sentence in which the surface structure subject is the deep structure or logical object of the action, such as in The child was scolded by the mother. Patient A thematic or semantic role corresponding to the individual acted on, such as the elderly man in The neighborhood frightened the elderly man. Pattern recognition A process of matching information in the sensory stores with information retrieved from permanent memory. Perceptual span The size of the area from which a reader picks up visual information. Perlocutionary effect In speech act theory, the effect of a speech act on a listener. Permanent memory Memory that is essentially permanent (also called longterm memory). Includes semantic and episodic memory. Perseveration A speech error in which an earlier word or sound intrudes on a later one. Perseveratory coarticulation The type of coarticulation in which the shape of the vocal tract for a given speech sound is influenced by previous sounds. Personal settings A conversational setting in which there is a free exchange of turns among two or more participants. Phone The minimal unit of sound. Phoneme The minimal unit of sound that contributes to meaning. Phoneme monitoring An experimental task in which subjects listen for a particular phoneme while comprehending a passage and being timed for how long it takes them to monitor the phoneme. Phonemic restoration A topdown process in which the listener uses the context to restore phonemes missing from the speech signal. Phonemic similarity effect The observation that speech errors and targets are phonemically similar. Phonetic level A level of speech perception in which the speech signal is represented in terms of acoustic cues, such as formant transitions. Phonetics The study of speech sounds. Phonetic trading relations The notion that different acoustic cues have tradeoff effects on speech perception. Phonological bias technique A method of inducing speech errors by having a subject read a series of words with similar phonological patterns. Phonological dyslexia A form of reading disability in which a person’s ability to read words aloud is disrupted. Phonological level A level of speech perception in which the speech signal is converted into a phoneme and phonological rules are applied to the sound sequence. Phonology The sound system of a language, including the rules determining how different phonemes may be arranged in a word. Phrase marker A tree diagram that represents the phrase structure of a sentence. Phrase structure The hierarchical organization of sentences into phrases. Phrasestructure ambiguity A form of ambiguity in which a sentence has multiple meanings that may be revealed by regrouping the sentence constituents. Phrasestructure rule A rule that rewrites one constituent into one or more constituents. For example, a verb phrase may be rewritten as a verb and a noun phrase. Pidgin An auxiliary language that is created when speakers of mutually unintelligible languages are in close contact. Place of articulation The location within the vocal tract where articulation of a speech sound is produced (for example, bilabial, alveolar, and so on). Planum temporale An area in the temporal lobe known to be related to language functioning. Positive evidence Evidence that a particular linguistic expression (a word or sentence) is appropriate or acceptable. Positive evidence may be presented explicitly (when someone approves of another’s word or utterance) or implicitly (for example, when a person responds to another’s utterance without explicitly commenting on its appropriateness). Poverty of stimulus argument The argument made by followers of nativism that the environmental input presented to children is too weak and degenerate to account for the child’s language acquisition. Pragmatics The social rules underlying language use. Pragmatic theory In figurative language comprehension, the position that we comprehend figurative language by considering the literal meaning, then rejecting it. Preemption principle The principle that the speech of a child’s linguistic environment preempts or suppresses the language bioprogram. Preoperational period The second of Piaget’s periods of cognitive development. Pretend play The use of an object in a playful or unconventional manner, such as using a toy rake to comb a doll’s hair. Processing capacity The overall amount of mental capacity available for various tasks or activities. Prodrop parameter See nullsubject parameter. Proposition A unit of meaning consisting of a predicate (verb, adjective, or conjunction) plus one or more arguments (noun or pronoun). Simple sentences express a single proposition, whereas complex sentences express more than one proposition. Propositional representation In sentence or discourse memory, memory for the meaning apart from the exact words used. Prosodic factors Factors such as intonation and stress that are superimposed on speech segments. Also called suprasegmentals. Psycholinguistics The study of the comprehension, production, and acquisition of language. Psychologically realistic grammar A grammar or theory of language that takes psychological or processing considerations into account. Pure word deafness An aphasia in which a person is unable to comprehend language in the auditory modality. Comprehension of visual language and production in both modalities are normal. Radical In a logography, a group of strokes related to meaning. Rate The speed at which speech is articulated. Rate normalization The process of taking the rate of speech into consideration when using acoustic cues during speech perception. Rationalism The philosophical tradition that emphasizes the use of argument and the belief that innate knowledge guides human behavior. Reading span task A measure of working memory capacity during reading. Subjects read aloud a series of sentences and then try to recall the last word in each sentence. The number of words recalled is the measure of the subject’s reading span. Receptive aphasia See Wernicke’s aphasia. Recipient A semantic or thematic role referring to the person to whom something is given—for example, Susan in John gave the flower to Susan. Reciprocity In American Sign Language, the distinction between whether the subject is the agent of the action and the object is the recipient (they pinched them) and whether there is mutual interchange between the subject and object (they pinched each other). Recognition point In auditory word recognition, the point at which a word diverges from other possible words. Recursive rule A rule that applies to its own output, such as a rule for selfembedded sentences. Reduction A phonological process in child language in which one or more phonemes are deleted. Also called cluster reduction because consonant clusters are often reduced, such as saying take for steak. Reduplicated babbling A form of babbling in which infants use the same sounds over and over, as in gagagaga. Reduplication A phonological process in which the repetition of one syllable is used to mark a multisyllabic word (for example, dada for daddy). Reference The relationship between a linguistic expression and a person, object, or event in the world. GLOSSARY 431 Reference cohesion A form of cohesion in which the information needed to interpret a linguistic expression is found elsewhere in the text. See also anaphoric reference and cataphoric reference. Referent The person, object, or event to which a linguistic expression refers. Referential communication task An experimental task in which the subject must formulate a message about an object in the environment (as opposed to one’s thoughts or feelings). Referential gesture A form of gesture used in conversation to refer to some aspect of the content of a conversation. Referential strategy The style of child language that emphasizes a high ratio of nouns to pronouns, clear articulation, and an emphasis on naming. Regression Backward eye movement during reading. Reinstatement The timeconsuming process in which antecedents are retrieved from permanent memory into working memory to comprehend a current sentence. Relational processing A style of processing, associated with the left hemisphere, which emphasizes the analysis of whole units into parts. Relative clause A wh clause that modifies a noun—for example, that you found in Show me the book that you found. Relative terms Spatial terms that indicate the relationship between an object in space and a person (for example, in front of me, to the left of her). Request A communicative act in which a person attempts to influence the behavior of another—for example, a child pointing at a milk bottle in order to be given some. Requests may occur in words or gestures. Respiratory system The system of muscles that regulates the flow of air from the lungs to the vocal tract. Retention interval The time between when information is presented and when it is to be recalled. Saccade An eye movement during reading. Sapir–Whorf hypothesis See Whorf hypothesis. Schema (plural, schemata) A structure in semantic memory that specifies the expected sequence of events. Secondlanguage acquisition When an individual (child or adult) acquires a second language after already acquiring a native language. Also called sequential bilingualism. Selfreference effect The tendency to remember information better when one relates it to oneself. Selfrepairs Selfcorrection of speech errors. Semantic bootstrapping The process of using semantics to acquire syntax. Semantic complexity The complexity of the ideas expressed in a sentence or phrase (also called conceptual complexity). See also syntactic complexity. Semantic differential A tool for measuring the associative meanings of words by asking people to rate words on dimensions such as good/bad and strong/weak. Semantic memory The portion of permanent memory that contains organized knowledge of words, concepts, symbols, and objects. See also internal lexicon. Semantic network A model of semantic memory in which words are represented as nodes and connected to other nodes by various semantic relationships. Semantic priming An experimental procedure in which one word is presented in advance of another, target word, which reduces the time needed to retrieve or activate the target word. Semantics The domain of language that pertains to the meanings of words and sentences. Semantic verification task An experimental task in which subjects view sentences of the form An A is a B and rapidly decide whether the sentence is true or false. Sense The relationship a word has with other words in the lexicon. Sensorimotor period The first of Piaget’s periods of cognitive development, characterized by sensory and motor development and the inability to fully represent objects symbolically. See object permanence. Sensory stores The initial memory system for sensory stimuli. There is a separate store for each sense (vision, audition, and so on). Sequential bilingualism See secondlanguage acquisition. Serial processing Processes that occur one at a time. Shadowing An experimental task in which subjects repeat what they hear. Shift A speech error in which a speech sound or word moves from one location to another. Shortterm memory The memory system that holds information for about 30 seconds. See also working memory. Side participant An individual who is taking part in a conversation but is not currently being addressed. Simultaneous bilingualism See bilingual firstlanguage acquisition. Situational model A mental model of discourse. Sociolinguistics The study of how language functions in social situations. Somatosensory regions Areas in the brain’s parietal lobe controlling the sense of touch. Sound spectrogram A visual representation of the speech signal. Sound spectrograph A device used to create a sound spectrogram. Speaker normalization The process of taking the pitch of the speaker into account when using acoustic cues during speech perception. Speech act An utterance with an illocutionary force. Speech perception The process of using acoustic information to arrive at a recognition of the speech sounds in a message. Spreading activation The process by which one node in a semantic network, when active, activates related nodes. Steady state The portion of a formant that is of relatively constant frequency. Stop A consonant in which the vocal tract is completely closed, building up air pressure, which is then abruptly released, such as in the b in bat. Story grammar The mental representation (schema) of an expected series of events in a story. Stress The emphasis given to a word or syllable during the articulation of a sentence (for example, blackBIRD versus BLACKbird ). Structure dependence The fact that linguistic rules apply to grammatical structures (or constituents) rather than to individual words. Subset principle The notion that languages may be considered as subsets of one another. Substitution A form of cohesion in which one word is replaced by another as an alternative to repeating the first word. Also, a speech error or phonological process in which one sound or word replaces another. Sufficient condition A condition that, if present, ensures that a specified event will occur. Supralaryngeal system The system of muscles that manipulates the size and shape of the vocal tract. Suprasegmentals Prosodic factors such as stress and intonational patterns that lie ‘‘on top of ’’ speech segments. Surface dyslexia A form of reading disability in which a person retains the ability to name nonwords but not words. Surface representation In sentence or discourse memory, representation of the exact words that were presented. Surface structure The level of syntactic structure assumed in transformational grammar that is closer to the phonetic specification of an utterance. Syllabary An orthography in which syllables are represented by visual symbols. Synonymy A semantic relationship in which two or more words have a similar meaning. Syntactic category Another term for part of speech, such as noun, verb, and so forth. Syntactic complexity The complexity of the grammatical operations required to express an idea in a given language; also called formal complexity. See also semantic complexity. Syntax The domain of language that pertains to the grammatical arrangement of words in a sentence. Syrinx The major structure in the vocal system of the chaffinch. Tachistoscope A machine that presents visual stimuli for very brief periods of time. Tacit knowledge Knowledge of how to perform an act. See also explicit knowledge. Tag question A question that is ‘‘tagged’’ onto a declarative sentence such as isn’t it in It sure is cold in here, isn’t it? Task specificity The notion that certain cognitive processes are restricted to language and are not employed in other intellectual domains. Taxonomic bias A cognitive constraint in which children assume that a word refers to a class of individuals rather than to a single person or animal. Taxonomic relations Relations among words that indicate the position of words in a taxonomy. For example, for the word dog, mammal is a superordinate term, cat is a coordinate term, and collie is a subordinate term. Temporal lobes Auditory regions at each side of the brain. GLOSSARY 433 Tenor The topic of a metaphor. Theory of mind The ability to view another person as an intentional being. For example, interpreting a person’s bumping into you as an intentional rather than an accidental act is an example of a theory of mind. Tipofthetongue (TOT) phenomenon When we know a word but are temporarily unable to retrieve it. Topdown processing A process in which higher levels influence lower levels of processing. For example, the perception of phonemes may be influenced by the words in which they appear. TRACE model A connectionist model of speech perception. Transformational rule A rule that transforms one phrase structure into another by adding, deleting, or moving grammatical constituents. Also called transformation. Truth conditions The conditions that need to be present in the world in order for a sentence to be true. Turnyielding signal A set of cues given by a speaker to indicate that he or she is ready to yield the floor. Typicality effect The fact that it takes longer to verify a statement of the form An A is a B when A is not typical or characteristic of B. Underextension When a child uses a word in a more limited way than adults do (for example, refusing to call a taxi a car). Undershooting In speech production, the tendency for articulators to fall short of target locations for different speech sounds, owing to coarticulation. Variegated babbling A form of babbling consisting of syllable strings with varying consonants and vowels. Vehicle What is predicated of the topic in a metaphor. Velar A consonant articulated at the velum, such as the c in collar. Visual field task An experimental task in which visual stimuli are presented to either the right or the left visual field. Vocal cords Two bands of muscular tissue in the larynx that vibrate during the production of speech sounds; also called vocal folds. Vocal tract The structures above the larynx that participate in speech production, principally the mouth (oral cavity) and nose (nasal cavity) regions. Voiced Speech sound in which the vocal cords are vibrating during the production of sound. Voiceless Speech sound in which the vocal cords are not vibrating during the production of sound. Voice onset time The period of time from when a consonant is released until the vocal cords vibrate. Voicing Whether or not the vocal cords are vibrating when air from the lungs passes over them. If the cords are vibrating, the speech sound is called voiced; if not, voiceless. Vowel A speech sound in which the vocal tract is open during production. Wernicke’s aphasia An aphasia characterized by fluent speech that is not informational and by disorders of comprehension. Also called receptive aphasia. Wernicke’s area A brain region in the temporal lobe of the left hemisphere. Damage to this region leads to Wernicke’s aphasia. Whole object bias A cognitive constraint in which children assume that a word refers to an entire object, not a part of it. Whorf hypothesis The hypothesis that languages shape thought processes; also called the Sapir–Whorf hypothesis. See also linguistic determinism and linguistic relativity. Whquestion A question beginning with a whword, such as who, what, where, or when. Word association test A test in which a person is presented with a word and asked to respond with the first word that comes to mind. Word initial cohort In auditory word recognition, the initial set of lexical candidates activated by the comprehender. Word level A level of written language perception in which a visual stimulus is represented as a familiar word. Wordsuperiority effect An experimental finding that it is easier to perceive a letter in a word context than in isolation. Working memory A form of memory with both storage and processing functions. Working memory is used to hold information for a short period of time as well as to perform various operations on the stored information. Yes/no question A question that can be answered with a yes or no answer. Zipf ’s law The fact that the length of a word is negatively correlated with its frequency of use.</p>
]]></content>
      <categories>
        <category>语言学</category>
        <category>心理语言学</category>
      </categories>
      <tags>
        <tag>psycholinguistics</tag>
      </tags>
  </entry>
  <entry>
    <title>一文读懂Flask Web开发实战！</title>
    <url>/flask-2/</url>
    <content><![CDATA[
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title>基础：用flask搭建RESTful API</title>
    <url>/flask-api-1/</url>
    <content><![CDATA[<p><a href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a>是目前发展最迅速的 Python 框架之一，它是一个微型的 Python 开发的 Web 框架，基于<a href="https://www.oschina.net/p/werkzeug">Werkzeug</a> WSGI工具箱和<a href="https://www.oschina.net/p/jinja">Jinja2</a> 模板引擎。以下是用flask来构建 RESTful API 的全过程实录，力求完整、准确、无误地记录每个操作步骤与细节。我用的是windows系统，但我也会提到其他系统的操作。 <span id="more"></span> # 环境搭建与工具安装</p>
<p>首先依然是创建虚拟环境并安装 flask 和 <a href="https://flask-restful.readthedocs.io/en/latest/">flask-restful</a> package。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkvirtualenv flask-api</span><br><span class="line">pip install flask</span><br><span class="line">pip install flask-restful</span><br></pre></td></tr></table></figure>
<h1 id="使用flask搭建api">使用flask搭建API</h1>
<h2 id="最简单的flask应用">最简单的flask应用</h2>
<p>新建一个python脚本，linux用touch，windows操作如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">workon flask-api <span class="comment"># 激活虚拟环境</span></span><br><span class="line">D:</span><br><span class="line"><span class="built_in">type</span> nul&gt;hello.py <span class="comment"># 新建文件</span></span><br><span class="line">atom hello.py <span class="comment"># 用atom编辑器打开</span></span><br></pre></td></tr></table></figure>
<p>粘贴以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)  </span><span class="comment"># 把Flask对象中的route()函数作为一个装饰器，增强该函数的功能</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello, World!&#x27;</span></span><br></pre></td></tr></table></figure>
<p>再打开 Command Prompt 输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set FLASK_APP&#x3D;hello.py</span><br><span class="line">set FLASK_ENV&#x3D;development</span><br><span class="line">flask run</span><br></pre></td></tr></table></figure>
<p>按下快捷键Alt+Shift+=左右分屏，输入：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl http://127.0.0.1:5000/</span><br><span class="line">curl -v http://127.0.0.1:5000/</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">&lt; Content-Type: text&#x2F;html; charset&#x3D;utf-8</span><br><span class="line">&lt; Content-Length: 13</span><br><span class="line">...</span><br><span class="line">&lt;</span><br><span class="line">Hello, World!* Closing connection 0</span><br></pre></td></tr></table></figure>
<p>我们发现返回的是html类型，而不是通常从 RESTful API 获得的 json 文件，于是修改hello.py的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, jsonify</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;about&#x27;</span>: <span class="string">&#x27;Hello, World!&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者：</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(about=<span class="string">&#x27;Hello, World!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>调试模式下，服务器会监测你的代码更新并自动加载，所以无须重启才看到效果。这时curl得到json file：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">&lt; Content-Type: application&#x2F;json</span><br><span class="line">&lt; Content-Length: 31</span><br><span class="line">...</span><br><span class="line">&lt;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;hello&quot;: &quot;Hello, World!&quot;</span><br><span class="line">&#125;</span><br><span class="line">* Closing connection 0</span><br></pre></td></tr></table></figure>
<h2 id="通过url传递参数">通过URL传递参数</h2>
<p>我们可以通过url传递参数给flask api，例如<code>/&lt;int:num&gt;/</code></p>
<p>将hello.py修改为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, jsonify, request</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/multi/&lt;int:num&gt;&#x27;</span>, methods=[<span class="string">&#x27;GET&#x27;</span>]</span>) </span><span class="comment"># methods一般默认为GET</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_multiply10</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;result&#x27;</span>: num*<span class="number">10</span>&#125;) <span class="comment"># 返回数字乘以10后的结果</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run(debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后在cmd输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;multi&#x2F;10</span><br></pre></td></tr></table></figure>
<p>我们得到10乘以10后的结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;result&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然，我们也可以在浏览器内访问以下地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;multi&#x2F;10</span><br></pre></td></tr></table></figure>
<h2 id="使用post方法">使用POST方法</h2>
<p>这一小节我们将使用POST method。</p>
<p>在hello.py中加入以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span>, methods=[<span class="string">&quot;GET&quot;</span>, <span class="string">&quot;POST&quot;</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span>():</span></span><br><span class="line">    <span class="keyword">if</span> (request.method == <span class="string">&quot;POST&quot;</span>):</span><br><span class="line">        some_json = request.get_json()</span><br><span class="line">        <span class="keyword">return</span> jsonify(&#123;<span class="string">&#x27;You sent&#x27;</span>: some_json&#125;), <span class="number">201</span> <span class="comment"># 如果是post则返回post内容</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> jsonify(&#123;<span class="string">&quot;about&quot;</span>: <span class="string">&quot;Hello World!&quot;</span>&#125;) <span class="comment"># 如果是get则返回hello world</span></span><br></pre></td></tr></table></figure>
<p>现在，我们可以发送如下curl请求并返回POST的内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application&#x2F;json&quot; -X POST -d &#39;&#123;&quot;name&quot;:&quot;Example&quot;,&quot;email&quot;:&quot;example@example.com&quot;&#125;&#39; http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;</span><br></pre></td></tr></table></figure>
<p>注意：Windows的命令行不支持单引号、且需要将双引号转义，所以需要改成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application&#x2F;json&quot; -X POST -d &quot;&#123;\&quot;name\&quot;:\&quot;Example\&quot;,\&quot;email\&quot;:\&quot;example@example.com\&quot;&#125;&quot; http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;</span><br></pre></td></tr></table></figure>
<p>因此，Windows用户建议在git bash上完成以上操作。</p>
<h1 id="使用flask-restful搭建api">使用flask-restful搭建API</h1>
<p>我们把hello.py修改为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"><span class="keyword">from</span> flask_restful <span class="keyword">import</span> Resource, Api</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">api = Api(app)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span>(<span class="params">Resource</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self</span>):</span></span><br><span class="line">        some_json = request.get_json()</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;You sent&#x27;</span>: some_json&#125;, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Multi</span>(<span class="params">Resource</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self, num</span>):</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;result&#x27;</span>: num*<span class="number">10</span>&#125;</span><br><span class="line"></span><br><span class="line">api.add_resource(HelloWorld, <span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">api.add_resource(Multi, <span class="string">&#x27;/multi/&lt;int:num&gt;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>以上代码实现完全相同的功能，不过看起来更简洁和舒服。</p>
<p>参考教程：</p>
<p>https://www.youtube.com/watch?v=s_ht4AKnWZg</p>
<p>参考文献：</p>
<p>https://blog.csdn.net/weixin_41010198/article/details/85230424#API_3</p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>部署Flask开发的API到Heroku</title>
    <url>/flask-api-2/</url>
    <content><![CDATA[<p>上篇文章我们介绍了如何用flask开发简单的web api，下面我们把它部署到heroku上，方便更多人使用。 <span id="more"></span></p>
<p><strong>步骤总结：</strong></p>
<ul>
<li><p>注册<a href="https://signup.heroku.com/">Heroku帐号</a></p></li>
<li><p>下载客户端</p></li>
<li><p>在本地命令行登录</p>
<ul>
<li>如果出现IP Address Mismatch，复制并粘贴<code>heroku login -i</code>到终端，用邮箱密码登录</li>
</ul></li>
<li><p>创建应用</p>
<ul>
<li>``` heroku apps:create flask-microblog <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line">- 初始化</span><br><span class="line">  </span><br><span class="line">  - &#96;&#96;&#96;</span><br><span class="line">    mkdir flask-api</span><br><span class="line">    cd flask-api</span><br><span class="line">    git init</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>编写应用代码</p>
<ul>
<li>run.py</li>
<li>requirements.txt</li>
<li><code>echo web: gunicorn run:app &gt; Procfile</code>
<ul>
<li><code>web: gunicorn &lt;filename&gt;:&lt;main method name&gt;</code></li>
</ul></li>
<li>用<code>tree/F</code>检验上述文件是否齐全</li>
</ul></li>
<li><p>部署应用</p>
<ul>
<li><p>关联github，自动部署</p></li>
<li><pre><code>  git add .
  git commit -m &quot;Initialize repo&quot;
  git push -u origin master</code></pre></li>
</ul></li>
<li><p>访问应用地址：https://nlp-ch.herokuapp.com/</p></li>
</ul>
<p><strong>参考文献：</strong></p>
<p>官方教程：</p>
<p>https://devcenter.heroku.com/articles/getting-started-with-python</p>
<p>以及这些博客：</p>
<p>https://noviachen.github.io/posts/b4cb2e1c.html</p>
<p>https://wizardforcel.gitbooks.io/the-flask-mega-tutorial-2017-zh/content/docs/18.html</p>
<p>http://www.bjhee.com/flask-heroku.html</p>
<p>windows上部署参考（建议使用git bash）：https://caijialinxx.github.io/2018/07/25/deploy-on-heroku/</p>
<p>git bash创建并编辑文件参考：https://blog.csdn.net/qq_34289537/article/details/53994070</p>
<p>windows cmd常用命令参考：https://www.jianshu.com/p/80c3ac7bea8f</p>
<p>上传文件到github: https://www.jianshu.com/p/5227f837070b</p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title>flask干货总结</title>
    <url>/flask/</url>
    <content><![CDATA[<h1 id="flask">Flask</h1>
<blockquote>
<p>在一个Web应用里，客户端和服务器上的Flask程序的交互可以简单概括为以下几步：</p>
<p>1）用户在浏览器输入URL访问某个资源。</p>
<p>2）Flask接收用户请求并分析请求的URL。</p>
<p>3）为这个URL找到对应的处理函数。</p>
<p>4）执行函数并生成响应，返回给浏览器。</p>
<p>5）浏览器接收并解析响应，将信息显示在页面中。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span id="more"></span></p>
</blockquote>
<h1 id="术语解释">术语解释</h1>
<ul>
<li>路由：作为动词时，含义是“按某路线发送”，即调用与请求URL对应的视图函数。</li>
<li>视图函数（view function）：处理请求并生成响应的函数。当用户访问URL时会触发视图函数，该函数可执行任意操作，比如从数据库中获取信息，获取请求信息，对用户输入的数据进行计算和处理等。最后，视图函数返回的值将作为响应的主体，一般来说就是HTML页面。</li>
<li>模板：包含程序页面的HTML文件。</li>
<li>静态文件：需要在HTML文件中加载的CSS和Java Script文件，以及图片、字体文件等资源文件。</li>
<li>模板文件存放在项目根目录中的templates文件夹中，静态文件存放在static文件夹下，这两个文件夹需要和包含程序实例的模块处于同一个目录下。</li>
<li>HTTP（Hypertext TransferProtocol，超文本传输协议）定义了服务器和客户端之间信息交流的格式和传递方式，它是万维网（World Wide Web）中数据交换的基础。</li>
<li>WSGI：将HTTP格式的请求数据转换成Flask程序能够使用的Python数据，并把python程序的响应经过WSGI转换生成HTTP响应。</li>
<li>URL中的查询字符串用来向指定的资源传递参数。查询字符串从问号?开始，以键值对的形式写出，多个键值对之间使用&amp;分隔。</li>
<li>这种浏览器与服务器之间交互的数据被称为报文（message），请求时浏览器发送的数据被称为请求报文（request message），而服务器返回的数据被称为响应报文（responsemessage）。</li>
<li>请求报文由请求的方法、URL、协议版本、首部字段（header）以及内容实体组成。</li>
<li>报文由报文首部和报文主体组成，两者由空行分隔，请求报文的主体一般为空。如果URL中包含查询字符串，或是提交了表单，那么报文主体将会是查询字符串和表单数据。</li>
<li>用来映射到数据库表的Python类通常被称为数据库模型（model），一个数据库模型类对应数据库中的一个表。所有的模型类都需要继承Flask-SQLAlchemy提供的db.Model基类。</li>
</ul>
<h1 id="架构">架构</h1>
<p>在MVC架构中，程序被分为三个组件：数据处理（Model）、用户界面（View）、交互逻辑（Controller）。</p>
<p>如果想要使用Flask来编写一个MVC架构的程序，那么视图函数可以作为控制器（Controller），视图（View）则是使用Jinja2渲染的HTML模板，而模型（Model）可以使用其他库来实现。</p>
<p>参考文献：</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://book.douban.com/subject/30310340/">《Flask Web开发实战》</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title>Django网站开发全过程实录-1</title>
    <url>/django-1/</url>
    <content><![CDATA[<p>网站一般需要实现三种基本功能：<strong>连接数据库、处理用户请求、页面设计的删改</strong>。Django的优势在于将这些功能设计成独立的模块，形成一套web框架。利用Django框架开发网站，能让我们专注于编写应用程序而无需重新造轮子。 <span id="more"></span> Django 采用了 <strong>MVT 的软件设计模式</strong>，即<strong>模型</strong>（Model）、<strong>视图</strong>（View）和<strong>模板</strong>（Template）。这种设计模式的优势在于<strong>各个组件松散结合</strong>，每个APP应用都有明确的目的，并且可独立更改而不影响其它部分。如此，使得页面设计与业务逻辑互不影响。同时，Django是一套出色的<strong>动态内容管理系统</strong>，擅长动态提供数据库驱动的信息。</p>
<p>以下是我使用Django 3.1.7搭建网站过程的实录，力求完整、准确、无误地记录每个操作步骤与细节。</p>
<h2 id="环境搭建与工具安装"><strong>环境搭建与工具安装</strong></h2>
<blockquote>
<p><em>参考：</em><a href="https://stormsha.com/article/2026/"><em>https://stormsha.com/article/2026/</em></a></p>
</blockquote>
<p>我们需要在合适的目录内创建一个<strong>虚拟环境</strong>（用virtualenv, virtualenvwrapper皆可，参考<a href="https://blog.csdn.net/a200822146085/article/details/89048172">virtualenvwrapper的使用</a>），我给它取名为webdev。</p>
<p>在该虚拟环境，安装<strong>django</strong>和<strong>psycopg2</strong>工具包（用于管理PostgreSQL数据库）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install virtualenv, virtualenvwrapper-win</span><br><span class="line">mkvirtualenv webdev</span><br><span class="line">workon webdev</span><br><span class="line">pip install django</span><br><span class="line">pip install psycopg2</span><br><span class="line">pip list</span><br><span class="line">pip freeze</span><br><span class="line"></span><br><span class="line"># 如果需要退出或删除虚拟环境</span><br><span class="line">deactivate</span><br><span class="line">rmvirtualenv webdev</span><br></pre></td></tr></table></figure>
<p><strong>安装数据库：</strong></p>
<p>Django支持四种数据库：PostgreSQL、SQLite 3、MySQL、Oracle。</p>
<p>我选择PostgreSQL，它比MySQL更适合Django，Django的创建者如是说：</p>
<blockquote>
<p>如果您不受任何遗留系统的束缚，并且可以自由选择数据库后端，那么我们建议您使用PostgreSQL，它可以在成本、功能、速度和稳定性之间取得很好的平衡。（《 Django权威指南》第15页）</p>
</blockquote>
<p>PostgreSQL的安装步骤参考：https://www.runoob.com/postgresql/windows-install-postgresql.html</p>
<p>打开后设置语言为中文，然后关闭。</p>
<h2 id="创建项目project"><strong>创建项目（project）</strong></h2>
<p>下面在刚刚的虚拟环境webdev内创建一个项目mysite（你可以选择任意其他名字），项目是我们所建立的网站上所有应用程序的集合，并共用一套数据库配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd webdev</span><br><span class="line">django-admin startproject mysite</span><br></pre></td></tr></table></figure>
<p>我们看到新建了一个文件夹mysite及下面的子文件夹mysite/mysite：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysite&#x2F;</span><br><span class="line">    manage.py</span><br><span class="line">    mysite&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">        settings.py</span><br><span class="line">        urls.py</span><br><span class="line">        asgi.py</span><br><span class="line">        wsgi.py</span><br></pre></td></tr></table></figure>
<p>我们可以把子文件夹mysite/mysite视为整个项目的配置，其中的settings.py和urls.py这两个文件是我们以后需要经常修改的。</p>
<h2 id="启动服务器server"><strong>启动服务器（server）</strong></h2>
<p>启动服务器，服务器会监测你的代码更新并自动加载，所以无须重启才看到效果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd mysite</span><br><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<p>在settings.py内将语言改成中文，时区改为上海：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LANGUAGE_CODE &#x3D; &#39;zh-hans&#39;</span><br><span class="line"></span><br><span class="line">TIME_ZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39;</span><br></pre></td></tr></table></figure>
<p>刷新浏览器看到中文页面。</p>
<p>我们可以指定服务器的端口和IP地址。比如，把地址设为自己的IP地址（例如192.168.1.110）或0.0.0.0，让联网的其他计算机可见：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py runserver 0.0.0.0:8000</span><br></pre></td></tr></table></figure>
<p>使用Windows的用户用ipconfig命令获取本地网络中的IP 地址，然后复制到setting.py中，比如我是这个：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALLOWED_HOSTS &#x3D; [&#39;192.168.1.110&#39;]</span><br></pre></td></tr></table></figure>
<p>于是，在其他电脑或手机浏览器打开 http://192.168.1.110:8000/ 就可以访问啦！完美！不过网站还在开发中，就不要随便开放共享啦~</p>
<h2 id="创建应用程序app"><strong>创建应用程序（APP）</strong></h2>
<p><strong>项目和应用的区分：</strong></p>
<ul>
<li><strong>应用</strong>是用于执行某项具体操作的程序，<strong>项目</strong>是特定网站的配置和应用程序的集合。</li>
<li><strong>多对多的关系</strong>：一个项目可以包含多个应用程序，一个应用程序可以用在多个项目中。</li>
</ul>
<p>应用放在任意路径都可以，但我们一般放在<strong>manage.py文件相同的目录</strong>中，与mysite子文件夹平行。在这里我创建一个成语检索的app，名为idiom：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py startapp idiom</span><br></pre></td></tr></table></figure>
<p>看看这个应用程序下有哪些文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">idiom&#x2F;</span><br><span class="line">    __init__.py</span><br><span class="line">    admin.py</span><br><span class="line">    apps.py</span><br><span class="line">    migrations&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">    models.py</span><br><span class="line">    tests.py</span><br><span class="line">    views.py</span><br></pre></td></tr></table></figure>
<h2 id="编写视图views"><strong>编写视图（views）</strong></h2>
<p>下面为这个应用程序idiom添砖加瓦，分为三个步骤：</p>
<ol type="1">
<li>创建视图函数</li>
<li>将视图函数映射到APP的urls</li>
<li>将APP中的urls连入网站的根urls</li>
</ol>
<p>这样看逻辑可能更清晰：视图函数 --&gt; APP的urls --&gt; 网站的urls</p>
<h2 id="创建视图函数"><strong>创建视图函数</strong></h2>
<p>打开文件idiom/views.py ，加入以下Python代码，<strong>创建index视图函数</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    return HttpResponse(&quot;Hello, world. You&#39;re at the idiom index.&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="映射到app的urls"><strong>映射到APP的urls</strong></h2>
<p>要调用该视图，我们要将其映射到URL，为此，我们需要添加一个URL配置（URLconf）。<strong>URLconf</strong>相当于网站的目录，也就是<strong>URL模式与视图函数之间的映射表</strong>。</p>
<p>我们在idiom应用的目录下创建一个名为urls.py的文件，windows操作如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type nul&gt;urls.py</span><br></pre></td></tr></table></figure>
<p>看看现在的应用目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">idiom&#x2F;</span><br><span class="line">    __init__.py</span><br><span class="line">    admin.py</span><br><span class="line">    apps.py</span><br><span class="line">    migrations&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">    models.py</span><br><span class="line">    tests.py</span><br><span class="line">    urls.py</span><br><span class="line">    views.py</span><br></pre></td></tr></table></figure>
<p>然后在idiom/urls.py这个空文件中加入以下代码，将index视图映射到APP的url模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.urls import path</span><br><span class="line"></span><br><span class="line">from . import views</span><br><span class="line"></span><br><span class="line">urlpatterns &#x3D; [</span><br><span class="line">    path(&#39;&#39;, views.index, name&#x3D;&#39;index&#39;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="连入网站的urls"><strong>连入网站的urls</strong></h2>
<p>下一步是将根URLconf（mysite/urls.py）指向idiom.urls模块，使得网站域名连接到app的url。我们打开mysite/urls.py，修改为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.contrib import admin</span><br><span class="line">from django.urls import include, path</span><br><span class="line"></span><br><span class="line">urlpatterns &#x3D; [</span><br><span class="line">    path(&#39;idiom&#x2F;&#39;, include(&#39;idiom.urls&#39;)),</span><br><span class="line">    path(&#39;admin&#x2F;&#39;, admin.site.urls),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>该include()功能允许引用其他URLconf，这样我们就将刚刚创建的index视图连接到了网站的URLconf。</p>
<p>也就是说，目前我们可以打开两个网址：</p>
<p><a href="http://127.0.0.1:8000/idiom/">http://example.com/idiom/</a></p>
<p><a href="http://127.0.0.1:8000/idiom/">http://example.com/admin/</a></p>
<p>注意idiom和admin在引用URL模式时的区别：除了admin.site.urls（用于管理后台），我们引用其他URL模式时，都应使用include()。</p>
<p>最后，验证下是否正常运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<p>打开http://127.0.0.1:8000/idiom/ ，可以看到 Hello, world. You're at the idiom index. 这行文字。</p>
<h2 id="urlconf的工作原理"><strong>URLconf的工作原理</strong></h2>
<p>Django允许我们根据需要设计每个应用程序的URL，通过创建<strong>URLconf</strong>（URL配置）。</p>
<p>在 idiom和mysite文件夹下的urls.py中，我们都使用了<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.path">path()</a>函数，这个函数有两个必需的参数 route和view。</p>
<p>path（<em>route</em>，<em>view</em>，<em>kwargs = None</em>，<em>name = None</em>）</p>
<ul>
<li>route是包含URL模式的字符串，比如目前我们有idiom/和admin/。在处理请求时，Django从第一个模式开始，沿列表的顺序，将请求的URL（域名后的部分）与每个模式进行比较，直到找到匹配的URL。这个字符串支持用尖括号匹配和捕获URL的一部分并将其作为关键字参数发送到视图，</li>
<li>view就是指定的视图函数，也可以是一个<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.include">django.urls.include()</a>。kwargs参数允许我们将其他参数传递给视图函数。</li>
<li>name不是必须的，但是命名URL的好处是便于在Django中的其他地方（尤其是在模板内部）明确地引用它。</li>
</ul>
<p>参考：<a href="https://docs.djangoproject.com/en/3.1/ref/urls/#django.urls.path">django.urls functions for use in URLconfs</a></p>
]]></content>
      <categories>
        <category>网站开发</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理综论》第14章-依存分析（中）</title>
    <url>/dependency-parsing-2/</url>
    <content><![CDATA[<h1 id="基于转换的依存分析">14.4 基于转换的依存分析</h1>
<p>我们的第一个依存分析方法是由一种基于堆栈的方法启发的，这种方法被称为shift-reduce parsing，最初是为分析程序语言而开发的(Aho and Ullman, 1972)。这个经典的方法简单而优雅，采用了一个上下文无关语法、一个堆栈和一个待解析的标记列表。输入的标记被连续地移动到堆栈上，堆栈的前两个元素与语法中的右侧规则进行匹配；当发现匹配时，匹配的元素在堆栈上被匹配的规则左侧的非终端替换（还原）。在将这种方法改编为依存性解析时，我们放弃了对语法的明确使用，并改变了reduce操作，使其不是在解析树上添加一个非终端，而是引入了一个词与其头部之间的依存关系。更具体地说，reduce操作被两种可能的操作所取代：在堆栈顶部的词和它下面的词之间断言一个词头依存关系，或者反之。图14.5说明了这种解析器的基本操作。 配置 在基于过渡的解析中，一个关键的元素是配置的概念，它由一个堆栈、一个词或标记的输入缓冲区和一组代表依存树的关系组成。在这个框架下，解析过程由一个通过可能配置空间的过渡序列组成。这个过程的目标是找到一个最终的配置，在这个配置中，所有的词都已经被计算在内，并且已经合成了一个合适的依存树。 为了实现这样的搜索，我们将定义一组过渡运算符，当它们应用于一个配置时，会产生新的配置。考虑到这个设置，我们可以将解析器的操作看作是在配置空间中搜索从起始状态到目标状态的过渡序列。在这个过程的开始，我们创建一个初始配置，其中堆栈包含ROOT节点，t</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>依存分析</category>
      </categories>
      <tags>
        <tag>dependency</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
</search>
